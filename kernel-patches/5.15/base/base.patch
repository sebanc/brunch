diff -ruN a/arch/x86/entry/common.c b/arch/x86/entry/common.c
--- a/arch/x86/entry/common.c	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/entry/common.c	2021-12-23 08:35:14.000000000 +0100
@@ -44,13 +44,26 @@
 	 * numbers for comparisons.
 	 */
 	unsigned int unr = nr;
+#ifdef CONFIG_ALT_SYSCALL
+	struct thread_info *ti;
+#endif
 
+#ifdef CONFIG_ALT_SYSCALL
+	ti = current_thread_info();
+	if (likely(unr < ti->nr_syscalls)) {
+		unr = array_index_nospec(unr, ti->nr_syscalls);
+		regs->ax = ti->sys_call_table[unr](regs);
+		return true;
+	}
+	return false;
+#else
 	if (likely(unr < NR_syscalls)) {
 		unr = array_index_nospec(unr, NR_syscalls);
 		regs->ax = sys_call_table[unr](regs);
 		return true;
 	}
 	return false;
+#endif
 }
 
 static __always_inline bool do_syscall_x32(struct pt_regs *regs, int nr)
@@ -106,13 +119,20 @@
 	 * numbers for comparisons.
 	 */
 	unsigned int unr = nr;
-
+#ifdef CONFIG_ALT_SYSCALL
+	struct thread_info *ti = current_thread_info();
+	if (likely(unr < ti->ia32_nr_syscalls)) {
+		unr = array_index_nospec(unr, ti->ia32_nr_syscalls);
+		regs->ax = ti->ia32_sys_call_table[unr](regs);
+	}
+#else
 	if (likely(unr < IA32_NR_syscalls)) {
 		unr = array_index_nospec(unr, IA32_NR_syscalls);
 		regs->ax = ia32_sys_call_table[unr](regs);
 	} else if (nr != -1) {
 		regs->ax = __ia32_sys_ni_syscall(regs);
 	}
+#endif
 }
 
 /* Handles int $0x80 */
diff -ruN a/arch/x86/include/asm/syscall.h b/arch/x86/include/asm/syscall.h
--- a/arch/x86/include/asm/syscall.h	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/include/asm/syscall.h	2021-12-23 08:35:14.000000000 +0100
@@ -21,6 +21,7 @@
 
 #if defined(CONFIG_X86_32)
 #define ia32_sys_call_table sys_call_table
+#define ia32_nr_syscalls nr_syscalls
 #else
 /*
  * These may not exist, but still put the prototypes in so we
diff -ruN a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
--- a/arch/x86/include/asm/thread_info.h	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/include/asm/thread_info.h	2021-12-23 08:35:14.000000000 +0100
@@ -50,6 +50,10 @@
  */
 #ifndef __ASSEMBLY__
 struct task_struct;
+
+/* same as sys_call_ptr_t from asm/syscall.h */
+typedef asmlinkage long (*ti_sys_call_ptr_t)(const struct pt_regs *);
+
 #include <asm/cpufeature.h>
 #include <linux/atomic.h>
 
@@ -57,11 +61,42 @@
 	unsigned long		flags;		/* low level flags */
 	unsigned long		syscall_work;	/* SYSCALL_WORK_ flags */
 	u32			status;		/* thread synchronous flags */
+#ifdef CONFIG_ALT_SYSCALL
+	/*
+	 * This uses nr_syscalls instead of nr_syscall_max because we want
+	 * to be able to entirely disable a syscall table (e.g. compat) by
+	 * setting nr_syscalls to 0. This requires some careful work in
+	 * the syscall entry assembly code, most variations use ..._max.
+	 */
+	unsigned int		nr_syscalls;	/* size of below */
+	const ti_sys_call_ptr_t	*sys_call_table;
+# ifdef CONFIG_IA32_EMULATION
+	unsigned int		ia32_nr_syscalls;	/* size of below */
+	const ti_sys_call_ptr_t	*ia32_sys_call_table;
+# endif
+#endif
 };
 
+#ifdef CONFIG_ALT_SYSCALL
+# ifdef CONFIG_IA32_EMULATION
+#  define INIT_THREAD_INFO_SYSCALL_COMPAT			\
+	.ia32_nr_syscalls	= IA32_NR_syscalls,		\
+	.ia32_sys_call_table	= ia32_sys_call_table,
+# else
+#  define INIT_THREAD_INFO_SYSCALL_COMPAT /* */
+# endif
+# define INIT_THREAD_INFO_SYSCALL \
+	.nr_syscalls	= NR_syscalls,		\
+	.sys_call_table	= sys_call_table,	\
+	INIT_THREAD_INFO_SYSCALL_COMPAT
+#else
+# define INIT_THREAD_INFO_SYSCALL /* */
+#endif
+
 #define INIT_THREAD_INFO(tsk)			\
 {						\
 	.flags		= 0,			\
+	INIT_THREAD_INFO_SYSCALL		\
 }
 
 #else /* !__ASSEMBLY__ */
diff -ruN a/arch/x86/Kconfig b/arch/x86/Kconfig
--- a/arch/x86/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/Kconfig	2021-12-23 08:35:14.000000000 +0100
@@ -266,6 +266,7 @@
 	select X86_FEATURE_NAMES		if PROC_FS
 	select PROC_PID_ARCH_STATUS		if PROC_FS
 	imply IMA_SECURE_AND_OR_TRUSTED_BOOT    if EFI
+	select ARCH_HAS_ALT_SYSCALL		if X86_64
 
 config INSTRUCTION_DECODER
 	def_bool y
diff -ruN a/arch/x86/kernel/acpi/cstate.c b/arch/x86/kernel/acpi/cstate.c
--- a/arch/x86/kernel/acpi/cstate.c	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/kernel/acpi/cstate.c	2021-12-23 08:35:14.000000000 +0100
@@ -79,6 +79,21 @@
 		 */
 		flags->bm_control = 0;
 	}
+	if (c->x86_vendor == X86_VENDOR_AMD && c->x86 >= 0x17) {
+		/*
+		 * For all AMD Zen or newer CPUs that support C3, caches
+		 * should not be flushed by software while entering C3
+		 * type state. Set bm->check to 1 so that kernel doesn't
+		 * need to execute cache flush operation.
+		 */
+		flags->bm_check = 1;
+		/*
+		 * In current AMD C state implementation ARB_DIS is no longer
+		 * used. So set bm_control to zero to indicate ARB_DIS is not
+		 * required while entering C3 type state.
+		 */
+		flags->bm_control = 0;
+	}
 }
 EXPORT_SYMBOL(acpi_processor_power_init_bm_check);
 
diff -ruN a/arch/x86/kernel/alt-syscall.c b/arch/x86/kernel/alt-syscall.c
--- a/arch/x86/kernel/alt-syscall.c	1970-01-01 01:00:00.000000000 +0100
+++ b/arch/x86/kernel/alt-syscall.c	2021-12-23 08:35:14.000000000 +0100
@@ -0,0 +1,70 @@
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/unistd.h>
+#include <linux/slab.h>
+#include <linux/stddef.h>
+#include <linux/syscalls.h>
+#include <linux/alt-syscall.h>
+
+#include <asm/syscall.h>
+#include <asm/syscalls.h>
+
+int arch_dup_sys_call_table(struct alt_sys_call_table *entry)
+{
+	if (!entry)
+		return -EINVAL;
+	/* Table already allocated. */
+	if (entry->table)
+		return -EINVAL;
+#ifdef CONFIG_IA32_EMULATION
+	if (entry->compat_table)
+		return -EINVAL;
+#endif
+	entry->size = NR_syscalls;
+	entry->table = kcalloc(entry->size, sizeof(sys_call_ptr_t),
+			       GFP_KERNEL);
+	if (!entry->table)
+		goto failed;
+
+	memcpy(entry->table, sys_call_table,
+	       entry->size * sizeof(sys_call_ptr_t));
+
+#ifdef CONFIG_IA32_EMULATION
+	entry->compat_size = IA32_NR_syscalls;
+	entry->compat_table = kcalloc(entry->compat_size,
+				      sizeof(sys_call_ptr_t), GFP_KERNEL);
+	if (!entry->compat_table)
+		goto failed;
+	memcpy(entry->compat_table, ia32_sys_call_table,
+	       entry->compat_size * sizeof(sys_call_ptr_t));
+#endif
+
+	return 0;
+
+failed:
+	entry->size = 0;
+	kfree(entry->table);
+	entry->table = NULL;
+#ifdef CONFIG_IA32_EMULATION
+	entry->compat_size = 0;
+#endif
+	return -ENOMEM;
+}
+
+/* Operates on "current", which isn't racey, since it's _in_ a syscall. */
+int arch_set_sys_call_table(struct alt_sys_call_table *entry)
+{
+	if (!entry)
+		return -EINVAL;
+
+	current_thread_info()->nr_syscalls = entry->size;
+	current_thread_info()->sys_call_table = entry->table;
+#ifdef CONFIG_IA32_EMULATION
+	current_thread_info()->ia32_nr_syscalls = entry->compat_size;
+	current_thread_info()->ia32_sys_call_table = entry->compat_table;
+#endif
+
+	return 0;
+}
diff -ruN a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
--- a/arch/x86/kernel/cpu/bugs.c	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/kernel/cpu/bugs.c	2021-12-23 08:35:14.000000000 +0100
@@ -43,6 +43,7 @@
 static void __init mds_print_mitigation(void);
 static void __init taa_select_mitigation(void);
 static void __init srbds_select_mitigation(void);
+static void __init coresched_select(void);
 static void __init l1d_flush_select_mitigation(void);
 
 /* The base value of the SPEC_CTRL MSR that always has to be preserved. */
@@ -111,6 +112,9 @@
 	if (boot_cpu_has(X86_FEATURE_STIBP))
 		x86_spec_ctrl_mask |= SPEC_CTRL_STIBP;
 
+	/* Update whether core-scheduling is needed. */
+	coresched_select();
+
 	/* Select the proper CPU mitigations before patching alternatives: */
 	spectre_v1_select_mitigation();
 	spectre_v2_select_mitigation();
@@ -1016,7 +1020,7 @@
 /* Update the static key controlling the evaluation of TIF_SPEC_IB */
 static void update_indir_branch_cond(void)
 {
-	if (sched_smt_active())
+	if (!IS_ENABLED(CONFIG_SCHED_CORE) && sched_smt_active())
 		static_branch_enable(&switch_to_cond_stibp);
 	else
 		static_branch_disable(&switch_to_cond_stibp);
@@ -1824,4 +1828,19 @@
 {
 	return cpu_show_common(dev, attr, buf, X86_BUG_SRBDS);
 }
+
 #endif
+
+/*
+ * When coresched=secure command line option is passed (default), disable core
+ * scheduling if CPU does not have MDS/L1TF vulnerability.
+ */
+static void __init coresched_select(void)
+{
+#ifdef CONFIG_SCHED_CORE
+	if (coresched_cmd_secure() &&
+	    !boot_cpu_has_bug(X86_BUG_MDS) &&
+	    !boot_cpu_has_bug(X86_BUG_L1TF))
+		static_branch_disable(&sched_coresched_supported);
+#endif
+}
diff -ruN a/arch/x86/kernel/itmt.c b/arch/x86/kernel/itmt.c
--- a/arch/x86/kernel/itmt.c	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/kernel/itmt.c	2021-12-23 08:35:14.000000000 +0100
@@ -198,7 +198,7 @@
 		 * of the priority chain and only used when
 		 * all other high priority cpus are out of capacity.
 		 */
-		smt_prio = prio * smp_num_siblings / i;
+		smt_prio = prio * smp_num_siblings / (i * i);
 		per_cpu(sched_core_priority, cpu) = smt_prio;
 		i++;
 	}
diff -ruN a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile
--- a/arch/x86/kernel/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/kernel/Makefile	2021-12-23 08:35:14.000000000 +0100
@@ -153,6 +153,8 @@
 
 obj-$(CONFIG_ARCH_HAS_CC_PLATFORM)	+= cc_platform.o
 
+obj-$(CONFIG_ALT_SYSCALL)		+= alt-syscall.o
+
 ###
 # 64 bit specific files
 ifeq ($(CONFIG_X86_64),y)
diff -ruN a/arch/x86/kernel/reboot.c b/arch/x86/kernel/reboot.c
--- a/arch/x86/kernel/reboot.c	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/kernel/reboot.c	2021-12-23 08:35:14.000000000 +0100
@@ -620,6 +620,7 @@
 		switch (reboot_type) {
 		case BOOT_ACPI:
 			acpi_reboot();
+			mdelay(15);
 			reboot_type = BOOT_KBD;
 			break;
 
diff -ruN a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
--- a/arch/x86/kernel/tsc.c	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/kernel/tsc.c	2021-12-23 08:35:15.000000000 +0100
@@ -644,10 +644,24 @@
 	 * Denverton SoCs don't report crystal clock, and also don't support
 	 * CPUID.0x16 for the calculation below, so hardcode the 25MHz crystal
 	 * clock.
-	 */
-	if (crystal_khz == 0 &&
-			boot_cpu_data.x86_model == INTEL_FAM6_ATOM_GOLDMONT_D)
-		crystal_khz = 25000;
+	 * Also estimation code is not reliable and gives 1.5%  difference for
+	 * tsc/clock ratio on Skylake mobile. Therefore below is a hardcoded
+	 * crystal frequency for Skylake which was removed by upstream commit
+	 * "x86/tsc: Use CPUID.0x16 to calculate missing crystal frequency"
+	 * This is temporary workaround for bugs:
+	 * b/148108096, b/154283905, b/146787525, b/153400677, b/148178929
+	 * chromium/1031054
+	 */
+	if (crystal_khz == 0) {
+		switch (boot_cpu_data.x86_model) {
+			case INTEL_FAM6_SKYLAKE_L:
+			crystal_khz = 24000;	/* 24.0 MHz */
+			break;
+		case INTEL_FAM6_ATOM_GOLDMONT_D:
+			crystal_khz = 25000;	/* 25.0 MHz */
+			break;
+		}
+	}
 
 	/*
 	 * TSC frequency reported directly by CPUID is a "hardware reported"
@@ -1353,7 +1367,12 @@
 		 */
 		hpet = is_hpet_enabled();
 		tsc_start = tsc_read_refs(&ref_start, hpet);
-		schedule_delayed_work(&tsc_irqwork, HZ);
+		/* temporary workaround for AMD Cezanne. BUG=b:191845735 */
+		if ((boot_cpu_data.x86_vendor == X86_VENDOR_AMD) && (boot_cpu_data.x86 == 25)
+			&& (boot_cpu_data.x86_model == 80))
+			schedule_delayed_work(&tsc_irqwork, HZ/2);
+		else
+			schedule_delayed_work(&tsc_irqwork, HZ);
 		return;
 	}
 
@@ -1437,6 +1456,8 @@
 
 static bool __init determine_cpu_tsc_frequencies(bool early)
 {
+	u64 initial_tsc;
+
 	/* Make sure that cpu and tsc are not already calibrated */
 	WARN_ON(cpu_khz || tsc_khz);
 
@@ -1452,6 +1473,8 @@
 		cpu_khz = pit_hpet_ptimer_calibrate_cpu();
 	}
 
+	initial_tsc = rdtsc();
+
 	/*
 	 * Trust non-zero tsc_khz as authoritative,
 	 * and use it to sanity check cpu_khz,
@@ -1465,6 +1488,10 @@
 	if (tsc_khz == 0)
 		return false;
 
+	do_div(initial_tsc, cpu_khz / 1000);
+	pr_info("Initial usec timer %llu\n",
+		(unsigned long long)initial_tsc);
+
 	pr_info("Detected %lu.%03lu MHz processor\n",
 		(unsigned long)cpu_khz / KHZ,
 		(unsigned long)cpu_khz % KHZ);
diff -ruN a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
--- a/arch/x86/kvm/emulate.c	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/kvm/emulate.c	2021-12-23 08:35:15.000000000 +0100
@@ -1122,6 +1122,31 @@
 	return X86EMUL_CONTINUE;
 }
 
+static u8 simd_prefix_to_bytes(const struct x86_emulate_ctxt *ctxt,
+			       int simd_prefix)
+{
+	u8 bytes;
+
+	switch (ctxt->b) {
+	case 0x11:
+		/* movss xmm, m32 */
+		/* movsd xmm, m64 */
+		/* movups xmm, m128 */
+		if (simd_prefix == 0xf3) {
+			bytes = 4;
+			break;
+		} else if (simd_prefix == 0xf2) {
+			bytes = 8;
+			break;
+		}
+		fallthrough;
+	default:
+		bytes = 16;
+		break;
+	}
+	return bytes;
+}
+
 static void decode_register_operand(struct x86_emulate_ctxt *ctxt,
 				    struct operand *op)
 {
@@ -1132,7 +1157,7 @@
 
 	if (ctxt->d & Sse) {
 		op->type = OP_XMM;
-		op->bytes = 16;
+		op->bytes = ctxt->op_bytes;
 		op->addr.xmm = reg;
 		kvm_read_sse_reg(reg, &op->vec_val);
 		return;
@@ -1183,7 +1208,7 @@
 				ctxt->d & ByteOp);
 		if (ctxt->d & Sse) {
 			op->type = OP_XMM;
-			op->bytes = 16;
+			op->bytes = ctxt->op_bytes;
 			op->addr.xmm = ctxt->modrm_rm;
 			kvm_read_sse_reg(ctxt->modrm_rm, &op->vec_val);
 			return rc;
@@ -4446,7 +4471,7 @@
 };
 
 static const struct gprefix pfx_0f_10_0f_11 = {
-	I(Unaligned, em_mov), I(Unaligned, em_mov), N, N,
+	I(Unaligned, em_mov), I(Unaligned, em_mov), I(Unaligned, em_mov), I(Unaligned, em_mov),
 };
 
 static const struct gprefix pfx_0f_28_0f_29 = {
@@ -5018,7 +5043,7 @@
 {
 	int rc = X86EMUL_CONTINUE;
 	int mode = ctxt->mode;
-	int def_op_bytes, def_ad_bytes, goffset, simd_prefix;
+	int def_op_bytes, def_ad_bytes, goffset, simd_prefix = 0;
 	bool op_prefix = false;
 	bool has_seg_override = false;
 	struct opcode opcode;
@@ -5260,7 +5285,8 @@
 			ctxt->op_bytes = 4;
 
 		if (ctxt->d & Sse)
-			ctxt->op_bytes = 16;
+			ctxt->op_bytes = simd_prefix_to_bytes(ctxt,
+							      simd_prefix);
 		else if (ctxt->d & Mmx)
 			ctxt->op_bytes = 8;
 	}
diff -ruN a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
--- a/arch/x86/kvm/x86.c	2021-12-08 09:04:57.000000000 +0100
+++ b/arch/x86/kvm/x86.c	2021-12-23 08:35:15.000000000 +0100
@@ -2211,6 +2211,7 @@
 #endif
 
 static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz);
+static DEFINE_PER_CPU(unsigned long, cpu_tsc_khz_changed);
 static unsigned long max_tsc_khz;
 
 static u32 adjust_tsc_khz(u32 khz, s32 ppm)
@@ -2918,6 +2919,14 @@
 		kvm_make_request(KVM_REQ_CLOCK_UPDATE, v);
 		return 1;
 	}
+	/*
+	 * Use the refined tsc_khz instead of the tsc_khz at boot (which was
+	 * not refined yet when we got it), if the tsc frequency hasn't changed.
+	 * If the frequency does change, it does not get refined any further,
+	 * so it is safe ot use the one gotten from the notifiers.
+	 */
+	if (!__this_cpu_read(cpu_tsc_khz_changed))
+		tgt_tsc_khz = tsc_khz;
 	if (!use_master_clock) {
 		host_tsc = rdtsc();
 		kernel_ns = get_kvmclock_base_ns();
@@ -7624,6 +7633,7 @@
 {
 	gpa_t gpa = cr2_or_gpa;
 	kvm_pfn_t pfn;
+	struct page *page;
 
 	if (!(emulation_type & EMULTYPE_ALLOW_RETRY_PF))
 		return false;
@@ -7653,7 +7663,7 @@
 	 * retry instruction -> write #PF -> emulation fail -> retry
 	 * instruction -> ...
 	 */
-	pfn = gfn_to_pfn(vcpu->kvm, gpa_to_gfn(gpa));
+	pfn = gfn_to_pfn_page(vcpu->kvm, gpa_to_gfn(gpa), &page);
 
 	/*
 	 * If the instruction failed on the error pfn, it can not be fixed,
@@ -7662,7 +7672,8 @@
 	if (is_error_noslot_pfn(pfn))
 		return false;
 
-	kvm_release_pfn_clean(pfn);
+	if (page)
+		put_page(page);
 
 	/* The instructions are well-emulated on direct mmu. */
 	if (vcpu->arch.mmu->direct_map) {
@@ -8202,6 +8213,8 @@
 		khz = freq->new;
 	else if (!boot_cpu_has(X86_FEATURE_CONSTANT_TSC))
 		khz = cpufreq_quick_get(raw_smp_processor_id());
+	if (khz)
+		__this_cpu_write(cpu_tsc_khz_changed, true);
 	if (!khz)
 		khz = tsc_khz;
 	__this_cpu_write(cpu_tsc_khz, khz);
diff -ruN a/drivers/acpi/sleep.c b/drivers/acpi/sleep.c
--- a/drivers/acpi/sleep.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/acpi/sleep.c	2021-12-23 08:35:16.000000000 +0100
@@ -565,6 +565,7 @@
 	acpi_status status = AE_OK;
 	u32 acpi_state = acpi_target_sleep_state;
 	int error;
+	u64 tsc;
 
 	ACPI_FLUSH_CPU_CACHE();
 
@@ -581,6 +582,9 @@
 		error = acpi_suspend_lowlevel();
 		if (error)
 			return error;
+		tsc = rdtsc_ordered();
+		printk(KERN_INFO "TSC at resume: %llu\n",
+				(unsigned long long)tsc);
 		pr_info("Low-level resume complete\n");
 		pm_set_resume_via_firmware();
 		break;
diff -ruN a/drivers/base/dd.c b/drivers/base/dd.c
--- a/drivers/base/dd.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/base/dd.c	2021-12-23 08:35:16.000000000 +0100
@@ -708,6 +708,7 @@
 		return -EBUSY;
 	return 0;
 }
+EXPORT_SYMBOL(driver_probe_done);
 
 /**
  * wait_for_device_probe
diff -ruN a/drivers/base/devtmpfs.c b/drivers/base/devtmpfs.c
--- a/drivers/base/devtmpfs.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/base/devtmpfs.c	2021-12-23 08:35:16.000000000 +0100
@@ -356,6 +356,7 @@
 int __init devtmpfs_mount(void)
 {
 	int err;
+	int mflags = MS_SILENT;
 
 	if (!mount_dev)
 		return 0;
@@ -363,7 +364,10 @@
 	if (!thread)
 		return 0;
 
-	err = init_mount("devtmpfs", "dev", "devtmpfs", MS_SILENT, NULL);
+#ifdef CONFIG_DEVTMPFS_SAFE
+	mflags |= MS_NOEXEC | MS_NOSUID;
+#endif
+	err = init_mount("devtmpfs", "dev", "devtmpfs", mflags, NULL);
 	if (err)
 		printk(KERN_INFO "devtmpfs: error mounting %i\n", err);
 	else
diff -ruN a/drivers/base/Kconfig b/drivers/base/Kconfig
--- a/drivers/base/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/base/Kconfig	2021-12-23 08:35:16.000000000 +0100
@@ -62,6 +62,15 @@
 	  rescue mode with init=/bin/sh, even when the /dev directory
 	  on the rootfs is completely empty.
 
+config DEVTMPFS_SAFE
+	bool "Automount devtmpfs with nosuid/noexec"
+	depends on DEVTMPFS_MOUNT
+	default y
+	help
+	  This instructs the kernel to automount devtmpfs with the
+	  MS_NOEXEC and MS_NOSUID mount flags, which can prevent
+	  certain kinds of code-execution attack on embedded platforms.
+
 config STANDALONE
 	bool "Select only drivers that don't need compile-time external firmware"
 	default y
diff -ruN a/drivers/base/power/main.c b/drivers/base/power/main.c
--- a/drivers/base/power/main.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/base/power/main.c	2021-12-23 08:35:16.000000000 +0100
@@ -35,6 +35,7 @@
 #include <linux/cpuidle.h>
 #include <linux/devfreq.h>
 #include <linux/timer.h>
+#include <linux/wakeup_reason.h>
 
 #include "../base.h"
 #include "power.h"
@@ -1244,6 +1245,8 @@
 	error = dpm_run_callback(callback, dev, state, info);
 	if (error) {
 		async_error = error;
+		log_suspend_abort_reason("Callback failed on %s in %pS returned %d",
+					 dev_name(dev), callback, error);
 		goto Complete;
 	}
 
@@ -1440,6 +1443,8 @@
 	error = dpm_run_callback(callback, dev, state, info);
 	if (error) {
 		async_error = error;
+		log_suspend_abort_reason("Callback failed on %s in %pS returned %d",
+					 dev_name(dev), callback, error);
 		goto Complete;
 	}
 	dpm_propagate_wakeup_to_parent(dev);
@@ -1651,7 +1656,12 @@
 		dev->power.direct_complete = false;
 
 	if (dev->power.direct_complete) {
-		if (pm_runtime_status_suspended(dev)) {
+		/*
+		 * Check if we're runtime suspended. If not, try to runtime
+		 * suspend for autosuspend cases.
+		 */
+		if (pm_runtime_status_suspended(dev) ||
+		    !pm_runtime_suspend(dev)) {
 			pm_runtime_disable(dev);
 			if (pm_runtime_status_suspended(dev)) {
 				pm_dev_dbg(dev, state, "direct-complete ");
@@ -1715,6 +1725,9 @@
 
 		dpm_propagate_wakeup_to_parent(dev);
 		dpm_clear_superiors_direct_complete(dev);
+	} else {
+		log_suspend_abort_reason("Callback failed on %s in %pS returned %d",
+					 dev_name(dev), callback, error);
 	}
 
 	device_unlock(dev);
@@ -1928,6 +1941,8 @@
 		} else {
 			dev_info(dev, "not prepared for power transition: code %d\n",
 				 error);
+			log_suspend_abort_reason("Device %s not prepared for power transition: code %d",
+						 dev_name(dev), error);
 		}
 
 		mutex_unlock(&dpm_list_mtx);
diff -ruN a/drivers/base/power/wakeup.c b/drivers/base/power/wakeup.c
--- a/drivers/base/power/wakeup.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/base/power/wakeup.c	2021-12-23 08:35:16.000000000 +0100
@@ -15,6 +15,9 @@
 #include <linux/seq_file.h>
 #include <linux/debugfs.h>
 #include <linux/pm_wakeirq.h>
+#include <linux/irq.h>
+#include <linux/irqdesc.h>
+#include <linux/wakeup_reason.h>
 #include <trace/events/power.h>
 
 #include "power.h"
@@ -873,6 +876,37 @@
 }
 EXPORT_SYMBOL_GPL(pm_wakeup_dev_event);
 
+void pm_get_active_wakeup_sources(char *pending_wakeup_source, size_t max)
+{
+	struct wakeup_source *ws, *last_active_ws = NULL;
+	int len = 0;
+	bool active = false;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(ws, &wakeup_sources, entry) {
+		if (ws->active && len < max) {
+			if (!active)
+				len += scnprintf(pending_wakeup_source, max,
+						"Pending Wakeup Sources: ");
+			len += scnprintf(pending_wakeup_source + len, max - len,
+				"%s ", ws->name);
+			active = true;
+		} else if (!active &&
+			   (!last_active_ws ||
+			    ktime_to_ns(ws->last_time) >
+			    ktime_to_ns(last_active_ws->last_time))) {
+			last_active_ws = ws;
+		}
+	}
+	if (!active && last_active_ws) {
+		scnprintf(pending_wakeup_source, max,
+				"Last active Wakeup Source: %s",
+				last_active_ws->name);
+	}
+	rcu_read_unlock();
+}
+EXPORT_SYMBOL_GPL(pm_get_active_wakeup_sources);
+
 void pm_print_active_wakeup_sources(void)
 {
 	struct wakeup_source *ws;
@@ -911,6 +945,7 @@
 {
 	unsigned long flags;
 	bool ret = false;
+	char suspend_abort[MAX_SUSPEND_ABORT_LEN];
 
 	raw_spin_lock_irqsave(&events_lock, flags);
 	if (events_check_enabled) {
@@ -925,6 +960,10 @@
 	if (ret) {
 		pm_pr_dbg("Wakeup pending, aborting suspend\n");
 		pm_print_active_wakeup_sources();
+		pm_get_active_wakeup_sources(suspend_abort,
+					     MAX_SUSPEND_ABORT_LEN);
+		log_suspend_abort_reason(suspend_abort);
+		pr_info("PM: %s\n", suspend_abort);
 	}
 
 	return ret || atomic_read(&pm_abort_suspend) > 0;
@@ -951,8 +990,21 @@
 
 	raw_spin_unlock_irqrestore(&wakeup_irq_lock, flags);
 
-	if (irq_number)
+	if (irq_number) {
+		struct irq_desc *desc;
+		const char *name = "null";
+
+		desc = irq_to_desc(irq_number);
+		if (desc == NULL)
+			name = "stray irq";
+		else if (desc->action && desc->action->name)
+			name = desc->action->name;
+
+		log_irq_wakeup_reason(irq_number);
+		pr_warn("%s: %d triggered %s\n", __func__, irq_number, name);
+
 		pm_system_wakeup();
+	}
 }
 
 unsigned int pm_wakeup_irq(void)
diff -ruN a/drivers/base/syscore.c b/drivers/base/syscore.c
--- a/drivers/base/syscore.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/base/syscore.c	2021-12-23 08:35:16.000000000 +0100
@@ -10,6 +10,7 @@
 #include <linux/module.h>
 #include <linux/suspend.h>
 #include <trace/events/power.h>
+#include <linux/wakeup_reason.h>
 
 static LIST_HEAD(syscore_ops_list);
 static DEFINE_MUTEX(syscore_ops_lock);
@@ -73,7 +74,9 @@
 	return 0;
 
  err_out:
-	pr_err("PM: System core suspend callback %pS failed.\n", ops->suspend);
+	log_suspend_abort_reason("System core suspend callback %pS failed",
+		ops->suspend);
+	pr_err("PM: System core suspend callback %pF failed.\n", ops->suspend);
 
 	list_for_each_entry_continue(ops, &syscore_ops_list, node)
 		if (ops->resume)
diff -ruN a/drivers/block/zram/zram_drv.c b/drivers/block/zram/zram_drv.c
--- a/drivers/block/zram/zram_drv.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/block/zram/zram_drv.c	2021-12-23 08:35:16.000000000 +0100
@@ -1810,6 +1810,16 @@
 
 	WARN_ON(!mutex_is_locked(&bdev->bd_disk->open_mutex));
 
+	/*
+	 * Chromium OS specific behavior:
+	 * sys_swapon opens the device once to populate its swapinfo->swap_file
+	 * and once when it claims the block device (blkdev_get).  By limiting
+	 * the maximum number of opens to 2, we ensure there are no prior open
+	 * references before swap is enabled.
+	 */
+	if (bdev->bd_openers > 1)
+		return -EBUSY;
+
 	zram = bdev->bd_disk->private_data;
 	/* zram was claimed to reset so open request fails */
 	if (zram->claim)
diff -ruN a/drivers/char/mem.c b/drivers/char/mem.c
--- a/drivers/char/mem.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/char/mem.c	2021-12-23 08:35:16.000000000 +0100
@@ -30,6 +30,7 @@
 #include <linux/uio.h>
 #include <linux/uaccess.h>
 #include <linux/security.h>
+#include <linux/low-mem-notify.h>
 
 #ifdef CONFIG_IA64
 # include <linux/efi.h>
@@ -707,6 +708,9 @@
 #ifdef CONFIG_PRINTK
 	[11] = { "kmsg", 0644, &kmsg_fops, 0 },
 #endif
+#ifdef CONFIG_LOW_MEM_NOTIFY
+	[12] = { "chromeos-low-mem", 0666, &low_mem_notify_fops, 0 },
+#endif
 };
 
 static int memory_open(struct inode *inode, struct file *filp)
diff -ruN a/drivers/char/tpm/cr50_i2c.c b/drivers/char/tpm/cr50_i2c.c
--- a/drivers/char/tpm/cr50_i2c.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/char/tpm/cr50_i2c.c	2021-12-23 08:35:16.000000000 +0100
@@ -0,0 +1,683 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright 2016 Google Inc.
+ *
+ * Based on Linux Kernel TPM driver by
+ * Peter Huewe <peter.huewe@infineon.com>
+ * Copyright (C) 2011 Infineon Technologies
+ */
+
+/*
+ * cr50 is a firmware for H1 secure modules that requires special
+ * handling for the I2C interface.
+ *
+ * - Use an interrupt for transaction status instead of hardcoded delays
+ * - Must use write+wait+read read protocol
+ * - All 4 bytes of status register must be read/written at once
+ * - Burst count max is 63 bytes, and burst count behaves
+ *   slightly differently than other I2C TPMs
+ * - When reading from FIFO the full burstcnt must be read
+ *   instead of just reading header and determining the remainder
+ */
+
+#include <linux/acpi.h>
+#include <linux/completion.h>
+#include <linux/i2c.h>
+#include <linux/module.h>
+#include <linux/pm.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/wait.h>
+#include "tpm.h"
+#include "tpm_tis_core.h"
+
+#define CR50_MAX_BUFSIZE	63
+#define CR50_TIMEOUT_SHORT_MS	2	/* Short timeout during transactions */
+#define CR50_TIMEOUT_NOIRQ_MS	20	/* Timeout for TPM ready without IRQ */
+#define CR50_I2C_DID_VID	0x00281ae0L
+#define CR50_I2C_MAX_RETRIES	3	/* Max retries due to I2C errors */
+#define CR50_I2C_RETRY_DELAY_LO	55	/* Min usecs between retries on I2C */
+#define CR50_I2C_RETRY_DELAY_HI	65	/* Max usecs between retries on I2C */
+
+struct priv_data {
+	int irq;
+	int locality;
+	struct completion tpm_ready;
+	u8 buf[CR50_MAX_BUFSIZE + sizeof(u8)];
+};
+
+/*
+ * The cr50 interrupt handler just signals waiting threads that the
+ * interrupt was asserted.  It does not do any processing triggered
+ * by interrupts but is instead used to avoid fixed delays.
+ */
+static irqreturn_t cr50_i2c_int_handler(int dummy, void *dev_id)
+{
+	struct tpm_chip *chip = dev_id;
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+
+	complete(&priv->tpm_ready);
+
+	return IRQ_HANDLED;
+}
+
+/*
+ * Wait for completion interrupt if available, otherwise use a fixed
+ * delay for the TPM to be ready.
+ *
+ * Returns negative number for error, positive number for success.
+ */
+static int cr50_i2c_wait_tpm_ready(struct tpm_chip *chip)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+	long rc;
+
+	/* Use a safe fixed delay if interrupt is not supported */
+	if (priv->irq <= 0) {
+		msleep(CR50_TIMEOUT_NOIRQ_MS);
+		return 1;
+	}
+
+	/* Wait for interrupt to indicate TPM is ready to respond */
+	rc = wait_for_completion_timeout(&priv->tpm_ready,
+		msecs_to_jiffies(chip->timeout_a));
+
+	if (rc == 0)
+		dev_warn(&chip->dev, "Timeout waiting for TPM ready\n");
+
+	return rc;
+}
+
+static void cr50_i2c_enable_tpm_irq(struct tpm_chip *chip)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+
+	if (priv->irq > 0) {
+		reinit_completion(&priv->tpm_ready);
+		enable_irq(priv->irq);
+	}
+}
+
+static void cr50_i2c_disable_tpm_irq(struct tpm_chip *chip)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+
+	if (priv->irq > 0)
+		disable_irq(priv->irq);
+}
+
+/*
+ * cr50_i2c_transfer - transfer messages over i2c
+ *
+ * @adapter: i2c adapter
+ * @msgs: array of messages to transfer
+ * @num: number of messages in the array
+ *
+ * Call unlocked i2c transfer routine with the provided parameters and retry
+ * in case of bus errors. Returns the number of transferred messages.
+ */
+static int cr50_i2c_transfer(struct device *dev, struct i2c_adapter *adapter,
+			     struct i2c_msg *msgs, int num)
+{
+	int rc, try;
+
+	for (try = 0; try < CR50_I2C_MAX_RETRIES; try++) {
+		rc = __i2c_transfer(adapter, msgs, num);
+		if (rc > 0)
+			break;
+		if (try)
+			dev_warn(dev, "i2c transfer failed (attempt %d/%d): %d\n",
+				 try+1, CR50_I2C_MAX_RETRIES, rc);
+		usleep_range(CR50_I2C_RETRY_DELAY_LO, CR50_I2C_RETRY_DELAY_HI);
+	}
+
+	return rc;
+}
+
+/*
+ * cr50_i2c_read() - read from TPM register
+ *
+ * @chip: TPM chip information
+ * @addr: register address to read from
+ * @buffer: provided by caller
+ * @len: number of bytes to read
+ *
+ * 1) send register address byte 'addr' to the TPM
+ * 2) wait for TPM to indicate it is ready
+ * 3) read 'len' bytes of TPM response into the provided 'buffer'
+ *
+ * Returns negative number for error, 0 for success.
+ */
+static int cr50_i2c_read(struct tpm_chip *chip, u8 addr, u8 *buffer, size_t len)
+{
+	struct i2c_client *client = to_i2c_client(chip->dev.parent);
+	struct i2c_msg msg1 = {
+		.addr = client->addr,
+		.len = 1,
+		.buf = &addr
+	};
+	struct i2c_msg msg2 = {
+		.addr = client->addr,
+		.flags = I2C_M_RD,
+		.len = len,
+		.buf = buffer
+	};
+	int rc;
+
+	i2c_lock_bus(client->adapter, I2C_LOCK_SEGMENT);
+
+	/* Prepare for completion interrupt */
+	cr50_i2c_enable_tpm_irq(chip);
+
+	/* Send the register address byte to the TPM */
+	rc = cr50_i2c_transfer(&chip->dev, client->adapter, &msg1, 1);
+	if (rc <= 0)
+		goto out;
+
+	/* Wait for TPM to be ready with response data */
+	rc = cr50_i2c_wait_tpm_ready(chip);
+	if (rc < 0)
+		goto out;
+
+	/* Read response data from the TPM */
+	rc = cr50_i2c_transfer(&chip->dev, client->adapter, &msg2, 1);
+
+out:
+	cr50_i2c_disable_tpm_irq(chip);
+	i2c_unlock_bus(client->adapter, I2C_LOCK_SEGMENT);
+
+	if (rc < 0)
+		return rc;
+	if (rc == 0)
+		return -EIO; /* No i2c segments transferred */
+
+	return 0;
+}
+
+/*
+ * cr50_i2c_write() - write to TPM register
+ *
+ * @chip: TPM chip information
+ * @addr: register address to write to
+ * @buffer: data to write
+ * @len: number of bytes to write
+ *
+ * 1) prepend the provided address to the provided data
+ * 2) send the address+data to the TPM
+ * 3) wait for TPM to indicate it is done writing
+ *
+ * Returns negative number for error, 0 for success.
+ */
+static int cr50_i2c_write(struct tpm_chip *chip, u8 addr, u8 *buffer,
+			  size_t len)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+	struct i2c_client *client = to_i2c_client(chip->dev.parent);
+	struct i2c_msg msg1 = {
+		.addr = client->addr,
+		.len = len + 1,
+		.buf = priv->buf
+	};
+	int rc;
+
+	if (len > CR50_MAX_BUFSIZE)
+		return -EINVAL;
+
+	i2c_lock_bus(client->adapter, I2C_LOCK_SEGMENT);
+
+	/* Prepend the 'register address' to the buffer */
+	priv->buf[0] = addr;
+	memcpy(priv->buf + 1, buffer, len);
+
+	/* Prepare for completion interrupt */
+	cr50_i2c_enable_tpm_irq(chip);
+
+	/* Send write request buffer with address */
+	rc = cr50_i2c_transfer(&chip->dev, client->adapter, &msg1, 1);
+	if (rc <= 0)
+		goto out;
+
+	/* Wait for TPM to be ready, ignore timeout */
+	cr50_i2c_wait_tpm_ready(chip);
+
+out:
+	cr50_i2c_disable_tpm_irq(chip);
+	i2c_unlock_bus(client->adapter, I2C_LOCK_SEGMENT);
+
+	if (rc < 0)
+		return rc;
+	if (rc == 0)
+		return -EIO; /* No i2c segments transferred */
+
+	return 0;
+}
+
+#undef	TPM_ACCESS
+#undef	TPM_STS
+#undef	TPM_DATA_FIFO
+#undef	TPM_DID_VID
+
+#define	TPM_ACCESS(l)			(0x0000 | ((l) << 4))
+#define	TPM_STS(l)			(0x0001 | ((l) << 4))
+#define	TPM_DATA_FIFO(l)		(0x0005 | ((l) << 4))
+#define	TPM_DID_VID(l)			(0x0006 | ((l) << 4))
+
+static int check_locality(struct tpm_chip *chip, int loc)
+{
+	u8 mask = TPM_ACCESS_VALID | TPM_ACCESS_ACTIVE_LOCALITY;
+	u8 buf;
+	int rc;
+
+	rc = cr50_i2c_read(chip, TPM_ACCESS(loc), &buf, 1);
+	if (rc < 0)
+		return rc;
+
+	if ((buf & mask) == mask)
+		return loc;
+
+	return -EIO;
+}
+
+static void release_locality(struct tpm_chip *chip, int force)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+	u8 mask = TPM_ACCESS_VALID | TPM_ACCESS_REQUEST_PENDING;
+	u8 addr = TPM_ACCESS(priv->locality);
+	u8 buf;
+
+	if (cr50_i2c_read(chip, addr, &buf, 1) < 0)
+		return;
+
+	if (force || (buf & mask) == mask) {
+		buf = TPM_ACCESS_ACTIVE_LOCALITY;
+		cr50_i2c_write(chip, addr, &buf, 1);
+	}
+
+	priv->locality = 0;
+}
+
+static int request_locality(struct tpm_chip *chip, int loc)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+	u8 buf = TPM_ACCESS_REQUEST_USE;
+	unsigned long stop;
+	int rc;
+
+	if (check_locality(chip, loc) == loc)
+		return loc;
+
+	rc = cr50_i2c_write(chip, TPM_ACCESS(loc), &buf, 1);
+	if (rc < 0)
+		return rc;
+
+	stop = jiffies + chip->timeout_a;
+	do {
+		if (check_locality(chip, loc) == loc) {
+			priv->locality = loc;
+			return loc;
+		}
+		msleep(CR50_TIMEOUT_SHORT_MS);
+	} while (time_before(jiffies, stop));
+
+	return -ETIMEDOUT;
+}
+
+/* cr50 requires all 4 bytes of status register to be read */
+static u8 cr50_i2c_tis_status(struct tpm_chip *chip)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+	u8 buf[4];
+
+	if (cr50_i2c_read(chip, TPM_STS(priv->locality), buf, sizeof(buf)) < 0)
+		return 0;
+	return buf[0];
+}
+
+/* cr50 requires all 4 bytes of status register to be written */
+static void cr50_i2c_tis_ready(struct tpm_chip *chip)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+	u8 buf[4] = { TPM_STS_COMMAND_READY };
+
+	cr50_i2c_write(chip, TPM_STS(priv->locality), buf, sizeof(buf));
+	msleep(CR50_TIMEOUT_SHORT_MS);
+}
+
+/*
+ * cr50 uses bytes 3:2 of status register for burst count and
+ * all 4 bytes must be read
+ */
+static int cr50_i2c_wait_burststs(struct tpm_chip *chip, u8 mask,
+				  size_t *burst, int *status)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+	unsigned long stop;
+	u8 buf[4];
+
+	/* wait for burstcount */
+	stop = jiffies + chip->timeout_b;
+	do {
+		if (cr50_i2c_read(chip, TPM_STS(priv->locality),
+				  (u8 *)&buf, sizeof(buf)) < 0) {
+			msleep(CR50_TIMEOUT_SHORT_MS);
+			continue;
+		}
+
+		*status = *buf;
+		*burst = le16_to_cpup((__le16 *)(buf + 1));
+
+		if ((*status & mask) == mask &&
+		    *burst > 0 && *burst <= CR50_MAX_BUFSIZE)
+			return 0;
+
+		msleep(CR50_TIMEOUT_SHORT_MS);
+	} while (time_before(jiffies, stop));
+
+	dev_err(&chip->dev, "Timeout reading burst and status\n");
+	return -ETIMEDOUT;
+}
+
+static int cr50_i2c_tis_recv(struct tpm_chip *chip, u8 *buf, size_t buf_len)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+	int status, rc;
+	size_t burstcnt, cur, len, expected;
+	u8 addr = TPM_DATA_FIFO(priv->locality);
+	u8 mask = TPM_STS_VALID | TPM_STS_DATA_AVAIL;
+
+	if (buf_len < TPM_HEADER_SIZE)
+		return -EINVAL;
+
+	rc = cr50_i2c_wait_burststs(chip, mask, &burstcnt, &status);
+	if (rc < 0)
+		goto out_err;
+
+	if (burstcnt > buf_len || burstcnt < TPM_HEADER_SIZE) {
+		dev_err(&chip->dev,
+			"Unexpected burstcnt: %zu (max=%zu, min=%d)\n",
+			burstcnt, buf_len, TPM_HEADER_SIZE);
+		rc = -EIO;
+		goto out_err;
+	}
+
+	/* Read first chunk of burstcnt bytes */
+	rc = cr50_i2c_read(chip, addr, buf, burstcnt);
+	if (rc < 0) {
+		dev_err(&chip->dev, "Read of first chunk failed\n");
+		goto out_err;
+	}
+
+	/* Determine expected data in the return buffer */
+	expected = be32_to_cpup((__be32 *)(buf + 2));
+	if (expected > buf_len) {
+		dev_err(&chip->dev, "Too much data in FIFO\n");
+		goto out_err;
+	}
+
+	/* Now read the rest of the data */
+	cur = burstcnt;
+	while (cur < expected) {
+		/* Read updated burst count and check status */
+		rc = cr50_i2c_wait_burststs(chip, mask, &burstcnt, &status);
+		if (rc < 0)
+			goto out_err;
+
+		len = min_t(size_t, burstcnt, expected - cur);
+		rc = cr50_i2c_read(chip, addr, buf + cur, len);
+		if (rc < 0) {
+			dev_err(&chip->dev, "Read failed\n");
+			goto out_err;
+		}
+
+		cur += len;
+	}
+
+	/* Ensure TPM is done reading data */
+	rc = cr50_i2c_wait_burststs(chip, TPM_STS_VALID, &burstcnt, &status);
+	if (rc < 0)
+		goto out_err;
+	if (status & TPM_STS_DATA_AVAIL) {
+		dev_err(&chip->dev, "Data still available\n");
+		rc = -EIO;
+		goto out_err;
+	}
+
+	release_locality(chip, 0);
+	return cur;
+
+out_err:
+	/* Abort current transaction if still pending */
+	if (cr50_i2c_tis_status(chip) & TPM_STS_COMMAND_READY)
+		cr50_i2c_tis_ready(chip);
+
+	release_locality(chip, 0);
+	return rc;
+}
+
+static int cr50_i2c_tis_send(struct tpm_chip *chip, u8 *buf, size_t len)
+{
+	struct priv_data *priv = dev_get_drvdata(&chip->dev);
+	int rc, status;
+	size_t burstcnt, limit, sent = 0;
+	u8 tpm_go[4] = { TPM_STS_GO };
+	unsigned long stop;
+
+	rc = request_locality(chip, 0);
+	if (rc < 0)
+		return rc;
+
+	/* Wait until TPM is ready for a command */
+	stop = jiffies + chip->timeout_b;
+	while (!(cr50_i2c_tis_status(chip) & TPM_STS_COMMAND_READY)) {
+		if (time_after(jiffies, stop)) {
+			rc = -ETIMEDOUT;
+			goto out_err;
+		}
+
+		cr50_i2c_tis_ready(chip);
+	}
+
+	while (len > 0) {
+		u8 mask = TPM_STS_VALID;
+
+		/* Wait for data if this is not the first chunk */
+		if (sent > 0)
+			mask |= TPM_STS_DATA_EXPECT;
+
+		/* Read burst count and check status */
+		rc = cr50_i2c_wait_burststs(chip, mask, &burstcnt, &status);
+		if (rc < 0)
+			goto out_err;
+
+		/*
+		 * Use burstcnt - 1 to account for the address byte
+		 * that is inserted by cr50_i2c_write()
+		 */
+		limit = min_t(size_t, burstcnt - 1, len);
+		rc = cr50_i2c_write(chip, TPM_DATA_FIFO(priv->locality),
+				    &buf[sent], limit);
+		if (rc < 0) {
+			dev_err(&chip->dev, "Write failed\n");
+			goto out_err;
+		}
+
+		sent += limit;
+		len -= limit;
+	}
+
+	/* Ensure TPM is not expecting more data */
+	rc = cr50_i2c_wait_burststs(chip, TPM_STS_VALID, &burstcnt, &status);
+	if (rc < 0)
+		goto out_err;
+	if (status & TPM_STS_DATA_EXPECT) {
+		dev_err(&chip->dev, "Data still expected\n");
+		rc = -EIO;
+		goto out_err;
+	}
+
+	/* Start the TPM command */
+	rc = cr50_i2c_write(chip, TPM_STS(priv->locality), tpm_go,
+			    sizeof(tpm_go));
+	if (rc < 0) {
+		dev_err(&chip->dev, "Start command failed\n");
+		goto out_err;
+	}
+	return 0;
+
+out_err:
+	/* Abort current transaction if still pending */
+	if (cr50_i2c_tis_status(chip) & TPM_STS_COMMAND_READY)
+		cr50_i2c_tis_ready(chip);
+
+	release_locality(chip, 0);
+	return rc;
+}
+
+static bool cr50_i2c_req_canceled(struct tpm_chip *chip, u8 status)
+{
+	return (status == TPM_STS_COMMAND_READY);
+}
+
+static const struct tpm_class_ops cr50_i2c = {
+	.flags = TPM_OPS_AUTO_STARTUP,
+	.status = &cr50_i2c_tis_status,
+	.recv = &cr50_i2c_tis_recv,
+	.send = &cr50_i2c_tis_send,
+	.cancel = &cr50_i2c_tis_ready,
+	.req_complete_mask = TPM_STS_DATA_AVAIL | TPM_STS_VALID,
+	.req_complete_val = TPM_STS_DATA_AVAIL | TPM_STS_VALID,
+	.req_canceled = &cr50_i2c_req_canceled,
+};
+
+static int cr50_i2c_init(struct i2c_client *client)
+{
+	struct device *dev = &client->dev;
+	struct tpm_chip *chip;
+	struct priv_data *priv;
+	u8 buf[4];
+	u32 vendor;
+	int rc;
+
+	chip = tpmm_chip_alloc(dev, &cr50_i2c);
+	if (IS_ERR(chip))
+		return PTR_ERR(chip);
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	/* cr50 is a TPM 2.0 chip */
+	chip->flags |= TPM_CHIP_FLAG_TPM2;
+	chip->flags |= TPM_CHIP_FLAG_FIRMWARE_POWER_MANAGED;
+
+	/* Default timeouts */
+	chip->timeout_a = msecs_to_jiffies(TIS_SHORT_TIMEOUT);
+	chip->timeout_b = msecs_to_jiffies(TIS_LONG_TIMEOUT);
+	chip->timeout_c = msecs_to_jiffies(TIS_SHORT_TIMEOUT);
+	chip->timeout_d = msecs_to_jiffies(TIS_SHORT_TIMEOUT);
+
+	dev_set_drvdata(&chip->dev, priv);
+	init_completion(&priv->tpm_ready);
+
+	if (client->irq > 0) {
+		rc = devm_request_irq(dev, client->irq, cr50_i2c_int_handler,
+				      IRQF_TRIGGER_FALLING | IRQF_ONESHOT,
+				      dev->driver->name, chip);
+		if (rc < 0) {
+			dev_err(dev, "Failed to probe IRQ %d\n", client->irq);
+			return rc;
+		}
+
+		disable_irq(client->irq);
+		priv->irq = client->irq;
+	} else {
+		dev_warn(dev, "No IRQ, will use %ums delay for TPM ready\n",
+			 CR50_TIMEOUT_NOIRQ_MS);
+	}
+
+	rc = request_locality(chip, 0);
+	if (rc < 0) {
+		dev_err(dev, "Could not request locality\n");
+		return rc;
+	}
+
+	/* Read four bytes from DID_VID register */
+	rc = cr50_i2c_read(chip, TPM_DID_VID(0), buf, sizeof(buf));
+	if (rc < 0) {
+		dev_err(dev, "Could not read vendor id\n");
+		release_locality(chip, 1);
+		return rc;
+	}
+
+	vendor = le32_to_cpup((__le32 *)buf);
+	if (vendor != CR50_I2C_DID_VID) {
+		dev_err(dev, "Vendor ID did not match! ID was %08x\n", vendor);
+		release_locality(chip, 1);
+		return -ENODEV;
+	}
+
+	dev_info(dev, "cr50 TPM 2.0 (i2c 0x%02x irq %d id 0x%x)\n",
+		 client->addr, client->irq, vendor >> 16);
+
+	return tpm_chip_register(chip);
+}
+
+static const struct i2c_device_id cr50_i2c_table[] = {
+	{"cr50_i2c", 0},
+	{}
+};
+MODULE_DEVICE_TABLE(i2c, cr50_i2c_table);
+
+#ifdef CONFIG_ACPI
+static const struct acpi_device_id cr50_i2c_acpi_id[] = {
+	{ "GOOG0005", 0 },
+	{}
+};
+MODULE_DEVICE_TABLE(acpi, cr50_i2c_acpi_id);
+#endif
+
+#ifdef CONFIG_OF
+static const struct of_device_id of_cr50_i2c_match[] = {
+	{ .compatible = "google,cr50", },
+	{}
+};
+MODULE_DEVICE_TABLE(of, of_cr50_i2c_match);
+#endif
+
+static int cr50_i2c_probe(struct i2c_client *client,
+			  const struct i2c_device_id *id)
+{
+	if (!i2c_check_functionality(client->adapter, I2C_FUNC_I2C))
+		return -ENODEV;
+
+	return cr50_i2c_init(client);
+}
+
+static int cr50_i2c_remove(struct i2c_client *client)
+{
+	struct tpm_chip *chip = i2c_get_clientdata(client);
+
+	tpm_chip_unregister(chip);
+	release_locality(chip, 1);
+
+	return 0;
+}
+
+static SIMPLE_DEV_PM_OPS(cr50_i2c_pm, tpm_pm_suspend, tpm_pm_resume);
+
+static struct i2c_driver cr50_i2c_driver = {
+	.id_table = cr50_i2c_table,
+	.probe = cr50_i2c_probe,
+	.remove = cr50_i2c_remove,
+	.driver = {
+		.name = "cr50_i2c",
+		.pm = &cr50_i2c_pm,
+		.acpi_match_table = ACPI_PTR(cr50_i2c_acpi_id),
+		.of_match_table = of_match_ptr(of_cr50_i2c_match),
+	},
+};
+
+module_i2c_driver(cr50_i2c_driver);
+
+MODULE_DESCRIPTION("cr50 TPM I2C Driver");
+MODULE_LICENSE("GPL");
diff -ruN a/drivers/char/tpm/Kconfig b/drivers/char/tpm/Kconfig
--- a/drivers/char/tpm/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/char/tpm/Kconfig	2021-12-23 08:35:16.000000000 +0100
@@ -142,6 +142,15 @@
 	  will be accessible from within Linux.  To compile this driver 
 	  as a module, choose M here; the module will be called tpm_atmel.
 
+config TCG_CR50_I2C
+	tristate "Cr50 I2C Interface"
+	depends on I2C
+	help
+	  If you have a H1 secure module running Cr50 firmware on I2C bus,
+	  say Yes and it will be accessible from within Linux. To compile
+	  this driver as a module, choose M here; the module will be called
+	  cr50_i2c.
+
 config TCG_INFINEON
 	tristate "Infineon Technologies TPM Interface"
 	depends on PNP
@@ -198,5 +207,14 @@
 	help
 	  This driver proxies for firmware TPM running in TEE.
 
+config TCG_VIRTIO_VTPM
+	tristate "Virtio vTPM"
+	depends on TCG_TPM && VIRTIO
+	help
+	  This driver provides the guest kernel side of TPM over Virtio. If
+	  you are building Linux to run inside of a hypervisor that supports
+	  TPM over Virtio, say Yes and the virtualized TPM will be
+	  accessible from the guest.
+
 source "drivers/char/tpm/st33zp24/Kconfig"
 endif # TCG_TPM
diff -ruN a/drivers/char/tpm/Makefile b/drivers/char/tpm/Makefile
--- a/drivers/char/tpm/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/char/tpm/Makefile	2021-12-23 08:35:16.000000000 +0100
@@ -34,6 +34,7 @@
 obj-$(CONFIG_TCG_TIS_I2C_NUVOTON) += tpm_i2c_nuvoton.o
 obj-$(CONFIG_TCG_NSC) += tpm_nsc.o
 obj-$(CONFIG_TCG_ATMEL) += tpm_atmel.o
+obj-$(CONFIG_TCG_CR50_I2C) += cr50_i2c.o
 obj-$(CONFIG_TCG_INFINEON) += tpm_infineon.o
 obj-$(CONFIG_TCG_IBMVTPM) += tpm_ibmvtpm.o
 obj-$(CONFIG_TCG_TIS_ST33ZP24) += st33zp24/
@@ -41,3 +42,4 @@
 obj-$(CONFIG_TCG_CRB) += tpm_crb.o
 obj-$(CONFIG_TCG_VTPM_PROXY) += tpm_vtpm_proxy.o
 obj-$(CONFIG_TCG_FTPM_TEE) += tpm_ftpm_tee.o
+obj-$(CONFIG_TCG_VIRTIO_VTPM) += tpm_virtio.o
diff -ruN a/drivers/char/tpm/tpm1-cmd.c b/drivers/char/tpm/tpm1-cmd.c
--- a/drivers/char/tpm/tpm1-cmd.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/char/tpm/tpm1-cmd.c	2021-12-23 08:35:16.000000000 +0100
@@ -708,11 +708,9 @@
 	rc = tpm1_get_timeouts(chip);
 	if (rc)
 		goto out;
-	rc = tpm1_do_selftest(chip);
-	if (rc) {
-		dev_err(&chip->dev, "TPM self test failed\n");
-		goto out;
-	}
+
+	if (tpm1_do_selftest(chip))
+		dev_err(&chip->dev, "TPM self test failed - ignoring\n");
 
 	return rc;
 out:
diff -ruN a/drivers/char/tpm/tpm-chip.c b/drivers/char/tpm/tpm-chip.c
--- a/drivers/char/tpm/tpm-chip.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/char/tpm/tpm-chip.c	2021-12-23 08:35:16.000000000 +0100
@@ -296,7 +296,7 @@
 	struct tpm_chip *chip = container_of(dev, struct tpm_chip, dev);
 
 	down_write(&chip->ops_sem);
-	if (chip->flags & TPM_CHIP_FLAG_TPM2) {
+	if (chip->ops && (chip->flags & TPM_CHIP_FLAG_TPM2)) {
 		if (!tpm_chip_start(chip)) {
 			tpm2_shutdown(chip, TPM2_SU_CLEAR);
 			tpm_chip_stop(chip);
@@ -488,7 +488,7 @@
 {
 	struct attribute **i;
 
-	if (chip->flags & (TPM_CHIP_FLAG_TPM2 | TPM_CHIP_FLAG_VIRTUAL))
+	if (chip->flags & TPM_CHIP_FLAG_VIRTUAL)
 		return;
 
 	sysfs_remove_link(&chip->dev.parent->kobj, "ppi");
@@ -506,7 +506,7 @@
 	struct attribute **i;
 	int rc;
 
-	if (chip->flags & (TPM_CHIP_FLAG_TPM2 | TPM_CHIP_FLAG_VIRTUAL))
+	if (chip->flags & TPM_CHIP_FLAG_VIRTUAL)
 		return 0;
 
 	rc = compat_only_sysfs_link_entry_to_kobj(
@@ -543,6 +543,7 @@
 		 "tpm-rng-%d", chip->dev_num);
 	chip->hwrng.name = chip->hwrng_name;
 	chip->hwrng.read = tpm_hwrng_read;
+	chip->hwrng.quality = 1000;
 	return hwrng_register(&chip->hwrng);
 }
 
diff -ruN a/drivers/char/tpm/tpm-interface.c b/drivers/char/tpm/tpm-interface.c
--- a/drivers/char/tpm/tpm-interface.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/char/tpm/tpm-interface.c	2021-12-23 08:35:16.000000000 +0100
@@ -82,6 +82,11 @@
 		return -E2BIG;
 	}
 
+	if (chip->is_suspended) {
+		dev_warn(&chip->dev, "blocking transmit while suspended\n");
+		return -EAGAIN;
+	}
+
 	rc = chip->ops->send(chip, buf, count);
 	if (rc < 0) {
 		if (rc != -EPIPE)
@@ -410,6 +415,8 @@
 		tpm_chip_stop(chip);
 	}
 
+	if (!rc)
+		chip->is_suspended = true;
 suspended:
 	return rc;
 }
@@ -426,6 +433,7 @@
 	if (chip == NULL)
 		return -ENODEV;
 
+	chip->is_suspended = false;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(tpm_pm_resume);
diff -ruN a/drivers/char/tpm/tpm-sysfs.c b/drivers/char/tpm/tpm-sysfs.c
--- a/drivers/char/tpm/tpm-sysfs.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/char/tpm/tpm-sysfs.c	2021-12-23 08:35:16.000000000 +0100
@@ -324,7 +324,137 @@
 	NULL,
 };
 
+struct tpm2_prop_flag_dev_attribute {
+	struct device_attribute attr;
+	u32 property_id;
+	u32 flag_mask;
+};
+
+struct tpm2_prop_u32_dev_attribute {
+	struct device_attribute attr;
+	u32 property_id;
+};
+
+static ssize_t tpm2_prop_flag_show(struct device *dev,
+				   struct device_attribute *attr,
+				   char *buf)
+{
+	struct tpm2_prop_flag_dev_attribute *pa =
+		container_of(attr, struct tpm2_prop_flag_dev_attribute, attr);
+	struct tpm_chip *chip = to_tpm_chip(dev);
+	u32 flags;
+	ssize_t rc;
+
+	rc = tpm_try_get_ops(chip);
+	if (rc)
+		return rc;
+
+	rc = tpm2_get_tpm_pt(chip, pa->property_id, &flags, "reading property");
+	if (rc) {
+		rc = 0;
+		goto error;
+	}
+
+	rc = sprintf(buf, "%d\n", !!(flags & pa->flag_mask));
+error:
+	tpm_put_ops(chip);
+	return rc;
+}
+
+static ssize_t tpm2_prop_u32_show(struct device *dev,
+				  struct device_attribute *attr,
+				  char *buf)
+{
+	struct tpm2_prop_u32_dev_attribute *pa =
+		container_of(attr, struct tpm2_prop_u32_dev_attribute, attr);
+	struct tpm_chip *chip = to_tpm_chip(dev);
+	u32 value;
+	ssize_t rc;
+
+	rc = tpm_try_get_ops(chip);
+	if (rc)
+		return rc;
+
+	rc = tpm2_get_tpm_pt(chip, pa->property_id, &value, "reading property");
+	if (rc) {
+		rc = 0;
+		goto error;
+	}
+
+	rc = sprintf(buf, "%u\n", value);
+error:
+	tpm_put_ops(chip);
+	return rc;
+}
+
+#define TPM2_PROP_FLAG_ATTR(_name, _property_id, _flag_mask)           \
+	struct tpm2_prop_flag_dev_attribute attr_tpm2_prop_##_name = { \
+		__ATTR(_name, S_IRUGO, tpm2_prop_flag_show, NULL),     \
+		_property_id, _flag_mask                               \
+	}
+
+#define TPM2_PROP_U32_ATTR(_name, _property_id)                        \
+	struct tpm2_prop_u32_dev_attribute attr_tpm2_prop_##_name = {  \
+		__ATTR(_name, S_IRUGO, tpm2_prop_u32_show, NULL),      \
+		_property_id                                           \
+	}
+
+TPM2_PROP_FLAG_ATTR(owner_auth_set,
+		    TPM2_PT_PERMANENT, TPM2_ATTR_OWNER_AUTH_SET);
+TPM2_PROP_FLAG_ATTR(endorsement_auth_set,
+		    TPM2_PT_PERMANENT, TPM2_ATTR_ENDORSEMENT_AUTH_SET);
+TPM2_PROP_FLAG_ATTR(lockout_auth_set,
+		    TPM2_PT_PERMANENT, TPM2_ATTR_LOCKOUT_AUTH_SET);
+TPM2_PROP_FLAG_ATTR(disable_clear,
+		    TPM2_PT_PERMANENT, TPM2_ATTR_DISABLE_CLEAR);
+TPM2_PROP_FLAG_ATTR(in_lockout,
+		    TPM2_PT_PERMANENT, TPM2_ATTR_IN_LOCKOUT);
+TPM2_PROP_FLAG_ATTR(tpm_generated_eps,
+		    TPM2_PT_PERMANENT, TPM2_ATTR_TPM_GENERATED_EPS);
+
+TPM2_PROP_FLAG_ATTR(ph_enable,
+		    TPM2_PT_STARTUP_CLEAR, TPM2_ATTR_PH_ENABLE);
+TPM2_PROP_FLAG_ATTR(sh_enable,
+		    TPM2_PT_STARTUP_CLEAR, TPM2_ATTR_SH_ENABLE);
+TPM2_PROP_FLAG_ATTR(eh_enable,
+		    TPM2_PT_STARTUP_CLEAR, TPM2_ATTR_EH_ENABLE);
+TPM2_PROP_FLAG_ATTR(ph_enable_nv,
+		    TPM2_PT_STARTUP_CLEAR, TPM2_ATTR_PH_ENABLE_NV);
+TPM2_PROP_FLAG_ATTR(orderly,
+		    TPM2_PT_STARTUP_CLEAR, TPM2_ATTR_ORDERLY);
+
+/* Aliases for userland scripts in TPM2 case */
+TPM2_PROP_FLAG_ATTR(enabled,
+		    TPM2_PT_STARTUP_CLEAR, TPM2_ATTR_SH_ENABLE);
+TPM2_PROP_FLAG_ATTR(owned,
+		    TPM2_PT_PERMANENT, TPM2_ATTR_OWNER_AUTH_SET);
+
+TPM2_PROP_U32_ATTR(lockout_counter, TPM2_PT_LOCKOUT_COUNTER);
+TPM2_PROP_U32_ATTR(max_auth_fail, TPM2_PT_MAX_AUTH_FAIL);
+TPM2_PROP_U32_ATTR(lockout_interval, TPM2_PT_LOCKOUT_INTERVAL);
+TPM2_PROP_U32_ATTR(lockout_recovery, TPM2_PT_LOCKOUT_RECOVERY);
+
+#define ATTR_FOR_TPM2_PROP(_name) (&attr_tpm2_prop_##_name.attr.attr)
 static struct attribute *tpm2_dev_attrs[] = {
+	ATTR_FOR_TPM2_PROP(owner_auth_set),
+	ATTR_FOR_TPM2_PROP(endorsement_auth_set),
+	ATTR_FOR_TPM2_PROP(lockout_auth_set),
+	ATTR_FOR_TPM2_PROP(disable_clear),
+	ATTR_FOR_TPM2_PROP(in_lockout),
+	ATTR_FOR_TPM2_PROP(tpm_generated_eps),
+	ATTR_FOR_TPM2_PROP(ph_enable),
+	ATTR_FOR_TPM2_PROP(sh_enable),
+	ATTR_FOR_TPM2_PROP(eh_enable),
+	ATTR_FOR_TPM2_PROP(ph_enable_nv),
+	ATTR_FOR_TPM2_PROP(orderly),
+	ATTR_FOR_TPM2_PROP(enabled),
+	ATTR_FOR_TPM2_PROP(owned),
+	ATTR_FOR_TPM2_PROP(lockout_counter),
+	ATTR_FOR_TPM2_PROP(max_auth_fail),
+	ATTR_FOR_TPM2_PROP(lockout_interval),
+	ATTR_FOR_TPM2_PROP(lockout_recovery),
+	&dev_attr_durations.attr,
+	&dev_attr_timeouts.attr,
 	&dev_attr_tpm_version_major.attr,
 	NULL
 };
@@ -476,6 +606,7 @@
 
 void tpm_sysfs_add_device(struct tpm_chip *chip)
 {
+	/* FIXME: update tpm_sysfs to explicitly lock chip->ops for TPM 2.0 */
 	int i;
 
 	WARN_ON(chip->groups_cnt != 0);
diff -ruN a/drivers/char/tpm/tpm_tis_core.c b/drivers/char/tpm/tpm_tis_core.c
--- a/drivers/char/tpm/tpm_tis_core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/char/tpm/tpm_tis_core.c	2021-12-23 08:35:16.000000000 +0100
@@ -247,8 +247,15 @@
 			return rc;
 
 		burstcnt = (value >> 8) & 0xFFFF;
-		if (burstcnt)
+		if (burstcnt) {
+			if (priv->phy_ops->max_xfer_size &&
+			    (burstcnt > priv->phy_ops->max_xfer_size)) {
+				dev_warn(&chip->dev,
+					 "Bad burstcnt read: %d\n", burstcnt);
+				burstcnt = priv->phy_ops->max_xfer_size;
+			}
 			return burstcnt;
+		}
 		usleep_range(TPM_TIMEOUT_USECS_MIN, TPM_TIMEOUT_USECS_MAX);
 	} while (time_before(jiffies, stop));
 	return -EBUSY;
@@ -919,6 +926,31 @@
 	.clk_enable = tpm_tis_clkrun_enable,
 };
 
+static ssize_t did_vid_show(struct device *dev, struct device_attribute *attr,
+			    char *buf)
+{
+	struct tpm_tis_data *priv = dev_get_drvdata(dev);
+	u32 did_vid = 0;
+	int rc;
+
+	rc = tpm_tis_read32(priv, TPM_DID_VID(0), &did_vid);
+	if (rc < 0) {
+		dev_warn(dev, "%s: failed to read did_vid: %d\n", __func__, rc);
+		return rc;
+	}
+	return sprintf(buf, "0x%08X\n", did_vid);
+}
+static DEVICE_ATTR_RO(did_vid);
+
+static struct attribute *tpm_tis_attrs[] = {
+	&dev_attr_did_vid.attr,
+	NULL,
+};
+
+static const struct attribute_group tpm_tis_group = {
+	.attrs = tpm_tis_attrs,
+};
+
 int tpm_tis_core_init(struct device *dev, struct tpm_tis_data *priv, int irq,
 		      const struct tpm_tis_phy_ops *phy_ops,
 		      acpi_handle acpi_dev_handle)
@@ -1012,6 +1044,9 @@
 		 (chip->flags & TPM_CHIP_FLAG_TPM2) ? "2.0" : "1.2",
 		 vendor >> 16, rid);
 
+	/* Expose the DID_VID information to userspace */
+	chip->groups[chip->groups_cnt++] = &tpm_tis_group;
+
 	probe = probe_itpm(chip);
 	if (probe < 0) {
 		rc = -ENODEV;
diff -ruN a/drivers/char/tpm/tpm_tis_core.h b/drivers/char/tpm/tpm_tis_core.h
--- a/drivers/char/tpm/tpm_tis_core.h	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/char/tpm/tpm_tis_core.h	2021-12-23 08:35:16.000000000 +0100
@@ -112,6 +112,7 @@
 	int (*read16)(struct tpm_tis_data *data, u32 addr, u16 *result);
 	int (*read32)(struct tpm_tis_data *data, u32 addr, u32 *result);
 	int (*write32)(struct tpm_tis_data *data, u32 addr, u32 src);
+	u16 max_xfer_size;
 };
 
 static inline int tpm_tis_read_bytes(struct tpm_tis_data *data, u32 addr,
diff -ruN a/drivers/char/tpm/tpm_tis_spi_main.c b/drivers/char/tpm/tpm_tis_spi_main.c
--- a/drivers/char/tpm/tpm_tis_spi_main.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/char/tpm/tpm_tis_spi_main.c	2021-12-23 08:35:16.000000000 +0100
@@ -208,6 +208,7 @@
 	.read16 = tpm_tis_spi_read16,
 	.read32 = tpm_tis_spi_read32,
 	.write32 = tpm_tis_spi_write32,
+	.max_xfer_size = MAX_SPI_FRAMESIZE,
 };
 
 static int tpm_tis_spi_probe(struct spi_device *dev)
diff -ruN a/drivers/char/tpm/tpm_virtio.c b/drivers/char/tpm/tpm_virtio.c
--- a/drivers/char/tpm/tpm_virtio.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/char/tpm/tpm_virtio.c	2021-12-23 08:35:16.000000000 +0100
@@ -0,0 +1,463 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright 2019 Google Inc.
+ *
+ * Author: David Tolnay <dtolnay@gmail.com>
+ *
+ * ---
+ *
+ * Device driver for TPM over virtio.
+ *
+ * This driver employs a single virtio queue to handle both send and recv. TPM
+ * commands are sent over virtio to the hypervisor during a TPM send operation
+ * and responses are received over the same queue during a recv operation.
+ *
+ * The driver contains a single buffer that is the only buffer we ever place on
+ * the virtio queue. Commands are copied from the caller's command buffer into
+ * the driver's buffer before handing off to virtio, and responses are received
+ * into the driver's buffer then copied into the caller's response buffer. This
+ * allows us to be resilient to timeouts. When a send or recv operation times
+ * out, the caller is free to destroy their buffer; we don't want the hypervisor
+ * continuing to perform reads or writes against that destroyed buffer.
+ *
+ * This driver does not support concurrent send and recv operations. Mutual
+ * exclusion is upheld by the tpm_mutex lock held in tpm-interface.c around the
+ * calls to chip->ops->send and chip->ops->recv.
+ *
+ * The intended hypervisor-side implementation is as follows.
+ *
+ *     while true:
+ *         await next virtio buffer.
+ *         expect first descriptor in chain to be guest-to-host.
+ *         read tpm command from that buffer.
+ *         synchronously perform TPM work determined by the command.
+ *         expect second descriptor in chain to be host-to-guest.
+ *         write TPM response into that buffer.
+ *         place buffer on virtio used queue indicating how many bytes written.
+ */
+
+#include <linux/virtio_config.h>
+
+#include "tpm.h"
+
+/*
+ * Timeout duration when waiting on the hypervisor to complete its end of the
+ * TPM operation. This timeout is relatively high because certain TPM operations
+ * can take dozens of seconds.
+ */
+#define TPM_VIRTIO_TIMEOUT (120 * HZ)
+
+struct vtpm_device {
+	/*
+	 * Data structure for integration with the common code of the TPM driver
+	 * in tpm-chip.c.
+	 */
+	struct tpm_chip *chip;
+
+	/*
+	 * Virtio queue for sending TPM commands out of the virtual machine and
+	 * receiving TPM responses back from the hypervisor.
+	 */
+	struct virtqueue *vq;
+
+	/*
+	 * Completion that is notified when a virtio operation has been
+	 * fulfilled by the hypervisor.
+	 */
+	struct completion complete;
+
+	/*
+	 * Whether driver currently holds ownership of the virtqueue buffer.
+	 * When false, the hypervisor is in the process of reading or writing
+	 * the buffer and the driver must not touch it.
+	 */
+	bool driver_has_buffer;
+
+	/*
+	 * Whether during the most recent TPM operation, a virtqueue_kick failed
+	 * or a wait timed out.
+	 *
+	 * The next send or recv operation will attempt a kick upon seeing this
+	 * status. That should clear up the queue in the case that the
+	 * hypervisor became temporarily nonresponsive, such as by resource
+	 * exhaustion on the host. The extra kick enables recovery from kicks
+	 * going unnoticed by the hypervisor as well as recovery from virtio
+	 * callbacks going unnoticed by the guest kernel.
+	 */
+	bool needs_kick;
+
+	/* Number of bytes available to read from the virtqueue buffer. */
+	unsigned int readable;
+
+	/*
+	 * Buffer in which all virtio transfers take place. Buffer size is the
+	 * maximum legal TPM command or response message size.
+	 */
+	u8 virtqueue_buffer[TPM_BUFSIZE];
+};
+
+/*
+ * Wait for ownership of the virtqueue buffer.
+ *
+ * The why-string should begin with "waiting to..." or "waiting for..." with no
+ * trailing newline. It will appear in log output.
+ *
+ * Returns zero for success, otherwise negative error.
+ */
+static int vtpm_wait_for_buffer(struct vtpm_device *dev, const char *why)
+{
+	int ret;
+	struct tpm_chip *chip = dev->chip;
+	unsigned long deadline = jiffies + TPM_VIRTIO_TIMEOUT;
+
+	/* Kick queue if needed. */
+	if (dev->needs_kick) {
+		bool did_kick = virtqueue_kick(dev->vq);
+		if (!did_kick) {
+			dev_notice(&chip->dev, "kick failed; will retry\n");
+			return -EBUSY;
+		}
+		dev->needs_kick = false;
+	}
+
+	while (!dev->driver_has_buffer) {
+		unsigned long now = jiffies;
+
+		/* Check timeout, otherwise `deadline - now` may underflow. */
+		if time_after_eq(now, deadline) {
+			dev_warn(&chip->dev, "timed out %s\n", why);
+			dev->needs_kick = true;
+			return -ETIMEDOUT;
+		}
+
+		/*
+		 * Wait to be signaled by virtio callback.
+		 *
+		 * Positive ret is jiffies remaining until timeout when the
+		 * completion occurred, which means successful completion. Zero
+		 * ret is timeout. Negative ret is error.
+		 */
+		ret = wait_for_completion_killable_timeout(
+				&dev->complete, deadline - now);
+
+		/* Log if completion did not occur. */
+		if (ret == -ERESTARTSYS) {
+			/* Not a warning if it was simply interrupted. */
+			dev_dbg(&chip->dev, "interrupted %s\n", why);
+		} else if (ret == 0) {
+			dev_warn(&chip->dev, "timed out %s\n", why);
+			ret = -ETIMEDOUT;
+		} else if (ret < 0) {
+			dev_warn(&chip->dev, "failed while %s: error %d\n",
+					why, -ret);
+		}
+
+		/*
+		 * Return error if completion did not occur. Schedule kick to be
+		 * retried at the start of the next send/recv to help unblock
+		 * the queue.
+		 */
+		if (ret < 0) {
+			dev->needs_kick = true;
+			return ret;
+		}
+
+		/* Completion occurred. Expect response buffer back. */
+		if (virtqueue_get_buf(dev->vq, &dev->readable)) {
+			dev->driver_has_buffer = true;
+
+			if (dev->readable > TPM_BUFSIZE) {
+				dev_crit(&chip->dev,
+						"hypervisor bug: response exceeds max size,"
+						" %u > %u\n",
+						dev->readable,
+						(unsigned int) TPM_BUFSIZE);
+				dev->readable = TPM_BUFSIZE;
+				return -EPROTO;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int vtpm_op_send(struct tpm_chip *chip, u8 *caller_buf, size_t len)
+{
+	int ret;
+	bool did_kick;
+	struct scatterlist sg_outbuf, sg_inbuf;
+	struct scatterlist *sgs[2] = { &sg_outbuf, &sg_inbuf };
+	struct vtpm_device *dev = dev_get_drvdata(&chip->dev);
+	u8 *virtqueue_buf = dev->virtqueue_buffer;
+
+	dev_dbg(&chip->dev, "vtpm_op_send %zu bytes\n", len);
+
+	if (len > TPM_BUFSIZE) {
+		dev_err(&chip->dev,
+				"command is too long, %zu > %zu\n",
+				len, (size_t) TPM_BUFSIZE);
+		return -EINVAL;
+	}
+
+	/*
+	 * Wait until hypervisor relinquishes ownership of the virtqueue buffer.
+	 *
+	 * This may block if the previous recv operation timed out in the guest
+	 * kernel but is still being processed by the hypervisor. Also may block
+	 * if send operations are performed back-to-back, such as if something
+	 * in the caller failed in between a send and recv.
+	 *
+	 * During normal operation absent of any errors or timeouts, this does
+	 * not block.
+	 */
+	ret = vtpm_wait_for_buffer(dev, "waiting to begin send");
+	if (ret) {
+		return ret;
+	}
+
+	/* Driver owns virtqueue buffer and may now write into it. */
+	memcpy(virtqueue_buf, caller_buf, len);
+
+	/*
+	 * Enqueue the virtqueue buffer once as outgoing virtio data (written by
+	 * the virtual machine and read by the hypervisor) and again as incoming
+	 * data (written by the hypervisor and read by the virtual machine).
+	 * This step moves ownership of the virtqueue buffer from driver to
+	 * hypervisor.
+	 *
+	 * Note that we don't know here how big of a buffer the caller will use
+	 * with their later call to recv. We allow the hypervisor to write up to
+	 * the TPM max message size. If the caller ends up using a smaller
+	 * buffer with recv that is too small to hold the entire response, the
+	 * recv will return an error. This conceivably breaks TPM
+	 * implementations that want to produce a different verbosity of
+	 * response depending on the receiver's buffer size.
+	 */
+	sg_init_one(&sg_outbuf, virtqueue_buf, len);
+	sg_init_one(&sg_inbuf, virtqueue_buf, TPM_BUFSIZE);
+	ret = virtqueue_add_sgs(dev->vq, sgs, 1, 1, virtqueue_buf, GFP_KERNEL);
+	if (ret) {
+		dev_err(&chip->dev, "failed virtqueue_add_sgs\n");
+		return ret;
+	}
+
+	/* Kick the other end of the virtqueue after having added a buffer. */
+	did_kick = virtqueue_kick(dev->vq);
+	if (!did_kick) {
+		dev->needs_kick = true;
+		dev_notice(&chip->dev, "kick failed; will retry\n");
+
+		/*
+		 * We return 0 anyway because what the caller doesn't know can't
+		 * hurt them. They can call recv and it will retry the kick. If
+		 * that works, everything is fine.
+		 *
+		 * If the retry in recv fails too, they will get -EBUSY from
+		 * recv.
+		 */
+	}
+
+	/*
+	 * Hypervisor is now processing the TPM command asynchronously. It will
+	 * read the command from the output buffer and write the response into
+	 * the input buffer (which are the same buffer). When done, it will send
+	 * back the buffers over virtio and the driver's virtio callback will
+	 * complete dev->complete so that we know the response is ready to be
+	 * read.
+	 *
+	 * It is important to have copied data out of the caller's buffer into
+	 * the driver's virtqueue buffer because the caller is free to destroy
+	 * their buffer when this call returns. We can't avoid copying by
+	 * waiting here for the hypervisor to finish reading, because if that
+	 * wait times out, we return and the caller may destroy their buffer
+	 * while the hypervisor is continuing to read from it.
+	 */
+	dev->driver_has_buffer = false;
+	return 0;
+}
+
+static int vtpm_op_recv(struct tpm_chip *chip, u8 *caller_buf, size_t len)
+{
+	int ret;
+	struct vtpm_device *dev = dev_get_drvdata(&chip->dev);
+	u8 *virtqueue_buf = dev->virtqueue_buffer;
+
+	dev_dbg(&chip->dev, "vtpm_op_recv\n");
+
+	/*
+	 * Wait until the virtqueue buffer is owned by the driver.
+	 *
+	 * This will usually block while the hypervisor finishes processing the
+	 * most recent TPM command.
+	 */
+	ret = vtpm_wait_for_buffer(dev, "waiting for TPM response");
+	if (ret) {
+		return ret;
+	}
+
+	dev_dbg(&chip->dev, "received %u bytes\n", dev->readable);
+
+	if (dev->readable > len) {
+		dev_notice(&chip->dev,
+				"TPM response is bigger than receiver's buffer:"
+				" %u > %zu\n",
+				dev->readable, len);
+		return -EINVAL;
+	}
+
+	/* Copy response over to the caller. */
+	memcpy(caller_buf, virtqueue_buf, dev->readable);
+
+	return dev->readable;
+}
+
+static void vtpm_op_cancel(struct tpm_chip *chip)
+{
+	/*
+	 * Cancel is not called in this driver's use of tpm-interface.c. It may
+	 * be triggered through tpm-sysfs but that can be implemented as needed.
+	 * Be aware that tpm-sysfs performs cancellation without holding the
+	 * tpm_mutex that protects our send and recv operations, so a future
+	 * implementation will need to consider thread safety of concurrent
+	 * send/recv and cancel.
+	 */
+	dev_notice(&chip->dev, "cancellation is not implemented\n");
+}
+
+static u8 vtpm_op_status(struct tpm_chip *chip)
+{
+	/*
+	 * Status is for TPM drivers that want tpm-interface.c to poll for
+	 * completion before calling recv. Usually this is when the hardware
+	 * needs to be polled i.e. there is no other way for recv to block on
+	 * the TPM command completion.
+	 *
+	 * Polling goes until `(status & complete_mask) == complete_val`. This
+	 * driver defines both complete_mask and complete_val as 0 and blocks on
+	 * our own completion object in recv instead.
+	 */
+	return 0;
+}
+
+static const struct tpm_class_ops vtpm_ops = {
+	.flags = TPM_OPS_AUTO_STARTUP,
+	.send = vtpm_op_send,
+	.recv = vtpm_op_recv,
+	.cancel = vtpm_op_cancel,
+	.status = vtpm_op_status,
+	.req_complete_mask = 0,
+	.req_complete_val = 0,
+};
+
+static void vtpm_virtio_complete(struct virtqueue *vq)
+{
+	struct virtio_device *vdev = vq->vdev;
+	struct vtpm_device *dev = vdev->priv;
+
+	complete(&dev->complete);
+}
+
+static int vtpm_probe(struct virtio_device *vdev)
+{
+	int err;
+	struct vtpm_device *dev;
+	struct virtqueue *vq;
+	struct tpm_chip *chip;
+
+	dev_dbg(&vdev->dev, "vtpm_probe\n");
+
+	dev = kzalloc(sizeof(struct vtpm_device), GFP_KERNEL);
+	if (!dev) {
+		err = -ENOMEM;
+		dev_err(&vdev->dev, "failed kzalloc\n");
+		goto err_dev_alloc;
+	}
+	vdev->priv = dev;
+
+	vq = virtio_find_single_vq(vdev, vtpm_virtio_complete, "vtpm");
+	if (IS_ERR(vq)) {
+		err = PTR_ERR(vq);
+		dev_err(&vdev->dev, "failed virtio_find_single_vq\n");
+		goto err_virtio_find;
+	}
+	dev->vq = vq;
+
+	chip = tpm_chip_alloc(&vdev->dev, &vtpm_ops);
+	if (IS_ERR(chip)) {
+		err = PTR_ERR(chip);
+		dev_err(&vdev->dev, "failed tpm_chip_alloc\n");
+		goto err_chip_alloc;
+	}
+	dev_set_drvdata(&chip->dev, dev);
+	chip->flags |= TPM_CHIP_FLAG_TPM2;
+	dev->chip = chip;
+
+	init_completion(&dev->complete);
+	dev->driver_has_buffer = true;
+	dev->needs_kick = false;
+	dev->readable = 0;
+
+	/*
+	 * Required in order to enable vq use in probe function for auto
+	 * startup.
+	 */
+	virtio_device_ready(vdev);
+
+	err = tpm_chip_register(dev->chip);
+	if (err) {
+		dev_err(&vdev->dev, "failed tpm_chip_register\n");
+		goto err_chip_register;
+	}
+
+	return 0;
+
+err_chip_register:
+	put_device(&dev->chip->dev);
+err_chip_alloc:
+	vdev->config->del_vqs(vdev);
+err_virtio_find:
+	kfree(dev);
+err_dev_alloc:
+	return err;
+}
+
+static void vtpm_remove(struct virtio_device *vdev)
+{
+	struct vtpm_device *dev = vdev->priv;
+
+	/* Undo tpm_chip_register. */
+	tpm_chip_unregister(dev->chip);
+
+	/* Undo tpm_chip_alloc. */
+	put_device(&dev->chip->dev);
+
+	vdev->config->reset(vdev);
+	vdev->config->del_vqs(vdev);
+
+	kfree(dev);
+}
+
+#define VIRTIO_ID_TPM 31
+
+static struct virtio_device_id id_table[] = {
+	{
+		.device = VIRTIO_ID_TPM,
+		.vendor = VIRTIO_DEV_ANY_ID,
+	},
+	{},
+};
+
+static struct virtio_driver vtpm_driver = {
+	.driver.name = KBUILD_MODNAME,
+	.driver.owner = THIS_MODULE,
+	.id_table = id_table,
+	.probe = vtpm_probe,
+	.remove = vtpm_remove,
+};
+
+module_virtio_driver(vtpm_driver);
+
+MODULE_AUTHOR("David Tolnay (dtolnay@gmail.com)");
+MODULE_DESCRIPTION("Virtio vTPM Driver");
+MODULE_VERSION("1.0");
+MODULE_LICENSE("GPL");
diff -ruN a/drivers/clk/clk.c b/drivers/clk/clk.c
--- a/drivers/clk/clk.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/clk/clk.c	2021-12-23 08:35:17.000000000 +0100
@@ -2503,11 +2503,14 @@
 EXPORT_SYMBOL_GPL(clk_has_parent);
 
 static int clk_core_set_parent_nolock(struct clk_core *core,
-				      struct clk_core *parent)
+				      struct clk_core *parent,
+				      bool invalidate_parent)
 {
+	struct clk_core *old_parent = core->parent;
 	int ret = 0;
 	int p_index = 0;
 	unsigned long p_rate = 0;
+	int i;
 
 	lockdep_assert_held(&prepare_lock);
 
@@ -2561,6 +2564,14 @@
 		__clk_recalc_accuracies(core);
 	}
 
+	/* invalidate the parent cache */
+	if (!parent && invalidate_parent) {
+		for (i = 0; i < core->num_parents; i++) {
+			if (core->parents[i].core == old_parent)
+				core->parents[i].core = NULL;
+		}
+	}
+
 runtime_put:
 	clk_pm_runtime_put(core);
 
@@ -2569,7 +2580,7 @@
 
 int clk_hw_set_parent(struct clk_hw *hw, struct clk_hw *parent)
 {
-	return clk_core_set_parent_nolock(hw->core, parent->core);
+	return clk_core_set_parent_nolock(hw->core, parent->core, false);
 }
 EXPORT_SYMBOL_GPL(clk_hw_set_parent);
 
@@ -2603,7 +2614,8 @@
 		clk_core_rate_unprotect(clk->core);
 
 	ret = clk_core_set_parent_nolock(clk->core,
-					 parent ? parent->core : NULL);
+					 parent ? parent->core : NULL,
+					 false);
 
 	if (clk->exclusive_count)
 		clk_core_rate_protect(clk->core);
@@ -4076,7 +4088,7 @@
 		/* Reparent all children to the orphan list. */
 		hlist_for_each_entry_safe(child, t, &clk->core->children,
 					  child_node)
-			clk_core_set_parent_nolock(child, NULL);
+			clk_core_set_parent_nolock(child, NULL, true);
 	}
 
 	clk_core_evict_parent_cache(clk->core);
diff -ruN a/drivers/cpufreq/cpu-boost.c b/drivers/cpufreq/cpu-boost.c
--- a/drivers/cpufreq/cpu-boost.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/cpufreq/cpu-boost.c	2021-12-23 08:35:17.000000000 +0100
@@ -0,0 +1,306 @@
+/*
+ * Copyright (C) 2014 Google, Inc.
+ *
+ * Loosely based on cpu-boost.c from Android tree
+ * Copyright (c) 2013-2014, The Linux Foundation. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/cpufreq.h>
+#include <linux/cpu.h>
+#include <linux/init.h>
+#include <linux/input.h>
+#include <linux/jiffies.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/notifier.h>
+#include <linux/pm_qos.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+
+static unsigned int cpuboost_input_boost_freq_percent;
+module_param_named(input_boost_freq_percent,
+		   cpuboost_input_boost_freq_percent, uint, 0644);
+MODULE_PARM_DESC(input_boost_freq_percent,
+		 "Percentage of max frequency of CPU to be used as boost frequency");
+
+static unsigned int cpuboost_input_boost_ms = 40;
+module_param_named(input_boost_ms,
+		   cpuboost_input_boost_ms, uint, 0644);
+MODULE_PARM_DESC(input_boost_ms, "Duration of input boost (msec)");
+
+static unsigned int cpuboost_input_boost_interval_ms = 150;
+module_param_named(input_boost_interval_ms,
+		   cpuboost_input_boost_interval_ms, uint, 0644);
+MODULE_PARM_DESC(input_boost_interval_ms,
+		 "Interval between input events to reactivate input boost (msec)");
+
+DEFINE_MUTEX(cpuboost_mutex);
+
+static bool cpuboost_boost_active;
+
+static LIST_HEAD(cpuboost_policy_list);
+
+struct cpuboost_policy_node {
+	struct list_head policy_list;
+	struct freq_qos_request qos_req;
+	struct cpufreq_policy *policy;
+};
+
+static int cpuboost_policy_notifier(struct notifier_block *nb,
+				    unsigned long val, void *data)
+{
+	struct cpufreq_policy *policy = data;
+	struct cpuboost_policy_node *node;
+	int ret;
+	bool found;
+
+	switch (val) {
+	case CPUFREQ_CREATE_POLICY:
+		node = kzalloc(sizeof(*node), GFP_KERNEL);
+		if (!node)
+			break;
+
+		node->policy = policy;
+
+		/*
+		 * Always init to no boost and we'll get the boost the next
+		 * time input comes in.
+		 */
+		ret = freq_qos_add_request(&policy->constraints,
+					   &node->qos_req, FREQ_QOS_MIN, 0);
+		if (ret < 0) {
+			pr_warn("Failed to add input boost: %d\n", ret);
+			kfree(node);
+			break;
+		}
+
+		mutex_lock(&cpuboost_mutex);
+		list_add(&node->policy_list, &cpuboost_policy_list);
+		mutex_unlock(&cpuboost_mutex);
+
+		return NOTIFY_OK;
+
+	case CPUFREQ_REMOVE_POLICY:
+		mutex_lock(&cpuboost_mutex);
+		found = false;
+		list_for_each_entry(node, &cpuboost_policy_list, policy_list) {
+			if (node->policy == policy) {
+				found = true;
+				break;
+			}
+		}
+
+		if (!found) {
+			pr_warn("Couldn't find input boost for policy\n");
+			mutex_unlock(&cpuboost_mutex);
+			break;
+		}
+		list_del(&node->policy_list);
+		mutex_unlock(&cpuboost_mutex);
+
+		ret = freq_qos_remove_request(&node->qos_req);
+		kfree(node);
+		if (ret < 0) {
+			pr_warn("Failed to remove input boost: %d\n", ret);
+			break;
+		}
+
+		return NOTIFY_OK;
+	}
+
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block cpuboost_policy_nb = {
+	.notifier_call = cpuboost_policy_notifier,
+};
+
+static void cpuboost_toggle_boost(bool boost_active)
+{
+	struct cpuboost_policy_node *node;
+	int ret;
+	s32 freq = 0;
+
+	mutex_lock(&cpuboost_mutex);
+	cpuboost_boost_active = boost_active;
+	list_for_each_entry(node, &cpuboost_policy_list, policy_list) {
+		if (boost_active)
+			freq = node->policy->cpuinfo.max_freq / 100 *
+			       cpuboost_input_boost_freq_percent;
+		ret = freq_qos_update_request(&node->qos_req, freq);
+		if (ret < 0)
+			pr_warn("Error updating cpuboost request: %d\n", ret);
+	}
+	mutex_unlock(&cpuboost_mutex);
+}
+
+static void cpuboost_cancel_input_boost(struct work_struct *work)
+{
+	cpuboost_toggle_boost(false);
+}
+static DECLARE_DELAYED_WORK(cpuboost_cancel_boost_work,
+			    cpuboost_cancel_input_boost);
+
+static void cpuboost_do_input_boost(struct work_struct *work)
+{
+	mod_delayed_work(system_wq, &cpuboost_cancel_boost_work,
+			 msecs_to_jiffies(cpuboost_input_boost_ms));
+
+	cpuboost_toggle_boost(true);
+}
+static DECLARE_WORK(cpuboost_input_boost_work, cpuboost_do_input_boost);
+
+static void cpuboost_input_event(struct input_handle *handle,
+				 unsigned int type, unsigned int code,
+				 int value)
+{
+	static unsigned long last_event_time;
+	unsigned long now = jiffies;
+	unsigned int threshold;
+
+	if (!cpuboost_input_boost_freq_percent)
+		return;
+
+	threshold = msecs_to_jiffies(cpuboost_input_boost_interval_ms);
+	if (time_after(now, last_event_time + threshold))
+		queue_work(system_highpri_wq, &cpuboost_input_boost_work);
+
+	last_event_time = now;
+}
+
+static int cpuboost_input_connect(struct input_handler *handler,
+				  struct input_dev *dev,
+				  const struct input_device_id *id)
+{
+	struct input_handle *handle;
+	int error;
+
+	handle = kzalloc(sizeof(struct input_handle), GFP_KERNEL);
+	if (!handle)
+		return -ENOMEM;
+
+	handle->dev = dev;
+	handle->handler = handler;
+	handle->name = "cpu-boost";
+
+	error = input_register_handle(handle);
+	if (error)
+		goto err2;
+
+	error = input_open_device(handle);
+	if (error)
+		goto err1;
+
+	return 0;
+
+err1:
+	input_unregister_handle(handle);
+err2:
+	kfree(handle);
+	return error;
+}
+
+static void cpuboost_input_disconnect(struct input_handle *handle)
+{
+	input_close_device(handle);
+	input_unregister_handle(handle);
+	kfree(handle);
+}
+
+static const struct input_device_id cpuboost_ids[] = {
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT |
+			 INPUT_DEVICE_ID_MATCH_ABSBIT,
+		.evbit = { BIT_MASK(EV_ABS) },
+		.absbit = { [BIT_WORD(ABS_MT_POSITION_X)] =
+			    BIT_MASK(ABS_MT_POSITION_X) |
+			    BIT_MASK(ABS_MT_POSITION_Y) },
+	}, /* multi-touch touchscreen */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT,
+		.evbit = { BIT_MASK(EV_ABS) },
+		.absbit = { [BIT_WORD(ABS_X)] = BIT_MASK(ABS_X) }
+
+	}, /* stylus or joystick device */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT,
+		.evbit = { BIT_MASK(EV_KEY) },
+		.keybit = { [BIT_WORD(BTN_LEFT)] = BIT_MASK(BTN_LEFT) },
+	}, /* pointer (e.g. trackpad, mouse) */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT,
+		.evbit = { BIT_MASK(EV_KEY) },
+		.keybit = { [BIT_WORD(KEY_ESC)] = BIT_MASK(KEY_ESC) },
+	}, /* keyboard */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT |
+				INPUT_DEVICE_ID_MATCH_KEYBIT,
+		.evbit = { BIT_MASK(EV_KEY) },
+		.keybit = {[BIT_WORD(BTN_JOYSTICK)] = BIT_MASK(BTN_JOYSTICK) },
+	}, /* joysticks not caught by ABS_X above */
+	{
+		.flags = INPUT_DEVICE_ID_MATCH_EVBIT |
+				INPUT_DEVICE_ID_MATCH_KEYBIT,
+		.evbit = { BIT_MASK(EV_KEY) },
+		.keybit = { [BIT_WORD(BTN_GAMEPAD)] = BIT_MASK(BTN_GAMEPAD) },
+	}, /* gamepad */
+	{ },
+};
+
+static struct input_handler cpuboost_input_handler = {
+	.event          = cpuboost_input_event,
+	.connect        = cpuboost_input_connect,
+	.disconnect     = cpuboost_input_disconnect,
+	.name           = "cpu-boost",
+	.id_table       = cpuboost_ids,
+};
+
+static int __init cpuboost_init(void)
+{
+	int error;
+
+	error = cpufreq_register_notifier(&cpuboost_policy_nb,
+					  CPUFREQ_POLICY_NOTIFIER);
+	if (error) {
+		pr_err("failed to register input handler: %d\n", error);
+		return error;
+	}
+
+	error = input_register_handler(&cpuboost_input_handler);
+	if (error) {
+		pr_err("failed to register input handler: %d\n", error);
+		cpufreq_unregister_notifier(&cpuboost_policy_nb,
+					    CPUFREQ_POLICY_NOTIFIER);
+		return error;
+	}
+
+	return 0;
+}
+module_init(cpuboost_init);
+
+static void __exit cpuboost_exit(void)
+{
+	input_unregister_handler(&cpuboost_input_handler);
+
+	flush_work(&cpuboost_input_boost_work);
+	cancel_delayed_work_sync(&cpuboost_cancel_boost_work);
+
+	cpufreq_unregister_notifier(&cpuboost_policy_nb,
+				    CPUFREQ_POLICY_NOTIFIER);
+}
+module_exit(cpuboost_exit);
+
+MODULE_DESCRIPTION("Input event based short term CPU frequency booster");
+MODULE_LICENSE("GPL v2");
diff -ruN a/drivers/cpufreq/cpufreq.c b/drivers/cpufreq/cpufreq.c
--- a/drivers/cpufreq/cpufreq.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/cpufreq/cpufreq.c	2021-12-23 08:35:17.000000000 +0100
@@ -16,6 +16,7 @@
 
 #include <linux/cpu.h>
 #include <linux/cpufreq.h>
+#include <linux/cpufreq_times.h>
 #include <linux/cpu_cooling.h>
 #include <linux/delay.h>
 #include <linux/device.h>
@@ -386,6 +387,7 @@
 					 CPUFREQ_POSTCHANGE, freqs);
 
 		cpufreq_stats_record_transition(policy, freqs->new);
+		cpufreq_times_record_transition(policy, freqs->new);
 		policy->cur = freqs->new;
 	}
 }
@@ -1486,6 +1488,7 @@
 			goto out_destroy_policy;
 
 		cpufreq_stats_create_table(policy);
+		cpufreq_times_create_policy(policy);
 
 		write_lock_irqsave(&cpufreq_driver_lock, flags);
 		list_add(&policy->policy_list, &cpufreq_policy_list);
@@ -2567,7 +2570,6 @@
 		ret = cpufreq_start_governor(policy);
 		if (!ret) {
 			pr_debug("governor change\n");
-			sched_cpufreq_governor_change(policy, old_gov);
 			return 0;
 		}
 		cpufreq_exit_governor(policy);
diff -ruN a/drivers/cpufreq/cpufreq_times.c b/drivers/cpufreq/cpufreq_times.c
--- a/drivers/cpufreq/cpufreq_times.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/cpufreq/cpufreq_times.c	2021-12-23 08:35:17.000000000 +0100
@@ -0,0 +1,211 @@
+/* drivers/cpufreq/cpufreq_times.c
+ *
+ * Copyright (C) 2018 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/cpufreq.h>
+#include <linux/cpufreq_times.h>
+#include <linux/jiffies.h>
+#include <linux/sched.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/threads.h>
+
+static DEFINE_SPINLOCK(task_time_in_state_lock); /* task->time_in_state */
+
+/**
+ * struct cpu_freqs - per-cpu frequency information
+ * @offset: start of these freqs' stats in task time_in_state array
+ * @max_state: number of entries in freq_table
+ * @last_index: index in freq_table of last frequency switched to
+ * @freq_table: list of available frequencies
+ */
+struct cpu_freqs {
+	unsigned int offset;
+	unsigned int max_state;
+	unsigned int last_index;
+	unsigned int freq_table[0];
+};
+
+static struct cpu_freqs *all_freqs[NR_CPUS];
+
+static unsigned int next_offset;
+
+void cpufreq_task_times_init(struct task_struct *p)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&task_time_in_state_lock, flags);
+	p->time_in_state = NULL;
+	spin_unlock_irqrestore(&task_time_in_state_lock, flags);
+	p->max_state = 0;
+}
+
+void cpufreq_task_times_alloc(struct task_struct *p)
+{
+	void *temp;
+	unsigned long flags;
+	unsigned int max_state = READ_ONCE(next_offset);
+
+	/* We use one array to avoid multiple allocs per task */
+	temp = kcalloc(max_state, sizeof(p->time_in_state[0]), GFP_ATOMIC);
+	if (!temp)
+		return;
+
+	spin_lock_irqsave(&task_time_in_state_lock, flags);
+	p->time_in_state = temp;
+	spin_unlock_irqrestore(&task_time_in_state_lock, flags);
+	p->max_state = max_state;
+}
+
+/* Caller must hold task_time_in_state_lock */
+static int cpufreq_task_times_realloc_locked(struct task_struct *p)
+{
+	void *temp;
+	unsigned int max_state = READ_ONCE(next_offset);
+
+	temp = krealloc(p->time_in_state, max_state * sizeof(u64), GFP_ATOMIC);
+	if (!temp)
+		return -ENOMEM;
+	p->time_in_state = temp;
+	memset(p->time_in_state + p->max_state, 0,
+	       (max_state - p->max_state) * sizeof(u64));
+	p->max_state = max_state;
+	return 0;
+}
+
+void cpufreq_task_times_exit(struct task_struct *p)
+{
+	unsigned long flags;
+	void *temp;
+
+	if (!p->time_in_state)
+		return;
+
+	spin_lock_irqsave(&task_time_in_state_lock, flags);
+	temp = p->time_in_state;
+	p->time_in_state = NULL;
+	spin_unlock_irqrestore(&task_time_in_state_lock, flags);
+	kfree(temp);
+}
+
+int proc_time_in_state_show(struct seq_file *m, struct pid_namespace *ns,
+	struct pid *pid, struct task_struct *p)
+{
+	unsigned int cpu, i;
+	u64 cputime;
+	unsigned long flags;
+	struct cpu_freqs *freqs;
+	struct cpu_freqs *last_freqs = NULL;
+
+	spin_lock_irqsave(&task_time_in_state_lock, flags);
+	for_each_possible_cpu(cpu) {
+		freqs = all_freqs[cpu];
+		if (!freqs || freqs == last_freqs)
+			continue;
+		last_freqs = freqs;
+
+		seq_printf(m, "cpu%u\n", cpu);
+		for (i = 0; i < freqs->max_state; i++) {
+			cputime = 0;
+			if (freqs->offset + i < p->max_state &&
+			    p->time_in_state)
+				cputime = p->time_in_state[freqs->offset + i];
+			seq_printf(m, "%u %lu\n", freqs->freq_table[i],
+				   (unsigned long)nsec_to_clock_t(cputime));
+		}
+	}
+	spin_unlock_irqrestore(&task_time_in_state_lock, flags);
+	return 0;
+}
+
+void cpufreq_acct_update_power(struct task_struct *p, u64 cputime)
+{
+	unsigned long flags;
+	unsigned int state;
+	struct cpu_freqs *freqs = all_freqs[task_cpu(p)];
+
+	if (!freqs || is_idle_task(p) || p->flags & PF_EXITING)
+		return;
+
+	state = freqs->offset + READ_ONCE(freqs->last_index);
+
+	spin_lock_irqsave(&task_time_in_state_lock, flags);
+	if ((state < p->max_state || !cpufreq_task_times_realloc_locked(p)) &&
+	    p->time_in_state)
+		p->time_in_state[state] += cputime;
+	spin_unlock_irqrestore(&task_time_in_state_lock, flags);
+}
+
+static int cpufreq_times_get_index(struct cpu_freqs *freqs, unsigned int freq)
+{
+	int index;
+        for (index = 0; index < freqs->max_state; ++index) {
+		if (freqs->freq_table[index] == freq)
+			return index;
+        }
+	return -1;
+}
+
+void cpufreq_times_create_policy(struct cpufreq_policy *policy)
+{
+	int cpu, index = 0;
+	unsigned int count = 0;
+	struct cpufreq_frequency_table *pos, *table;
+	struct cpu_freqs *freqs;
+	void *tmp;
+
+	if (all_freqs[policy->cpu])
+		return;
+
+	table = policy->freq_table;
+	if (!table)
+		return;
+
+	cpufreq_for_each_valid_entry(pos, table)
+		count++;
+
+	tmp =  kzalloc(sizeof(*freqs) + sizeof(freqs->freq_table[0]) * count,
+		       GFP_KERNEL);
+	if (!tmp)
+		return;
+
+	freqs = tmp;
+	freqs->max_state = count;
+
+	cpufreq_for_each_valid_entry(pos, table)
+		freqs->freq_table[index++] = pos->frequency;
+
+	index = cpufreq_times_get_index(freqs, policy->cur);
+	if (index >= 0)
+		WRITE_ONCE(freqs->last_index, index);
+
+	freqs->offset = next_offset;
+	WRITE_ONCE(next_offset, freqs->offset + count);
+	for_each_cpu(cpu, policy->related_cpus)
+		all_freqs[cpu] = freqs;
+}
+
+void cpufreq_times_record_transition(struct cpufreq_policy *policy,
+	unsigned int new_freq)
+{
+	int index;
+	struct cpu_freqs *freqs = all_freqs[policy->cpu];
+	if (!freqs)
+		return;
+
+	index = cpufreq_times_get_index(freqs, new_freq);
+	if (index >= 0)
+		WRITE_ONCE(freqs->last_index, index);
+}
diff -ruN a/drivers/cpufreq/dummy-cpufreq.c b/drivers/cpufreq/dummy-cpufreq.c
--- a/drivers/cpufreq/dummy-cpufreq.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/cpufreq/dummy-cpufreq.c	2021-12-23 08:35:17.000000000 +0100
@@ -0,0 +1,60 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2019 Google, Inc.
+ */
+#include <linux/cpufreq.h>
+#include <linux/module.h>
+
+static struct cpufreq_frequency_table freq_table[] = {
+	{ .frequency = 1 },
+	{ .frequency = 2 },
+	{ .frequency = CPUFREQ_TABLE_END },
+};
+
+static int dummy_cpufreq_target_index(struct cpufreq_policy *policy,
+				   unsigned int index)
+{
+	return 0;
+}
+
+static int dummy_cpufreq_driver_init(struct cpufreq_policy *policy)
+{
+	policy->freq_table = freq_table;
+	return 0;
+}
+
+static unsigned int dummy_cpufreq_get(unsigned int cpu)
+{
+	return 1;
+}
+
+static int dummy_cpufreq_verify(struct cpufreq_policy_data *data)
+{
+	return 0;
+}
+
+static struct cpufreq_driver dummy_cpufreq_driver = {
+	.name = "dummy",
+	.target_index = dummy_cpufreq_target_index,
+	.init = dummy_cpufreq_driver_init,
+	.get = dummy_cpufreq_get,
+	.verify = dummy_cpufreq_verify,
+	.attr = cpufreq_generic_attr,
+};
+
+static int __init dummy_cpufreq_init(void)
+{
+	return cpufreq_register_driver(&dummy_cpufreq_driver);
+}
+
+static void __exit dummy_cpufreq_exit(void)
+{
+	cpufreq_unregister_driver(&dummy_cpufreq_driver);
+}
+
+module_init(dummy_cpufreq_init);
+module_exit(dummy_cpufreq_exit);
+
+MODULE_AUTHOR("Connor O'Brien <connoro@google.com>");
+MODULE_DESCRIPTION("dummy cpufreq driver");
+MODULE_LICENSE("GPL");
diff -ruN a/drivers/cpufreq/Kconfig b/drivers/cpufreq/Kconfig
--- a/drivers/cpufreq/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/cpufreq/Kconfig	2021-12-23 08:35:17.000000000 +0100
@@ -35,6 +35,13 @@
 
 	  If in doubt, say N.
 
+config CPU_FREQ_TIMES
+       bool "CPU frequency time-in-state statistics"
+       help
+         Export CPU time-in-state information through procfs.
+
+         If in doubt, say N.
+
 choice
 	prompt "Default CPUFreq governor"
 	default CPU_FREQ_DEFAULT_GOV_USERSPACE if ARM_SA1100_CPUFREQ || ARM_SA1110_CPUFREQ
@@ -204,6 +211,17 @@
 
 	  If in doubt, say N.
 
+config CPU_BOOST
+	tristate "Input event based short term CPU freq booster"
+	depends on INPUT
+	help
+	  This driver monitors events from input devices, such as
+	  touchscreen, trackpad, keyboard, etc., and boosts frequency
+	  of all CPUs in the system in response to user interacting with
+	  the device.
+
+	  If in doubt, say N.
+
 comment "CPU frequency scaling drivers"
 
 config CPUFREQ_DT
@@ -227,6 +245,15 @@
 
 	  If in doubt, say N.
 
+config CPUFREQ_DUMMY
+	tristate "Dummy CPU frequency driver"
+	help
+	  This option adds a generic dummy CPUfreq driver, which sets a fake
+	  2-frequency table when initializing each policy and otherwise does
+	  nothing.
+
+	  If in doubt, say N
+
 if X86
 source "drivers/cpufreq/Kconfig.x86"
 endif
diff -ruN a/drivers/cpufreq/Makefile b/drivers/cpufreq/Makefile
--- a/drivers/cpufreq/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/cpufreq/Makefile	2021-12-23 08:35:17.000000000 +0100
@@ -5,7 +5,10 @@
 # CPUfreq stats
 obj-$(CONFIG_CPU_FREQ_STAT)             += cpufreq_stats.o
 
-# CPUfreq governors 
+# CPUfreq times
+obj-$(CONFIG_CPU_FREQ_TIMES)		+= cpufreq_times.o
+
+# CPUfreq governors
 obj-$(CONFIG_CPU_FREQ_GOV_PERFORMANCE)	+= cpufreq_performance.o
 obj-$(CONFIG_CPU_FREQ_GOV_POWERSAVE)	+= cpufreq_powersave.o
 obj-$(CONFIG_CPU_FREQ_GOV_USERSPACE)	+= cpufreq_userspace.o
@@ -13,10 +16,13 @@
 obj-$(CONFIG_CPU_FREQ_GOV_CONSERVATIVE)	+= cpufreq_conservative.o
 obj-$(CONFIG_CPU_FREQ_GOV_COMMON)		+= cpufreq_governor.o
 obj-$(CONFIG_CPU_FREQ_GOV_ATTR_SET)	+= cpufreq_governor_attr_set.o
+obj-$(CONFIG_CPU_BOOST)			+= cpu-boost.o
 
 obj-$(CONFIG_CPUFREQ_DT)		+= cpufreq-dt.o
 obj-$(CONFIG_CPUFREQ_DT_PLATDEV)	+= cpufreq-dt-platdev.o
 
+obj-$(CONFIG_CPUFREQ_DUMMY)		+= dummy-cpufreq.o
+
 ##################################################################################
 # x86 drivers.
 # Link order matters. K8 is preferred to ACPI because of firmware bugs in early
diff -ruN a/drivers/cpufreq/mediatek-cpufreq.c b/drivers/cpufreq/mediatek-cpufreq.c
--- a/drivers/cpufreq/mediatek-cpufreq.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/cpufreq/mediatek-cpufreq.c	2021-12-23 08:35:17.000000000 +0100
@@ -42,6 +42,11 @@
 	struct list_head list_head;
 	int intermediate_voltage;
 	bool need_voltage_tracking;
+	int old_vproc;
+	struct mutex lock; /* avoid notify and policy race condition */
+	struct notifier_block opp_nb;
+	int opp_cpu;
+	unsigned long opp_freq;
 };
 
 static LIST_HEAD(dvfs_info_list);
@@ -192,11 +197,16 @@
 
 static int mtk_cpufreq_set_voltage(struct mtk_cpu_dvfs_info *info, int vproc)
 {
+	int ret;
+
 	if (info->need_voltage_tracking)
-		return mtk_cpufreq_voltage_tracking(info, vproc);
+		ret = mtk_cpufreq_voltage_tracking(info, vproc);
 	else
-		return regulator_set_voltage(info->proc_reg, vproc,
-					     vproc + VOLT_TOL);
+		ret = regulator_set_voltage(info->proc_reg, vproc,
+					    MAX_VOLT_LIMIT);
+	if (!ret)
+		info->old_vproc = vproc;
+	return ret;
 }
 
 static int mtk_cpufreq_set_target(struct cpufreq_policy *policy,
@@ -214,7 +224,9 @@
 	inter_vproc = info->intermediate_voltage;
 
 	old_freq_hz = clk_get_rate(cpu_clk);
-	old_vproc = regulator_get_voltage(info->proc_reg);
+	old_vproc = info->old_vproc;
+	if (old_vproc == 0)
+		old_vproc = regulator_get_voltage(info->proc_reg);
 	if (old_vproc < 0) {
 		pr_err("%s: invalid Vproc value: %d\n", __func__, old_vproc);
 		return old_vproc;
@@ -231,6 +243,7 @@
 	vproc = dev_pm_opp_get_voltage(opp);
 	dev_pm_opp_put(opp);
 
+	mutex_lock(&info->lock);
 	/*
 	 * If the new voltage or the intermediate voltage is higher than the
 	 * current voltage, scale up voltage first.
@@ -242,6 +255,7 @@
 			pr_err("cpu%d: failed to scale up voltage!\n",
 			       policy->cpu);
 			mtk_cpufreq_set_voltage(info, old_vproc);
+			mutex_unlock(&info->lock);
 			return ret;
 		}
 	}
@@ -253,6 +267,7 @@
 		       policy->cpu);
 		mtk_cpufreq_set_voltage(info, old_vproc);
 		WARN_ON(1);
+		mutex_unlock(&info->lock);
 		return ret;
 	}
 
@@ -263,6 +278,7 @@
 		       policy->cpu);
 		clk_set_parent(cpu_clk, armpll);
 		mtk_cpufreq_set_voltage(info, old_vproc);
+		mutex_unlock(&info->lock);
 		return ret;
 	}
 
@@ -273,6 +289,7 @@
 		       policy->cpu);
 		mtk_cpufreq_set_voltage(info, inter_vproc);
 		WARN_ON(1);
+		mutex_unlock(&info->lock);
 		return ret;
 	}
 
@@ -288,15 +305,69 @@
 			clk_set_parent(cpu_clk, info->inter_clk);
 			clk_set_rate(armpll, old_freq_hz);
 			clk_set_parent(cpu_clk, armpll);
+			mutex_unlock(&info->lock);
 			return ret;
 		}
 	}
 
+	info->opp_freq = freq_hz;
+	mutex_unlock(&info->lock);
+
 	return 0;
 }
 
 #define DYNAMIC_POWER "dynamic-power-coefficient"
 
+static int mtk_cpufreq_opp_notifier(struct notifier_block *nb,
+				    unsigned long event, void *data)
+{
+	struct dev_pm_opp *opp = data;
+	struct dev_pm_opp *new_opp;
+	struct mtk_cpu_dvfs_info *info;
+	unsigned long freq, volt;
+	struct cpufreq_policy *policy;
+	int ret = 0;
+
+	info = container_of(nb, struct mtk_cpu_dvfs_info, opp_nb);
+
+	if (event == OPP_EVENT_ADJUST_VOLTAGE) {
+		freq = dev_pm_opp_get_freq(opp);
+
+		mutex_lock(&info->lock);
+		if (info->opp_freq == freq) {
+			volt = dev_pm_opp_get_voltage(opp);
+			ret = mtk_cpufreq_set_voltage(info, volt);
+			if (ret)
+				dev_err(info->cpu_dev, "failed to scale voltage: %d\n",
+					ret);
+		}
+		mutex_unlock(&info->lock);
+	} else if (event == OPP_EVENT_DISABLE) {
+		freq = dev_pm_opp_get_freq(opp);
+		/* case of current opp item is disabled */
+		if (info->opp_freq == freq) {
+			freq = 1;
+			new_opp = dev_pm_opp_find_freq_ceil(info->cpu_dev,
+							    &freq);
+			if (!IS_ERR(new_opp)) {
+				dev_pm_opp_put(new_opp);
+				policy = cpufreq_cpu_get(info->opp_cpu);
+				if (policy) {
+					cpufreq_driver_target(policy,
+							      freq / 1000,
+							      CPUFREQ_RELATION_L);
+					cpufreq_cpu_put(policy);
+				}
+			} else {
+				pr_err("%s: all opp items are disabled\n",
+				       __func__);
+			}
+		}
+	}
+
+	return notifier_from_errno(ret);
+}
+
 static int mtk_cpu_dvfs_info_init(struct mtk_cpu_dvfs_info *info, int cpu)
 {
 	struct device *cpu_dev;
@@ -350,6 +421,11 @@
 		ret = PTR_ERR(proc_reg);
 		goto out_free_resources;
 	}
+	ret = regulator_enable(proc_reg);
+	if (ret) {
+		pr_warn("enable vproc for cpu%d fail\n", cpu);
+		goto out_free_resources;
+	}
 
 	/* Both presence and absence of sram regulator are valid cases. */
 	sram_reg = regulator_get_exclusive(cpu_dev, "sram");
@@ -368,22 +444,40 @@
 		goto out_free_resources;
 	}
 
+	ret = clk_prepare_enable(cpu_clk);
+	if (ret)
+		goto out_free_opp_table;
+
+	ret = clk_prepare_enable(inter_clk);
+	if (ret)
+		goto out_disable_mux_clock;
+
 	/* Search a safe voltage for intermediate frequency. */
 	rate = clk_get_rate(inter_clk);
 	opp = dev_pm_opp_find_freq_ceil(cpu_dev, &rate);
 	if (IS_ERR(opp)) {
 		pr_err("failed to get intermediate opp for cpu%d\n", cpu);
 		ret = PTR_ERR(opp);
-		goto out_free_opp_table;
+		goto out_disable_inter_clock;
 	}
 	info->intermediate_voltage = dev_pm_opp_get_voltage(opp);
 	dev_pm_opp_put(opp);
 
+	info->opp_cpu = cpu;
+	info->opp_nb.notifier_call = mtk_cpufreq_opp_notifier;
+	ret = dev_pm_opp_register_notifier(cpu_dev, &info->opp_nb);
+	if (ret) {
+		pr_warn("cannot register opp notification\n");
+		goto out_disable_inter_clock;
+	}
+
+	mutex_init(&info->lock);
 	info->cpu_dev = cpu_dev;
 	info->proc_reg = proc_reg;
 	info->sram_reg = IS_ERR(sram_reg) ? NULL : sram_reg;
 	info->cpu_clk = cpu_clk;
 	info->inter_clk = inter_clk;
+	info->opp_freq = clk_get_rate(cpu_clk);
 
 	/*
 	 * If SRAM regulator is present, software "voltage tracking" is needed
@@ -393,6 +487,12 @@
 
 	return 0;
 
+out_disable_inter_clock:
+	clk_disable_unprepare(inter_clk);
+
+out_disable_mux_clock:
+	clk_disable_unprepare(cpu_clk);
+
 out_free_opp_table:
 	dev_pm_opp_of_cpumask_remove_table(&info->cpus);
 
@@ -411,14 +511,20 @@
 
 static void mtk_cpu_dvfs_info_release(struct mtk_cpu_dvfs_info *info)
 {
-	if (!IS_ERR(info->proc_reg))
+	if (!IS_ERR(info->proc_reg)) {
+		regulator_disable(info->proc_reg);
 		regulator_put(info->proc_reg);
+	}
 	if (!IS_ERR(info->sram_reg))
 		regulator_put(info->sram_reg);
-	if (!IS_ERR(info->cpu_clk))
+	if (!IS_ERR(info->cpu_clk)) {
+		clk_disable_unprepare(info->cpu_clk);
 		clk_put(info->cpu_clk);
-	if (!IS_ERR(info->inter_clk))
+	}
+	if (!IS_ERR(info->inter_clk)) {
+		clk_disable_unprepare(info->inter_clk);
 		clk_put(info->inter_clk);
+	}
 
 	dev_pm_opp_of_cpumask_remove_table(&info->cpus);
 }
diff -ruN a/drivers/devfreq/devfreq.c b/drivers/devfreq/devfreq.c
--- a/drivers/devfreq/devfreq.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/devfreq/devfreq.c	2021-12-23 08:35:18.000000000 +0100
@@ -112,16 +112,16 @@
 }
 
 /**
- * get_freq_range() - Get the current freq range
+ * devfreq_get_freq_range() - Get the current freq range
  * @devfreq:	the devfreq instance
  * @min_freq:	the min frequency
  * @max_freq:	the max frequency
  *
  * This takes into consideration all constraints.
  */
-static void get_freq_range(struct devfreq *devfreq,
-			   unsigned long *min_freq,
-			   unsigned long *max_freq)
+void devfreq_get_freq_range(struct devfreq *devfreq,
+			    unsigned long *min_freq,
+			    unsigned long *max_freq)
 {
 	unsigned long *freq_table = devfreq->profile->freq_table;
 	s32 qos_min_freq, qos_max_freq;
@@ -158,6 +158,7 @@
 	if (*min_freq > *max_freq)
 		*min_freq = *max_freq;
 }
+EXPORT_SYMBOL(devfreq_get_freq_range);
 
 /**
  * devfreq_get_freq_level() - Lookup freq_table for the frequency
@@ -418,7 +419,7 @@
 	err = devfreq->governor->get_target_freq(devfreq, &freq);
 	if (err)
 		return err;
-	get_freq_range(devfreq, &min_freq, &max_freq);
+	devfreq_get_freq_range(devfreq, &min_freq, &max_freq);
 
 	if (freq < min_freq) {
 		freq = min_freq;
@@ -785,6 +786,7 @@
 {
 	struct devfreq *devfreq;
 	struct devfreq_governor *governor;
+	unsigned long min_freq, max_freq;
 	int err = 0;
 
 	if (!dev || !profile || !governor_name) {
@@ -849,6 +851,8 @@
 		goto err_dev;
 	}
 
+	devfreq_get_freq_range(devfreq, &min_freq, &max_freq);
+
 	devfreq->suspend_freq = dev_pm_opp_get_suspend_opp_freq(dev);
 	devfreq->opp_table = dev_pm_opp_get_opp_table(dev);
 	if (IS_ERR(devfreq->opp_table))
@@ -1561,7 +1565,7 @@
 	unsigned long min_freq, max_freq;
 
 	mutex_lock(&df->lock);
-	get_freq_range(df, &min_freq, &max_freq);
+	devfreq_get_freq_range(df, &min_freq, &max_freq);
 	mutex_unlock(&df->lock);
 
 	return sprintf(buf, "%lu\n", min_freq);
@@ -1615,7 +1619,7 @@
 	unsigned long min_freq, max_freq;
 
 	mutex_lock(&df->lock);
-	get_freq_range(df, &min_freq, &max_freq);
+	devfreq_get_freq_range(df, &min_freq, &max_freq);
 	mutex_unlock(&df->lock);
 
 	return sprintf(buf, "%lu\n", max_freq);
@@ -1929,7 +1933,7 @@
 
 		mutex_lock(&devfreq->lock);
 		cur_freq = devfreq->previous_freq;
-		get_freq_range(devfreq, &min_freq, &max_freq);
+		devfreq_get_freq_range(devfreq, &min_freq, &max_freq);
 		timer = devfreq->profile->timer;
 
 		if (IS_SUPPORTED_ATTR(devfreq->governor->attrs, POLLING_INTERVAL))
diff -ruN a/drivers/devfreq/governor.h b/drivers/devfreq/governor.h
--- a/drivers/devfreq/governor.h	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/devfreq/governor.h	2021-12-23 08:35:18.000000000 +0100
@@ -48,6 +48,28 @@
 #define DEVFREQ_GOV_ATTR_TIMER				BIT(1)
 
 /**
+ * struct devfreq_cpu_data - Hold the per-cpu data
+ * @dev:	reference to cpu device.
+ * @first_cpu:	the cpumask of the first cpu of a policy.
+ * @opp_table:	reference to cpu opp table.
+ * @cur_freq:	the current frequency of the cpu.
+ * @min_freq:	the min frequency of the cpu.
+ * @max_freq:	the max frequency of the cpu.
+ *
+ * This structure stores the required cpu_data of a cpu.
+ * This is auto-populated by the governor.
+ */
+struct devfreq_cpu_data {
+	struct device *dev;
+	unsigned int first_cpu;
+
+	struct opp_table *opp_table;
+	unsigned int cur_freq;
+	unsigned int min_freq;
+	unsigned int max_freq;
+};
+
+/**
  * struct devfreq_governor - Devfreq policy governor
  * @node:		list node - contains registered devfreq governors
  * @name:		Governor's name
@@ -86,6 +108,8 @@
 
 int devfreq_update_status(struct devfreq *devfreq, unsigned long freq);
 int devfreq_update_target(struct devfreq *devfreq, unsigned long freq);
+void devfreq_get_freq_range(struct devfreq *devfreq, unsigned long *min_freq,
+			    unsigned long *max_freq);
 
 static inline int devfreq_update_stats(struct devfreq *df)
 {
diff -ruN a/drivers/devfreq/governor_passive.c b/drivers/devfreq/governor_passive.c
--- a/drivers/devfreq/governor_passive.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/devfreq/governor_passive.c	2021-12-23 08:35:18.000000000 +0100
@@ -8,19 +8,129 @@
  */
 
 #include <linux/module.h>
+#include <linux/cpu.h>
+#include <linux/cpufreq.h>
+#include <linux/cpumask.h>
+#include <linux/slab.h>
 #include <linux/device.h>
 #include <linux/devfreq.h>
 #include "governor.h"
 
-static int devfreq_passive_get_target_freq(struct devfreq *devfreq,
+#define HZ_PER_KHZ	1000
+
+static unsigned long get_taget_freq_by_required_opp(struct device *p_dev,
+						struct opp_table *p_opp_table,
+						struct opp_table *opp_table,
+						unsigned long freq)
+{
+	struct dev_pm_opp *opp = NULL, *p_opp = NULL;
+
+	if (!p_dev || !p_opp_table || !opp_table || !freq)
+		return 0;
+
+	p_opp = devfreq_recommended_opp(p_dev, &freq, 0);
+	if (IS_ERR(p_opp))
+		return 0;
+
+	opp = dev_pm_opp_xlate_required_opp(p_opp_table, opp_table, p_opp);
+	dev_pm_opp_put(p_opp);
+
+	if (IS_ERR(opp))
+		return 0;
+
+	freq = dev_pm_opp_get_freq(opp);
+	dev_pm_opp_put(opp);
+
+	return freq;
+}
+
+static int get_target_freq_with_cpufreq(struct devfreq *devfreq,
+					unsigned long *target_freq)
+{
+	struct devfreq_passive_data *p_data =
+				(struct devfreq_passive_data *)devfreq->data;
+	struct devfreq_cpu_data *cpudata;
+	unsigned long cpu, cpu_cur, cpu_min, cpu_max, cpu_percent;
+	unsigned long dev_min, dev_max;
+	unsigned long freq = 0;
+
+	for_each_online_cpu(cpu) {
+		cpudata = p_data->cpudata[cpu];
+		if (!cpudata || cpudata->first_cpu != cpu)
+			continue;
+
+		/* Get target freq via required opps */
+		cpu_cur = cpudata->cur_freq * HZ_PER_KHZ;
+		freq = get_taget_freq_by_required_opp(cpudata->dev,
+					cpudata->opp_table,
+					devfreq->opp_table, cpu_cur);
+		if (freq) {
+			*target_freq = max(freq, *target_freq);
+			continue;
+		}
+
+		/* Use Interpolation if required opps is not available */
+		devfreq_get_freq_range(devfreq, &dev_min, &dev_max);
+
+		cpu_min = cpudata->min_freq;
+		cpu_max = cpudata->max_freq;
+		cpu_cur = cpudata->cur_freq;
+
+		cpu_percent = ((cpu_cur - cpu_min) * 100) / (cpu_max - cpu_min);
+		freq = dev_min + mult_frac(dev_max - dev_min, cpu_percent, 100);
+
+		*target_freq = max(freq, *target_freq);
+	}
+
+	return 0;
+}
+
+static int get_target_freq_with_devfreq(struct devfreq *devfreq,
 					unsigned long *freq)
 {
 	struct devfreq_passive_data *p_data
 			= (struct devfreq_passive_data *)devfreq->data;
 	struct devfreq *parent_devfreq = (struct devfreq *)p_data->parent;
-	unsigned long child_freq = ULONG_MAX;
-	struct dev_pm_opp *opp, *p_opp;
-	int i, count;
+	unsigned long target_freq;
+	int i;
+
+	/* Get target freq via required opps */
+	target_freq = get_taget_freq_by_required_opp(parent_devfreq->dev.parent,
+						parent_devfreq->opp_table,
+						devfreq->opp_table, *freq);
+	if (target_freq)
+		goto out;
+
+	/* Use Interpolation if required opps is not available */
+	for (i = 0; i < parent_devfreq->profile->max_state; i++)
+		if (parent_devfreq->profile->freq_table[i] == *freq)
+			break;
+
+	if (i == parent_devfreq->profile->max_state)
+		return -EINVAL;
+
+	if (i < devfreq->profile->max_state) {
+		target_freq = devfreq->profile->freq_table[i];
+	} else {
+		i = devfreq->profile->max_state;
+		target_freq = devfreq->profile->freq_table[i - 1];
+	}
+
+out:
+	*freq = target_freq;
+
+	return 0;
+}
+
+static int devfreq_passive_get_target_freq(struct devfreq *devfreq,
+					   unsigned long *freq)
+{
+	struct devfreq_passive_data *p_data =
+				(struct devfreq_passive_data *)devfreq->data;
+	int ret;
+
+	if (!p_data)
+		return -EINVAL;
 
 	/*
 	 * If the devfreq device with passive governor has the specific method
@@ -30,71 +140,150 @@
 	if (p_data->get_target_freq)
 		return p_data->get_target_freq(devfreq, freq);
 
-	/*
-	 * If the parent and passive devfreq device uses the OPP table,
-	 * get the next frequency by using the OPP table.
-	 */
-
-	/*
-	 * - parent devfreq device uses the governors except for passive.
-	 * - passive devfreq device uses the passive governor.
-	 *
-	 * Each devfreq has the OPP table. After deciding the new frequency
-	 * from the governor of parent devfreq device, the passive governor
-	 * need to get the index of new frequency on OPP table of parent
-	 * device. And then the index is used for getting the suitable
-	 * new frequency for passive devfreq device.
-	 */
-	if (!devfreq->profile || !devfreq->profile->freq_table
-		|| devfreq->profile->max_state <= 0)
-		return -EINVAL;
+	switch (p_data->parent_type) {
+	case DEVFREQ_PARENT_DEV:
+		ret = get_target_freq_with_devfreq(devfreq, freq);
+		break;
+	case CPUFREQ_PARENT_DEV:
+		ret = get_target_freq_with_cpufreq(devfreq, freq);
+		break;
+	default:
+		ret = -EINVAL;
+		dev_err(&devfreq->dev, "Invalid parent type\n");
+		break;
+	}
 
-	/*
-	 * The passive governor have to get the correct frequency from OPP
-	 * list of parent device. Because in this case, *freq is temporary
-	 * value which is decided by ondemand governor.
-	 */
-	if (devfreq->opp_table && parent_devfreq->opp_table) {
-		p_opp = devfreq_recommended_opp(parent_devfreq->dev.parent,
-						freq, 0);
-		if (IS_ERR(p_opp))
-			return PTR_ERR(p_opp);
-
-		opp = dev_pm_opp_xlate_required_opp(parent_devfreq->opp_table,
-						    devfreq->opp_table, p_opp);
-		dev_pm_opp_put(p_opp);
+	return ret;
+}
 
-		if (IS_ERR(opp))
-			goto no_required_opp;
+static int cpufreq_passive_notifier_call(struct notifier_block *nb,
+					 unsigned long event, void *ptr)
+{
+	struct devfreq_passive_data *data =
+			container_of(nb, struct devfreq_passive_data, nb);
+	struct devfreq *devfreq = (struct devfreq *)data->this;
+	struct devfreq_cpu_data *cpudata;
+	struct cpufreq_freqs *freqs = ptr;
+	unsigned int cur_freq;
+	int ret;
 
-		*freq = dev_pm_opp_get_freq(opp);
-		dev_pm_opp_put(opp);
+	if (event != CPUFREQ_POSTCHANGE || !freqs ||
+		!data->cpudata[freqs->policy->cpu])
+		return 0;
 
+	cpudata = data->cpudata[freqs->policy->cpu];
+	if (cpudata->cur_freq == freqs->new)
 		return 0;
+
+	cur_freq = cpudata->cur_freq;
+	cpudata->cur_freq = freqs->new;
+
+	mutex_lock(&devfreq->lock);
+	ret = devfreq_update_target(devfreq, freqs->new);
+	mutex_unlock(&devfreq->lock);
+	if (ret) {
+		cpudata->cur_freq = cur_freq;
+		dev_err(&devfreq->dev, "failed to update the frequency.\n");
+		return ret;
 	}
 
-no_required_opp:
-	/*
-	 * Get the OPP table's index of decided frequency by governor
-	 * of parent device.
-	 */
-	for (i = 0; i < parent_devfreq->profile->max_state; i++)
-		if (parent_devfreq->profile->freq_table[i] == *freq)
-			break;
+	return 0;
+}
 
-	if (i == parent_devfreq->profile->max_state)
-		return -EINVAL;
+static int cpufreq_passive_register_notifier(struct devfreq *devfreq)
+{
+	struct devfreq_passive_data *p_data
+			= (struct devfreq_passive_data *)devfreq->data;
+	struct device *dev = devfreq->dev.parent;
+	struct opp_table *opp_table = NULL;
+	struct devfreq_cpu_data *cpudata;
+	struct cpufreq_policy *policy;
+	struct device *cpu_dev;
+	unsigned int cpu;
+	int ret;
+
+	cpus_read_lock();
+
+	p_data->nb.notifier_call = cpufreq_passive_notifier_call;
+	ret = cpufreq_register_notifier(&p_data->nb, CPUFREQ_TRANSITION_NOTIFIER);
+	if (ret) {
+		dev_err(dev, "failed to register cpufreq notifier\n");
+		p_data->nb.notifier_call = NULL;
+		goto out;
+	}
 
-	/* Get the suitable frequency by using index of parent device. */
-	if (i < devfreq->profile->max_state) {
-		child_freq = devfreq->profile->freq_table[i];
-	} else {
-		count = devfreq->profile->max_state;
-		child_freq = devfreq->profile->freq_table[count - 1];
+	for_each_online_cpu(cpu) {
+		if (p_data->cpudata[cpu])
+			continue;
+
+		policy = cpufreq_cpu_get(cpu);
+		if (policy) {
+			cpudata = kzalloc(sizeof(*cpudata), GFP_KERNEL);
+			if (!cpudata) {
+				ret = -ENOMEM;
+				goto out;
+			}
+
+			cpu_dev = get_cpu_device(cpu);
+			if (!cpu_dev) {
+				dev_err(dev, "failed to get cpu device\n");
+				ret = -ENODEV;
+				goto out;
+			}
+
+			opp_table = dev_pm_opp_get_opp_table(cpu_dev);
+			if (IS_ERR(opp_table)) {
+				ret = PTR_ERR(opp_table);
+				goto out;
+			}
+
+			cpudata->dev = cpu_dev;
+			cpudata->opp_table = opp_table;
+			cpudata->first_cpu = cpumask_first(policy->related_cpus);
+			cpudata->cur_freq = policy->cur;
+			cpudata->min_freq = policy->cpuinfo.min_freq;
+			cpudata->max_freq = policy->cpuinfo.max_freq;
+
+			p_data->cpudata[cpu] = cpudata;
+			cpufreq_cpu_put(policy);
+		} else {
+			ret = -EPROBE_DEFER;
+			goto out;
+		}
 	}
+out:
+	cpus_read_unlock();
+	if (ret)
+		return ret;
+
+	mutex_lock(&devfreq->lock);
+	ret = devfreq_update_target(devfreq, 0L);
+	mutex_unlock(&devfreq->lock);
+	if (ret)
+		dev_err(dev, "failed to update the frequency\n");
 
-	/* Return the suitable frequency for passive device. */
-	*freq = child_freq;
+	return ret;
+}
+
+static int cpufreq_passive_unregister_notifier(struct devfreq *devfreq)
+{
+	struct devfreq_passive_data *p_data
+			= (struct devfreq_passive_data *)devfreq->data;
+	struct devfreq_cpu_data *cpudata;
+	int cpu;
+
+	if (p_data->nb.notifier_call)
+		cpufreq_unregister_notifier(&p_data->nb, CPUFREQ_TRANSITION_NOTIFIER);
+
+	for_each_possible_cpu(cpu) {
+		cpudata = p_data->cpudata[cpu];
+		if (cpudata) {
+			if (cpudata->opp_table)
+				dev_pm_opp_put_opp_table(cpudata->opp_table);
+			kfree(cpudata);
+			cpudata = NULL;
+		}
+	}
 
 	return 0;
 }
@@ -140,7 +329,7 @@
 	struct notifier_block *nb = &p_data->nb;
 	int ret = 0;
 
-	if (!parent)
+	if (p_data->parent_type == DEVFREQ_PARENT_DEV && !parent)
 		return -EPROBE_DEFER;
 
 	switch (event) {
@@ -148,13 +337,24 @@
 		if (!p_data->this)
 			p_data->this = devfreq;
 
-		nb->notifier_call = devfreq_passive_notifier_call;
-		ret = devfreq_register_notifier(parent, nb,
-					DEVFREQ_TRANSITION_NOTIFIER);
+		if (p_data->parent_type == DEVFREQ_PARENT_DEV) {
+			nb->notifier_call = devfreq_passive_notifier_call;
+			ret = devfreq_register_notifier(parent, nb,
+						DEVFREQ_TRANSITION_NOTIFIER);
+		} else if (p_data->parent_type == CPUFREQ_PARENT_DEV) {
+			ret = cpufreq_passive_register_notifier(devfreq);
+		} else {
+			ret = -EINVAL;
+		}
 		break;
 	case DEVFREQ_GOV_STOP:
-		WARN_ON(devfreq_unregister_notifier(parent, nb,
-					DEVFREQ_TRANSITION_NOTIFIER));
+		if (p_data->parent_type == DEVFREQ_PARENT_DEV)
+			WARN_ON(devfreq_unregister_notifier(parent, nb,
+						DEVFREQ_TRANSITION_NOTIFIER));
+		else if (p_data->parent_type == CPUFREQ_PARENT_DEV)
+			WARN_ON(cpufreq_passive_unregister_notifier(devfreq));
+		else
+			ret = -EINVAL;
 		break;
 	default:
 		break;
diff -ruN a/drivers/devfreq/Kconfig b/drivers/devfreq/Kconfig
--- a/drivers/devfreq/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/devfreq/Kconfig	2021-12-23 08:35:18.000000000 +0100
@@ -108,6 +108,16 @@
 	  This adds the DEVFREQ driver for the i.MX8M DDR Controller. It allows
 	  adjusting DRAM frequency.
 
+config ARM_MT8183_CCI_DEVFREQ
+	tristate "MT8183 CCI DEVFREQ Driver"
+	depends on ARM_MEDIATEK_CPUFREQ
+	help
+		This adds a devfreq driver for Cache Coherent Interconnect
+		of Mediatek MT8183, which is shared the same regulator
+		with cpu cluster.
+		It can track buck voltage and update a proper CCI frequency.
+		Use notification to get regulator status.
+
 config ARM_TEGRA_DEVFREQ
 	tristate "NVIDIA Tegra30/114/124/210 DEVFREQ Driver"
 	depends on ARCH_TEGRA_3x_SOC || ARCH_TEGRA_114_SOC || \
diff -ruN a/drivers/devfreq/Makefile b/drivers/devfreq/Makefile
--- a/drivers/devfreq/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/devfreq/Makefile	2021-12-23 08:35:18.000000000 +0100
@@ -11,6 +11,7 @@
 obj-$(CONFIG_ARM_EXYNOS_BUS_DEVFREQ)	+= exynos-bus.o
 obj-$(CONFIG_ARM_IMX_BUS_DEVFREQ)	+= imx-bus.o
 obj-$(CONFIG_ARM_IMX8M_DDRC_DEVFREQ)	+= imx8m-ddrc.o
+obj-$(CONFIG_ARM_MT8183_CCI_DEVFREQ)	+= mt8183-cci-devfreq.o
 obj-$(CONFIG_ARM_RK3399_DMC_DEVFREQ)	+= rk3399_dmc.o
 obj-$(CONFIG_ARM_TEGRA_DEVFREQ)		+= tegra30-devfreq.o
 
diff -ruN a/drivers/devfreq/mt8183-cci-devfreq.c b/drivers/devfreq/mt8183-cci-devfreq.c
--- a/drivers/devfreq/mt8183-cci-devfreq.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/devfreq/mt8183-cci-devfreq.c	2021-12-23 08:35:18.000000000 +0100
@@ -0,0 +1,225 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2021 MediaTek Inc.
+
+ * Author: Andrew-sh.Cheng <andrew-sh.cheng@mediatek.com>
+ */
+
+#include <linux/clk.h>
+#include <linux/devfreq.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/regulator/consumer.h>
+#include <linux/time.h>
+
+#define MAX_VOLT_LIMIT		(1150000)
+
+struct cci_devfreq {
+	struct devfreq *devfreq;
+	struct regulator *cpu_reg;
+	struct clk *cci_clk;
+	int old_vproc;
+	unsigned long old_freq;
+	struct notifier_block opp_nb;
+};
+
+static int mtk_cci_set_voltage(struct cci_devfreq *cci_df, int vproc)
+{
+	int ret;
+
+	ret = regulator_set_voltage(cci_df->cpu_reg, vproc,
+				    MAX_VOLT_LIMIT);
+	if (!ret)
+		cci_df->old_vproc = vproc;
+	return ret;
+}
+
+static int mtk_cci_devfreq_target(struct device *dev, unsigned long *freq,
+				  u32 flags)
+{
+	int ret;
+	struct cci_devfreq *cci_df = dev_get_drvdata(dev);
+	struct dev_pm_opp *opp;
+	unsigned long opp_rate, opp_voltage, old_voltage;
+
+	if (!cci_df)
+		return -EINVAL;
+
+	if (cci_df->old_freq == *freq)
+		return 0;
+
+	opp_rate = *freq;
+	opp = devfreq_recommended_opp(dev, &opp_rate, 1);
+	opp_voltage = dev_pm_opp_get_voltage(opp);
+	dev_pm_opp_put(opp);
+
+	old_voltage = cci_df->old_vproc;
+	if (old_voltage == 0)
+		old_voltage = regulator_get_voltage(cci_df->cpu_reg);
+
+	// scale up: set voltage first then freq
+	if (opp_voltage > old_voltage) {
+		ret = mtk_cci_set_voltage(cci_df, opp_voltage);
+		if (ret) {
+			pr_err("cci: failed to scale up voltage\n");
+			return ret;
+		}
+	}
+
+	ret = clk_set_rate(cci_df->cci_clk, *freq);
+	if (ret) {
+		pr_err("%s: failed cci to set rate: %d\n", __func__,
+		       ret);
+		mtk_cci_set_voltage(cci_df, old_voltage);
+		return ret;
+	}
+
+	// scale down: set freq first then voltage
+	if (opp_voltage < old_voltage) {
+		ret = mtk_cci_set_voltage(cci_df, opp_voltage);
+		if (ret) {
+			pr_err("cci: failed to scale down voltage\n");
+			clk_set_rate(cci_df->cci_clk, cci_df->old_freq);
+			return ret;
+		}
+	}
+
+	cci_df->old_freq = *freq;
+
+	return 0;
+}
+
+static int ccidevfreq_opp_notifier(struct notifier_block *nb,
+				   unsigned long event, void *data)
+{
+	struct dev_pm_opp *opp = data;
+	struct cci_devfreq *cci_df = container_of(nb, struct cci_devfreq,
+						  opp_nb);
+	unsigned long	freq, volt;
+
+	if (event == OPP_EVENT_ADJUST_VOLTAGE) {
+		freq = dev_pm_opp_get_freq(opp);
+		/* current opp item is changed */
+		if (freq == cci_df->old_freq) {
+			volt = dev_pm_opp_get_voltage(opp);
+			mtk_cci_set_voltage(cci_df, volt);
+		}
+	}
+
+	return 0;
+}
+
+static struct devfreq_dev_profile cci_devfreq_profile = {
+	.target = mtk_cci_devfreq_target,
+};
+
+static int mtk_cci_devfreq_probe(struct platform_device *pdev)
+{
+	struct device *cci_dev = &pdev->dev;
+	struct cci_devfreq *cci_df;
+	struct devfreq_passive_data *passive_data;
+	struct notifier_block *opp_nb;
+	int ret;
+
+	cci_df = devm_kzalloc(cci_dev, sizeof(*cci_df), GFP_KERNEL);
+	if (!cci_df)
+		return -ENOMEM;
+
+	opp_nb = &cci_df->opp_nb;
+
+	cci_df->cci_clk = devm_clk_get(cci_dev, "cci_clock");
+	ret = PTR_ERR_OR_ZERO(cci_df->cci_clk);
+	if (ret) {
+		if (ret != -EPROBE_DEFER)
+			dev_err(cci_dev, "failed to get clock for CCI: %d\n",
+				ret);
+		return ret;
+	}
+	cci_df->cpu_reg = devm_regulator_get_optional(cci_dev, "proc");
+	ret = PTR_ERR_OR_ZERO(cci_df->cpu_reg);
+	if (ret) {
+		if (ret != -EPROBE_DEFER)
+			dev_err(cci_dev, "failed to get regulator for CCI: %d\n",
+				ret);
+		return ret;
+	}
+	ret = regulator_enable(cci_df->cpu_reg);
+	if (ret) {
+		dev_err(cci_dev, "enable buck for cci fail\n");
+		return ret;
+	}
+
+	ret = dev_pm_opp_of_add_table(cci_dev);
+	if (ret) {
+		dev_err(cci_dev, "Fail to get OPP table for CCI: %d\n", ret);
+		return ret;
+	}
+
+	platform_set_drvdata(pdev, cci_df);
+
+	passive_data = devm_kzalloc(cci_dev, sizeof(*passive_data), GFP_KERNEL);
+	if (!passive_data) {
+		ret = -ENOMEM;
+		goto err_opp;
+	}
+
+	passive_data->parent_type = CPUFREQ_PARENT_DEV;
+
+	cci_df->devfreq = devm_devfreq_add_device(cci_dev,
+						  &cci_devfreq_profile,
+						  DEVFREQ_GOV_PASSIVE,
+						  passive_data);
+	if (IS_ERR(cci_df->devfreq)) {
+		ret = PTR_ERR(cci_df->devfreq);
+		dev_err(cci_dev, "cannot create cci devfreq device:%d\n", ret);
+		goto err_opp;
+	}
+
+	opp_nb->notifier_call = ccidevfreq_opp_notifier;
+	dev_pm_opp_register_notifier(cci_dev, opp_nb);
+
+	return 0;
+
+err_opp:
+	dev_pm_opp_of_remove_table(cci_dev);
+	return ret;
+}
+
+static int mtk_cci_devfreq_remove(struct platform_device *pdev)
+{
+	struct device *cci_dev = &pdev->dev;
+	struct cci_devfreq *cci_df;
+	struct notifier_block *opp_nb;
+
+	cci_df = platform_get_drvdata(pdev);
+	opp_nb = &cci_df->opp_nb;
+
+	dev_pm_opp_unregister_notifier(cci_dev, opp_nb);
+	dev_pm_opp_of_remove_table(cci_dev);
+	regulator_disable(cci_df->cpu_reg);
+
+	return 0;
+}
+
+static const __maybe_unused struct of_device_id
+	mediatek_cci_of_match[] = {
+	{ .compatible = "mediatek,mt8183-cci" },
+	{ },
+};
+MODULE_DEVICE_TABLE(of, mediatek_cci_of_match);
+
+static struct platform_driver cci_devfreq_driver = {
+	.probe	= mtk_cci_devfreq_probe,
+	.remove	= mtk_cci_devfreq_remove,
+	.driver = {
+		.name = "mediatek-cci-devfreq",
+		.of_match_table = of_match_ptr(mediatek_cci_of_match),
+	},
+};
+
+module_platform_driver(cci_devfreq_driver);
+
+MODULE_DESCRIPTION("Mediatek CCI devfreq driver");
+MODULE_AUTHOR("Andrew-sh.Cheng <andrew-sh.cheng@mediatek.com>");
+MODULE_LICENSE("GPL v2");
diff -ruN a/drivers/dma-buf/dma-buf.c b/drivers/dma-buf/dma-buf.c
--- a/drivers/dma-buf/dma-buf.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/dma-buf/dma-buf.c	2021-12-23 08:35:18.000000000 +0100
@@ -20,6 +20,7 @@
 #include <linux/debugfs.h>
 #include <linux/module.h>
 #include <linux/seq_file.h>
+#include <linux/sync_file.h>
 #include <linux/poll.h>
 #include <linux/dma-resv.h>
 #include <linux/mm.h>
@@ -195,6 +196,9 @@
  * Note that this only signals the completion of the respective fences, i.e. the
  * DMA transfers are complete. Cache flushing and any other necessary
  * preparations before CPU access can begin still need to happen.
+ *
+ * As an alternative to poll(), the set of fences on DMA buffer can be
+ * exported as a &sync_file using &dma_buf_sync_file_export.
  */
 
 static void dma_buf_poll_cb(struct dma_fence *fence, struct dma_fence_cb *cb)
@@ -354,6 +358,64 @@
 	return ret;
 }
 
+#if IS_ENABLED(CONFIG_SYNC_FILE)
+static long dma_buf_export_sync_file(struct dma_buf *dmabuf,
+				     void __user *user_data)
+{
+	struct dma_buf_export_sync_file arg;
+	struct dma_fence *fence = NULL;
+	struct sync_file *sync_file;
+	int fd, ret;
+
+	if (copy_from_user(&arg, user_data, sizeof(arg)))
+		return -EFAULT;
+
+	if (arg.flags & ~DMA_BUF_SYNC_RW)
+		return -EINVAL;
+
+	if ((arg.flags & DMA_BUF_SYNC_RW) == 0)
+		return -EINVAL;
+
+	fd = get_unused_fd_flags(O_CLOEXEC);
+	if (fd < 0)
+		return fd;
+
+	if (arg.flags & DMA_BUF_SYNC_WRITE) {
+		fence = dma_resv_get_singleton(dmabuf->resv);
+		if (IS_ERR(fence)) {
+			ret = PTR_ERR(fence);
+			goto err_put_fd;
+		}
+	} else if (arg.flags & DMA_BUF_SYNC_READ) {
+		fence = dma_resv_get_excl_unlocked(dmabuf->resv);
+	}
+
+	if (!fence)
+		fence = dma_fence_get_stub();
+
+	sync_file = sync_file_create(fence);
+
+	dma_fence_put(fence);
+
+	if (!sync_file) {
+		ret = -ENOMEM;
+		goto err_put_fd;
+	}
+
+	fd_install(fd, sync_file->file);
+
+	arg.fd = fd;
+	if (copy_to_user(user_data, &arg, sizeof(arg)))
+		return -EFAULT;
+
+	return 0;
+
+err_put_fd:
+	put_unused_fd(fd);
+	return ret;
+}
+#endif
+
 static long dma_buf_ioctl(struct file *file,
 			  unsigned int cmd, unsigned long arg)
 {
@@ -397,6 +459,11 @@
 	case DMA_BUF_SET_NAME_B:
 		return dma_buf_set_name(dmabuf, (const char __user *)arg);
 
+#if IS_ENABLED(CONFIG_SYNC_FILE)
+	case DMA_BUF_IOCTL_EXPORT_SYNC_FILE:
+		return dma_buf_export_sync_file(dmabuf, (void __user *)arg);
+#endif
+
 	default:
 		return -ENOTTY;
 	}
diff -ruN a/drivers/dma-buf/dma-fence-array.c b/drivers/dma-buf/dma-fence-array.c
--- a/drivers/dma-buf/dma-fence-array.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/dma-buf/dma-fence-array.c	2021-12-23 08:35:18.000000000 +0100
@@ -201,3 +201,30 @@
 	return true;
 }
 EXPORT_SYMBOL(dma_fence_match_context);
+
+struct dma_fence *dma_fence_array_first(struct dma_fence *head)
+{
+	struct dma_fence_array *array;
+
+	if (!head)
+		return NULL;
+
+	array = to_dma_fence_array(head);
+	if (!array)
+		return head;
+
+	return array->fences[0];
+}
+EXPORT_SYMBOL(dma_fence_array_first);
+
+struct dma_fence *dma_fence_array_next(struct dma_fence *head,
+				       unsigned int index)
+{
+	struct dma_fence_array *array = to_dma_fence_array(head);
+
+	if (!array || index >= array->num_fences)
+		return NULL;
+
+	return array->fences[index];
+}
+EXPORT_SYMBOL(dma_fence_array_next);
diff -ruN a/drivers/dma-buf/dma-resv.c b/drivers/dma-buf/dma-resv.c
--- a/drivers/dma-buf/dma-resv.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/dma-buf/dma-resv.c	2021-12-23 08:35:18.000000000 +0100
@@ -34,6 +34,8 @@
  */
 
 #include <linux/dma-resv.h>
+#include <linux/dma-fence-chain.h>
+#include <linux/dma-fence-array.h>
 #include <linux/export.h>
 #include <linux/mm.h>
 #include <linux/sched/mm.h>
@@ -50,6 +52,10 @@
  * write-side updates.
  */
 
+#define dma_fence_deep_dive_for_each(fence, chain, index, head)	\
+	dma_fence_chain_for_each(chain, head)			\
+		dma_fence_array_for_each(fence, index, chain)
+
 DEFINE_WD_CLASS(reservation_ww_class);
 EXPORT_SYMBOL(reservation_ww_class);
 
@@ -496,6 +502,91 @@
 EXPORT_SYMBOL_GPL(dma_resv_get_fences);
 
 /**
+ * dma_resv_get_singleton - get a single fence for the dma_resv object
+ * @obj: the reservation object
+ *
+ * Get a single fence representing all unsignaled fences in the dma_resv object
+ * plus the given extra fence. If we got only one fence return a new
+ * reference to that, otherwise return a dma_fence_array object.
+ *
+ * RETURNS
+ * The singleton dma_fence on success or an ERR_PTR on failure
+ */
+struct dma_fence *dma_resv_get_singleton(struct dma_resv *obj)
+{
+	struct dma_fence *result, **resv_fences, *fence, *chain, **fences;
+	struct dma_fence_array *array;
+	unsigned int num_resv_fences, num_fences;
+	unsigned int err, i, j;
+
+	err = dma_resv_get_fences(obj, NULL, &num_resv_fences, &resv_fences);
+	if (err)
+		return ERR_PTR(err);
+
+	if (num_resv_fences == 0)
+		return NULL;
+
+	num_fences = 0;
+	result = NULL;
+
+	for (i = 0; i < num_resv_fences; ++i) {
+		dma_fence_deep_dive_for_each(fence, chain, j, resv_fences[i]) {
+			if (dma_fence_is_signaled(fence))
+				continue;
+
+			result = fence;
+			++num_fences;
+		}
+	}
+
+	if (num_fences <= 1) {
+		result = dma_fence_get(result);
+		goto put_resv_fences;
+	}
+
+	fences = kmalloc_array(num_fences, sizeof(struct dma_fence *),
+			       GFP_KERNEL);
+	if (!fences) {
+		result = ERR_PTR(-ENOMEM);
+		goto put_resv_fences;
+	}
+
+	num_fences = 0;
+	for (i = 0; i < num_resv_fences; ++i) {
+		dma_fence_deep_dive_for_each(fence, chain, j, resv_fences[i]) {
+			if (!dma_fence_is_signaled(fence))
+				fences[num_fences++] = dma_fence_get(fence);
+		}
+	}
+
+	if (num_fences <= 1) {
+		result = num_fences ? fences[0] : NULL;
+		kfree(fences);
+		goto put_resv_fences;
+	}
+
+	array = dma_fence_array_create(num_fences, fences,
+				       dma_fence_context_alloc(1),
+				       1, false);
+	if (array) {
+		result = &array->base;
+	} else {
+		result = ERR_PTR(-ENOMEM);
+		while (num_fences--)
+			dma_fence_put(fences[num_fences]);
+		kfree(fences);
+	}
+
+put_resv_fences:
+	while (num_resv_fences--)
+		dma_fence_put(resv_fences[num_resv_fences]);
+	kfree(resv_fences);
+
+	return result;
+}
+EXPORT_SYMBOL_GPL(dma_resv_get_singleton);
+
+/**
  * dma_resv_wait_timeout - Wait on reservation's objects
  * shared and/or exclusive fences.
  * @obj: the reservation object
diff -ruN a/drivers/dma-buf/sw_sync.c b/drivers/dma-buf/sw_sync.c
--- a/drivers/dma-buf/sw_sync.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/dma-buf/sw_sync.c	2021-12-23 08:35:18.000000000 +0100
@@ -130,16 +130,7 @@
 
 static void timeline_fence_release(struct dma_fence *fence)
 {
-	struct sync_pt *pt = dma_fence_to_sync_pt(fence);
 	struct sync_timeline *parent = dma_fence_parent(fence);
-	unsigned long flags;
-
-	spin_lock_irqsave(fence->lock, flags);
-	if (!list_empty(&pt->link)) {
-		list_del(&pt->link);
-		rb_erase(&pt->node, &parent->pt_tree);
-	}
-	spin_unlock_irqrestore(fence->lock, flags);
 
 	sync_timeline_put(parent);
 	dma_fence_free(fence);
@@ -203,18 +194,11 @@
 		if (!timeline_fence_signaled(&pt->base))
 			break;
 
-		list_del_init(&pt->link);
+		list_del(&pt->link);
 		rb_erase(&pt->node, &obj->pt_tree);
 
-		/*
-		 * A signal callback may release the last reference to this
-		 * fence, causing it to be freed. That operation has to be
-		 * last to avoid a use after free inside this loop, and must
-		 * be after we remove the fence from the timeline in order to
-		 * prevent deadlocking on timeline->lock inside
-		 * timeline_fence_release().
-		 */
 		dma_fence_signal_locked(&pt->base);
+		dma_fence_put(&pt->base);
 	}
 
 	spin_unlock_irq(&obj->lock);
@@ -261,13 +245,9 @@
 			} else if (cmp < 0) {
 				p = &parent->rb_left;
 			} else {
-				if (dma_fence_get_rcu(&other->base)) {
-					sync_timeline_put(obj);
-					kfree(pt);
-					pt = other;
-					goto unlock;
-				}
-				p = &parent->rb_left;
+				dma_fence_put(&pt->base);
+				pt = other;
+				goto unlock;
 			}
 		}
 		rb_link_node(&pt->node, parent, p);
@@ -278,6 +258,7 @@
 			      parent ? &rb_entry(parent, typeof(*pt), node)->link : &obj->pt_list);
 	}
 unlock:
+	dma_fence_get(&pt->base); /* keep a ref for the timeline */
 	spin_unlock_irq(&obj->lock);
 
 	return pt;
@@ -316,6 +297,7 @@
 	list_for_each_entry_safe(pt, next, &obj->pt_list, link) {
 		dma_fence_set_error(&pt->base, -ENOENT);
 		dma_fence_signal_locked(&pt->base);
+		dma_fence_put(&pt->base);
 	}
 
 	spin_unlock_irq(&obj->lock);
diff -ruN a/drivers/firmware/google/gsmi.c b/drivers/firmware/google/gsmi.c
--- a/drivers/firmware/google/gsmi.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/firmware/google/gsmi.c	2021-12-23 08:35:18.000000000 +0100
@@ -31,6 +31,7 @@
 #include <linux/module.h>
 #include <linux/ucs2_string.h>
 #include <linux/suspend.h>
+#include <linux/thermal.h>
 
 #define GSMI_SHUTDOWN_CLEAN	0	/* Clean Shutdown */
 /* TODO(mikew@google.com): Tie in HARDLOCKUP_DETECTOR with NMIWDT */
@@ -42,6 +43,7 @@
 #define GSMI_SHUTDOWN_SOFTWDT	6	/* Software Watchdog */
 #define GSMI_SHUTDOWN_MBE	7	/* Uncorrected ECC */
 #define GSMI_SHUTDOWN_TRIPLE	8	/* Triple Fault */
+#define GSMI_SHUTDOWN_THERMAL	9	/* Critical Thermal Threshold */
 
 #define DRIVER_VERSION		"1.0"
 #define GSMI_GUID_SIZE		16
@@ -689,6 +691,18 @@
 	.notifier_call = gsmi_panic_callback,
 };
 
+static int gsmi_thermal_callback(struct notifier_block *nb,
+				 unsigned long reason, void *arg)
+{
+	if (reason == THERMAL_TRIP_CRITICAL)
+		gsmi_shutdown_reason(GSMI_SHUTDOWN_THERMAL);
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block gsmi_thermal_notifier = {
+	.notifier_call = gsmi_thermal_callback
+};
+
 /*
  * This hash function was blatantly copied from include/linux/hash.h.
  * It is used by this driver to obfuscate a board name that requires a
@@ -1028,6 +1042,7 @@
 	}
 #endif
 
+	register_thermal_notifier(&gsmi_thermal_notifier);
 	register_reboot_notifier(&gsmi_reboot_notifier);
 	register_die_notifier(&gsmi_die_notifier);
 	atomic_notifier_chain_register(&panic_notifier_list,
@@ -1055,6 +1070,7 @@
 
 static void __exit gsmi_exit(void)
 {
+	unregister_thermal_notifier(&gsmi_thermal_notifier);
 	unregister_reboot_notifier(&gsmi_reboot_notifier);
 	unregister_die_notifier(&gsmi_die_notifier);
 	atomic_notifier_chain_unregister(&panic_notifier_list,
diff -ruN a/drivers/firmware/google/Kconfig b/drivers/firmware/google/Kconfig
--- a/drivers/firmware/google/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/firmware/google/Kconfig	2021-12-23 08:35:18.000000000 +0100
@@ -11,7 +11,7 @@
 
 config GOOGLE_SMI
 	tristate "SMI interface for Google platforms"
-	depends on X86 && ACPI && DMI
+	depends on X86 && ACPI && DMI && THERMAL
 	help
 	  Say Y here if you want to enable SMI callbacks for Google
 	  platforms.  This provides an interface for writing to and
diff -ruN a/drivers/gpu/drm/bridge/analogix/anx7625.c b/drivers/gpu/drm/bridge/analogix/anx7625.c
--- a/drivers/gpu/drm/bridge/analogix/anx7625.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/bridge/analogix/anx7625.c	2021-12-23 08:35:26.000000000 +0100
@@ -1329,7 +1329,6 @@
 	dsi->format = MIPI_DSI_FMT_RGB888;
 	dsi->mode_flags = MIPI_DSI_MODE_VIDEO	|
 		MIPI_DSI_MODE_VIDEO_SYNC_PULSE	|
-		MIPI_DSI_MODE_NO_EOT_PACKET	|
 		MIPI_DSI_MODE_VIDEO_HSE;
 
 	if (mipi_dsi_attach(dsi) < 0) {
diff -ruN a/drivers/gpu/drm/bridge/ite-it6505.c b/drivers/gpu/drm/bridge/ite-it6505.c
--- a/drivers/gpu/drm/bridge/ite-it6505.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/bridge/ite-it6505.c	2021-12-23 08:35:26.000000000 +0100
@@ -0,0 +1,3376 @@
+// SPDX-License-Identifier: (GPL-2.0-only OR BSD-2-Clause)
+/*
+ * Copyright (c) 2020, The Linux Foundation. All rights reserved.
+ */
+#include <linux/bits.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/err.h>
+#include <linux/extcon.h>
+#include <linux/fs.h>
+#include <linux/gpio/consumer.h>
+#include <linux/i2c.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pm_runtime.h>
+#include <linux/regmap.h>
+#include <linux/regulator/consumer.h>
+#include <linux/types.h>
+#include <linux/wait.h>
+
+#include <crypto/hash.h>
+
+#include <drm/drm_atomic_helper.h>
+#include <drm/drm_bridge.h>
+#include <drm/drm_crtc.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/drm_dp_helper.h>
+#include <drm/drm_edid.h>
+#include <drm/drm_hdcp.h>
+#include <drm/drm_print.h>
+#include <drm/drm_probe_helper.h>
+
+#include <sound/hdmi-codec.h>
+
+#define REG_IC_VER 0x04
+
+#define REG_RESET_CTRL 0x05
+#define VIDEO_RESET BIT(0)
+#define AUDIO_RESET BIT(1)
+#define ALL_LOGIC_RESET BIT(2)
+#define AUX_RESET BIT(3)
+#define HDCP_RESET BIT(4)
+
+#define INT_STATUS_01 0x06
+#define INT_MASK_01 0x09
+#define INT_HPD_CHANGE 0
+#define INT_RECEIVE_HPD_IRQ 1
+#define INT_SCDT_CHANGE 2
+#define INT_HDCP_FAIL 3
+#define INT_HDCP_DONE 4
+#define BIT_OFFSET(x) ((x - INT_STATUS_01) * BITS_PER_BYTE)
+#define BIT_INT_HPD INT_HPD_CHANGE
+#define BIT_INT_HPD_IRQ INT_RECEIVE_HPD_IRQ
+#define BIT_INT_SCDT INT_SCDT_CHANGE
+#define BIT_INT_HDCP_FAIL INT_HDCP_FAIL
+#define BIT_INT_HDCP_DONE INT_HDCP_DONE
+
+#define INT_STATUS_02 0x07
+#define INT_MASK_02 0x0A
+#define INT_AUX_CMD_FAIL 0
+#define INT_HDCP_KSV_CHECK 1
+#define INT_AUDIO_FIFO_ERROR 2
+#define BIT_INT_AUX_CMD_FAIL (BIT_OFFSET(0x07) + INT_AUX_CMD_FAIL)
+#define BIT_INT_HDCP_KSV_CHECK (BIT_OFFSET(0x07) + INT_HDCP_KSV_CHECK)
+#define BIT_INT_AUDIO_FIFO_ERROR (BIT_OFFSET(0x07) + INT_AUDIO_FIFO_ERROR)
+
+#define INT_STATUS_03 0x08
+#define INT_MASK_03 0x0B
+#define INT_LINK_TRAIN_FAIL 4
+#define INT_VID_FIFO_ERROR 5
+#define INT_IO_LATCH_FIFO_OVERFLOW 7
+#define BIT_INT_LINK_TRAIN_FAIL (BIT_OFFSET(0x08) + INT_LINK_TRAIN_FAIL)
+#define BIT_INT_VID_FIFO_ERROR (BIT_OFFSET(0x08) + INT_VID_FIFO_ERROR)
+#define BIT_INT_IO_FIFO_OVERFLOW (BIT_OFFSET(0x08) + INT_IO_LATCH_FIFO_OVERFLOW)
+
+#define REG_SYSTEM_STS 0x0D
+#define INT_STS BIT(0)
+#define HPD_STS BIT(1)
+#define VIDEO_STB BIT(2)
+
+#define REG_LINK_TRAIN_STS 0x0E
+#define LINK_STATE_CR BIT(2)
+#define LINK_STATE_EQ BIT(3)
+#define LINK_STATE_NORP BIT(4)
+
+#define REG_BANK_SEL 0x0F
+#define REG_CLK_CTRL0 0x10
+#define M_PCLK_DELAY 0x03
+
+#define REG_AUX_OPT 0x11
+#define AUX_AUTO_RST BIT(0)
+#define AUX_FIX_FREQ BIT(3)
+
+#define REG_DATA_CTRL0 0x12
+#define VIDEO_LATCH_EDGE BIT(4)
+#define ENABLE_PCLK_COUNTER BIT(7)
+
+#define REG_PCLK_COUNTER_VALUE 0x13
+
+#define REG_501_FIFO_CTRL 0x15
+#define RST_501_FIFO BIT(1)
+
+#define REG_TRAIN_CTRL0 0x16
+#define FORCE_LBR BIT(0)
+#define LANE_COUNT_MASK 0x06
+#define LANE_SWAP BIT(3)
+#define SPREAD_AMP_5 BIT(4)
+#define FORCE_CR_DONE BIT(5)
+#define FORCE_EQ_DONE BIT(6)
+
+#define REG_TRAIN_CTRL1 0x17
+#define AUTO_TRAIN BIT(0)
+#define MANUAL_TRAIN BIT(1)
+#define FORCE_RETRAIN BIT(2)
+
+#define REG_AUX_CTRL 0x23
+#define CLR_EDID_FIFO BIT(0)
+#define AUX_USER_MODE BIT(1)
+#define AUX_NO_SEGMENT_WR BIT(6)
+#define AUX_EN_FIFO_READ BIT(7)
+
+#define REG_AUX_ADR_0_7 0x24
+#define REG_AUX_ADR_8_15 0x25
+#define REG_AUX_ADR_16_19 0x26
+#define REG_AUX_OUT_DATA0 0x27
+
+#define REG_AUX_CMD_REQ 0x2B
+#define AUX_BUSY BIT(5)
+
+#define REG_AUX_DATA_0_7 0x2C
+#define REG_AUX_DATA_8_15 0x2D
+#define REG_AUX_DATA_16_23 0x2E
+#define REG_AUX_DATA_24_31 0x2F
+
+#define REG_AUX_DATA_FIFO 0x2F
+
+#define REG_AUX_ERROR_STS 0x9F
+#define M_AUX_REQ_FAIL 0x03
+
+#define REG_HDCP_CTRL1 0x38
+#define HDCP_CP_ENABLE BIT(0)
+
+#define REG_HDCP_TRIGGER 0x39
+#define HDCP_TRIGGER_START  BIT(0)
+#define HDCP_TRIGGER_CPIRQ  BIT(1)
+#define HDCP_TRIGGER_KSV_DONE  BIT(4)
+#define HDCP_TRIGGER_KSV_FAIL BIT(5)
+
+#define REG_HDCP_CTRL2 0x3A
+#define HDCP_AN_SEL BIT(0)
+#define HDCP_AN_GEN BIT(1)
+#define HDCP_HW_HPDIRQ_ACT BIT(2)
+#define HDCP_EN_M0_READ BIT(5)
+
+#define REG_M0_0_7 0x4C
+#define REG_AN_0_7 0x4C
+#define REG_SP_CTRL0 0x58
+#define REG_IP_CTRL1 0x59
+#define REG_IP_CTRL2 0x5A
+
+#define REG_LINK_DRV 0x5C
+#define DRV_HS BIT(1)
+
+#define REG_DRV_LN_DATA_SEL 0x5D
+
+#define REG_AUX 0x5E
+
+#define REG_VID_BUS_CTRL0 0x60
+#define IN_DDR BIT(2)
+#define DDR_CD (0x01 << 6)
+
+#define REG_VID_BUS_CTRL1 0x61
+#define TX_FIFO_RESET BIT(1)
+
+#define REG_INPUT_CTRL 0xA0
+#define INPUT_HSYNC_POL BIT(0)
+#define INPUT_VSYNC_POL BIT(2)
+#define INPUT_INTERLACED BIT(4)
+
+#define REG_INPUT_HTOTAL 0xA1
+#define REG_INPUT_HACTIVE_START 0xA3
+#define REG_INPUT_HACTIVE_WIDTH 0xA5
+#define REG_INPUT_HFRONT_PORCH 0xA7
+#define REG_INPUT_HSYNC_WIDTH 0xA9
+#define REG_INPUT_VTOTAL 0xAB
+#define REG_INPUT_VACTIVE_START 0xAD
+#define REG_INPUT_VACTIVE_WIDTH 0xAF
+#define REG_INPUT_VFRONT_PORCH 0xB1
+#define REG_INPUT_VSYNC_WIDTH 0xB3
+
+#define REG_AUDIO_SRC_CTRL 0xB8
+#define M_AUDIO_I2S_EN 0x0F
+#define EN_I2S0 BIT(0)
+#define EN_I2S1 BIT(1)
+#define EN_I2S2 BIT(2)
+#define EN_I2S3 BIT(3)
+#define AUDIO_FIFO_RESET BIT(7)
+
+#define REG_AUDIO_FMT 0xB9
+#define REG_AUDIO_FIFO_SEL 0xBA
+
+#define REG_AUDIO_CTRL0 0xBB
+#define AUDIO_FULL_PKT BIT(4)
+#define AUDIO_16B_BOUND BIT(5)
+
+#define REG_AUDIO_CTRL1 0xBC
+#define REG_AUDIO_INPUT_FREQ 0xBE
+
+#define REG_IEC958_STS0 0xBF
+#define REG_IEC958_STS1 0xC0
+#define REG_IEC958_STS2 0xC1
+#define REG_IEC958_STS3 0xC2
+#define REG_IEC958_STS4 0xC3
+
+#define REG_HPD_IRQ_TIME 0xC9
+#define REG_AUX_DEBUG_MODE 0xCA
+#define REG_AUX_OPT2 0xCB
+#define REG_HDCP_OPT 0xCE
+#define REG_USER_DRV_PRE 0xCF
+
+#define REG_DATA_MUTE_CTRL 0xD3
+#define ENABLE_ENHANCED_FRAME BIT(0)
+#define ENABLE_AUTO_VIDEO_FIFO_RESET BIT(1)
+#define EN_VID_MUTE BIT(4)
+#define EN_AUD_MUTE BIT(5)
+
+#define REG_TIME_STMP_CTRL 0xD4
+#define EN_ENHANCE_VID_STMP BIT(0)
+#define EN_ENHANCE_AUD_STMP BIT(2)
+#define M_STAMP_STEP 0x30
+#define EN_SSC_GAT BIT(6)
+
+#define REG_INFOFRAME_CTRL 0xE8
+#define EN_AVI_PKT BIT(0)
+#define EN_AUD_PKT BIT(1)
+#define EN_MPG_PKT BIT(2)
+#define EN_GEN_PKT BIT(3)
+#define EN_VID_TIME_STMP BIT(4)
+#define EN_AUD_TIME_STMP BIT(5)
+#define EN_VID_CTRL_PKT (EN_AVI_PKT | EN_VID_TIME_STMP)
+#define EN_AUD_CTRL_PKT (EN_AUD_PKT | EN_AUD_TIME_STMP)
+
+#define REG_AUDIO_N_0_7 0xDE
+#define REG_AUDIO_N_8_15 0xDF
+#define REG_AUDIO_N_16_23 0xE0
+
+#define REG_AVI_INFO_DB1 0xE9
+#define REG_AVI_INFO_DB2 0xEA
+#define REG_AVI_INFO_DB3 0xEB
+#define REG_AVI_INFO_DB4 0xEC
+#define REG_AVI_INFO_DB5 0xED
+#define REG_AVI_INFO_SUM 0xF6
+
+#define REG_AUD_INFOFRAM_DB1 0xF7
+#define REG_AUD_INFOFRAM_DB2 0xF8
+#define REG_AUD_INFOFRAM_DB3 0xF9
+#define REG_AUD_INFOFRAM_DB4 0xFA
+#define REG_AUD_INFOFRAM_SUM 0xFB
+
+/* the following six registers are in bank1 */
+#define REG_DRV_0_DB_800_MV 0x7E
+#define REG_PRE_0_DB_800_MV 0x7F
+#define REG_PRE_3P5_DB_800_MV 0x81
+#define REG_SSC_CTRL0 0x88
+#define REG_SSC_CTRL1 0x89
+#define REG_SSC_CTRL2 0x8A
+
+#define RBR DP_LINK_BW_1_62
+#define HBR DP_LINK_BW_2_7
+#define HBR2 DP_LINK_BW_5_4
+#define HBR3 DP_LINK_BW_8_1
+
+#define MISC_VERB 0xF0
+#define MISC_VERC 0x70
+#define I2S_INPUT_FORMAT_STANDARD 0
+#define I2S_INPUT_FORMAT_32BIT 1
+#define I2S_INPUT_LEFT_JUSTIFIED 0
+#define I2S_INPUT_RIGHT_JUSTIFIED 1
+#define I2S_DATA_1T_DELAY 0
+#define I2S_DATA_NO_DELAY 1
+#define I2S_WS_LEFT_CHANNEL 0
+#define I2S_WS_RIGHT_CHANNEL 1
+#define I2S_DATA_MSB_FIRST 0
+#define I2S_DATA_LSB_FIRST 1
+#define WORD_LENGTH_16BIT 0
+#define WORD_LENGTH_18BIT 1
+#define WORD_LENGTH_20BIT 2
+#define WORD_LENGTH_24BIT 3
+
+/* Vendor option */
+#define HDCP_DESIRED 1
+#define MAX_LANE_COUNT 4
+#define MAX_LINK_RATE HBR
+#define AUTO_TRAIN_RETRY 3
+#define MAX_HDCP_DOWN_STREAM_COUNT 10
+#define MAX_CR_LEVEL 0x03
+#define MAX_EQ_LEVEL 0x03
+#define AUX_WAIT_TIMEOUT_MS 15
+#define AUX_FIFO_MAX_SIZE 32
+#define PIXEL_CLK_DELAY 1
+#define PIXEL_CLK_INVERSE 0
+#define ADJUST_PHASE_THRESHOLD 80000
+#define DPI_PIXEL_CLK_MAX 95000
+#define HDCP_SHA1_FIFO_LEN (MAX_HDCP_DOWN_STREAM_COUNT * 5 + 10)
+#define DEFAULT_PWR_ON 0
+#define DEFAULT_DRV_HOLD 0
+
+#define AUDIO_SELECT I2S
+#define AUDIO_TYPE LPCM
+#define AUDIO_SAMPLE_RATE SAMPLE_RATE_48K
+#define AUDIO_CHANNEL_COUNT 2
+#define I2S_INPUT_FORMAT I2S_INPUT_FORMAT_32BIT
+#define I2S_JUSTIFIED I2S_INPUT_LEFT_JUSTIFIED
+#define I2S_DATA_DELAY I2S_DATA_1T_DELAY
+#define I2S_WS_CHANNEL I2S_WS_LEFT_CHANNEL
+#define I2S_DATA_SEQUENCE I2S_DATA_MSB_FIRST
+#define AUDIO_WORD_LENGTH WORD_LENGTH_24BIT
+
+enum aux_cmd_type {
+	CMD_AUX_NATIVE_READ = 0x0,
+	CMD_AUX_NATIVE_WRITE = 0x5,
+	CMD_AUX_I2C_EDID_READ = 0xB,
+};
+
+enum aux_cmd_reply {
+	REPLY_ACK,
+	REPLY_NACK,
+	REPLY_DEFER,
+};
+
+enum link_train_status {
+	LINK_IDLE,
+	LINK_BUSY,
+	LINK_OK,
+};
+
+enum hdcp_state {
+	HDCP_AUTH_IDLE,
+	HDCP_AUTH_GOING,
+	HDCP_AUTH_DONE,
+};
+
+struct it6505_platform_data {
+	struct regulator *pwr18;
+	struct regulator *ovdd;
+	struct gpio_desc *gpiod_reset;
+};
+
+enum it6505_audio_select {
+	I2S = 0,
+	SPDIF,
+};
+
+enum it6505_audio_sample_rate {
+	SAMPLE_RATE_24K = 0x6,
+	SAMPLE_RATE_32K = 0x3,
+	SAMPLE_RATE_48K = 0x2,
+	SAMPLE_RATE_96K = 0xA,
+	SAMPLE_RATE_192K = 0xE,
+	SAMPLE_RATE_44_1K = 0x0,
+	SAMPLE_RATE_88_2K = 0x8,
+	SAMPLE_RATE_176_4K = 0xC,
+};
+
+enum it6505_audio_type {
+	LPCM = 0,
+	NLPCM,
+	DSS,
+};
+
+struct it6505_audio_data {
+	enum it6505_audio_select select;
+	enum it6505_audio_sample_rate sample_rate;
+	enum it6505_audio_type type;
+	u8 word_length;
+	u8 channel_count;
+	u8 i2s_input_format;
+	u8 i2s_justified;
+	u8 i2s_data_delay;
+	u8 i2s_ws_channel;
+	u8 i2s_data_sequence;
+};
+
+struct it6505_audio_sample_rate_map {
+	enum it6505_audio_sample_rate rate;
+	int sample_rate_value;
+};
+
+struct it6505_drm_dp_link {
+	unsigned char revision;
+	unsigned int rate;
+	unsigned int num_lanes;
+	unsigned long capabilities;
+};
+
+struct it6505 {
+	struct drm_dp_aux aux;
+	struct drm_bridge bridge;
+	struct i2c_client *client;
+	struct edid *edid;
+	struct it6505_drm_dp_link link;
+	struct it6505_platform_data pdata;
+	struct mutex extcon_lock;
+	struct mutex mode_lock;
+	struct mutex aux_lock;
+	struct regmap *regmap;
+	struct drm_display_mode source_output_mode;
+	struct drm_display_mode video_info;
+	struct notifier_block event_nb;
+	struct extcon_dev *extcon;
+	struct work_struct extcon_wq;
+	enum drm_connector_status connector_status;
+	enum link_train_status link_state;
+	struct work_struct link_works;
+	u8 dpcd[DP_RECEIVER_CAP_SIZE];
+	u8 lane_count;
+	u8 link_rate_bw_code;
+	u8 sink_count;
+	bool step_train;
+	bool branch_device;
+	bool enable_ssc;
+	bool lane_swap_disabled;
+	bool lane_swap;
+	bool powered;
+	bool hpd_state;
+	u32 afe_setting;
+	enum hdcp_state hdcp_status;
+	struct delayed_work hdcp_work;
+	struct work_struct hdcp_wait_ksv_list;
+	struct completion wait_edid_complete;
+	u8 auto_train_retry;
+	bool hdcp_desired;
+	bool is_repeater;
+	u8 hdcp_down_stream_count;
+	u8 bksvs[DRM_HDCP_KSV_LEN];
+	u8 sha1_input[HDCP_SHA1_FIFO_LEN];
+	bool enable_enhanced_frame;
+	hdmi_codec_plugged_cb plugged_cb;
+	struct device *codec_dev;
+	struct delayed_work delayed_audio;
+	struct it6505_audio_data audio;
+
+	/* it6505 driver hold option */
+	bool enable_drv_hold;
+};
+
+struct it6505_step_train_para {
+	u8 voltage_swing[MAX_LANE_COUNT];
+	u8 pre_emphasis[MAX_LANE_COUNT];
+};
+
+/*
+ * Vendor option afe settings for different platforms
+ * 0: without FPC cable
+ * 1: with FPC cable
+ */
+
+static u8 const afe_setting_table[][3] = {
+	{0x82, 0x00, 0x45},
+	{0x93, 0x2A, 0x85}
+};
+
+static const struct it6505_audio_sample_rate_map audio_sample_rate_map[] = {
+	{SAMPLE_RATE_24K, 24000},
+	{SAMPLE_RATE_32K, 32000},
+	{SAMPLE_RATE_48K, 48000},
+	{SAMPLE_RATE_96K, 96000},
+	{SAMPLE_RATE_192K, 192000},
+	{SAMPLE_RATE_44_1K, 44100},
+	{SAMPLE_RATE_88_2K, 88200},
+	{SAMPLE_RATE_176_4K, 176400},
+};
+
+static const struct regmap_range it6505_bridge_volatile_ranges[] = {
+	{ .range_min = 0, .range_max = 0xFF },
+};
+
+static const struct regmap_access_table it6505_bridge_volatile_table = {
+	.yes_ranges = it6505_bridge_volatile_ranges,
+	.n_yes_ranges = ARRAY_SIZE(it6505_bridge_volatile_ranges),
+};
+
+static const struct regmap_config it6505_regmap_config = {
+	.reg_bits = 8,
+	.val_bits = 8,
+	.volatile_table = &it6505_bridge_volatile_table,
+	.cache_type = REGCACHE_NONE,
+};
+
+static int it6505_read(struct it6505 *it6505, unsigned int reg_addr)
+{
+	unsigned int value;
+	int err;
+	struct device *dev = &it6505->client->dev;
+
+	err = regmap_read(it6505->regmap, reg_addr, &value);
+	if (err < 0) {
+		DRM_DEV_ERROR(dev, "read failed reg[0x%x] err: %d", reg_addr,
+			      err);
+		return err;
+	}
+
+	return value;
+}
+
+static int it6505_write(struct it6505 *it6505, unsigned int reg_addr,
+		      unsigned int reg_val)
+{
+	int err;
+	struct device *dev = &it6505->client->dev;
+
+	err = regmap_write(it6505->regmap, reg_addr, reg_val);
+
+	if (err < 0) {
+		DRM_DEV_ERROR(dev, "write failed reg[0x%x] = 0x%x err = %d",
+			      reg_addr, reg_val, err);
+		return err;
+	}
+
+	return 0;
+}
+
+static int it6505_set_bits(struct it6505 *it6505, unsigned int reg,
+			 unsigned int mask, unsigned int value)
+{
+	int err;
+	struct device *dev = &it6505->client->dev;
+
+	err = regmap_update_bits(it6505->regmap, reg, mask, value);
+	if (err < 0) {
+		DRM_DEV_ERROR(
+			dev, "write reg[0x%x] = 0x%x mask = 0x%x failed err %d",
+			reg, value, mask, err);
+		return err;
+	}
+
+	return 0;
+}
+
+static void it6505_debug_print(struct it6505 *it6505, unsigned int reg,
+			     const char *prefix)
+{
+	struct device *dev = &it6505->client->dev;
+	int val;
+
+	if (likely(!drm_debug_enabled(DRM_UT_DRIVER)))
+		return;
+
+	val = it6505_read(it6505, reg);
+	if (val < 0)
+		DRM_DEV_DEBUG_DRIVER(dev, "%s reg[%02x] read error (%d)",
+				     prefix, reg, val);
+	else
+		DRM_DEV_DEBUG_DRIVER(dev, "%s reg[%02x] = 0x%02x", prefix, reg,
+				     val);
+}
+
+static int it6505_dpcd_read(struct it6505 *it6505, unsigned long offset)
+{
+	u8 value;
+	int ret;
+	struct device *dev = &it6505->client->dev;
+
+	ret = drm_dp_dpcd_readb(&it6505->aux, offset, &value);
+	if (ret < 0) {
+		DRM_DEV_ERROR(dev, "DPCD read failed [0x%lx] ret: %d", offset,
+			      ret);
+		return ret;
+	}
+	return value;
+}
+
+static int it6505_dpcd_write(struct it6505 *it6505, unsigned long offset,
+			     unsigned long datain)
+{
+	int ret;
+	struct device *dev = &it6505->client->dev;
+
+	ret = drm_dp_dpcd_writeb(&it6505->aux, offset, datain);
+	if (ret < 0) {
+		DRM_DEV_ERROR(dev, "DPCD write failed [0x%lx] ret: %d", offset,
+			      ret);
+		return ret;
+	}
+	return 0;
+}
+
+static int it6505_get_dpcd(struct it6505 *it6505, int offset, u8 *dpcd, int num)
+{
+	int ret;
+	struct device *dev = &it6505->client->dev;
+
+	ret = drm_dp_dpcd_read(&it6505->aux, offset, dpcd, num);
+
+	if (ret < 0)
+		return ret;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "ret = %d DPCD[0x%x] = 0x%*ph", ret, offset,
+			     num, dpcd);
+
+	return 0;
+}
+
+static void it6505_dump(struct it6505 *it6505)
+{
+	unsigned int i, j;
+	u8 regs[16];
+	struct device *dev = &it6505->client->dev;
+
+	for (i = 0; i <= 0xff; i += 16) {
+		for (j = 0; j < 16; j++)
+			regs[j] = it6505_read(it6505, i + j);
+
+		DRM_DEV_DEBUG_DRIVER(dev, "[0x%02x] = %16ph", i, regs);
+	}
+}
+
+static bool it6505_get_sink_hpd_status(struct it6505 *it6505)
+{
+	int reg_0d;
+
+	reg_0d = it6505_read(it6505, REG_SYSTEM_STS);
+
+	if (reg_0d < 0)
+		return false;
+
+	return reg_0d & HPD_STS;
+}
+
+static int it6505_read_word(struct it6505 *it6505, unsigned int reg)
+{
+	int val0, val1;
+
+	val0 = it6505_read(it6505, reg);
+	if (val0 < 0)
+		return val0;
+
+	val1 = it6505_read(it6505, reg + 1);
+	if (val1 < 0)
+		return val1;
+
+	return (val1 << 8) | val0;
+}
+
+static void it6505_calc_video_info(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	int hsync_pol, vsync_pol, interlaced;
+	int htotal, hdes, hdew, hfph, hsyncw;
+	int vtotal, vdes, vdew, vfph, vsyncw;
+	int rddata, i, pclk, sum = 0;
+
+	usleep_range(10000, 15000);
+	rddata = it6505_read(it6505, REG_INPUT_CTRL);
+	hsync_pol = rddata & INPUT_HSYNC_POL;
+	vsync_pol = (rddata & INPUT_VSYNC_POL) >> 2;
+	interlaced = (rddata & INPUT_INTERLACED) >> 4;
+
+	htotal = it6505_read_word(it6505, REG_INPUT_HTOTAL) & 0x1FFF;
+	hdes = it6505_read_word(it6505, REG_INPUT_HACTIVE_START) & 0x1FFF;
+	hdew = it6505_read_word(it6505, REG_INPUT_HACTIVE_WIDTH) & 0x1FFF;
+	hfph = it6505_read_word(it6505, REG_INPUT_HFRONT_PORCH) & 0x1FFF;
+	hsyncw = it6505_read_word(it6505, REG_INPUT_HSYNC_WIDTH) & 0x1FFF;
+
+	vtotal = it6505_read_word(it6505, REG_INPUT_VTOTAL) & 0xFFF;
+	vdes = it6505_read_word(it6505, REG_INPUT_VACTIVE_START) & 0xFFF;
+	vdew = it6505_read_word(it6505, REG_INPUT_VACTIVE_WIDTH) & 0xFFF;
+	vfph = it6505_read_word(it6505, REG_INPUT_VFRONT_PORCH) & 0xFFF;
+	vsyncw = it6505_read_word(it6505, REG_INPUT_VSYNC_WIDTH) & 0xFFF;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "hsync_pol:%d, vsync_pol:%d, interlaced:%d",
+			     hsync_pol, vsync_pol, interlaced);
+	DRM_DEV_DEBUG_DRIVER(dev, "hactive_start:%d, vactive_start:%d",
+			     hdes, vdes);
+
+	for (i = 0; i < 10; i++) {
+		it6505_set_bits(it6505, REG_DATA_CTRL0, ENABLE_PCLK_COUNTER,
+				ENABLE_PCLK_COUNTER);
+		usleep_range(10000, 15000);
+		it6505_set_bits(it6505, REG_DATA_CTRL0, ENABLE_PCLK_COUNTER,
+				0x00);
+		rddata = it6505_read_word(it6505, REG_PCLK_COUNTER_VALUE) &
+			 0xFFF;
+
+		sum += rddata;
+	}
+
+	if (sum == 0) {
+		DRM_DEV_DEBUG_DRIVER(dev, "calc video timing error");
+		return;
+	}
+
+	sum /= 10;
+	pclk = 13500 * 2048 / sum;
+	it6505->video_info.clock = pclk;
+	it6505->video_info.hdisplay = hdew;
+	it6505->video_info.hsync_start = hdew + hfph;
+	it6505->video_info.hsync_end = hdew + hfph + hsyncw;
+	it6505->video_info.htotal = htotal;
+	it6505->video_info.vdisplay = vdew;
+	it6505->video_info.vsync_start = vdew + vfph;
+	it6505->video_info.vsync_end = vdew + vfph + vsyncw;
+	it6505->video_info.vtotal = vtotal;
+
+	DRM_DEV_DEBUG_DRIVER(dev, DRM_MODE_FMT,
+			     DRM_MODE_ARG(&it6505->video_info));
+}
+
+static int it6505_drm_dp_link_probe(struct drm_dp_aux *aux,
+				    struct it6505_drm_dp_link *link)
+{
+	u8 values[3];
+	int err;
+
+	memset(link, 0, sizeof(*link));
+
+	err = drm_dp_dpcd_read(aux, DP_DPCD_REV, values, sizeof(values));
+	if (err < 0)
+		return err;
+
+	link->revision = values[0];
+	link->rate = drm_dp_bw_code_to_link_rate(values[1]);
+	link->num_lanes = values[2] & DP_MAX_LANE_COUNT_MASK;
+
+	if (values[2] & DP_ENHANCED_FRAME_CAP)
+		link->capabilities = 1;
+
+	return 0;
+}
+
+static int it6505_drm_dp_link_power_up(struct drm_dp_aux *aux,
+				       struct it6505_drm_dp_link *link)
+{
+	u8 value;
+	int err;
+
+	/* DP_SET_POWER register is only available on DPCD v1.1 and later */
+	if (link->revision < 0x11)
+		return 0;
+
+	err = drm_dp_dpcd_readb(aux, DP_SET_POWER, &value);
+	if (err < 0)
+		return err;
+
+	value &= ~DP_SET_POWER_MASK;
+	value |= DP_SET_POWER_D0;
+
+	err = drm_dp_dpcd_writeb(aux, DP_SET_POWER, value);
+	if (err < 0)
+		return err;
+
+	/*
+	 * According to the DP 1.1 specification, a "Sink Device must exit the
+	 * power saving state within 1 ms" (Section 2.5.3.1, Table 5-52, "Sink
+	 * Control Field" (register 0x600).
+	 */
+	usleep_range(1000, 2000);
+
+	return 0;
+}
+
+static void it6505_clear_int(struct it6505 *it6505)
+{
+	it6505_write(it6505, INT_STATUS_01, 0xFF);
+	it6505_write(it6505, INT_STATUS_02, 0xFF);
+	it6505_write(it6505, INT_STATUS_03, 0xFF);
+}
+
+static void it6505_int_mask_enable(struct it6505 *it6505)
+{
+	it6505_write(it6505, INT_MASK_01, 0x1F);
+	it6505_write(it6505, INT_MASK_02, 0x07);
+	it6505_write(it6505, INT_MASK_03, 0xB0);
+}
+
+static void it6505_int_mask_disable(struct it6505 *it6505)
+{
+	it6505_write(it6505, INT_MASK_01, 0x00);
+	it6505_write(it6505, INT_MASK_02, 0x00);
+	it6505_write(it6505, INT_MASK_03, 0x00);
+}
+
+static void it6505_lane_termination_on(struct it6505 *it6505)
+{
+	int regcf;
+
+	regcf = it6505_read(it6505, REG_USER_DRV_PRE);
+
+	if (regcf == MISC_VERB)
+		it6505_set_bits(it6505, REG_DRV_LN_DATA_SEL, 0x80, 0x00);
+
+	if (regcf == MISC_VERC) {
+		if (it6505->lane_swap) {
+			switch (it6505->lane_count) {
+			case 1:
+			case 2:
+				it6505_set_bits(it6505, REG_DRV_LN_DATA_SEL,
+						0x0C, 0x08);
+				break;
+			default:
+				it6505_set_bits(it6505, REG_DRV_LN_DATA_SEL,
+						0x0C, 0x0C);
+				break;
+			}
+		} else {
+			switch (it6505->lane_count) {
+			case 1:
+			case 2:
+				it6505_set_bits(it6505, REG_DRV_LN_DATA_SEL,
+						0x0C, 0x04);
+				break;
+			default:
+				it6505_set_bits(it6505, REG_DRV_LN_DATA_SEL,
+						0x0C, 0x0C);
+				break;
+			}
+		}
+	}
+}
+
+static void it6505_lane_termination_off(struct it6505 *it6505)
+{
+	int regcf;
+
+	regcf = it6505_read(it6505, REG_USER_DRV_PRE);
+
+	if (regcf == MISC_VERB)
+		it6505_set_bits(it6505, REG_DRV_LN_DATA_SEL, 0x80, 0x80);
+
+	if (regcf == MISC_VERC)
+		it6505_set_bits(it6505, REG_DRV_LN_DATA_SEL, 0x0C, 0x00);
+}
+
+static void it6505_lane_power_on(struct it6505 *it6505)
+{
+	it6505_set_bits(it6505, REG_LINK_DRV, 0xF1,
+			(it6505->lane_swap ?
+				 GENMASK(7, 8 - it6505->lane_count) :
+				 GENMASK(3 + it6505->lane_count, 4)) |
+				0x01);
+}
+
+static void it6505_lane_power_off(struct it6505 *it6505)
+{
+	it6505_set_bits(it6505, REG_LINK_DRV, 0xF0, 0x00);
+}
+
+static void it6505_lane_off(struct it6505 *it6505)
+{
+	it6505_lane_power_off(it6505);
+	it6505_lane_termination_off(it6505);
+}
+
+static void it6505_aux_termination_on(struct it6505 *it6505)
+{
+	int regcf;
+
+	regcf = it6505_read(it6505, REG_USER_DRV_PRE);
+
+	if (regcf == MISC_VERB)
+		it6505_lane_termination_on(it6505);
+
+	if (regcf == MISC_VERC)
+		it6505_set_bits(it6505, REG_DRV_LN_DATA_SEL, 0x80, 0x80);
+}
+
+static void it6505_aux_power_on(struct it6505 *it6505)
+{
+	it6505_set_bits(it6505, REG_AUX, 0x02, 0x02);
+}
+
+static void it6505_aux_on(struct it6505 *it6505)
+{
+	it6505_aux_power_on(it6505);
+	it6505_aux_termination_on(it6505);
+}
+
+static void it6505_aux_reset(struct it6505 *it6505)
+{
+	it6505_set_bits(it6505, REG_RESET_CTRL, AUX_RESET, AUX_RESET);
+	it6505_set_bits(it6505, REG_RESET_CTRL, AUX_RESET, 0x00);
+}
+
+static void it6505_reset_logic(struct it6505 *it6505)
+{
+	regmap_write(it6505->regmap, REG_RESET_CTRL, ALL_LOGIC_RESET);
+	usleep_range(1000, 1500);
+}
+
+static bool it6505_aux_op_finished(struct it6505 *it6505)
+{
+	int reg2b = it6505_read(it6505, REG_AUX_CMD_REQ);
+
+	if (reg2b < 0)
+		return false;
+
+	return (reg2b & AUX_BUSY) == 0;
+}
+
+static int it6505_aux_wait(struct it6505 *it6505)
+{
+	int status;
+	unsigned long timeout;
+	struct device *dev = &it6505->client->dev;
+
+	timeout = jiffies + msecs_to_jiffies(AUX_WAIT_TIMEOUT_MS) + 1;
+
+	while (!it6505_aux_op_finished(it6505)) {
+		if (time_after(jiffies, timeout)) {
+			DRM_DEV_ERROR(dev, "Timed out waiting AUX to finish");
+			return -ETIMEDOUT;
+		}
+		usleep_range(1000, 2000);
+	}
+
+	status = it6505_read(it6505, REG_AUX_ERROR_STS);
+	if (status < 0) {
+		DRM_DEV_ERROR(dev, "Failed to read AUX channel: %d", status);
+		return status;
+	}
+
+	return 0;
+}
+
+static ssize_t it6505_aux_operation(struct it6505 *it6505,
+				    enum aux_cmd_type cmd,
+				    unsigned int address, u8 *buffer,
+				    size_t size, enum aux_cmd_reply *reply)
+{
+	int i, ret;
+	bool aux_write_check = false;
+
+	if (!it6505_get_sink_hpd_status(it6505))
+		return -EIO;
+
+	/* set AUX user mode */
+	it6505_set_bits(it6505, REG_AUX_CTRL, AUX_USER_MODE, AUX_USER_MODE);
+
+aux_op_start:
+	if (cmd == CMD_AUX_I2C_EDID_READ) {
+		/* AUX EDID FIFO has max length of AUX_FIFO_MAX_SIZE bytes. */
+		size = min_t(size_t, size, AUX_FIFO_MAX_SIZE);
+		/* Enable AUX FIFO read back and clear FIFO */
+		it6505_set_bits(it6505, REG_AUX_CTRL,
+				AUX_EN_FIFO_READ | CLR_EDID_FIFO,
+				AUX_EN_FIFO_READ | CLR_EDID_FIFO);
+
+		it6505_set_bits(it6505, REG_AUX_CTRL,
+				AUX_EN_FIFO_READ | CLR_EDID_FIFO,
+				AUX_EN_FIFO_READ);
+	} else {
+		/* The DP AUX transmit buffer has 4 bytes. */
+		size = min_t(size_t, size, 4);
+		it6505_set_bits(it6505, REG_AUX_CTRL, AUX_NO_SEGMENT_WR,
+				AUX_NO_SEGMENT_WR);
+	}
+
+	/* Start Address[7:0] */
+	it6505_write(it6505, REG_AUX_ADR_0_7, (address >> 0) & 0xFF);
+	/* Start Address[15:8] */
+	it6505_write(it6505, REG_AUX_ADR_8_15, (address >> 8) & 0xFF);
+	/* WriteNum[3:0]+StartAdr[19:16] */
+	it6505_write(it6505, REG_AUX_ADR_16_19,
+		     ((address >> 16) & 0x0F) | ((size - 1) << 4));
+
+	if (cmd == CMD_AUX_NATIVE_WRITE)
+		regmap_bulk_write(it6505->regmap, REG_AUX_OUT_DATA0, buffer,
+				  size);
+
+	/* Aux Fire */
+	it6505_write(it6505, REG_AUX_CMD_REQ, cmd);
+
+	ret = it6505_aux_wait(it6505);
+	if (ret < 0)
+		goto aux_op_err;
+
+	ret = it6505_read(it6505, REG_AUX_ERROR_STS);
+	if (ret < 0)
+		goto aux_op_err;
+
+	switch ((ret >> 6) & 0x3) {
+	case 0:
+		*reply = REPLY_ACK;
+		break;
+	case 1:
+		*reply = REPLY_DEFER;
+		ret = -EAGAIN;
+		goto aux_op_err;
+	case 2:
+		*reply = REPLY_NACK;
+		ret = -EIO;
+		goto aux_op_err;
+	case 3:
+		ret = -ETIMEDOUT;
+		goto aux_op_err;
+	}
+
+	/* Read back Native Write data */
+	if (cmd == CMD_AUX_NATIVE_WRITE) {
+		aux_write_check = true;
+		cmd = CMD_AUX_NATIVE_READ;
+		goto aux_op_start;
+	}
+
+	if (cmd == CMD_AUX_I2C_EDID_READ) {
+		for (i = 0; i < size; i++) {
+			ret = it6505_read(it6505, REG_AUX_DATA_FIFO);
+			if (ret < 0)
+				goto aux_op_err;
+			buffer[i] = ret;
+		}
+	} else {
+		for (i = 0; i < size; i++) {
+			ret = it6505_read(it6505, REG_AUX_DATA_0_7 + i);
+			if (ret < 0)
+				goto aux_op_err;
+
+			if (aux_write_check && buffer[size - 1 - i] != ret) {
+				ret = -EINVAL;
+				goto aux_op_err;
+			}
+
+			buffer[size - 1 - i] = ret;
+		}
+	}
+
+	ret = i;
+
+aux_op_err:
+	if (cmd == CMD_AUX_I2C_EDID_READ) {
+		/* clear AUX FIFO */
+		it6505_set_bits(it6505, REG_AUX_CTRL,
+				AUX_EN_FIFO_READ | CLR_EDID_FIFO,
+				AUX_EN_FIFO_READ | CLR_EDID_FIFO);
+		it6505_set_bits(it6505, REG_AUX_CTRL,
+				AUX_EN_FIFO_READ | CLR_EDID_FIFO, 0x00);
+	}
+
+	/* Leave AUX user mode */
+	it6505_set_bits(it6505, REG_AUX_CTRL, AUX_USER_MODE, 0);
+
+	return ret;
+}
+
+static ssize_t it6505_aux_do_transfer(struct it6505 *it6505,
+				      enum aux_cmd_type cmd,
+				      unsigned int address, u8 *buffer,
+				      size_t size, enum aux_cmd_reply *reply)
+{
+	int i, ret_size, ret = 0, request_size;
+
+	mutex_lock(&it6505->aux_lock);
+	for (i = 0; i < size; i += 4) {
+		request_size = min((int)size - i, 4);
+		ret_size = it6505_aux_operation(it6505, cmd, address + i,
+						buffer + i, request_size,
+						reply);
+		if (ret_size < 0) {
+			ret = ret_size;
+			goto aux_op_err;
+		}
+
+		ret += ret_size;
+	}
+
+aux_op_err:
+	mutex_unlock(&it6505->aux_lock);
+	return ret;
+}
+
+static ssize_t it6505_aux_transfer(struct drm_dp_aux *aux,
+				   struct drm_dp_aux_msg *msg)
+{
+	struct it6505 *it6505 = container_of(aux, struct it6505, aux);
+	u8 cmd;
+	bool is_i2c = !(msg->request & DP_AUX_NATIVE_WRITE);
+	int ret;
+	enum aux_cmd_reply reply;
+
+	/* IT6505 doesn't support arbitrary I2C read / write. */
+	if (is_i2c)
+		return -EINVAL;
+
+	switch (msg->request) {
+	case DP_AUX_NATIVE_READ:
+		cmd = CMD_AUX_NATIVE_READ;
+		break;
+	case DP_AUX_NATIVE_WRITE:
+		cmd = CMD_AUX_NATIVE_WRITE;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	ret = it6505_aux_do_transfer(it6505, cmd, msg->address, msg->buffer,
+				     msg->size, &reply);
+	if (ret < 0)
+		return ret;
+
+	switch (reply) {
+	case REPLY_ACK:
+		msg->reply = DP_AUX_NATIVE_REPLY_ACK;
+		break;
+	case REPLY_NACK:
+		msg->reply = DP_AUX_NATIVE_REPLY_NACK;
+		break;
+	case REPLY_DEFER:
+		msg->reply = DP_AUX_NATIVE_REPLY_DEFER;
+		break;
+	}
+
+	return ret;
+}
+
+static int it6505_get_edid_block(void *data, u8 *buf, unsigned int block,
+				 size_t len)
+{
+	struct it6505 *it6505 = data;
+	struct device *dev = &it6505->client->dev;
+	enum aux_cmd_reply reply;
+	int offset, ret, aux_retry = 100;
+
+	it6505_aux_reset(it6505);
+	DRM_DEV_DEBUG_DRIVER(dev, "block number = %d", block);
+
+	for (offset = 0; offset < EDID_LENGTH;) {
+		ret = it6505_aux_do_transfer(it6505, CMD_AUX_I2C_EDID_READ,
+					     block * EDID_LENGTH + offset,
+					     buf + offset, 8, &reply);
+
+		if (ret < 0 && ret != -EAGAIN)
+			return ret;
+
+		switch (reply) {
+		case REPLY_ACK:
+			DRM_DEV_DEBUG_DRIVER(dev, "[0x%02x]: %8ph", offset,
+					     buf + offset);
+			offset += 8;
+			aux_retry = 100;
+			break;
+		case REPLY_NACK:
+			return -EIO;
+		case REPLY_DEFER:
+			msleep(20);
+			if (!(--aux_retry))
+				return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+static void it6505_variable_config(struct it6505 *it6505)
+{
+	it6505->link_rate_bw_code = HBR;
+	it6505->lane_count = MAX_LANE_COUNT;
+	it6505->link_state = LINK_IDLE;
+	it6505->hdcp_desired = HDCP_DESIRED;
+	it6505->auto_train_retry = AUTO_TRAIN_RETRY;
+	it6505->audio.select = AUDIO_SELECT;
+	it6505->audio.sample_rate = AUDIO_SAMPLE_RATE;
+	it6505->audio.channel_count = AUDIO_CHANNEL_COUNT;
+	it6505->audio.type = AUDIO_TYPE;
+	it6505->audio.i2s_input_format = I2S_INPUT_FORMAT;
+	it6505->audio.i2s_justified = I2S_JUSTIFIED;
+	it6505->audio.i2s_data_delay = I2S_DATA_DELAY;
+	it6505->audio.i2s_ws_channel = I2S_WS_CHANNEL;
+	it6505->audio.i2s_data_sequence = I2S_DATA_SEQUENCE;
+	it6505->audio.word_length = AUDIO_WORD_LENGTH;
+	memset(it6505->sha1_input, 0, sizeof(it6505->sha1_input));
+	memset(it6505->bksvs, 0, sizeof(it6505->bksvs));
+}
+
+static int it6505_send_video_infoframe(struct it6505 *it6505,
+				       struct hdmi_avi_infoframe *frame)
+{
+	u8 buffer[HDMI_INFOFRAME_HEADER_SIZE + HDMI_AVI_INFOFRAME_SIZE];
+	int err;
+	struct device *dev = &it6505->client->dev;
+
+	err = hdmi_avi_infoframe_pack(frame, buffer, sizeof(buffer));
+	if (err < 0) {
+		DRM_DEV_ERROR(dev, "Failed to pack AVI infoframe: %d", err);
+		return err;
+	}
+
+	err = it6505_set_bits(it6505, REG_INFOFRAME_CTRL, EN_AVI_PKT, 0x00);
+	if (err)
+		return err;
+
+	err = regmap_bulk_write(it6505->regmap, REG_AVI_INFO_DB1,
+				buffer + HDMI_INFOFRAME_HEADER_SIZE,
+				frame->length);
+	if (err)
+		return err;
+
+	err = it6505_set_bits(it6505, REG_INFOFRAME_CTRL, EN_AVI_PKT,
+			      EN_AVI_PKT);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static void it6505_get_extcon_property(struct it6505 *it6505)
+{
+	int err;
+	union extcon_property_value property;
+	struct device *dev = &it6505->client->dev;
+
+	if (it6505->extcon && !it6505->lane_swap_disabled) {
+		err = extcon_get_property(it6505->extcon, EXTCON_DISP_DP,
+					  EXTCON_PROP_USB_TYPEC_POLARITY,
+					  &property);
+		if (err) {
+			DRM_DEV_ERROR(dev, "get property fail!");
+			return;
+		}
+		it6505->lane_swap = property.intval;
+	}
+}
+
+static void it6505_clk_phase_adjustment(struct it6505 *it6505,
+					const struct drm_display_mode *mode)
+{
+	it6505_set_bits(it6505, REG_CLK_CTRL0, M_PCLK_DELAY,
+		mode->clock < ADJUST_PHASE_THRESHOLD ? PIXEL_CLK_DELAY : 0);
+	it6505_set_bits(it6505, REG_DATA_CTRL0, VIDEO_LATCH_EDGE,
+			PIXEL_CLK_INVERSE << 4);
+}
+
+static void it6505_link_reset_step_train(struct it6505 *it6505)
+{
+	it6505_set_bits(it6505, REG_TRAIN_CTRL0,
+			FORCE_CR_DONE | FORCE_EQ_DONE, 0x00);
+	it6505_dpcd_write(it6505, DP_TRAINING_PATTERN_SET,
+			  DP_TRAINING_PATTERN_DISABLE);
+}
+
+static void it6505_init(struct it6505 *it6505)
+{
+	it6505_write(it6505, REG_AUX_OPT, AUX_AUTO_RST | AUX_FIX_FREQ);
+	it6505_write(it6505, REG_AUX_CTRL, AUX_NO_SEGMENT_WR);
+	it6505_write(it6505, REG_HDCP_CTRL2, HDCP_AN_SEL | HDCP_HW_HPDIRQ_ACT);
+	it6505_write(it6505, REG_VID_BUS_CTRL0, IN_DDR | DDR_CD);
+	it6505_write(it6505, REG_VID_BUS_CTRL1, 0x01);
+	it6505_write(it6505, REG_AUDIO_CTRL0, AUDIO_16B_BOUND);
+
+	/* chip internal setting, don't modify */
+	it6505_write(it6505, REG_HPD_IRQ_TIME, 0xF5);
+	it6505_write(it6505, REG_AUX_DEBUG_MODE, 0x4D);
+	it6505_write(it6505, REG_AUX_OPT2, 0x17);
+	it6505_write(it6505, REG_HDCP_OPT, 0x60);
+	it6505_write(it6505, REG_DATA_MUTE_CTRL,
+		     EN_VID_MUTE | EN_AUD_MUTE | ENABLE_AUTO_VIDEO_FIFO_RESET);
+	it6505_write(it6505, REG_TIME_STMP_CTRL,
+		     EN_SSC_GAT | EN_ENHANCE_VID_STMP | EN_ENHANCE_AUD_STMP);
+	it6505_write(it6505, REG_INFOFRAME_CTRL, 0x00);
+	it6505_write(it6505, REG_BANK_SEL, 0x01);
+	it6505_write(it6505, REG_DRV_0_DB_800_MV,
+		     afe_setting_table[it6505->afe_setting][0]);
+	it6505_write(it6505, REG_PRE_0_DB_800_MV,
+		     afe_setting_table[it6505->afe_setting][1]);
+	it6505_write(it6505, REG_PRE_3P5_DB_800_MV,
+		     afe_setting_table[it6505->afe_setting][2]);
+	it6505_write(it6505, REG_SSC_CTRL0, 0x9E);
+	it6505_write(it6505, REG_SSC_CTRL1, 0x1C);
+	it6505_write(it6505, REG_SSC_CTRL2, 0x42);
+	it6505_write(it6505, REG_BANK_SEL, 0x00);
+}
+
+static void it6505_video_disable(struct it6505 *it6505)
+{
+	it6505_set_bits(it6505, REG_DATA_MUTE_CTRL, EN_VID_MUTE, EN_VID_MUTE);
+	it6505_set_bits(it6505, REG_INFOFRAME_CTRL, EN_VID_CTRL_PKT, 0x00);
+	it6505_set_bits(it6505, REG_RESET_CTRL, VIDEO_RESET, VIDEO_RESET);
+}
+
+static void it6505_video_reset(struct it6505 *it6505)
+{
+	it6505_link_reset_step_train(it6505);
+	it6505_set_bits(it6505, REG_DATA_MUTE_CTRL, EN_VID_MUTE, EN_VID_MUTE);
+	it6505_set_bits(it6505, REG_INFOFRAME_CTRL, EN_VID_CTRL_PKT, 0x00);
+	it6505_set_bits(it6505, REG_RESET_CTRL, VIDEO_RESET, VIDEO_RESET);
+	it6505_set_bits(it6505, REG_501_FIFO_CTRL, RST_501_FIFO, RST_501_FIFO);
+	it6505_set_bits(it6505, REG_501_FIFO_CTRL, RST_501_FIFO, 0x00);
+	it6505_set_bits(it6505, REG_RESET_CTRL, VIDEO_RESET, 0x00);
+}
+
+static void it6505_update_video_parameter(struct it6505 *it6505,
+					  const struct drm_display_mode *mode)
+{
+	it6505_clk_phase_adjustment(it6505, mode);
+	it6505_video_disable(it6505);
+}
+
+static bool it6505_audio_input(struct it6505 *it6505)
+{
+	int reg05, regbe;
+
+	reg05 = it6505_read(it6505, REG_RESET_CTRL);
+	it6505_set_bits(it6505, REG_RESET_CTRL, AUDIO_RESET, 0x00);
+	usleep_range(3000, 4000);
+	regbe = it6505_read(it6505, REG_AUDIO_INPUT_FREQ);
+	it6505_write(it6505, REG_RESET_CTRL, reg05);
+
+	return regbe != 0xFF;
+}
+
+static void it6505_setup_audio_channel_status(struct it6505 *it6505)
+{
+	enum it6505_audio_sample_rate sample_rate = it6505->audio.sample_rate;
+	u8 audio_word_length_map[] = { 0x02, 0x04, 0x03, 0x0B };
+
+	/* Channel Status */
+	it6505_write(it6505, REG_IEC958_STS0, it6505->audio.type << 1);
+	it6505_write(it6505, REG_IEC958_STS1, 0x00);
+	it6505_write(it6505, REG_IEC958_STS2, 0x00);
+	it6505_write(it6505, REG_IEC958_STS3, sample_rate);
+	it6505_write(it6505, REG_IEC958_STS4, (~sample_rate << 4) |
+		     audio_word_length_map[it6505->audio.word_length]);
+}
+
+static void it6505_setup_audio_format(struct it6505 *it6505)
+{
+	/* I2S MODE */
+	it6505_write(it6505, REG_AUDIO_FMT,
+		     (it6505->audio.word_length << 5) |
+		     (it6505->audio.i2s_data_sequence << 4) |
+		     (it6505->audio.i2s_ws_channel << 3) |
+		     (it6505->audio.i2s_data_delay << 2) |
+		     (it6505->audio.i2s_justified << 1) |
+		     it6505->audio.i2s_input_format);
+	if (it6505->audio.select == SPDIF) {
+		it6505_write(it6505, REG_AUDIO_FIFO_SEL, 0x00);
+		/* 0x30 = 128*FS */
+		it6505_set_bits(it6505, REG_AUX_OPT, 0xF0, 0x30);
+	} else {
+		it6505_write(it6505, REG_AUDIO_FIFO_SEL, 0xE4);
+	}
+
+	it6505_write(it6505, REG_AUDIO_CTRL0, 0x20);
+	it6505_write(it6505, REG_AUDIO_CTRL1, 0x00);
+}
+
+static void it6505_enable_audio_source(struct it6505 *it6505)
+{
+	unsigned int audio_source_count;
+
+	audio_source_count = BIT(DIV_ROUND_UP(it6505->audio.channel_count, 2))
+				 - 1;
+
+	audio_source_count |= it6505->audio.select << 4;
+
+	it6505_write(it6505, REG_AUDIO_SRC_CTRL, audio_source_count);
+}
+
+static void it6505_enable_audio_infoframe(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	u8 audio_info_ca[] = { 0x00, 0x00, 0x01, 0x03, 0x07, 0x0B, 0x0F, 0x1F };
+
+	DRM_DEV_DEBUG_DRIVER(dev, "infoframe channel_allocation:0x%02x",
+			     audio_info_ca[it6505->audio.channel_count - 1]);
+
+	it6505_write(it6505, REG_AUD_INFOFRAM_DB1, it6505->audio.channel_count
+		     - 1);
+	it6505_write(it6505, REG_AUD_INFOFRAM_DB2, 0x00);
+	it6505_write(it6505, REG_AUD_INFOFRAM_DB3,
+		     audio_info_ca[it6505->audio.channel_count - 1]);
+	it6505_write(it6505, REG_AUD_INFOFRAM_DB4, 0x00);
+	it6505_write(it6505, REG_AUD_INFOFRAM_SUM, 0x00);
+
+	/* Enable Audio InfoFrame */
+	it6505_set_bits(it6505, REG_INFOFRAME_CTRL, EN_AUD_CTRL_PKT,
+			EN_AUD_CTRL_PKT);
+}
+
+static void it6505_disable_audio(struct it6505 *it6505)
+{
+	it6505_set_bits(it6505, REG_DATA_MUTE_CTRL, EN_AUD_MUTE, EN_AUD_MUTE);
+	it6505_set_bits(it6505, REG_AUDIO_SRC_CTRL, M_AUDIO_I2S_EN, 0x00);
+	it6505_set_bits(it6505, REG_INFOFRAME_CTRL, EN_AUD_CTRL_PKT, 0x00);
+	it6505_set_bits(it6505, REG_RESET_CTRL, AUDIO_RESET, AUDIO_RESET);
+}
+
+static void it6505_enable_audio(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	int regbe;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "start");
+	it6505_disable_audio(it6505);
+
+	it6505_setup_audio_channel_status(it6505);
+	it6505_setup_audio_format(it6505);
+	it6505_enable_audio_source(it6505);
+	it6505_enable_audio_infoframe(it6505);
+
+	it6505_write(it6505, REG_AUDIO_N_0_7, 0x00);
+	it6505_write(it6505, REG_AUDIO_N_8_15, 0x80);
+	it6505_write(it6505, REG_AUDIO_N_16_23, 0x00);
+
+	it6505_set_bits(it6505, REG_AUDIO_SRC_CTRL, AUDIO_FIFO_RESET,
+			AUDIO_FIFO_RESET);
+	it6505_set_bits(it6505, REG_AUDIO_SRC_CTRL, AUDIO_FIFO_RESET, 0x00);
+	it6505_set_bits(it6505, REG_RESET_CTRL, AUDIO_RESET, 0x00);
+	regbe = it6505_read(it6505, REG_AUDIO_INPUT_FREQ);
+	DRM_DEV_DEBUG_DRIVER(dev, "regbe:0x%02x audio input fs: %d.%d kHz",
+			     regbe, 6750 / regbe, (6750 % regbe) * 10 / regbe);
+	it6505_set_bits(it6505, REG_DATA_MUTE_CTRL, EN_AUD_MUTE, 0x00);
+}
+
+static bool it6505_use_step_train_check(struct it6505 *it6505)
+{
+	if (it6505->link.revision >= 0x12)
+		return it6505->dpcd[DP_TRAINING_AUX_RD_INTERVAL] >= 0x01;
+
+	return true;
+}
+
+static void it6505_parse_link_capabilities(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	struct it6505_drm_dp_link *link = &it6505->link;
+	int bcaps;
+
+	if (it6505->dpcd[0] == 0) {
+		it6505_aux_on(it6505);
+		it6505_get_dpcd(it6505, DP_DPCD_REV, it6505->dpcd,
+				ARRAY_SIZE(it6505->dpcd));
+	}
+
+	DRM_DEV_DEBUG_DRIVER(dev, "DPCD Rev.: %d.%d",
+			     link->revision >> 4, link->revision & 0x0F);
+
+	DRM_DEV_DEBUG_DRIVER(dev, "Sink max link rate: %d.%02d Gbps per lane",
+			     link->rate / 100000, link->rate / 1000 % 100);
+
+	it6505->link_rate_bw_code = drm_dp_link_rate_to_bw_code(link->rate);
+	DRM_DEV_DEBUG_DRIVER(dev, "link rate bw code:0x%02x",
+			     it6505->link_rate_bw_code);
+	it6505->link_rate_bw_code = min((int)it6505->link_rate_bw_code,
+					MAX_LINK_RATE);
+
+	it6505->lane_count = link->num_lanes;
+	DRM_DEV_DEBUG_DRIVER(dev, "Sink support %d lanes training",
+			     it6505->lane_count);
+	it6505->lane_count = min((int)it6505->lane_count, MAX_LANE_COUNT);
+
+	it6505->branch_device = drm_dp_is_branch(it6505->dpcd);
+	DRM_DEV_DEBUG_DRIVER(dev, "Sink %sbranch device",
+			     it6505->branch_device ? "" : "Not ");
+
+	it6505->enable_enhanced_frame = link->capabilities;
+	DRM_DEV_DEBUG_DRIVER(dev, "Sink %sSupport Enhanced Framing",
+			     it6505->enable_enhanced_frame ? "" : "Not ");
+
+	it6505->enable_ssc = (it6505->dpcd[DP_MAX_DOWNSPREAD] &
+				DP_MAX_DOWNSPREAD_0_5);
+	DRM_DEV_DEBUG_DRIVER(dev, "Maximum Down-Spread: %s, %ssupport SSC!",
+			     it6505->enable_ssc ? "0.5" : "0",
+			     it6505->enable_ssc ? "" : "Not ");
+
+	it6505->step_train = it6505_use_step_train_check(it6505);
+	if (it6505->step_train)
+		DRM_DEV_DEBUG_DRIVER(dev, "auto train fail, will step train");
+
+	bcaps = it6505_dpcd_read(it6505, DP_AUX_HDCP_BCAPS);
+	DRM_DEV_DEBUG_DRIVER(dev, "bcaps:0x%02x", bcaps);
+	if (bcaps & DP_BCAPS_HDCP_CAPABLE) {
+		it6505->is_repeater = (bcaps & DP_BCAPS_REPEATER_PRESENT);
+		DRM_DEV_DEBUG_DRIVER(dev, "Support HDCP! Downstream is %s!",
+				     it6505->is_repeater ? "repeater" :
+				     "receiver");
+	} else {
+		DRM_DEV_DEBUG_DRIVER(dev, "Sink not support HDCP!");
+		it6505->hdcp_desired = false;
+	}
+	DRM_DEV_DEBUG_DRIVER(dev, "HDCP %s",
+			     it6505->hdcp_desired ? "desired" : "undesired");
+}
+
+static void it6505_setup_ssc(struct it6505 *it6505)
+{
+	it6505_set_bits(it6505, REG_TRAIN_CTRL0, SPREAD_AMP_5,
+			it6505->enable_ssc ? SPREAD_AMP_5 : 0x00);
+	if (it6505->enable_ssc) {
+		it6505_write(it6505, REG_BANK_SEL, 0x01);
+		it6505_write(it6505, REG_SSC_CTRL0, 0x9E);
+		it6505_write(it6505, REG_SSC_CTRL1, 0x1C);
+		it6505_write(it6505, REG_SSC_CTRL2, 0x42);
+		it6505_write(it6505, REG_BANK_SEL, 0x00);
+		it6505_write(it6505, REG_SP_CTRL0, 0x07);
+		it6505_write(it6505, REG_IP_CTRL1, 0x29);
+		it6505_write(it6505, REG_IP_CTRL2, 0x03);
+		/* Stamp Interrupt Step */
+		it6505_set_bits(it6505, REG_TIME_STMP_CTRL, M_STAMP_STEP,
+				0x10);
+		it6505_dpcd_write(it6505, DP_DOWNSPREAD_CTRL,
+				  DP_SPREAD_AMP_0_5);
+	} else {
+		it6505_dpcd_write(it6505, DP_DOWNSPREAD_CTRL, 0x00);
+		it6505_set_bits(it6505, REG_TIME_STMP_CTRL, M_STAMP_STEP,
+				0x00);
+	}
+}
+
+static inline void it6505_link_rate_setup(struct it6505 *it6505)
+{
+	it6505_set_bits(it6505, REG_TRAIN_CTRL0, FORCE_LBR,
+			(it6505->link_rate_bw_code == RBR) ? FORCE_LBR : 0x00);
+	it6505_set_bits(it6505, REG_LINK_DRV, DRV_HS,
+			(it6505->link_rate_bw_code == RBR) ? 0x00 : DRV_HS);
+}
+
+static void it6505_lane_count_setup(struct it6505 *it6505)
+{
+	it6505_get_extcon_property(it6505);
+	it6505_set_bits(it6505, REG_TRAIN_CTRL0, LANE_SWAP,
+			it6505->lane_swap ? LANE_SWAP : 0x00);
+	it6505_set_bits(it6505, REG_TRAIN_CTRL0, LANE_COUNT_MASK,
+			(it6505->lane_count - 1) << 1);
+}
+
+static void it6505_link_training_setup(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	if (it6505->enable_enhanced_frame)
+		it6505_set_bits(it6505, REG_DATA_MUTE_CTRL,
+				ENABLE_ENHANCED_FRAME, ENABLE_ENHANCED_FRAME);
+
+	it6505_link_rate_setup(it6505);
+	it6505_lane_count_setup(it6505);
+	it6505_setup_ssc(it6505);
+	DRM_DEV_DEBUG_DRIVER(dev,
+			     "%s, %d lanes, %sable ssc, %sable enhanced frame",
+			     it6505->link_rate_bw_code != RBR ? "HBR" : "RBR",
+			     it6505->lane_count,
+			     it6505->enable_ssc ? "en" : "dis",
+			     it6505->enable_enhanced_frame ? "en" : "dis");
+}
+
+static bool it6505_link_start_auto_train(struct it6505 *it6505)
+{
+	int timeout = 500, link_training_state;
+	bool state = false;
+
+	mutex_lock(&it6505->aux_lock);
+	it6505_set_bits(it6505, REG_TRAIN_CTRL0,
+				FORCE_CR_DONE | FORCE_EQ_DONE, 0x00);
+	it6505_write(it6505, REG_TRAIN_CTRL1, FORCE_RETRAIN);
+	it6505_write(it6505, REG_TRAIN_CTRL1, AUTO_TRAIN);
+
+	while (timeout > 0) {
+		usleep_range(1000, 2000);
+		link_training_state = it6505_read(it6505, REG_LINK_TRAIN_STS);
+
+		if ((link_training_state > 0) &&
+		    (link_training_state & LINK_STATE_NORP)) {
+			state = true;
+			goto unlock;
+		}
+
+		timeout--;
+	}
+unlock:
+	mutex_unlock(&it6505->aux_lock);
+
+	return state;
+}
+
+static int it6505_drm_dp_link_configure(struct it6505 *it6505)
+{
+	u8 values[2];
+	int err;
+	struct drm_dp_aux *aux = &it6505->aux;
+
+	values[0] = it6505->link_rate_bw_code;
+	values[1] = it6505->lane_count;
+
+	if (it6505->enable_enhanced_frame)
+		values[1] |= DP_LANE_COUNT_ENHANCED_FRAME_EN;
+
+	err = drm_dp_dpcd_write(aux, DP_LINK_BW_SET, values, sizeof(values));
+	if (err < 0)
+		return err;
+
+	return 0;
+}
+
+static bool it6505_check_voltage_swing_max(u8 lane_voltage_swing_pre_emphasis)
+{
+	return ((lane_voltage_swing_pre_emphasis & 0x03) == MAX_CR_LEVEL);
+}
+
+static bool it6505_check_pre_emphasis_max(u8 lane_voltage_swing_pre_emphasis)
+{
+	return ((lane_voltage_swing_pre_emphasis & 0x03) == MAX_EQ_LEVEL);
+}
+
+static bool it6505_check_max_voltage_swing_reached(u8 *lane_voltage_swing,
+						   u8 lane_count)
+{
+	u8 i;
+
+	for (i = 0; i < lane_count; i++) {
+		if (lane_voltage_swing[i] & DP_TRAIN_MAX_SWING_REACHED)
+			return true;
+	}
+
+	return false;
+}
+
+static bool it6505_step_train_lane_voltage_pre_emphasis_set(
+	struct it6505 *it6505,
+	struct it6505_step_train_para *lane_voltage_pre_emphasis,
+	u8 *lane_voltage_pre_emphasis_set)
+{
+	u8 i;
+
+	for (i = 0; i < it6505->lane_count; i++) {
+		lane_voltage_pre_emphasis->voltage_swing[i] &= 0x03;
+		lane_voltage_pre_emphasis_set[i] =
+			lane_voltage_pre_emphasis->voltage_swing[i];
+		if (it6505_check_voltage_swing_max(
+			    lane_voltage_pre_emphasis->voltage_swing[i]))
+			lane_voltage_pre_emphasis_set[i] |=
+				DP_TRAIN_MAX_SWING_REACHED;
+
+		lane_voltage_pre_emphasis->pre_emphasis[i] &= 0x03;
+		lane_voltage_pre_emphasis_set[i] |=
+			lane_voltage_pre_emphasis->pre_emphasis[i]
+			<< DP_TRAIN_PRE_EMPHASIS_SHIFT;
+		if (it6505_check_pre_emphasis_max(
+			    lane_voltage_pre_emphasis->pre_emphasis[i]))
+			lane_voltage_pre_emphasis_set[i] |=
+				DP_TRAIN_MAX_PRE_EMPHASIS_REACHED;
+		it6505_dpcd_write(it6505, DP_TRAINING_LANE0_SET + i,
+				  lane_voltage_pre_emphasis_set[i]);
+
+		if (lane_voltage_pre_emphasis_set[i] !=
+		    it6505_dpcd_read(it6505, DP_TRAINING_LANE0_SET + i))
+			return false;
+	}
+
+	return true;
+}
+
+static bool
+it6505_step_cr_train(struct it6505 *it6505,
+		     struct it6505_step_train_para *lane_voltage_pre_emphasis)
+{
+	u8 loop_count = 0, i = 0, j;
+	u8 link_status[DP_LINK_STATUS_SIZE] = { 0 };
+	u8 lane_level_config[MAX_LANE_COUNT] = { 0 };
+	int pre_emphasis_adjust = -1, voltage_swing_adjust = -1;
+	const struct drm_dp_aux *aux = &it6505->aux;
+
+	it6505_dpcd_write(it6505, DP_DOWNSPREAD_CTRL,
+			  it6505->enable_ssc ? DP_SPREAD_AMP_0_5 : 0x00);
+	it6505_dpcd_write(it6505, DP_TRAINING_PATTERN_SET,
+			  DP_TRAINING_PATTERN_1);
+
+	while (loop_count < 5 && i < 10) {
+		i++;
+		if (!it6505_step_train_lane_voltage_pre_emphasis_set(
+			    it6505, lane_voltage_pre_emphasis,
+			    lane_level_config))
+			continue;
+		drm_dp_link_train_clock_recovery_delay(aux, it6505->dpcd);
+		drm_dp_dpcd_read_link_status(&it6505->aux, link_status);
+
+		if (drm_dp_clock_recovery_ok(link_status, it6505->lane_count)) {
+			it6505_set_bits(it6505, REG_TRAIN_CTRL0, FORCE_CR_DONE,
+					FORCE_CR_DONE);
+			return true;
+		}
+		DRM_DEV_DEBUG_DRIVER(&it6505->client->dev, "cr not done");
+
+		if (it6505_check_max_voltage_swing_reached(lane_level_config,
+							   it6505->lane_count))
+			goto cr_train_fail;
+
+		for (j = 0; j < it6505->lane_count; j++) {
+			lane_voltage_pre_emphasis->voltage_swing[j] =
+				drm_dp_get_adjust_request_voltage(link_status,
+								  j) >>
+				DP_TRAIN_VOLTAGE_SWING_SHIFT;
+			lane_voltage_pre_emphasis->pre_emphasis[j] =
+				drm_dp_get_adjust_request_pre_emphasis(
+					link_status, j) >>
+				DP_TRAIN_PRE_EMPHASIS_SHIFT;
+			if ((voltage_swing_adjust ==
+			     lane_voltage_pre_emphasis->voltage_swing[j]) &&
+			    (pre_emphasis_adjust ==
+			     lane_voltage_pre_emphasis->pre_emphasis[j])) {
+				loop_count++;
+				continue;
+			}
+
+			voltage_swing_adjust =
+				lane_voltage_pre_emphasis->voltage_swing[j];
+			pre_emphasis_adjust =
+				lane_voltage_pre_emphasis->pre_emphasis[j];
+			loop_count = 0;
+
+			if (voltage_swing_adjust + pre_emphasis_adjust >
+			    MAX_EQ_LEVEL)
+				lane_voltage_pre_emphasis->voltage_swing[j] =
+					MAX_EQ_LEVEL -
+					lane_voltage_pre_emphasis
+						->pre_emphasis[j];
+		}
+	}
+
+cr_train_fail:
+	it6505_dpcd_write(it6505, DP_TRAINING_PATTERN_SET,
+			  DP_TRAINING_PATTERN_DISABLE);
+
+	return false;
+}
+
+static bool
+it6505_step_eq_train(struct it6505 *it6505,
+		     struct it6505_step_train_para *lane_voltage_pre_emphasis)
+{
+	u8 loop_count = 0, i, link_status[DP_LINK_STATUS_SIZE] = { 0 };
+	u8 lane_level_config[MAX_LANE_COUNT] = { 0 };
+	const struct drm_dp_aux *aux = &it6505->aux;
+
+	it6505_dpcd_write(it6505, DP_TRAINING_PATTERN_SET,
+			  DP_TRAINING_PATTERN_2);
+
+	while (loop_count < 6) {
+		loop_count++;
+
+		if (!it6505_step_train_lane_voltage_pre_emphasis_set(
+			    it6505, lane_voltage_pre_emphasis,
+			    lane_level_config))
+			continue;
+
+		drm_dp_link_train_channel_eq_delay(aux, it6505->dpcd);
+		drm_dp_dpcd_read_link_status(&it6505->aux, link_status);
+
+		if (!drm_dp_clock_recovery_ok(link_status, it6505->lane_count))
+			goto eq_train_fail;
+
+		if (drm_dp_channel_eq_ok(link_status, it6505->lane_count)) {
+			it6505_dpcd_write(it6505, DP_TRAINING_PATTERN_SET,
+					  DP_TRAINING_PATTERN_DISABLE);
+			it6505_set_bits(it6505, REG_TRAIN_CTRL0, FORCE_EQ_DONE,
+					FORCE_EQ_DONE);
+			return true;
+		}
+		DRM_DEV_DEBUG_DRIVER(&it6505->client->dev, "eq not done");
+
+		for (i = 0; i < it6505->lane_count; i++) {
+			lane_voltage_pre_emphasis->voltage_swing[i] =
+				drm_dp_get_adjust_request_voltage(link_status,
+								  i) >>
+				DP_TRAIN_VOLTAGE_SWING_SHIFT;
+			lane_voltage_pre_emphasis->pre_emphasis[i] =
+				drm_dp_get_adjust_request_pre_emphasis(
+					link_status, i) >>
+				DP_TRAIN_PRE_EMPHASIS_SHIFT;
+
+			if (lane_voltage_pre_emphasis->voltage_swing[i] +
+				    lane_voltage_pre_emphasis->pre_emphasis[i] >
+			    MAX_EQ_LEVEL)
+				lane_voltage_pre_emphasis->voltage_swing[i] =
+					0x03 - lane_voltage_pre_emphasis
+						       ->pre_emphasis[i];
+		}
+	}
+
+eq_train_fail:
+	it6505_dpcd_write(it6505, DP_TRAINING_PATTERN_SET,
+			  DP_TRAINING_PATTERN_DISABLE);
+	return false;
+}
+
+static bool it6505_link_start_step_train(struct it6505 *it6505)
+{
+	int err;
+	struct it6505_step_train_para lane_voltage_pre_emphasis = {
+		.voltage_swing = { 0 },
+		.pre_emphasis = { 0 },
+	};
+
+	DRM_DEV_DEBUG_DRIVER(&it6505->client->dev, "start");
+	err = it6505_drm_dp_link_configure(it6505);
+
+	if (err < 0)
+		return false;
+	if (!it6505_step_cr_train(it6505, &lane_voltage_pre_emphasis))
+		return false;
+	if (!it6505_step_eq_train(it6505, &lane_voltage_pre_emphasis))
+		return false;
+	return true;
+}
+
+static bool it6505_get_video_status(struct it6505 *it6505)
+{
+	int reg_0d;
+
+	reg_0d = it6505_read(it6505, REG_SYSTEM_STS);
+
+	if (reg_0d < 0)
+		return false;
+
+	return reg_0d & VIDEO_STB;
+}
+
+static void it6505_reset_hdcp(struct it6505 *it6505)
+{
+	it6505->hdcp_status = HDCP_AUTH_IDLE;
+	/* Disable CP_Desired */
+	it6505_set_bits(it6505, REG_HDCP_CTRL1, HDCP_CP_ENABLE, 0x00);
+	it6505_set_bits(it6505, REG_RESET_CTRL, HDCP_RESET, HDCP_RESET);
+}
+
+static void it6505_start_hdcp(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "start");
+	it6505_reset_hdcp(it6505);
+	queue_delayed_work(system_wq, &it6505->hdcp_work,
+			   msecs_to_jiffies(2400));
+}
+
+static void it6505_stop_hdcp(struct it6505 *it6505)
+{
+	it6505_reset_hdcp(it6505);
+	cancel_delayed_work(&it6505->hdcp_work);
+}
+
+static bool it6505_hdcp_is_ksv_valid(u8 *ksv)
+{
+	int i, ones = 0;
+
+	/* KSV has 20 1's and 20 0's */
+	for (i = 0; i < DRM_HDCP_KSV_LEN; i++)
+		ones += hweight8(ksv[i]);
+	if (ones != 20)
+		return false;
+	return true;
+}
+
+static void it6505_hdcp_part1_auth(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	u8 hdcp_bcaps;
+
+	it6505_set_bits(it6505, REG_RESET_CTRL, HDCP_RESET, 0x00);
+	/* Disable CP_Desired */
+	it6505_set_bits(it6505, REG_HDCP_CTRL1, HDCP_CP_ENABLE, 0x00);
+
+	usleep_range(1000, 1500);
+	hdcp_bcaps = it6505_dpcd_read(it6505, DP_AUX_HDCP_BCAPS);
+	DRM_DEV_DEBUG_DRIVER(dev, "DPCD[0x68028]: 0x%02x",
+			     hdcp_bcaps);
+
+	if (!hdcp_bcaps)
+		return;
+
+	/* clear the repeater List Chk Done and fail bit */
+	it6505_set_bits(it6505, REG_HDCP_TRIGGER,
+			HDCP_TRIGGER_KSV_DONE | HDCP_TRIGGER_KSV_FAIL,
+			0x00);
+
+	/* Enable An Generator */
+	it6505_set_bits(it6505, REG_HDCP_CTRL2, HDCP_AN_GEN, HDCP_AN_GEN);
+	/* delay1ms(10);*/
+	usleep_range(10000, 15000);
+	/* Stop An Generator */
+	it6505_set_bits(it6505, REG_HDCP_CTRL2, HDCP_AN_GEN, 0x00);
+
+	it6505_set_bits(it6505, REG_HDCP_CTRL1, HDCP_CP_ENABLE, HDCP_CP_ENABLE);
+
+	it6505_set_bits(it6505, REG_HDCP_TRIGGER, HDCP_TRIGGER_START,
+			HDCP_TRIGGER_START);
+
+	it6505->hdcp_status = HDCP_AUTH_GOING;
+}
+
+static int it6505_sha1_digest(struct it6505 *it6505, u8 *sha1_input,
+			      unsigned int size, u8 *output_av)
+{
+	struct shash_desc *desc;
+	struct crypto_shash *tfm;
+	int err;
+	struct device *dev = &it6505->client->dev;
+
+	tfm = crypto_alloc_shash("sha1", 0, 0);
+	if (IS_ERR(tfm)) {
+		DRM_DEV_ERROR(dev, "crypto_alloc_shash sha1 failed");
+		return PTR_ERR(tfm);
+	}
+	desc = kzalloc(sizeof(*desc) + crypto_shash_descsize(tfm), GFP_KERNEL);
+	if (!desc) {
+		crypto_free_shash(tfm);
+		return -ENOMEM;
+	}
+
+	desc->tfm = tfm;
+	err = crypto_shash_digest(desc, sha1_input, size, output_av);
+	if (err)
+		DRM_DEV_ERROR(dev, "crypto_shash_digest sha1 failed");
+
+	crypto_free_shash(tfm);
+	kfree(desc);
+	return err;
+}
+
+static int it6505_setup_sha1_input(struct it6505 *it6505, u8 *sha1_input)
+{
+	struct device *dev = &it6505->client->dev;
+	u8 binfo[2];
+	int down_stream_count, i, err, msg_count = 0;
+
+	err = it6505_get_dpcd(it6505, DP_AUX_HDCP_BINFO, binfo,
+			      ARRAY_SIZE(binfo));
+
+	if (err < 0) {
+		DRM_DEV_ERROR(dev, "Read binfo value Fail");
+		return err;
+	}
+
+	down_stream_count = binfo[0] & 0x7F;
+	DRM_DEV_DEBUG_DRIVER(dev, "binfo:0x%*ph", (int)ARRAY_SIZE(binfo),
+			     binfo);
+
+	if ((binfo[0] & BIT(7)) || (binfo[1] & BIT(3))) {
+		DRM_DEV_ERROR(dev, "HDCP max cascade device exceed");
+		return 0;
+	}
+
+	if (!down_stream_count ||
+	    (down_stream_count > MAX_HDCP_DOWN_STREAM_COUNT)) {
+		DRM_DEV_ERROR(dev, "HDCP down stream count Error %d",
+			      down_stream_count);
+		return 0;
+	}
+
+	for (i = 0; i < down_stream_count; i++) {
+		err = it6505_get_dpcd(it6505, DP_AUX_HDCP_KSV_FIFO +
+				      (i % 3) * DRM_HDCP_KSV_LEN,
+				      sha1_input + msg_count,
+				      DRM_HDCP_KSV_LEN);
+
+		if (err < 0)
+			return err;
+
+		msg_count += 5;
+	}
+
+	it6505->hdcp_down_stream_count = down_stream_count;
+	sha1_input[msg_count++] = binfo[0];
+	sha1_input[msg_count++] = binfo[1];
+
+	it6505_set_bits(it6505, REG_HDCP_CTRL2, HDCP_EN_M0_READ,
+			HDCP_EN_M0_READ);
+
+	err = regmap_bulk_read(it6505->regmap, REG_M0_0_7,
+			       sha1_input + msg_count, 8);
+
+	it6505_set_bits(it6505, REG_HDCP_CTRL2, HDCP_EN_M0_READ, 0x00);
+
+	if (err < 0) {
+		DRM_DEV_ERROR(dev, " Warning, Read M value Fail");
+		return err;
+	}
+
+	msg_count += 8;
+
+	return msg_count;
+}
+
+static bool it6505_hdcp_part2_ksvlist_check(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	u8 av[5][4], bv[5][4];
+	int i, err;
+
+	i = it6505_setup_sha1_input(it6505, it6505->sha1_input);
+	if (i <= 0) {
+		DRM_DEV_ERROR(dev, "SHA-1 Input length error %d", i);
+		return false;
+	}
+
+	it6505_sha1_digest(it6505, it6505->sha1_input, i, (u8 *)av);
+
+	err = it6505_get_dpcd(it6505, DP_AUX_HDCP_V_PRIME(0), (u8 *)bv,
+			      sizeof(bv));
+
+	if (err < 0) {
+		DRM_DEV_ERROR(dev, "Read V' value Fail");
+		return false;
+	}
+
+	for (i = 0; i < 5; i++)
+		if ((bv[i][3] != av[i][0]) || (bv[i][2] != av[i][1]) ||
+		    (bv[i][1] != av[i][2]) || (bv[i][0] != av[i][3]))
+			return false;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "V' all match!!");
+	return true;
+}
+
+static void it6505_hdcp_wait_ksv_list(struct work_struct *work)
+{
+	struct it6505 *it6505 = container_of(work, struct it6505,
+					     hdcp_wait_ksv_list);
+	struct device *dev = &it6505->client->dev;
+	unsigned int timeout = 5000;
+	u8 bstatus = 0;
+	bool ksv_list_check;
+
+	timeout /= 20;
+	while (timeout > 0) {
+		if (!it6505_get_sink_hpd_status(it6505))
+			return;
+
+		bstatus = it6505_dpcd_read(it6505, DP_AUX_HDCP_BSTATUS);
+
+		if (bstatus & DP_BSTATUS_READY)
+			break;
+
+		msleep(20);
+		timeout--;
+	}
+
+	if (timeout == 0) {
+		DRM_DEV_DEBUG_DRIVER(dev, "timeout and ksv list wait failed");
+		goto timeout;
+	}
+
+	ksv_list_check = it6505_hdcp_part2_ksvlist_check(it6505);
+	DRM_DEV_DEBUG_DRIVER(dev, "ksv list ready, ksv list check %s",
+			     ksv_list_check ? "pass" : "fail");
+	if (ksv_list_check) {
+		it6505_set_bits(it6505, REG_HDCP_TRIGGER,
+				HDCP_TRIGGER_KSV_DONE, HDCP_TRIGGER_KSV_DONE);
+		return;
+	}
+timeout:
+	it6505_set_bits(it6505, REG_HDCP_TRIGGER,
+			HDCP_TRIGGER_KSV_DONE | HDCP_TRIGGER_KSV_FAIL,
+			HDCP_TRIGGER_KSV_DONE | HDCP_TRIGGER_KSV_FAIL);
+}
+
+static void it6505_hdcp_work(struct work_struct *work)
+{
+	struct it6505 *it6505 = container_of(work, struct it6505,
+					     hdcp_work.work);
+	struct device *dev = &it6505->client->dev;
+	int ret;
+	u8 link_status[DP_LINK_STATUS_SIZE] = { 0 };
+
+	DRM_DEV_DEBUG_DRIVER(dev, "start");
+
+	if (!it6505_get_sink_hpd_status(it6505))
+		return;
+
+	ret = drm_dp_dpcd_read_link_status(&it6505->aux, link_status);
+	DRM_DEV_DEBUG_DRIVER(dev, "ret: %d link_status: %*ph", ret,
+			     (int)sizeof(link_status), link_status);
+
+	if ((ret < 0) || !drm_dp_channel_eq_ok(link_status, it6505->lane_count)
+	    || !it6505_get_video_status(it6505)) {
+		DRM_DEV_DEBUG_DRIVER(dev, "link train not done or no video");
+		return;
+	}
+
+	ret = it6505_get_dpcd(it6505, DP_AUX_HDCP_BKSV, it6505->bksvs,
+			      ARRAY_SIZE(it6505->bksvs));
+	if (ret < 0) {
+		DRM_DEV_ERROR(dev, "fail to get bksv  ret: %d", ret);
+		it6505_set_bits(it6505, REG_HDCP_TRIGGER,
+				HDCP_TRIGGER_KSV_FAIL, HDCP_TRIGGER_KSV_FAIL);
+	}
+
+	DRM_DEV_DEBUG_DRIVER(dev, "bksv = 0x%*ph",
+			     (int)ARRAY_SIZE(it6505->bksvs), it6505->bksvs);
+
+	if (!it6505_hdcp_is_ksv_valid(it6505->bksvs)) {
+		DRM_DEV_ERROR(dev, "Display Port bksv not valid");
+		it6505_set_bits(it6505, REG_HDCP_TRIGGER,
+				HDCP_TRIGGER_KSV_FAIL, HDCP_TRIGGER_KSV_FAIL);
+	}
+
+	it6505_hdcp_part1_auth(it6505);
+}
+
+static void it6505_show_hdcp_info(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	int i;
+	u8 *sha1 = it6505->sha1_input;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "hdcp_status: %d is_repeater: %d",
+			     it6505->hdcp_status, it6505->is_repeater);
+	DRM_DEV_DEBUG_DRIVER(dev, "bksv = 0x%*ph",
+			     (int)ARRAY_SIZE(it6505->bksvs), it6505->bksvs);
+
+	if (it6505->is_repeater) {
+		DRM_DEV_DEBUG_DRIVER(dev, "hdcp_down_stream_count: %d",
+				     it6505->hdcp_down_stream_count);
+		DRM_DEV_DEBUG_DRIVER(dev, "sha1_input: 0x%*ph",
+				     (int)ARRAY_SIZE(it6505->sha1_input),
+				     it6505->sha1_input);
+		for (i = 0; i < it6505->hdcp_down_stream_count; i++) {
+			DRM_DEV_DEBUG_DRIVER(dev, "KSV_%d = 0x%*ph", i,
+					     DRM_HDCP_KSV_LEN, sha1);
+			sha1 += DRM_HDCP_KSV_LEN;
+		}
+		DRM_DEV_DEBUG_DRIVER(dev, "binfo: 0x%2ph M0: 0x%8ph",
+				     sha1, sha1 + 2);
+	}
+}
+
+static void it6505_stop_link_train(struct it6505 *it6505)
+{
+	it6505->link_state = LINK_IDLE;
+	cancel_work_sync(&it6505->link_works);
+	it6505_write(it6505, REG_TRAIN_CTRL1, FORCE_RETRAIN);
+}
+
+static void it6505_link_train_ok(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	it6505->link_state = LINK_OK;
+	/* disalbe mute enable avi info frame */
+	it6505_set_bits(it6505, REG_DATA_MUTE_CTRL, EN_VID_MUTE, 0x00);
+	it6505_set_bits(it6505, REG_INFOFRAME_CTRL,
+			EN_VID_CTRL_PKT, EN_VID_CTRL_PKT);
+
+	if (it6505_audio_input(it6505)) {
+		DRM_DEV_DEBUG_DRIVER(dev, "Enable audio!");
+		it6505_enable_audio(it6505);
+	}
+
+	if (it6505->hdcp_desired)
+		it6505_start_hdcp(it6505);
+}
+
+static void it6505_link_step_train_process(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	int ret, i, step_retry = 3;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "Start step train");
+
+	if (it6505->sink_count == 0) {
+		DRM_DEV_DEBUG_DRIVER(dev, "it6505->sink_count:%d, force eq",
+				     it6505->sink_count);
+		it6505_set_bits(it6505,	REG_TRAIN_CTRL0, FORCE_EQ_DONE,
+				FORCE_EQ_DONE);
+		return;
+	}
+
+	if (!it6505->step_train) {
+		DRM_DEV_DEBUG_DRIVER(dev, "not support step train");
+		return;
+	}
+
+	/* step training start here */
+	for (i = 0; i < step_retry; i++) {
+		it6505_link_reset_step_train(it6505);
+		ret = it6505_link_start_step_train(it6505);
+		DRM_DEV_DEBUG_DRIVER(dev, "step train %s, retry:%d times",
+				     ret ? "pass" : "failed", i + 1);
+		if (ret) {
+			it6505_link_train_ok(it6505);
+			return;
+		}
+	}
+
+	DRM_DEV_DEBUG_DRIVER(dev, "training fail");
+	it6505->link_state = LINK_IDLE;
+	it6505_video_reset(it6505);
+}
+
+static void it6505_link_training_work(struct work_struct *work)
+{
+	struct it6505 *it6505 = container_of(work, struct it6505, link_works);
+	struct device *dev = &it6505->client->dev;
+	int ret;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "it6505->sink_count: %d",
+			     it6505->sink_count);
+
+	if (!it6505_get_sink_hpd_status(it6505))
+		return;
+
+	it6505_link_training_setup(it6505);
+	it6505_reset_hdcp(it6505);
+	it6505_aux_reset(it6505);
+
+	if (it6505->auto_train_retry < 1) {
+		it6505_link_step_train_process(it6505);
+		return;
+	}
+
+	ret = it6505_link_start_auto_train(it6505);
+	DRM_DEV_DEBUG_DRIVER(dev, "auto train %s, auto_train_retry: %d",
+			     ret ? "pass" : "failed", it6505->auto_train_retry);
+	it6505->auto_train_retry--;
+
+	if (ret) {
+		it6505_link_train_ok(it6505);
+		return;
+	}
+
+	it6505_dump(it6505);
+}
+
+static void it6505_plugged_status_to_codec(struct it6505 *it6505)
+{
+	enum drm_connector_status status = it6505->connector_status;
+
+	if (it6505->plugged_cb && it6505->codec_dev)
+		it6505->plugged_cb(it6505->codec_dev,
+				   status == connector_status_connected);
+}
+
+static int it6505_process_hpd_irq(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	int ret, dpcd_sink_count, dp_irq_vector, bstatus;
+	u8 link_status[DP_LINK_STATUS_SIZE];
+
+	if (!it6505_get_sink_hpd_status(it6505)) {
+		DRM_DEV_DEBUG_DRIVER(dev, "HPD_IRQ HPD low");
+		it6505->sink_count = 0;
+		return 0;
+	}
+
+	ret = it6505_dpcd_read(it6505, DP_SINK_COUNT);
+	if (ret < 0)
+		return ret;
+
+	dpcd_sink_count = DP_GET_SINK_COUNT(ret);
+	DRM_DEV_DEBUG_DRIVER(dev, "dpcd_sink_count: %d it6505->sink_count:%d",
+			     dpcd_sink_count, it6505->sink_count);
+
+	if (it6505->branch_device && dpcd_sink_count != it6505->sink_count) {
+		memset(it6505->dpcd, 0, sizeof(it6505->dpcd));
+		it6505->sink_count = dpcd_sink_count;
+		kfree(it6505->edid);
+		it6505->edid = NULL;
+		it6505_reset_logic(it6505);
+		it6505_int_mask_enable(it6505);
+		it6505_init(it6505);
+		return 0;
+	}
+
+	dp_irq_vector = it6505_dpcd_read(it6505, DP_DEVICE_SERVICE_IRQ_VECTOR);
+	if (dp_irq_vector < 0)
+		return dp_irq_vector;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "dp_irq_vector = 0x%02x", dp_irq_vector);
+
+	if (dp_irq_vector & DP_CP_IRQ) {
+		it6505_set_bits(it6505, REG_HDCP_TRIGGER, HDCP_TRIGGER_CPIRQ,
+				HDCP_TRIGGER_CPIRQ);
+
+		bstatus = it6505_dpcd_read(it6505, DP_AUX_HDCP_BSTATUS);
+		if (bstatus < 0)
+			return bstatus;
+
+		DRM_DEV_DEBUG_DRIVER(dev, "Bstatus = 0x%02x", bstatus);
+	}
+
+	ret = drm_dp_dpcd_read_link_status(&it6505->aux, link_status);
+	if (ret < 0) {
+		DRM_DEV_ERROR(dev, "Fail to read link status ret: %d",
+			      ret);
+		return ret;
+	}
+
+	DRM_DEV_DEBUG_DRIVER(dev, "link status = 0x%*ph",
+			     (int)ARRAY_SIZE(link_status), link_status);
+
+	if (!drm_dp_channel_eq_ok(link_status, it6505->lane_count)) {
+		it6505->auto_train_retry = AUTO_TRAIN_RETRY;
+		it6505_video_reset(it6505);
+	}
+
+	return 0;
+}
+
+static void it6505_irq_hpd(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	it6505->hpd_state = it6505_get_sink_hpd_status(it6505);
+	DRM_DEV_DEBUG_DRIVER(dev, "hpd change interrupt, change to %s",
+			     it6505->hpd_state ? "high" : "low");
+
+	if (it6505->bridge.dev)
+		drm_helper_hpd_irq_event(it6505->bridge.dev);
+	DRM_DEV_DEBUG_DRIVER(dev, "it6505->sink_count: %d",
+			     it6505->sink_count);
+
+	if (it6505->hpd_state) {
+		wait_for_completion_timeout(&it6505->wait_edid_complete,
+					    msecs_to_jiffies(6000));
+		it6505_lane_termination_on(it6505);
+		it6505_lane_power_on(it6505);
+
+		/*
+		 * for some dongle which issue HPD_irq
+		 * when sink count change from  0->1
+		 * it6505 not able to receive HPD_IRQ
+		 * if HW never go into trainig done
+		 */
+
+		if (it6505->branch_device && it6505->sink_count == 0)
+			schedule_work(&it6505->link_works);
+
+		if (!it6505_get_video_status(it6505))
+			it6505_video_reset(it6505);
+
+		it6505_calc_video_info(it6505);
+	} else {
+		kfree(it6505->edid);
+		it6505->edid = NULL;
+		memset(it6505->dpcd, 0, sizeof(it6505->dpcd));
+
+		if (it6505->hdcp_desired)
+			it6505_stop_hdcp(it6505);
+
+		it6505_video_disable(it6505);
+		it6505_disable_audio(it6505);
+		it6505_stop_link_train(it6505);
+		it6505_lane_off(it6505);
+		it6505_link_reset_step_train(it6505);
+	}
+}
+
+static void it6505_irq_hpd_irq(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "hpd_irq interrupt");
+
+	if (it6505_process_hpd_irq(it6505) < 0)
+		DRM_DEV_DEBUG_DRIVER(dev, "process hpd_irq fail!");
+}
+
+static void it6505_irq_scdt(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	bool data;
+
+	data = it6505_get_video_status(it6505);
+	DRM_DEV_DEBUG_DRIVER(dev, "video stable change interrupt, %s",
+			     data ? "stable" : "unstable");
+	it6505_calc_video_info(it6505);
+	it6505_link_reset_step_train(it6505);
+
+	if (data)
+		schedule_work(&it6505->link_works);
+}
+
+static void it6505_irq_hdcp_done(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "hdcp done interrupt");
+	it6505->hdcp_status = HDCP_AUTH_DONE;
+	it6505_show_hdcp_info(it6505);
+}
+
+static void it6505_irq_hdcp_fail(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "hdcp fail interrupt");
+	it6505->hdcp_status = HDCP_AUTH_IDLE;
+	it6505_show_hdcp_info(it6505);
+	it6505_start_hdcp(it6505);
+}
+
+static void it6505_irq_aux_cmd_fail(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "AUX PC Request Fail Interrupt");
+}
+
+static void it6505_irq_hdcp_ksv_check(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "HDCP event Interrupt");
+	schedule_work(&it6505->hdcp_wait_ksv_list);
+}
+
+static void it6505_irq_audio_fifo_error(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "audio fifo error Interrupt");
+
+	if (it6505_audio_input(it6505))
+		it6505_enable_audio(it6505);
+}
+
+static void it6505_irq_link_train_fail(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "link training fail interrupt");
+	schedule_work(&it6505->link_works);
+}
+
+static void it6505_irq_video_fifo_error(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "video fifo overflow interrupt");
+	it6505->auto_train_retry = AUTO_TRAIN_RETRY;
+	flush_work(&it6505->link_works);
+	it6505_stop_hdcp(it6505);
+	it6505_video_reset(it6505);
+}
+
+static void it6505_irq_io_latch_fifo_overflow(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "IO latch fifo overflow interrupt");
+	it6505->auto_train_retry = AUTO_TRAIN_RETRY;
+	flush_work(&it6505->link_works);
+	it6505_stop_hdcp(it6505);
+	it6505_video_reset(it6505);
+}
+
+static bool it6505_test_bit(unsigned int bit, const unsigned int *addr)
+{
+	return 1 & (addr[bit / BITS_PER_BYTE] >> (bit % BITS_PER_BYTE));
+}
+
+static irqreturn_t it6505_int_threaded_handler(int unused, void *data)
+{
+	struct it6505 *it6505 = data;
+	struct device *dev = &it6505->client->dev;
+	static const struct {
+		int bit;
+		void (*handler)(struct it6505 *it6505);
+	} irq_vec[] = {
+		{ BIT_INT_HPD, it6505_irq_hpd },
+		{ BIT_INT_HPD_IRQ, it6505_irq_hpd_irq },
+		{ BIT_INT_SCDT, it6505_irq_scdt },
+		{ BIT_INT_HDCP_FAIL, it6505_irq_hdcp_fail },
+		{ BIT_INT_HDCP_DONE, it6505_irq_hdcp_done },
+		{ BIT_INT_AUX_CMD_FAIL, it6505_irq_aux_cmd_fail },
+		{ BIT_INT_HDCP_KSV_CHECK, it6505_irq_hdcp_ksv_check },
+		{ BIT_INT_AUDIO_FIFO_ERROR, it6505_irq_audio_fifo_error },
+		{ BIT_INT_LINK_TRAIN_FAIL, it6505_irq_link_train_fail },
+		{ BIT_INT_VID_FIFO_ERROR, it6505_irq_video_fifo_error },
+		{ BIT_INT_IO_FIFO_OVERFLOW, it6505_irq_io_latch_fifo_overflow },
+	};
+	int int_status[3], i;
+
+	msleep(100);
+	mutex_lock(&it6505->extcon_lock);
+
+	if (it6505->enable_drv_hold || !it6505->powered)
+		goto unlock;
+
+	int_status[0] = it6505_read(it6505, INT_STATUS_01);
+	int_status[1] = it6505_read(it6505, INT_STATUS_02);
+	int_status[2] = it6505_read(it6505, INT_STATUS_03);
+
+	it6505_write(it6505, INT_STATUS_01, int_status[0]);
+	it6505_write(it6505, INT_STATUS_02, int_status[1]);
+	it6505_write(it6505, INT_STATUS_03, int_status[2]);
+
+	DRM_DEV_DEBUG_DRIVER(dev, "reg06 = 0x%02x", int_status[0]);
+	DRM_DEV_DEBUG_DRIVER(dev, "reg07 = 0x%02x", int_status[1]);
+	DRM_DEV_DEBUG_DRIVER(dev, "reg08 = 0x%02x", int_status[2]);
+	it6505_debug_print(it6505, REG_SYSTEM_STS, "");
+
+	if (it6505_test_bit(irq_vec[0].bit, (unsigned int *)int_status))
+		irq_vec[0].handler(it6505);
+
+	if (!it6505->hpd_state)
+		goto unlock;
+
+	for (i = 1; i < ARRAY_SIZE(irq_vec); i++) {
+		if (it6505_test_bit(irq_vec[i].bit, (unsigned int *)int_status))
+			irq_vec[i].handler(it6505);
+	}
+
+unlock:
+	mutex_unlock(&it6505->extcon_lock);
+
+	return IRQ_HANDLED;
+}
+
+static int it6505_poweron(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	struct it6505_platform_data *pdata = &it6505->pdata;
+	int err;
+
+	if (it6505->powered) {
+		DRM_DEV_DEBUG_DRIVER(dev, "it6505 already powered on");
+		return 0;
+	}
+
+	if (pdata->pwr18) {
+		err = regulator_enable(pdata->pwr18);
+		if (err) {
+			DRM_DEV_DEBUG_DRIVER(dev, "Failed to enable VDD18: %d",
+					     err);
+			return err;
+		}
+	}
+
+	if (pdata->ovdd) {
+		/* time interval between IVDD and OVDD at least be 1ms */
+		usleep_range(1000, 2000);
+		err = regulator_enable(pdata->ovdd);
+		if (err) {
+			regulator_disable(pdata->pwr18);
+			return err;
+		}
+	}
+	/* time interval between OVDD and SYSRSTN at least be 10ms */
+	if (pdata->gpiod_reset) {
+		usleep_range(10000, 20000);
+		gpiod_set_value_cansleep(pdata->gpiod_reset, 0);
+		usleep_range(1000, 2000);
+		gpiod_set_value_cansleep(pdata->gpiod_reset, 1);
+		usleep_range(10000, 20000);
+	}
+
+	it6505_reset_logic(it6505);
+	it6505_int_mask_enable(it6505);
+	it6505_init(it6505);
+	it6505_lane_off(it6505);
+
+	it6505->powered = true;
+
+	return 0;
+}
+
+static int it6505_poweroff(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	struct it6505_platform_data *pdata = &it6505->pdata;
+	int err;
+
+	if (!it6505->powered) {
+		DRM_DEV_DEBUG_DRIVER(dev, "power had been already off");
+		return 0;
+	}
+
+	if (pdata->gpiod_reset)
+		gpiod_set_value_cansleep(pdata->gpiod_reset, 0);
+
+	if (pdata->pwr18) {
+		err = regulator_disable(pdata->pwr18);
+		if (err)
+			return err;
+	}
+
+	if (pdata->ovdd) {
+		err = regulator_disable(pdata->ovdd);
+		if (err)
+			return err;
+	}
+
+	it6505->powered = false;
+	kfree(it6505->edid);
+	it6505->edid = NULL;
+	it6505->sink_count = 0;
+
+	return 0;
+}
+
+static enum drm_connector_status it6505_detect(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	enum drm_connector_status status = connector_status_disconnected;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "it6505->sink_count:%d powered:%d",
+			     it6505->sink_count, it6505->powered);
+
+	mutex_lock(&it6505->mode_lock);
+
+	if (!it6505->powered)
+		goto unlock;
+
+	if (it6505->enable_drv_hold) {
+		status = it6505_get_sink_hpd_status(it6505) ?
+					connector_status_connected :
+					connector_status_disconnected;
+		goto unlock;
+	}
+
+	if (it6505_get_sink_hpd_status(it6505)) {
+		it6505_aux_on(it6505);
+		it6505_drm_dp_link_probe(&it6505->aux, &it6505->link);
+		it6505_drm_dp_link_power_up(&it6505->aux, &it6505->link);
+		it6505->auto_train_retry = AUTO_TRAIN_RETRY;
+
+		if (it6505->dpcd[0] == 0) {
+			it6505_get_dpcd(it6505, DP_DPCD_REV, it6505->dpcd,
+					ARRAY_SIZE(it6505->dpcd));
+			it6505_variable_config(it6505);
+			it6505_parse_link_capabilities(it6505);
+		}
+
+		it6505->sink_count = DP_GET_SINK_COUNT(it6505_dpcd_read(it6505,
+						       DP_SINK_COUNT));
+		DRM_DEV_DEBUG_DRIVER(dev, "it6505->sink_count:%d branch:%d",
+				     it6505->sink_count, it6505->branch_device);
+
+		if (it6505->branch_device) {
+			status = (it6505->sink_count != 0) ?
+				 connector_status_connected :
+				 connector_status_disconnected;
+		} else {
+			status = connector_status_connected;
+		}
+	} else {
+		it6505->sink_count = 0;
+		memset(it6505->dpcd, 0, sizeof(it6505->dpcd));
+	}
+
+unlock:
+	if (it6505->connector_status != status) {
+		it6505->connector_status = status;
+		it6505_plugged_status_to_codec(it6505);
+	}
+
+	mutex_unlock(&it6505->mode_lock);
+
+	return status;
+}
+
+static int it6505_extcon_notifier(struct notifier_block *self,
+				  unsigned long event, void *ptr)
+{
+	struct it6505 *it6505 = container_of(self, struct it6505, event_nb);
+
+	schedule_work(&it6505->extcon_wq);
+	return NOTIFY_DONE;
+}
+
+static void it6505_extcon_work(struct work_struct *work)
+{
+	struct it6505 *it6505 = container_of(work, struct it6505, extcon_wq);
+	struct device *dev = &it6505->client->dev;
+	int state = extcon_get_state(it6505->extcon, EXTCON_DISP_DP);
+	unsigned int pwroffretry = 0;
+
+	if (it6505->enable_drv_hold)
+		return;
+
+	mutex_lock(&it6505->extcon_lock);
+
+	DRM_DEV_DEBUG_DRIVER(dev, "EXTCON_DISP_DP = 0x%02x", state);
+	if (state > 0) {
+		DRM_DEV_DEBUG_DRIVER(dev, "start to power on");
+		msleep(100);
+		it6505_poweron(it6505);
+	} else {
+		DRM_DEV_DEBUG_DRIVER(dev, "start to power off");
+		while (it6505_poweroff(it6505) && pwroffretry++ < 5) {
+			DRM_DEV_DEBUG_DRIVER(dev, "power off fail %d times",
+					     pwroffretry);
+		}
+
+		drm_helper_hpd_irq_event(it6505->bridge.dev);
+		memset(it6505->dpcd, 0, sizeof(it6505->dpcd));
+		DRM_DEV_DEBUG_DRIVER(dev, "power off it6505 success!");
+	}
+
+	mutex_unlock(&it6505->extcon_lock);
+}
+
+static int it6505_use_notifier_module(struct it6505 *it6505)
+{
+	int ret;
+	struct device *dev = &it6505->client->dev;
+
+	it6505->event_nb.notifier_call = it6505_extcon_notifier;
+	INIT_WORK(&it6505->extcon_wq, it6505_extcon_work);
+	ret = devm_extcon_register_notifier(&it6505->client->dev,
+					    it6505->extcon, EXTCON_DISP_DP,
+					    &it6505->event_nb);
+	if (ret) {
+		DRM_DEV_ERROR(dev, "failed to register notifier for DP");
+		return ret;
+	}
+
+	schedule_work(&it6505->extcon_wq);
+
+	return 0;
+}
+
+static void it6505_remove_notifier_module(struct it6505 *it6505)
+{
+	if (it6505->extcon) {
+		devm_extcon_unregister_notifier(&it6505->client->dev,
+						it6505->extcon,	EXTCON_DISP_DP,
+						&it6505->event_nb);
+
+		flush_work(&it6505->extcon_wq);
+	}
+}
+
+static void __maybe_unused it6505_delayed_audio(struct work_struct *work)
+{
+	struct it6505 *it6505 = container_of(work, struct it6505,
+					     delayed_audio.work);
+
+	DRM_DEV_DEBUG_DRIVER(&it6505->client->dev, "start");
+
+	if (!it6505->powered)
+		return;
+
+	if (!it6505->enable_drv_hold)
+		it6505_enable_audio(it6505);
+}
+
+static int __maybe_unused it6505_audio_setup_hw_params(struct it6505 *it6505,
+					struct hdmi_codec_params *params)
+{
+	struct device *dev = &it6505->client->dev;
+	int i = 0;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "%s %d Hz, %d bit, %d channels\n", __func__,
+			     params->sample_rate, params->sample_width,
+			     params->cea.channels);
+
+	if (!it6505->bridge.encoder)
+		return -ENODEV;
+
+	if (params->cea.channels <= 1 || params->cea.channels > 8) {
+		DRM_DEV_DEBUG_DRIVER(dev, "channel number: %d not support",
+				     it6505->audio.channel_count);
+		return -EINVAL;
+	}
+
+	it6505->audio.channel_count = params->cea.channels;
+
+	while (i < ARRAY_SIZE(audio_sample_rate_map) &&
+	       params->sample_rate !=
+		       audio_sample_rate_map[i].sample_rate_value) {
+		i++;
+	}
+	if (i == ARRAY_SIZE(audio_sample_rate_map)) {
+		DRM_DEV_DEBUG_DRIVER(dev, "sample rate: %d Hz not support",
+				     params->sample_rate);
+		return -EINVAL;
+	}
+	it6505->audio.sample_rate = audio_sample_rate_map[i].rate;
+
+	switch (params->sample_width) {
+	case 16:
+		it6505->audio.word_length = WORD_LENGTH_16BIT;
+		break;
+	case 18:
+		it6505->audio.word_length = WORD_LENGTH_18BIT;
+		break;
+	case 20:
+		it6505->audio.word_length = WORD_LENGTH_20BIT;
+		break;
+	case 24:
+	case 32:
+		it6505->audio.word_length = WORD_LENGTH_24BIT;
+		break;
+	default:
+		DRM_DEV_DEBUG_DRIVER(dev, "wordlength: %d bit not support",
+				     params->sample_width);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int it6505_audio_hw_params(struct device *dev, void *data,
+				  struct hdmi_codec_daifmt *daifmt,
+				  struct hdmi_codec_params *params)
+{
+	struct it6505 *it6505 = dev_get_drvdata(dev);
+
+	return it6505_audio_setup_hw_params(it6505, params);
+}
+
+static int it6505_audio_setup_trigger(struct it6505 *it6505,
+						     int event)
+{
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "event: %d", event);
+
+	switch (event) {
+	case HDMI_CODEC_TRIGGER_EVENT_START:
+	case HDMI_CODEC_TRIGGER_EVENT_RESUME:
+		queue_delayed_work(system_wq, &it6505->delayed_audio,
+				   msecs_to_jiffies(180));
+		break;
+	case HDMI_CODEC_TRIGGER_EVENT_STOP:
+	case HDMI_CODEC_TRIGGER_EVENT_SUSPEND:
+		cancel_delayed_work(&it6505->delayed_audio);
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int it6505_audio_trigger(struct device *dev, int event)
+{
+	struct it6505 *it6505 = dev_get_drvdata(dev);
+
+	return it6505_audio_setup_trigger(it6505, event);
+}
+
+static void __maybe_unused it6505_audio_shutdown(struct device *dev, void *data)
+{
+	struct it6505 *it6505 = dev_get_drvdata(dev);
+
+	if (it6505->powered)
+		it6505_disable_audio(it6505);
+}
+
+static int __maybe_unused it6505_audio_hook_plugged_cb(struct device *dev,
+						       void *data,
+						       hdmi_codec_plugged_cb fn,
+						       struct device *codec_dev)
+{
+	struct it6505 *it6505 = data;
+
+	it6505->plugged_cb = fn;
+	it6505->codec_dev = codec_dev;
+	it6505_plugged_status_to_codec(it6505);
+
+	return 0;
+}
+
+static const struct hdmi_codec_ops it6505_audio_codec_ops = {
+	.hw_params = it6505_audio_hw_params,
+	.trigger = it6505_audio_trigger,
+	.audio_shutdown = it6505_audio_shutdown,
+	.hook_plugged_cb = it6505_audio_hook_plugged_cb,
+};
+
+static int it6505_register_audio_driver(struct device *dev)
+{
+	struct it6505 *it6505 = dev_get_drvdata(dev);
+	struct hdmi_codec_pdata codec_data = {
+		.ops = &it6505_audio_codec_ops,
+		.max_i2s_channels = 8,
+		.i2s = 1,
+		.data = it6505,
+	};
+	struct platform_device *pdev;
+
+	pdev = platform_device_register_data(dev, HDMI_CODEC_DRV_NAME,
+					     PLATFORM_DEVID_AUTO, &codec_data,
+					     sizeof(codec_data));
+	if (IS_ERR(pdev))
+		return PTR_ERR(pdev);
+
+	INIT_DELAYED_WORK(&it6505->delayed_audio, it6505_delayed_audio);
+	DRM_DEV_DEBUG_DRIVER(dev, "bound to %s", HDMI_CODEC_DRV_NAME);
+
+	return 0;
+}
+
+static inline struct it6505 *bridge_to_it6505(struct drm_bridge *bridge)
+{
+	return container_of(bridge, struct it6505, bridge);
+}
+
+static void it6505_bridge_mode_set(struct drm_bridge *bridge,
+				   const struct drm_display_mode *mode,
+				   const struct drm_display_mode *adjusted_mode)
+{
+	struct it6505 *it6505 = bridge_to_it6505(bridge);
+
+	if (WARN_ON(!it6505->powered))
+		return;
+
+	mutex_lock(&it6505->mode_lock);
+
+	memcpy(&it6505->source_output_mode, adjusted_mode,
+	       sizeof(it6505->source_output_mode));
+
+	mutex_unlock(&it6505->mode_lock);
+}
+
+static int it6505_bridge_attach(struct drm_bridge *bridge,
+				enum drm_bridge_attach_flags flags)
+{
+	struct it6505 *it6505 = bridge_to_it6505(bridge);
+	struct device *dev = &it6505->client->dev;
+	int ret;
+
+	if (!(flags & DRM_BRIDGE_ATTACH_NO_CONNECTOR)) {
+		DRM_ERROR("DRM_BRIDGE_ATTACH_NO_CONNECTOR must be supplied");
+		return -EINVAL;
+	}
+
+	if (!bridge->encoder) {
+		DRM_DEV_ERROR(dev, "Parent encoder object not found");
+		return -ENODEV;
+	}
+
+	if (it6505->extcon) {
+		ret = it6505_use_notifier_module(it6505);
+		if (ret < 0) {
+			DRM_DEV_ERROR(dev, "use notifier module failed");
+			return ret;
+		}
+	}
+
+	/* Register aux channel */
+	it6505->aux.name = "DP-AUX";
+	it6505->aux.dev = dev;
+	it6505->aux.drm_dev = it6505->bridge.dev;
+	it6505->aux.transfer = it6505_aux_transfer;
+
+	ret = drm_dp_aux_register(&it6505->aux);
+	if (ret < 0) {
+		DRM_DEV_ERROR(dev, "Failed to register aux: %d", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static void it6505_bridge_detach(struct drm_bridge *bridge)
+{
+	struct it6505 *it6505 = bridge_to_it6505(bridge);
+
+	flush_work(&it6505->link_works);
+	it6505_remove_notifier_module(it6505);
+}
+
+static enum drm_mode_status
+it6505_bridge_mode_valid(struct drm_bridge *bridge,
+			 const struct drm_display_info *info,
+			 const struct drm_display_mode *mode)
+{
+	struct it6505 *it6505 = bridge_to_it6505(bridge);
+
+	if (mode->flags & DRM_MODE_FLAG_INTERLACE)
+		return MODE_NO_INTERLACE;
+
+	if (mode->clock > DPI_PIXEL_CLK_MAX)
+		return MODE_CLOCK_HIGH;
+
+	it6505->video_info.clock = mode->clock;
+
+	return MODE_OK;
+}
+
+static void it6505_bridge_atomic_enable(struct drm_bridge *bridge,
+					struct drm_bridge_state *old_state)
+{
+	struct it6505 *it6505 = bridge_to_it6505(bridge);
+	struct device *dev = &it6505->client->dev;
+	struct drm_atomic_state *state = old_state->base.state;
+	struct hdmi_avi_infoframe frame;
+	struct drm_display_mode *adjusted_mode = &it6505->source_output_mode;
+	struct drm_connector *connector;
+	int ret;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "%s : start", __func__);
+
+	connector = drm_atomic_get_new_connector_for_encoder(
+		state, bridge->encoder);
+
+	ret = drm_hdmi_avi_infoframe_from_display_mode(&frame,
+						       connector,
+						       adjusted_mode);
+	if (ret)
+		DRM_DEV_ERROR(dev, "Failed to setup AVI infoframe: %d", ret);
+
+	it6505_update_video_parameter(it6505, adjusted_mode);
+
+	ret = it6505_send_video_infoframe(it6505, &frame);
+
+	if (ret)
+		DRM_DEV_ERROR(dev, "Failed to send AVI infoframe: %d", ret);
+
+	it6505_int_mask_enable(it6505);
+	it6505_video_reset(it6505);
+}
+
+static void it6505_bridge_atomic_disable(struct drm_bridge *bridge,
+					 struct drm_bridge_state *old_state)
+{
+	struct it6505 *it6505 = bridge_to_it6505(bridge);
+	struct device *dev = &it6505->client->dev;
+
+	DRM_DEV_DEBUG_DRIVER(dev, "%s : start", __func__);
+
+	if (it6505->powered)
+		it6505_video_disable(it6505);
+}
+
+static enum drm_connector_status
+it6505_bridge_detect(struct drm_bridge *bridge)
+{
+	struct it6505 *it6505 = bridge_to_it6505(bridge);
+
+	return it6505_detect(it6505);
+}
+
+static struct edid *it6505_bridge_get_edid(struct drm_bridge *bridge,
+					   struct drm_connector *connector)
+{
+	struct it6505 *it6505 = bridge_to_it6505(bridge);
+	struct device *dev = &it6505->client->dev;
+	struct edid *edid;
+
+	edid = drm_do_get_edid(connector, it6505_get_edid_block, it6505);
+
+	if (!edid) {
+		DRM_DEV_DEBUG_DRIVER(dev, "failed to get edid!");
+		return NULL;
+	}
+
+	return edid;
+}
+
+static const struct drm_bridge_funcs it6505_bridge_funcs = {
+	.atomic_duplicate_state = drm_atomic_helper_bridge_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_bridge_destroy_state,
+	.atomic_reset = drm_atomic_helper_bridge_reset,
+	.attach = it6505_bridge_attach,
+	.detach = it6505_bridge_detach,
+	.mode_valid = it6505_bridge_mode_valid,
+	.mode_set = it6505_bridge_mode_set,
+	.atomic_enable = it6505_bridge_atomic_enable,
+	.atomic_disable = it6505_bridge_atomic_disable,
+	.detect = it6505_bridge_detect,
+	.get_edid = it6505_bridge_get_edid,
+};
+
+#ifdef CONFIG_PM_SLEEP
+
+static int it6505_bridge_resume(struct device *dev)
+{
+	struct it6505 *it6505 = dev_get_drvdata(dev);
+
+	return it6505_poweron(it6505);
+}
+
+static int it6505_bridge_suspend(struct device *dev)
+{
+	struct it6505 *it6505 = dev_get_drvdata(dev);
+
+	return it6505_poweroff(it6505);
+}
+
+static SIMPLE_DEV_PM_OPS(it6505_bridge_pm_ops, it6505_bridge_suspend,
+			 it6505_bridge_resume);
+
+#endif
+
+static int it6505_init_pdata(struct it6505 *it6505)
+{
+	struct it6505_platform_data *pdata = &it6505->pdata;
+	struct device *dev = &it6505->client->dev;
+
+	/* 1.0V digital core power regulator  */
+	pdata->pwr18 = devm_regulator_get(dev, "pwr18");
+	if (IS_ERR(pdata->pwr18)) {
+		DRM_DEV_ERROR(dev, "pwr18 regulator not found");
+		return PTR_ERR(pdata->pwr18);
+	}
+
+	pdata->ovdd = devm_regulator_get(dev, "ovdd");
+	if (IS_ERR(pdata->ovdd)) {
+		DRM_DEV_ERROR(dev, "ovdd regulator not found");
+		return PTR_ERR(pdata->ovdd);
+	}
+
+	pdata->gpiod_reset = devm_gpiod_get(dev, "reset", GPIOD_OUT_HIGH);
+	if (IS_ERR(pdata->gpiod_reset)) {
+		DRM_DEV_ERROR(dev, "gpiod_reset gpio not found");
+		return PTR_ERR(pdata->gpiod_reset);
+	}
+
+	return 0;
+}
+
+static void it6505_parse_dt(struct it6505 *it6505)
+{
+	struct device *dev = &it6505->client->dev;
+	u32 *afe_setting = &it6505->afe_setting;
+
+	it6505->lane_swap_disabled =
+		device_property_read_bool(dev, "no-laneswap");
+
+	if (it6505->lane_swap_disabled)
+		it6505->lane_swap = false;
+
+	if (device_property_read_u32(dev, "afe-setting", afe_setting) == 0) {
+		if (*afe_setting >= ARRAY_SIZE(afe_setting_table)) {
+			DRM_DEV_ERROR(dev, "afe setting error, use default");
+			*afe_setting = 0;
+		}
+	} else {
+		*afe_setting = 0;
+	}
+	DRM_DEV_DEBUG_DRIVER(dev, "using afe_setting: %d", *afe_setting);
+}
+
+static ssize_t print_timing_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	struct it6505 *it6505 = dev_get_drvdata(dev);
+	struct drm_display_mode *vid = &it6505->video_info;
+	char *str = buf, *end = buf + PAGE_SIZE;
+
+	it6505_calc_video_info(it6505);
+	str += scnprintf(str, end - str, "---video timing---\n");
+	str += scnprintf(str, end - str, "PCLK:%d.%03dMHz\n",
+			 vid->clock / 1000, vid->clock % 1000);
+	str += scnprintf(str, end - str, "HTotal:%d\n", vid->htotal);
+	str += scnprintf(str, end - str, "HActive:%d\n", vid->hdisplay);
+	str += scnprintf(str, end - str, "HFrontPorch:%d\n",
+			 vid->hsync_start - vid->hdisplay);
+	str += scnprintf(str, end - str, "HSyncWidth:%d\n",
+			 vid->hsync_end - vid->hsync_start);
+	str += scnprintf(str, end - str, "HBackPorch:%d\n",
+			 vid->htotal - vid->hsync_end);
+	str += scnprintf(str, end - str, "VTotal:%d\n", vid->vtotal);
+	str += scnprintf(str, end - str, "VActive:%d\n", vid->vdisplay);
+	str += scnprintf(str, end - str, "VFrontPorch:%d\n",
+			 vid->vsync_start - vid->vdisplay);
+	str += scnprintf(str, end - str, "VSyncWidth:%d\n",
+			 vid->vsync_end - vid->vsync_start);
+	str += scnprintf(str, end - str, "VBackPorch:%d\n",
+			 vid->vtotal - vid->vsync_end);
+
+	return str - buf;
+}
+
+static ssize_t force_pwronoff_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+	struct it6505 *it6505 = dev_get_drvdata(dev);
+	int pwr;
+
+	if (kstrtoint(buf, 10, &pwr) < 0)
+		return -EINVAL;
+
+	if (pwr)
+		it6505_poweron(it6505);
+	else
+		it6505_poweroff(it6505);
+
+	return count;
+}
+
+static ssize_t enable_drv_hold_show(struct device *dev,
+				    struct device_attribute *attr, char *buf)
+{
+	struct it6505 *it6505 = dev_get_drvdata(dev);
+
+	return scnprintf(buf, PAGE_SIZE, "%d\n", it6505->enable_drv_hold);
+}
+
+static ssize_t enable_drv_hold_store(struct device *dev,
+				     struct device_attribute *attr,
+				     const char *buf, size_t count)
+{
+	struct it6505 *it6505 = dev_get_drvdata(dev);
+	unsigned int drv_hold;
+
+	if (kstrtoint(buf, 10, &drv_hold) < 0)
+		return -EINVAL;
+
+	it6505->enable_drv_hold = drv_hold;
+
+	if (it6505->enable_drv_hold) {
+		it6505_int_mask_disable(it6505);
+	} else {
+		it6505_clear_int(it6505);
+		it6505_int_mask_enable(it6505);
+
+		if (it6505->powered) {
+			it6505->connector_status =
+					it6505_get_sink_hpd_status(it6505) ?
+					connector_status_connected :
+					connector_status_disconnected;
+		} else {
+			it6505->connector_status =
+					connector_status_disconnected;
+		}
+	}
+	return count;
+}
+
+static DEVICE_ATTR_RO(print_timing);
+static DEVICE_ATTR_WO(force_pwronoff);
+static DEVICE_ATTR_RW(enable_drv_hold);
+
+static const struct attribute *it6505_attrs[] = {
+	&dev_attr_print_timing.attr,
+	&dev_attr_force_pwronoff.attr,
+	&dev_attr_enable_drv_hold.attr,
+	NULL,
+};
+
+static void it6505_shutdown(struct i2c_client *client)
+{
+	struct it6505 *it6505 = dev_get_drvdata(&client->dev);
+
+	kfree(it6505->edid);
+	it6505->edid = NULL;
+
+	if (it6505->powered)
+		it6505_lane_off(it6505);
+}
+
+static int it6505_i2c_probe(struct i2c_client *client,
+			    const struct i2c_device_id *id)
+{
+	struct it6505 *it6505;
+	struct device *dev = &client->dev;
+	struct extcon_dev *extcon;
+	int err, intp_irq;
+
+	it6505 = devm_kzalloc(&client->dev, sizeof(*it6505), GFP_KERNEL);
+	if (!it6505)
+		return -ENOMEM;
+
+	mutex_init(&it6505->extcon_lock);
+	mutex_init(&it6505->mode_lock);
+	mutex_init(&it6505->aux_lock);
+
+	it6505->bridge.of_node = client->dev.of_node;
+	it6505->connector_status = connector_status_disconnected;
+	it6505->client = client;
+	i2c_set_clientdata(client, it6505);
+
+	/* get extcon device from DTS */
+	extcon = extcon_get_edev_by_phandle(dev, 0);
+	if (PTR_ERR(extcon) == -EPROBE_DEFER)
+		return -EPROBE_DEFER;
+	if (IS_ERR(extcon)) {
+		DRM_DEV_ERROR(dev, "can not get extcon device!");
+		return PTR_ERR(extcon);
+	}
+
+	it6505->extcon = extcon;
+
+	it6505->regmap = devm_regmap_init_i2c(client, &it6505_regmap_config);
+	if (IS_ERR(it6505->regmap)) {
+		DRM_DEV_ERROR(dev, "regmap i2c init failed");
+		err = PTR_ERR(it6505->regmap);
+		return err;
+	}
+
+	err = it6505_init_pdata(it6505);
+	if (err) {
+		DRM_DEV_ERROR(dev, "Failed to initialize pdata: %d", err);
+		return err;
+	}
+
+	it6505_parse_dt(it6505);
+
+	intp_irq = client->irq;
+
+	if (!intp_irq) {
+		DRM_DEV_ERROR(dev, "Failed to get INTP IRQ");
+		err = -ENODEV;
+		return err;
+	}
+
+	err = devm_request_threaded_irq(&client->dev, intp_irq, NULL,
+					it6505_int_threaded_handler,
+					IRQF_TRIGGER_LOW | IRQF_ONESHOT,
+					"it6505-intp", it6505);
+	if (err) {
+		DRM_DEV_ERROR(dev, "Failed to request INTP threaded IRQ: %d",
+			      err);
+		return err;
+	}
+
+	err = it6505_register_audio_driver(dev);
+	if (err < 0) {
+		DRM_DEV_ERROR(dev, "Failed to register audio driver: %d", err);
+		return err;
+	}
+
+	INIT_WORK(&it6505->link_works, it6505_link_training_work);
+	INIT_WORK(&it6505->hdcp_wait_ksv_list, it6505_hdcp_wait_ksv_list);
+	INIT_DELAYED_WORK(&it6505->hdcp_work, it6505_hdcp_work);
+	init_completion(&it6505->wait_edid_complete);
+	memset(it6505->dpcd, 0, sizeof(it6505->dpcd));
+	it6505->powered = false;
+	it6505->enable_drv_hold = DEFAULT_DRV_HOLD;
+
+	if (DEFAULT_PWR_ON)
+		it6505_poweron(it6505);
+
+	err = sysfs_create_files(&client->dev.kobj, it6505_attrs);
+	if (err) {
+		return err;
+	}
+
+	it6505->bridge.funcs = &it6505_bridge_funcs;
+	it6505->bridge.type = DRM_MODE_CONNECTOR_DisplayPort;
+	it6505->bridge.ops = DRM_BRIDGE_OP_DETECT | DRM_BRIDGE_OP_EDID |
+			     DRM_BRIDGE_OP_HPD;
+	drm_bridge_add(&it6505->bridge);
+
+	return 0;
+}
+
+static int it6505_i2c_remove(struct i2c_client *client)
+{
+	struct it6505 *it6505 = i2c_get_clientdata(client);
+
+	drm_bridge_remove(&it6505->bridge);
+	drm_dp_aux_unregister(&it6505->aux);
+	sysfs_remove_files(&client->dev.kobj, it6505_attrs);
+	it6505_poweroff(it6505);
+
+	return 0;
+}
+
+static const struct i2c_device_id it6505_id[] = {
+	{ "it6505", 0 },
+	{ }
+};
+
+MODULE_DEVICE_TABLE(i2c, it6505_id);
+
+static const struct of_device_id it6505_of_match[] = {
+	{ .compatible = "ite,it6505" },
+	{ }
+};
+
+static struct i2c_driver it6505_i2c_driver = {
+	.driver = {
+		.name = "it6505",
+		.of_match_table = it6505_of_match,
+#ifdef CONFIG_PM_SLEEP
+		.pm = &it6505_bridge_pm_ops,
+#endif
+	},
+	.probe = it6505_i2c_probe,
+	.remove = it6505_i2c_remove,
+	.shutdown = it6505_shutdown,
+	.id_table = it6505_id,
+};
+
+module_i2c_driver(it6505_i2c_driver);
+
+MODULE_AUTHOR("Allen Chen <allen.chen@ite.com.tw>");
+MODULE_DESCRIPTION("IT6505 DisplayPort Transmitter driver");
+MODULE_LICENSE("GPL v2");
diff -ruN a/drivers/gpu/drm/bridge/Kconfig b/drivers/gpu/drm/bridge/Kconfig
--- a/drivers/gpu/drm/bridge/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/bridge/Kconfig	2021-12-23 08:35:26.000000000 +0100
@@ -74,6 +74,13 @@
 	  on ARM-based platforms. Saying Y here when this driver is not needed
 	  will not cause any issue.
 
+config DRM_ITE_IT6505
+        tristate "ITE IT6505 DisplayPort bridge"
+        depends on OF
+        select DRM_KMS_HELPER
+        help
+          ITE IT6505 DisplayPort bridge chip driver.
+
 config DRM_LONTIUM_LT8912B
 	tristate "Lontium LT8912B DSI/HDMI bridge"
 	depends on OF
diff -ruN a/drivers/gpu/drm/bridge/Makefile b/drivers/gpu/drm/bridge/Makefile
--- a/drivers/gpu/drm/bridge/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/bridge/Makefile	2021-12-23 08:35:26.000000000 +0100
@@ -4,6 +4,7 @@
 obj-$(CONFIG_DRM_CHRONTEL_CH7033) += chrontel-ch7033.o
 obj-$(CONFIG_DRM_CROS_EC_ANX7688) += cros-ec-anx7688.o
 obj-$(CONFIG_DRM_DISPLAY_CONNECTOR) += display-connector.o
+obj-$(CONFIG_DRM_ITE_IT6505) += ite-it6505.o
 obj-$(CONFIG_DRM_LONTIUM_LT8912B) += lontium-lt8912b.o
 obj-$(CONFIG_DRM_LONTIUM_LT9611) += lontium-lt9611.o
 obj-$(CONFIG_DRM_LONTIUM_LT9611UXC) += lontium-lt9611uxc.o
diff -ruN a/drivers/gpu/drm/drm_auth.c b/drivers/gpu/drm/drm_auth.c
--- a/drivers/gpu/drm/drm_auth.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/drm_auth.c	2021-12-23 08:35:26.000000000 +0100
@@ -235,7 +235,12 @@
 static int
 drm_master_check_perm(struct drm_device *dev, struct drm_file *file_priv)
 {
-	if (file_priv->pid == task_pid(current) && file_priv->was_master)
+	/*
+	 * Despite the above explanation and reasoning, we still have to check
+	 * drm_master_relax. With frecon, the was_master flag is true, but the
+	 * file pid and the task pid pointers (and the actual PIDs) don't match.
+	 */
+	if ((drm_master_relax || file_priv->pid == task_pid(current)) && file_priv->was_master)
 		return 0;
 
 	if (!capable(CAP_SYS_ADMIN))
diff -ruN a/drivers/gpu/drm/drm_drv.c b/drivers/gpu/drm/drm_drv.c
--- a/drivers/gpu/drm/drm_drv.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/drm_drv.c	2021-12-23 08:35:26.000000000 +0100
@@ -1021,6 +1021,9 @@
 	return err;
 }
 
+/* When set to true, allow set/drop master ioctls as normal user */
+bool drm_master_relax;
+
 static const struct file_operations drm_stub_fops = {
 	.owner = THIS_MODULE,
 	.open = drm_stub_open,
@@ -1052,6 +1058,9 @@
 
 	drm_debugfs_root = debugfs_create_dir("dri", NULL);
 
+	debugfs_create_bool("drm_master_relax", S_IRUSR | S_IWUSR,
+			    drm_debugfs_root, &drm_master_relax);
+
 	ret = register_chrdev(DRM_MAJOR, "drm", &drm_stub_fops);
 	if (ret < 0)
 		goto error;
diff -ruN a/drivers/gpu/drm/drm_ioctl.c b/drivers/gpu/drm/drm_ioctl.c
--- a/drivers/gpu/drm/drm_ioctl.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/drm_ioctl.c	2021-12-23 08:35:26.000000000 +0100
@@ -586,7 +586,7 @@
 	DRM_IOCTL_DEF(DRM_IOCTL_GET_CLIENT, drm_getclient, 0),
 	DRM_IOCTL_DEF(DRM_IOCTL_GET_STATS, drm_getstats, 0),
 	DRM_IOCTL_DEF(DRM_IOCTL_GET_CAP, drm_getcap, DRM_RENDER_ALLOW),
-	DRM_IOCTL_DEF(DRM_IOCTL_SET_CLIENT_CAP, drm_setclientcap, 0),
+	DRM_IOCTL_DEF(DRM_IOCTL_SET_CLIENT_CAP, drm_setclientcap, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF(DRM_IOCTL_SET_VERSION, drm_setversion, DRM_MASTER),
 
 	DRM_IOCTL_DEF(DRM_IOCTL_SET_UNIQUE, drm_invalid_op, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
@@ -663,10 +663,10 @@
 	DRM_IOCTL_DEF(DRM_IOCTL_PRIME_HANDLE_TO_FD, drm_prime_handle_to_fd_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF(DRM_IOCTL_PRIME_FD_TO_HANDLE, drm_prime_fd_to_handle_ioctl, DRM_RENDER_ALLOW),
 
-	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPLANERESOURCES, drm_mode_getplane_res, 0),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPLANERESOURCES, drm_mode_getplane_res, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETCRTC, drm_mode_getcrtc, 0),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_SETCRTC, drm_mode_setcrtc, DRM_MASTER),
-	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPLANE, drm_mode_getplane, 0),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPLANE, drm_mode_getplane, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_SETPLANE, drm_mode_setplane, DRM_MASTER),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_CURSOR, drm_mode_cursor_ioctl, DRM_MASTER),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETGAMMA, drm_mode_gamma_get_ioctl, 0),
@@ -675,7 +675,7 @@
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETCONNECTOR, drm_mode_getconnector, 0),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ATTACHMODE, drm_noop, DRM_MASTER),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_DETACHMODE, drm_noop, DRM_MASTER),
-	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPROPERTY, drm_mode_getproperty_ioctl, 0),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPROPERTY, drm_mode_getproperty_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_SETPROPERTY, drm_connector_property_set_ioctl, DRM_MASTER),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPROPBLOB, drm_mode_getblob_ioctl, 0),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETFB, drm_mode_getfb, 0),
@@ -685,10 +685,10 @@
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_RMFB, drm_mode_rmfb_ioctl, 0),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_PAGE_FLIP, drm_mode_page_flip_ioctl, DRM_MASTER),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_DIRTYFB, drm_mode_dirtyfb_ioctl, DRM_MASTER),
-	DRM_IOCTL_DEF(DRM_IOCTL_MODE_CREATE_DUMB, drm_mode_create_dumb_ioctl, 0),
-	DRM_IOCTL_DEF(DRM_IOCTL_MODE_MAP_DUMB, drm_mode_mmap_dumb_ioctl, 0),
-	DRM_IOCTL_DEF(DRM_IOCTL_MODE_DESTROY_DUMB, drm_mode_destroy_dumb_ioctl, 0),
-	DRM_IOCTL_DEF(DRM_IOCTL_MODE_OBJ_GETPROPERTIES, drm_mode_obj_get_properties_ioctl, 0),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_CREATE_DUMB, drm_mode_create_dumb_ioctl, DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_MAP_DUMB, drm_mode_mmap_dumb_ioctl, DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_DESTROY_DUMB, drm_mode_destroy_dumb_ioctl, DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_OBJ_GETPROPERTIES, drm_mode_obj_get_properties_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_OBJ_SETPROPERTY, drm_mode_obj_set_property_ioctl, DRM_MASTER),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_CURSOR2, drm_mode_cursor2_ioctl, DRM_MASTER),
 	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ATOMIC, drm_mode_atomic_ioctl, DRM_MASTER),
diff -ruN a/drivers/gpu/drm/evdi/dkms.conf b/drivers/gpu/drm/evdi/dkms.conf
--- a/drivers/gpu/drm/evdi/dkms.conf	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/dkms.conf	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,17 @@
+## @file
+# Linux DKMS config script for the EVDI kernel modules
+#
+
+#
+# Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+#
+
+PACKAGE_NAME="evdi"
+PACKAGE_VERSION=1.9.1
+AUTOINSTALL=yes
+
+MAKE[0]="make all INCLUDEDIR=/lib/modules/$kernelver/build/include KVERSION=$kernelver DKMS_BUILD=1"
+DEST_MODULE_LOCATION[0]="/kernel/drivers/gpu/drm/evdi"
+BUILT_MODULE_NAME[0]="evdi"
+CLEAN="make clean KERNELRELEASE=$kernelver DKMS_BUILD=1"
+
diff -ruN a/drivers/gpu/drm/evdi/evdi_connector.c b/drivers/gpu/drm/evdi/evdi_connector.c
--- a/drivers/gpu/drm/evdi/evdi_connector.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_connector.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,166 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2012 Red Hat
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * Based on parts on udlfb.c:
+ * Copyright (C) 2009 its respective authors
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include <linux/version.h>
+#include <drm/drm_crtc.h>
+#include <drm/drm_edid.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/drm_atomic_helper.h>
+#include "evdi_drm_drv.h"
+
+#if KERNEL_VERSION(5, 1, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#include <drm/drm_probe_helper.h>
+#endif
+
+/*
+ * dummy connector to just get EDID,
+ * all EVDI appear to have a DVI-D
+ */
+
+static int evdi_get_modes(struct drm_connector *connector)
+{
+	struct evdi_device *evdi = connector->dev->dev_private;
+	struct edid *edid = NULL;
+	int ret = 0;
+
+	edid = (struct edid *)evdi_painter_get_edid_copy(evdi);
+
+	if (!edid) {
+#if KERNEL_VERSION(4, 19, 0) <= LINUX_VERSION_CODE || defined(EL8)
+		drm_connector_update_edid_property(connector, NULL);
+#else
+		drm_mode_connector_update_edid_property(connector, NULL);
+#endif
+		return 0;
+	}
+
+#if KERNEL_VERSION(4, 19, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	ret = drm_connector_update_edid_property(connector, edid);
+#else
+	ret = drm_mode_connector_update_edid_property(connector, edid);
+#endif
+
+	if (ret) {
+		EVDI_ERROR("Failed to set edid property! error: %d", ret);
+		goto err;
+	}
+
+	ret = drm_add_edid_modes(connector, edid);
+	EVDI_INFO("(card%d) Edid property set", evdi->dev_index);
+err:
+	kfree(edid);
+	return ret;
+}
+
+static enum drm_mode_status evdi_mode_valid(struct drm_connector *connector,
+					    struct drm_display_mode *mode)
+{
+	struct evdi_device *evdi = connector->dev->dev_private;
+	uint32_t mode_area = mode->hdisplay * mode->vdisplay;
+
+	if (evdi->sku_area_limit == 0)
+		return MODE_OK;
+
+	if (mode_area > evdi->sku_area_limit) {
+		EVDI_WARN("(card%d) Mode %dx%d@%d rejected\n",
+			evdi->dev_index,
+			mode->hdisplay,
+			mode->vdisplay,
+			drm_mode_vrefresh(mode));
+		return MODE_BAD;
+	}
+
+	return MODE_OK;
+}
+
+static enum drm_connector_status
+evdi_detect(struct drm_connector *connector, __always_unused bool force)
+{
+	struct evdi_device *evdi = connector->dev->dev_private;
+
+	EVDI_CHECKPT();
+	if (evdi_painter_is_connected(evdi->painter)) {
+		EVDI_INFO("(card%d) Connector state: connected\n",
+			   evdi->dev_index);
+		return connector_status_connected;
+	}
+	EVDI_VERBOSE("(card%d) Connector state: disconnected\n",
+		   evdi->dev_index);
+	return connector_status_disconnected;
+}
+
+static void evdi_connector_destroy(struct drm_connector *connector)
+{
+	drm_connector_unregister(connector);
+	drm_connector_cleanup(connector);
+	kfree(connector);
+}
+
+static struct drm_encoder *evdi_best_encoder(struct drm_connector *connector)
+{
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	struct drm_encoder *encoder;
+
+	drm_connector_for_each_possible_encoder(connector, encoder) {
+		return encoder;
+	}
+
+	return NULL;
+#else
+	return drm_encoder_find(connector->dev,
+				NULL,
+				connector->encoder_ids[0]);
+#endif
+}
+
+static struct drm_connector_helper_funcs evdi_connector_helper_funcs = {
+	.get_modes = evdi_get_modes,
+	.mode_valid = evdi_mode_valid,
+	.best_encoder = evdi_best_encoder,
+};
+
+static const struct drm_connector_funcs evdi_connector_funcs = {
+	.detect = evdi_detect,
+	.fill_modes = drm_helper_probe_single_connector_modes,
+	.destroy = evdi_connector_destroy,
+	.reset = drm_atomic_helper_connector_reset,
+	.atomic_duplicate_state = drm_atomic_helper_connector_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_connector_destroy_state
+};
+
+int evdi_connector_init(struct drm_device *dev, struct drm_encoder *encoder)
+{
+	struct drm_connector *connector;
+	struct evdi_device *evdi = dev->dev_private;
+
+	connector = kzalloc(sizeof(struct drm_connector), GFP_KERNEL);
+	if (!connector)
+		return -ENOMEM;
+
+	/* TODO: Initialize connector with actual connector type */
+	drm_connector_init(dev, connector, &evdi_connector_funcs,
+			   DRM_MODE_CONNECTOR_DVII);
+	drm_connector_helper_add(connector, &evdi_connector_helper_funcs);
+	connector->polled = DRM_CONNECTOR_POLL_HPD;
+
+	drm_connector_register(connector);
+
+	evdi->conn = connector;
+
+#if KERNEL_VERSION(4, 19, 0) <= LINUX_VERSION_CODE  || defined(EL8)
+	drm_connector_attach_encoder(connector, encoder);
+#else
+	drm_mode_connector_attach_encoder(connector, encoder);
+#endif
+	return 0;
+}
diff -ruN a/drivers/gpu/drm/evdi/evdi_cursor.c b/drivers/gpu/drm/evdi/evdi_cursor.c
--- a/drivers/gpu/drm/evdi/evdi_cursor.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_cursor.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,291 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * evdi_cursor.c
+ *
+ * Copyright (c) 2016 The Chromium OS Authors
+ * Copyright (c) 2016 - 2020 DisplayLink (UK) Ltd.
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program. If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/compiler.h>
+#include <linux/mutex.h>
+#include <linux/version.h>
+
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#else
+#include <drm/drmP.h>
+#endif
+#include <drm/drm_crtc_helper.h>
+
+#include "evdi_cursor.h"
+#include "evdi_drm_drv.h"
+
+/*
+ * EVDI drm cursor private structure.
+ */
+struct evdi_cursor {
+	bool enabled;
+	int32_t x;
+	int32_t y;
+	uint32_t width;
+	uint32_t height;
+	int32_t hot_x;
+	int32_t hot_y;
+	uint32_t pixel_format;
+	uint32_t stride;
+	struct evdi_gem_object *obj;
+	struct mutex lock;
+};
+
+static void evdi_cursor_set_gem(struct evdi_cursor *cursor,
+				struct evdi_gem_object *obj)
+{
+	if (obj)
+		drm_gem_object_get(&obj->base);
+	if (cursor->obj)
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE || defined(EL8)
+		drm_gem_object_put(&cursor->obj->base);
+#else
+		drm_gem_object_put_unlocked(&cursor->obj->base);
+#endif
+
+	cursor->obj = obj;
+}
+
+struct evdi_gem_object *evdi_cursor_gem(struct evdi_cursor *cursor)
+{
+	return cursor->obj;
+}
+
+int evdi_cursor_init(struct evdi_cursor **cursor)
+{
+	if (WARN_ON(*cursor))
+		return -EINVAL;
+
+	*cursor = kzalloc(sizeof(struct evdi_cursor), GFP_KERNEL);
+	if (*cursor) {
+		mutex_init(&(*cursor)->lock);
+		return 0;
+	} else {
+		return -ENOMEM;
+	}
+}
+
+void evdi_cursor_lock(struct evdi_cursor *cursor)
+{
+	mutex_lock(&cursor->lock);
+}
+
+void evdi_cursor_unlock(struct evdi_cursor *cursor)
+{
+	mutex_unlock(&cursor->lock);
+}
+
+void evdi_cursor_free(struct evdi_cursor *cursor)
+{
+	if (WARN_ON(!cursor))
+		return;
+	evdi_cursor_set_gem(cursor, NULL);
+	kfree(cursor);
+}
+
+bool evdi_cursor_enabled(struct evdi_cursor *cursor)
+{
+	return cursor->enabled;
+}
+
+void evdi_cursor_enable(struct evdi_cursor *cursor, bool enable)
+{
+	evdi_cursor_lock(cursor);
+	cursor->enabled = enable;
+	if (!enable)
+		evdi_cursor_set_gem(cursor, NULL);
+	evdi_cursor_unlock(cursor);
+}
+
+void evdi_cursor_set(struct evdi_cursor *cursor,
+		     struct evdi_gem_object *obj,
+		     uint32_t width, uint32_t height,
+		     int32_t hot_x, int32_t hot_y,
+		     uint32_t pixel_format, uint32_t stride)
+{
+	int err = 0;
+
+	evdi_cursor_lock(cursor);
+	if (obj && !obj->vmapping)
+		err = evdi_gem_vmap(obj);
+
+	if (err != 0) {
+		EVDI_ERROR("Failed to map cursor.\n");
+		obj = NULL;
+	}
+
+	cursor->enabled = obj != NULL;
+	cursor->width = width;
+	cursor->height = height;
+	cursor->hot_x = hot_x;
+	cursor->hot_y = hot_y;
+	cursor->pixel_format = pixel_format;
+	cursor->stride = stride;
+	evdi_cursor_set_gem(cursor, obj);
+
+	evdi_cursor_unlock(cursor);
+}
+
+void evdi_cursor_move(struct evdi_cursor *cursor, int32_t x, int32_t y)
+{
+	evdi_cursor_lock(cursor);
+	cursor->x = x;
+	cursor->y = y;
+	evdi_cursor_unlock(cursor);
+}
+
+static inline uint32_t blend_component(uint32_t pixel,
+				  uint32_t blend,
+				  uint32_t alpha)
+{
+	uint32_t pre_blend = (pixel * (255 - alpha) + blend * alpha);
+
+	return (pre_blend + ((pre_blend + 1) << 8)) >> 16;
+}
+
+static inline uint32_t blend_alpha(const uint32_t pixel_val32,
+				uint32_t blend_val32)
+{
+	uint32_t alpha = (blend_val32 >> 24);
+
+	return blend_component(pixel_val32 & 0xff,
+			       blend_val32 & 0xff, alpha) |
+			blend_component((pixel_val32 & 0xff00) >> 8,
+				(blend_val32 & 0xff00) >> 8, alpha) << 8 |
+			blend_component((pixel_val32 & 0xff0000) >> 16,
+				(blend_val32 & 0xff0000) >> 16, alpha) << 16;
+}
+
+static int evdi_cursor_compose_pixel(char __user *buffer,
+				     int const cursor_value,
+				     int const fb_value,
+				     int cmd_offset)
+{
+	int const composed_value = blend_alpha(fb_value, cursor_value);
+
+	return copy_to_user(buffer + cmd_offset, &composed_value, 4);
+}
+
+int evdi_cursor_compose_and_copy(struct evdi_cursor *cursor,
+				 struct evdi_framebuffer *efb,
+				 char __user *buffer,
+				 int buf_byte_stride)
+{
+	int x, y;
+	struct drm_framebuffer *fb = &efb->base;
+	const int h_cursor_w = cursor->width >> 1;
+	const int h_cursor_h = cursor->height >> 1;
+	uint32_t *cursor_buffer = NULL;
+	uint32_t bytespp = 0;
+
+	if (!cursor->enabled)
+		return 0;
+
+	if (!cursor->obj)
+		return -EINVAL;
+
+	if (!cursor->obj->vmapping)
+		return -EINVAL;
+
+	bytespp = evdi_fb_get_bpp(cursor->pixel_format);
+	bytespp = DIV_ROUND_UP(bytespp, 8);
+	if (bytespp != 4) {
+		EVDI_ERROR("Unsupported cursor format bpp=%u\n", bytespp);
+		return -EINVAL;
+	}
+
+	if (cursor->width * cursor->height * bytespp >
+	    cursor->obj->base.size){
+		EVDI_ERROR("Wrong cursor size\n");
+		return -EINVAL;
+	}
+
+	cursor_buffer = (uint32_t *)cursor->obj->vmapping;
+
+	for (y = -h_cursor_h; y < h_cursor_h; ++y) {
+		for (x = -h_cursor_w; x < h_cursor_w; ++x) {
+			uint32_t curs_val;
+			int *fbsrc;
+			int fb_value;
+			int cmd_offset;
+			int cursor_pix;
+			int const mouse_pix_x = cursor->x + x + h_cursor_w;
+			int const mouse_pix_y = cursor->y + y + h_cursor_h;
+			bool const is_pix_sane =
+				mouse_pix_x >= 0 &&
+				mouse_pix_y >= 0 &&
+				mouse_pix_x < fb->width &&
+				mouse_pix_y < fb->height;
+
+			if (!is_pix_sane)
+				continue;
+
+			cursor_pix = h_cursor_w+x +
+				    (h_cursor_h+y)*cursor->width;
+			curs_val = le32_to_cpu(cursor_buffer[cursor_pix]);
+			fbsrc = (int *)(efb->obj->vmapping + fb->offsets[0]);
+			fb_value = *(fbsrc + ((fb->pitches[0]>>2) *
+						  mouse_pix_y + mouse_pix_x));
+			cmd_offset = (buf_byte_stride * mouse_pix_y) +
+						       (mouse_pix_x * bytespp);
+			if (evdi_cursor_compose_pixel(buffer,
+						      curs_val,
+						      fb_value,
+						      cmd_offset)) {
+				EVDI_ERROR("Failed to compose cursor pixel\n");
+				return -EFAULT;
+			}
+		}
+	}
+
+	return 0;
+}
+
+void evdi_cursor_position(struct evdi_cursor *cursor, int32_t *x, int32_t *y)
+{
+	*x = cursor->x;
+	*y = cursor->y;
+}
+
+void evdi_cursor_hotpoint(struct evdi_cursor *cursor,
+			  int32_t *hot_x, int32_t *hot_y)
+{
+	*hot_x = cursor->hot_x;
+	*hot_y = cursor->hot_y;
+}
+
+void evdi_cursor_size(struct evdi_cursor *cursor,
+		      uint32_t *width, uint32_t *height)
+{
+	*width = cursor->width;
+	*height = cursor->height;
+}
+
+void evdi_cursor_format(struct evdi_cursor *cursor, uint32_t *format)
+{
+	*format = cursor->pixel_format;
+}
+
+void evdi_cursor_stride(struct evdi_cursor *cursor, uint32_t *stride)
+{
+	*stride = cursor->stride;
+}
+
diff -ruN a/drivers/gpu/drm/evdi/evdi_cursor.h b/drivers/gpu/drm/evdi/evdi_cursor.h
--- a/drivers/gpu/drm/evdi/evdi_cursor.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_cursor.h	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,62 @@
+/* SPDX-License-Identifier: GPL-2.0-only
+ * evdi_cursor.h
+ *
+ * Copyright (c) 2016 The Chromium OS Authors
+ * Copyright (c) 2016 - 2020 DisplayLink (UK) Ltd.
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program. If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef _EVDI_CURSOR_H_
+#define _EVDI_CURSOR_H_
+
+#include <linux/version.h>
+#include <linux/module.h>
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#else
+#include <drm/drmP.h>
+#endif
+#include <drm/drm_crtc.h>
+
+struct evdi_cursor;
+struct evdi_framebuffer;
+struct evdi_gem_object;
+
+int evdi_cursor_init(struct evdi_cursor **cursor);
+void evdi_cursor_free(struct evdi_cursor *cursor);
+void evdi_cursor_lock(struct evdi_cursor *cursor);
+void evdi_cursor_unlock(struct evdi_cursor *cursor);
+bool evdi_cursor_enabled(struct evdi_cursor *cursor);
+void evdi_cursor_enable(struct evdi_cursor *cursor, bool enabled);
+void evdi_cursor_set(struct evdi_cursor *cursor,
+		     struct evdi_gem_object *obj,
+		     uint32_t width, uint32_t height,
+		     int32_t hot_x, int32_t hot_y,
+		     uint32_t pixel_format, uint32_t stride);
+
+void evdi_cursor_move(struct evdi_cursor *cursor, int32_t x, int32_t y);
+void evdi_cursor_position(struct evdi_cursor *cursor, int32_t *x, int32_t *y);
+void evdi_cursor_hotpoint(struct evdi_cursor *cursor,
+			  int32_t *hot_x, int32_t *hot_y);
+void evdi_cursor_size(struct evdi_cursor *cursor,
+		      uint32_t *width, uint32_t *height);
+void evdi_cursor_format(struct evdi_cursor *cursor, uint32_t *format);
+void evdi_cursor_stride(struct evdi_cursor *cursor, uint32_t *stride);
+struct evdi_gem_object *evdi_cursor_gem(struct evdi_cursor *cursor);
+
+int evdi_cursor_compose_and_copy(struct evdi_cursor *cursor,
+				 struct evdi_framebuffer *efb,
+				 char __user *buffer,
+				 int buf_byte_stride);
+#endif
diff -ruN a/drivers/gpu/drm/evdi/evdi_debug.c b/drivers/gpu/drm/evdi/evdi_debug.c
--- a/drivers/gpu/drm/evdi/evdi_debug.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_debug.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,34 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/* Copyright (c) 2015 - 2019 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include <linux/sched.h>
+
+#include "evdi_debug.h"
+
+void evdi_log_process(char *buf, size_t size)
+{
+	int task_pid = (int)task_pid_nr(current);
+	char task_comm[TASK_COMM_LEN] = { 0 };
+
+	get_task_comm(task_comm, current);
+
+	if (current->group_leader) {
+		char process_comm[TASK_COMM_LEN] = { 0 };
+
+		get_task_comm(process_comm, current->group_leader);
+		snprintf(buf, size, "Task %d (%s) of process %d (%s)",
+			  task_pid,
+			  task_comm,
+			  (int)task_pid_nr(current->group_leader),
+			  process_comm);
+	} else {
+		snprintf(buf, size, "Task %d (%s)",
+			  task_pid,
+			  task_comm);
+	}
+}
diff -ruN a/drivers/gpu/drm/evdi/evdi_debug.h b/drivers/gpu/drm/evdi/evdi_debug.h
--- a/drivers/gpu/drm/evdi/evdi_debug.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_debug.h	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,58 @@
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (c) 2015 - 2019 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#ifndef EVDI_DEBUG_H
+#define EVDI_DEBUG_H
+
+#include "evdi_params.h"
+
+#define EVDI_LOGLEVEL_ALWAYS  0
+#define EVDI_LOGLEVEL_FATAL   1
+#define EVDI_LOGLEVEL_ERROR   2
+#define EVDI_LOGLEVEL_WARN    3
+#define EVDI_LOGLEVEL_INFO    4
+#define EVDI_LOGLEVEL_DEBUG   5
+#define EVDI_LOGLEVEL_VERBOSE 6
+
+#define EVDI_PRINTK(KERN_LEVEL, LEVEL, FORMAT_STR, ...)	do { \
+	if (evdi_loglevel >= LEVEL) {\
+		printk(KERN_LEVEL "evdi: " FORMAT_STR, ##__VA_ARGS__); \
+	} \
+} while (0)
+
+#define EVDI_FATAL(FORMAT_STR, ...) \
+	EVDI_PRINTK(KERN_CRIT, EVDI_LOGLEVEL_FATAL,\
+		    "[F] %s:%d " FORMAT_STR, __func__, __LINE__, ##__VA_ARGS__)
+
+#define EVDI_ERROR(FORMAT_STR, ...) \
+	EVDI_PRINTK(KERN_ERR, EVDI_LOGLEVEL_ERROR,\
+		    "[E] %s:%d " FORMAT_STR, __func__, __LINE__, ##__VA_ARGS__)
+
+#define EVDI_WARN(FORMAT_STR, ...) \
+	EVDI_PRINTK(KERN_WARNING, EVDI_LOGLEVEL_WARN,\
+		    "[W] %s:%d " FORMAT_STR, __func__, __LINE__, ##__VA_ARGS__)
+
+#define EVDI_INFO(FORMAT_STR, ...) \
+	EVDI_PRINTK(KERN_DEFAULT, EVDI_LOGLEVEL_INFO,\
+		    "[I] " FORMAT_STR, ##__VA_ARGS__)
+
+#define EVDI_DEBUG(FORMAT_STR, ...) \
+	EVDI_PRINTK(KERN_DEFAULT, EVDI_LOGLEVEL_DEBUG,\
+		    "[D] %s:%d " FORMAT_STR, __func__, __LINE__, ##__VA_ARGS__)
+
+#define EVDI_VERBOSE(FORMAT_STR, ...) \
+	EVDI_PRINTK(KERN_DEFAULT, EVDI_LOGLEVEL_VERBOSE,\
+		    "[V] %s:%d " FORMAT_STR, __func__, __LINE__, ##__VA_ARGS__)
+
+#define EVDI_CHECKPT() EVDI_VERBOSE("\n")
+#define EVDI_ENTER() EVDI_VERBOSE("enter\n")
+#define EVDI_EXIT() EVDI_VERBOSE("exit\n")
+
+void evdi_log_process(char *buf, size_t size);
+
+#endif /* EVDI_DEBUG_H */
diff -ruN a/drivers/gpu/drm/evdi/evdi_drm_drv.c b/drivers/gpu/drm/evdi/evdi_drm_drv.c
--- a/drivers/gpu/drm/evdi/evdi_drm_drv.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_drm_drv.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,276 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2012 Red Hat
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * Based on parts on udlfb.c:
+ * Copyright (C) 2009 its respective authors
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include <linux/version.h>
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#else
+#include <drm/drmP.h>
+#endif
+#if KERNEL_VERSION(5, 1, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#include <drm/drm_probe_helper.h>
+#endif
+
+#include "evdi_drm_drv.h"
+#include "evdi_platform_drv.h"
+#include "evdi_cursor.h"
+#include "evdi_debug.h"
+#include "evdi_drm.h"
+
+static struct drm_driver driver;
+
+struct drm_ioctl_desc evdi_painter_ioctls[] = {
+	DRM_IOCTL_DEF_DRV(EVDI_CONNECT, evdi_painter_connect_ioctl,
+			  DRM_UNLOCKED),
+	DRM_IOCTL_DEF_DRV(EVDI_REQUEST_UPDATE,
+			  evdi_painter_request_update_ioctl, DRM_UNLOCKED),
+	DRM_IOCTL_DEF_DRV(EVDI_GRABPIX, evdi_painter_grabpix_ioctl,
+			  DRM_UNLOCKED),
+	DRM_IOCTL_DEF_DRV(EVDI_DDCCI_RESPONSE, evdi_painter_ddcci_response_ioctl,
+			  DRM_UNLOCKED),
+	DRM_IOCTL_DEF_DRV(EVDI_ENABLE_CURSOR_EVENTS, evdi_painter_enable_cursor_events_ioctl,
+			  DRM_UNLOCKED),
+};
+
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+#else
+static const struct vm_operations_struct evdi_gem_vm_ops = {
+	.fault = evdi_gem_fault,
+	.open = drm_gem_vm_open,
+	.close = drm_gem_vm_close,
+};
+#endif
+
+static const struct file_operations evdi_driver_fops = {
+	.owner = THIS_MODULE,
+	.open = drm_open,
+	.mmap = evdi_drm_gem_mmap,
+	.poll = drm_poll,
+	.read = drm_read,
+	.unlocked_ioctl = drm_ioctl,
+	.release = drm_release,
+	.llseek = noop_llseek,
+};
+
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+#else
+static int evdi_enable_vblank(__always_unused struct drm_device *dev,
+			      __always_unused unsigned int pipe)
+{
+	return 1;
+}
+
+static void evdi_disable_vblank(__always_unused struct drm_device *dev,
+				__always_unused unsigned int pipe)
+{
+}
+#endif
+
+static struct drm_driver driver = {
+#if KERNEL_VERSION(5, 4, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	.driver_features = DRIVER_MODESET | DRIVER_GEM | DRIVER_ATOMIC,
+#else
+	.driver_features = DRIVER_MODESET | DRIVER_GEM | DRIVER_PRIME
+			 | DRIVER_ATOMIC,
+#endif
+	.unload = evdi_driver_unload,
+
+	.open = evdi_driver_open,
+	.postclose = evdi_driver_postclose,
+
+	/* gem hooks */
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+#elif KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	.gem_free_object_unlocked = evdi_gem_free_object,
+#else
+	.gem_free_object = evdi_gem_free_object,
+#endif
+
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+#else
+	.gem_vm_ops = &evdi_gem_vm_ops,
+#endif
+
+	.dumb_create = evdi_dumb_create,
+	.dumb_map_offset = evdi_gem_mmap,
+#if KERNEL_VERSION(5, 12, 0) <= LINUX_VERSION_CODE
+#else
+	.dumb_destroy = drm_gem_dumb_destroy,
+#endif
+
+	.ioctls = evdi_painter_ioctls,
+	.num_ioctls = ARRAY_SIZE(evdi_painter_ioctls),
+
+	.fops = &evdi_driver_fops,
+
+	.prime_fd_to_handle = drm_gem_prime_fd_to_handle,
+	.gem_prime_import = drm_gem_prime_import,
+	.prime_handle_to_fd = drm_gem_prime_handle_to_fd,
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+#else
+	.preclose = evdi_driver_preclose,
+	.gem_prime_export = drm_gem_prime_export,
+	.gem_prime_get_sg_table = evdi_prime_get_sg_table,
+	.enable_vblank = evdi_enable_vblank,
+	.disable_vblank = evdi_disable_vblank,
+#endif
+	.gem_prime_import_sg_table = evdi_prime_import_sg_table,
+
+	.name = DRIVER_NAME,
+	.desc = DRIVER_DESC,
+	.date = DRIVER_DATE,
+	.major = DRIVER_MAJOR,
+	.minor = DRIVER_MINOR,
+	.patchlevel = DRIVER_PATCH,
+};
+
+static int evdi_driver_setup(struct drm_device *dev)
+{
+	struct evdi_device *evdi;
+	int ret;
+
+	EVDI_CHECKPT();
+	evdi = kzalloc(sizeof(struct evdi_device), GFP_KERNEL);
+	if (!evdi)
+		return -ENOMEM;
+
+	evdi->ddev = dev;
+	dev->dev_private = evdi;
+	evdi->dev_index = dev->primary->index;
+
+	evdi->cursor_events_enabled = false;
+	ret =  evdi_cursor_init(&evdi->cursor);
+	if (ret)
+		goto err_free;
+
+	EVDI_CHECKPT();
+	evdi_modeset_init(dev);
+
+#ifdef CONFIG_FB
+	ret = evdi_fbdev_init(dev);
+	if (ret)
+		goto err_cursor;
+#endif /* CONFIG_FB */
+
+	ret = drm_vblank_init(dev, 1);
+	if (ret)
+		goto err_fb;
+
+	ret = evdi_painter_init(evdi);
+	if (ret)
+		goto err_fb;
+
+	drm_kms_helper_poll_init(dev);
+
+	return 0;
+
+err_fb:
+#ifdef CONFIG_FB
+	evdi_fbdev_cleanup(dev);
+err_cursor:
+#endif /* CONFIG_FB */
+	evdi_cursor_free(evdi->cursor);
+err_free:
+	EVDI_ERROR("Failed to setup drm device %d\n", ret);
+	kfree(evdi);
+	return ret;
+}
+
+void evdi_driver_unload(struct drm_device *dev)
+{
+	struct evdi_device *evdi = dev->dev_private;
+
+	EVDI_CHECKPT();
+
+	drm_kms_helper_poll_fini(dev);
+
+#ifdef CONFIG_FB
+	evdi_fbdev_unplug(dev);
+#endif /* CONFIG_FB */
+	if (evdi->cursor)
+		evdi_cursor_free(evdi->cursor);
+
+	evdi_painter_cleanup(evdi->painter);
+#ifdef CONFIG_FB
+	evdi_fbdev_cleanup(dev);
+#endif /* CONFIG_FB */
+	evdi_modeset_cleanup(dev);
+
+	kfree(evdi);
+}
+
+int evdi_driver_open(struct drm_device *drm_dev, __always_unused struct drm_file *file)
+{
+	struct evdi_device *evdi = drm_dev->dev_private;
+	char buf[100];
+
+	evdi_log_process(buf, sizeof(buf));
+	EVDI_INFO("(card%d) Opened by %s\n", evdi->dev_index, buf);
+	return 0;
+}
+
+static void evdi_driver_close(struct drm_device *drm_dev, struct drm_file *file)
+{
+	struct evdi_device *evdi = drm_dev->dev_private;
+
+	EVDI_CHECKPT();
+	if (evdi)
+		evdi_painter_close(evdi, file);
+}
+
+void evdi_driver_preclose(struct drm_device *drm_dev, struct drm_file *file)
+{
+	evdi_driver_close(drm_dev, file);
+}
+
+void evdi_driver_postclose(struct drm_device *drm_dev, struct drm_file *file)
+{
+	struct evdi_device *evdi = drm_dev->dev_private;
+	char buf[100];
+
+	evdi_log_process(buf, sizeof(buf));
+	EVDI_INFO("(card%d) Closed by %s\n",
+		   evdi->dev_index, buf);
+
+	evdi_driver_close(drm_dev, file);
+}
+
+struct drm_device *evdi_drm_device_create(struct device *parent)
+{
+	struct drm_device *dev = NULL;
+	int ret;
+
+	dev = drm_dev_alloc(&driver, parent);
+	if (IS_ERR(dev))
+		return dev;
+
+	ret = evdi_driver_setup(dev);
+	if (ret)
+		goto err_free;
+
+	ret = drm_dev_register(dev, 0);
+	if (ret)
+		goto err_free;
+
+	return dev;
+
+err_free:
+	drm_dev_put(dev);
+	return ERR_PTR(ret);
+}
+
+int evdi_drm_device_remove(struct drm_device *dev)
+{
+	drm_dev_unplug(dev);
+	return 0;
+}
+
diff -ruN a/drivers/gpu/drm/evdi/evdi_drm_drv.h b/drivers/gpu/drm/evdi/evdi_drm_drv.h
--- a/drivers/gpu/drm/evdi/evdi_drm_drv.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_drm_drv.h	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,188 @@
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (C) 2012 Red Hat
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * Based on parts on udlfb.c:
+ * Copyright (C) 2009 its respective authors
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#ifndef EVDI_DRV_H
+#define EVDI_DRV_H
+
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/version.h>
+#include <linux/device.h>
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#include <drm/drm_drv.h>
+#include <drm/drm_fourcc.h>
+#include <drm/drm_ioctl.h>
+#include <drm/drm_vblank.h>
+#else
+#include <drm/drmP.h>
+#endif
+#include <drm/drm_crtc.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/drm_rect.h>
+#include <drm/drm_gem.h>
+#if KERNEL_VERSION(5, 4, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#include <linux/dma-resv.h>
+#else
+#include <linux/reservation.h>
+#endif
+#include "evdi_debug.h"
+
+
+struct evdi_fbdev;
+struct evdi_painter;
+
+struct evdi_device {
+	struct drm_device *ddev;
+	struct drm_connector *conn;
+	struct evdi_cursor *cursor;
+	bool cursor_events_enabled;
+
+	uint32_t sku_area_limit;
+
+	struct evdi_fbdev *fbdev;
+	struct evdi_painter *painter;
+	struct i2c_adapter *i2c_adapter;
+
+	int dev_index;
+};
+
+struct evdi_gem_object {
+	struct drm_gem_object base;
+	struct page **pages;
+	unsigned int pages_pin_count;
+	struct mutex pages_lock;
+	void *vmapping;
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+	bool vmap_is_iomem;
+#endif
+	struct sg_table *sg;
+#if KERNEL_VERSION(5, 4, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	struct dma_resv *resv;
+	struct dma_resv _resv;
+#else
+	struct reservation_object *resv;
+	struct reservation_object _resv;
+#endif
+};
+
+#define to_evdi_bo(x) container_of(x, struct evdi_gem_object, base)
+
+struct evdi_framebuffer {
+	struct drm_framebuffer base;
+	struct evdi_gem_object *obj;
+	bool active;
+};
+
+#define to_evdi_fb(x) container_of(x, struct evdi_framebuffer, base)
+
+/* modeset */
+void evdi_modeset_init(struct drm_device *dev);
+void evdi_modeset_cleanup(struct drm_device *dev);
+int evdi_connector_init(struct drm_device *dev, struct drm_encoder *encoder);
+
+struct drm_encoder *evdi_encoder_init(struct drm_device *dev);
+
+int evdi_driver_load(struct drm_device *dev, unsigned long flags);
+void evdi_driver_unload(struct drm_device *dev);
+int evdi_driver_open(struct drm_device *drm_dev, struct drm_file *file);
+void evdi_driver_preclose(struct drm_device *dev, struct drm_file *file_priv);
+void evdi_driver_postclose(struct drm_device *dev, struct drm_file *file_priv);
+
+#ifdef CONFIG_FB
+int evdi_fbdev_init(struct drm_device *dev);
+void evdi_fbdev_cleanup(struct drm_device *dev);
+void evdi_fbdev_unplug(struct drm_device *dev);
+#endif /* CONFIG_FB */
+struct drm_framebuffer *evdi_fb_user_fb_create(
+				struct drm_device *dev,
+				struct drm_file *file,
+				const struct drm_mode_fb_cmd2 *mode_cmd);
+
+int evdi_dumb_create(struct drm_file *file_priv,
+		     struct drm_device *dev, struct drm_mode_create_dumb *args);
+int evdi_gem_mmap(struct drm_file *file_priv,
+		  struct drm_device *dev, uint32_t handle, uint64_t *offset);
+
+void evdi_gem_free_object(struct drm_gem_object *gem_obj);
+struct evdi_gem_object *evdi_gem_alloc_object(struct drm_device *dev,
+					      size_t size);
+uint32_t evdi_gem_object_handle_lookup(struct drm_file *filp,
+				      struct drm_gem_object *obj);
+
+struct sg_table *evdi_prime_get_sg_table(struct drm_gem_object *obj);
+struct drm_gem_object *
+evdi_prime_import_sg_table(struct drm_device *dev,
+			   struct dma_buf_attachment *attach,
+			   struct sg_table *sg);
+
+int evdi_gem_vmap(struct evdi_gem_object *obj);
+void evdi_gem_vunmap(struct evdi_gem_object *obj);
+int evdi_drm_gem_mmap(struct file *filp, struct vm_area_struct *vma);
+
+#if KERNEL_VERSION(4, 17, 0) <= LINUX_VERSION_CODE
+vm_fault_t evdi_gem_fault(struct vm_fault *vmf);
+#else
+int evdi_gem_fault(struct vm_fault *vmf);
+#endif
+
+bool evdi_painter_is_connected(struct evdi_painter *painter);
+void evdi_painter_close(struct evdi_device *evdi, struct drm_file *file);
+u8 *evdi_painter_get_edid_copy(struct evdi_device *evdi);
+int evdi_painter_get_num_dirts(struct evdi_painter *painter);
+void evdi_painter_mark_dirty(struct evdi_device *evdi,
+			     const struct drm_clip_rect *rect);
+void evdi_painter_set_vblank(struct evdi_painter *painter,
+			     struct drm_crtc *crtc,
+			     struct drm_pending_vblank_event *vblank);
+void evdi_painter_send_update_ready_if_needed(struct evdi_painter *painter);
+void evdi_painter_dpms_notify(struct evdi_device *evdi, int mode);
+void evdi_painter_mode_changed_notify(struct evdi_device *evdi,
+				      struct drm_display_mode *mode);
+unsigned int evdi_painter_poll(struct file *filp,
+			       struct poll_table_struct *wait);
+
+int evdi_painter_status_ioctl(struct drm_device *drm_dev, void *data,
+			      struct drm_file *file);
+int evdi_painter_connect_ioctl(struct drm_device *drm_dev, void *data,
+			       struct drm_file *file);
+int evdi_painter_grabpix_ioctl(struct drm_device *drm_dev, void *data,
+			       struct drm_file *file);
+int evdi_painter_request_update_ioctl(struct drm_device *drm_dev, void *data,
+				      struct drm_file *file);
+int evdi_painter_ddcci_response_ioctl(struct drm_device *drm_dev, void *data,
+				      struct drm_file *file);
+int evdi_painter_enable_cursor_events_ioctl(struct drm_device *drm_dev, void *data,
+					  struct drm_file *file);
+
+int evdi_painter_init(struct evdi_device *evdi);
+void evdi_painter_cleanup(struct evdi_painter *painter);
+void evdi_painter_set_scanout_buffer(struct evdi_painter *painter,
+				     struct evdi_framebuffer *buffer);
+
+struct drm_clip_rect evdi_framebuffer_sanitize_rect(
+			const struct evdi_framebuffer *fb,
+			const struct drm_clip_rect *rect);
+
+struct drm_device *evdi_drm_device_create(struct device *parent);
+int evdi_drm_device_remove(struct drm_device *dev);
+
+void evdi_painter_send_cursor_set(struct evdi_painter *painter,
+				  struct evdi_cursor *cursor);
+void evdi_painter_send_cursor_move(struct evdi_painter *painter,
+				   struct evdi_cursor *cursor);
+bool evdi_painter_needs_full_modeset(struct evdi_painter *painter);
+void evdi_painter_force_full_modeset(struct evdi_painter *painter);
+struct drm_clip_rect evdi_painter_framebuffer_size(struct evdi_painter *painter);
+bool evdi_painter_i2c_data_notify(struct evdi_painter *painter, struct i2c_msg *msg);
+
+int evdi_fb_get_bpp(uint32_t format);
+#endif
diff -ruN a/drivers/gpu/drm/evdi/evdi_drm.h b/drivers/gpu/drm/evdi/evdi_drm.h
--- a/drivers/gpu/drm/evdi/evdi_drm.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_drm.h	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,131 @@
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (c) 2016 - 2020 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#ifndef __UAPI_EVDI_DRM_H__
+#define __UAPI_EVDI_DRM_H__
+
+/* Output events sent from driver to evdi lib */
+#define DRM_EVDI_EVENT_UPDATE_READY  0x80000000
+#define DRM_EVDI_EVENT_DPMS          0x80000001
+#define DRM_EVDI_EVENT_MODE_CHANGED  0x80000002
+#define DRM_EVDI_EVENT_CRTC_STATE    0x80000003
+#define DRM_EVDI_EVENT_CURSOR_SET    0x80000004
+#define DRM_EVDI_EVENT_CURSOR_MOVE   0x80000005
+#define DRM_EVDI_EVENT_DDCCI_DATA    0x80000006
+
+
+struct drm_evdi_event_update_ready {
+	struct drm_event base;
+};
+
+struct drm_evdi_event_dpms {
+	struct drm_event base;
+	int32_t mode;
+};
+
+struct drm_evdi_event_mode_changed {
+	struct drm_event base;
+	int32_t hdisplay;
+	int32_t vdisplay;
+	int32_t vrefresh;
+	int32_t bits_per_pixel;
+	uint32_t pixel_format;
+};
+
+struct drm_evdi_event_crtc_state {
+	struct drm_event base;
+	int32_t state;
+};
+
+struct drm_evdi_connect {
+	int32_t connected;
+	int32_t dev_index;
+	const unsigned char * __user edid;
+	uint32_t edid_length;
+	uint32_t sku_area_limit;
+};
+
+struct drm_evdi_request_update {
+	int32_t reserved;
+};
+
+enum drm_evdi_grabpix_mode {
+	EVDI_GRABPIX_MODE_RECTS = 0,
+	EVDI_GRABPIX_MODE_DIRTY = 1,
+};
+
+struct drm_evdi_grabpix {
+	enum drm_evdi_grabpix_mode mode;
+	int32_t buf_width;
+	int32_t buf_height;
+	int32_t buf_byte_stride;
+	unsigned char __user *buffer;
+	int32_t num_rects;
+	struct drm_clip_rect __user *rects;
+};
+
+struct drm_evdi_event_cursor_set {
+	struct drm_event base;
+	int32_t hot_x;
+	int32_t hot_y;
+	uint32_t width;
+	uint32_t height;
+	uint8_t enabled;
+	uint32_t buffer_handle;
+	uint32_t buffer_length;
+	uint32_t pixel_format;
+	uint32_t stride;
+};
+
+struct drm_evdi_event_cursor_move {
+	struct drm_event base;
+	int32_t x;
+	int32_t y;
+};
+
+struct drm_evdi_ddcci_response {
+	const unsigned char * __user buffer;
+	uint32_t buffer_length;
+	uint8_t result;
+};
+
+struct drm_evdi_enable_cursor_events {
+	struct drm_event base;
+	uint8_t enable;
+};
+
+#define DDCCI_BUFFER_SIZE 64
+
+struct drm_evdi_event_ddcci_data {
+	struct drm_event base;
+	unsigned char buffer[DDCCI_BUFFER_SIZE];
+	uint32_t buffer_length;
+	uint16_t flags;
+	uint16_t address;
+};
+
+/* Input ioctls from evdi lib to driver */
+#define DRM_EVDI_CONNECT          0x00
+#define DRM_EVDI_REQUEST_UPDATE   0x01
+#define DRM_EVDI_GRABPIX          0x02
+#define DRM_EVDI_DDCCI_RESPONSE   0x03
+#define DRM_EVDI_ENABLE_CURSOR_EVENTS 0x04
+/* LAST_IOCTL 0x5F -- 96 driver specific ioctls to use */
+
+#define DRM_IOCTL_EVDI_CONNECT DRM_IOWR(DRM_COMMAND_BASE +  \
+	DRM_EVDI_CONNECT, struct drm_evdi_connect)
+#define DRM_IOCTL_EVDI_REQUEST_UPDATE DRM_IOWR(DRM_COMMAND_BASE +  \
+	DRM_EVDI_REQUEST_UPDATE, struct drm_evdi_request_update)
+#define DRM_IOCTL_EVDI_GRABPIX DRM_IOWR(DRM_COMMAND_BASE +  \
+	DRM_EVDI_GRABPIX, struct drm_evdi_grabpix)
+#define DRM_IOCTL_EVDI_DDCCI_RESPONSE DRM_IOWR(DRM_COMMAND_BASE +  \
+	DRM_EVDI_DDCCI_RESPONSE, struct drm_evdi_ddcci_response)
+#define DRM_IOCTL_EVDI_ENABLE_CURSOR_EVENTS DRM_IOWR(DRM_COMMAND_BASE +  \
+	DRM_EVDI_ENABLE_CURSOR_EVENTS, struct drm_evdi_enable_cursor_events)
+
+#endif /* __EVDI_UAPI_DRM_H__ */
diff -ruN a/drivers/gpu/drm/evdi/evdi_encoder.c b/drivers/gpu/drm/evdi/evdi_encoder.c
--- a/drivers/gpu/drm/evdi/evdi_encoder.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_encoder.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,72 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2012 Red Hat
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * Based on parts on udlfb.c:
+ * Copyright (C) 2009 its respective authors
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include <linux/version.h>
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#else
+#include <drm/drmP.h>
+#endif
+#include <drm/drm_crtc.h>
+#include <drm/drm_crtc_helper.h>
+#include "evdi_drm_drv.h"
+
+/* dummy encoder */
+static void evdi_enc_destroy(struct drm_encoder *encoder)
+{
+	drm_encoder_cleanup(encoder);
+	kfree(encoder);
+}
+
+static void evdi_encoder_enable(__always_unused struct drm_encoder *encoder)
+{
+}
+
+static void evdi_encoder_disable(__always_unused struct drm_encoder *encoder)
+{
+}
+
+static const struct drm_encoder_helper_funcs evdi_enc_helper_funcs = {
+	.enable = evdi_encoder_enable,
+	.disable = evdi_encoder_disable
+};
+
+static const struct drm_encoder_funcs evdi_enc_funcs = {
+	.destroy = evdi_enc_destroy,
+};
+
+struct drm_encoder *evdi_encoder_init(struct drm_device *dev)
+{
+	struct drm_encoder *encoder;
+	int ret = 0;
+
+	encoder = kzalloc(sizeof(struct drm_encoder), GFP_KERNEL);
+	if (!encoder)
+		goto err;
+
+	ret = drm_encoder_init(dev, encoder, &evdi_enc_funcs,
+			       DRM_MODE_ENCODER_TMDS, dev_name(dev->dev));
+	if (ret) {
+		EVDI_ERROR("Failed to initialize encoder: %d\n", ret);
+		goto err_encoder;
+	}
+
+	drm_encoder_helper_add(encoder, &evdi_enc_helper_funcs);
+
+	encoder->possible_crtcs = 1;
+	return encoder;
+
+err_encoder:
+	kfree(encoder);
+err:
+	return NULL;
+}
diff -ruN a/drivers/gpu/drm/evdi/evdi_fb.c b/drivers/gpu/drm/evdi/evdi_fb.c
--- a/drivers/gpu/drm/evdi/evdi_fb.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_fb.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,609 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2012 Red Hat
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * Based on parts on udlfb.c:
+ * Copyright (C) 2009 its respective authors
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include <linux/slab.h>
+#ifdef CONFIG_FB
+#include <linux/fb.h>
+#endif /* CONFIG_FB */
+#include <linux/dma-buf.h>
+#include <linux/version.h>
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#else
+#include <drm/drmP.h>
+#endif
+#include <drm/drm_crtc.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/drm_fb_helper.h>
+#include <drm/drm_atomic.h>
+#if KERNEL_VERSION(5, 0, 0) <= LINUX_VERSION_CODE
+#include <drm/drm_damage_helper.h>
+#endif
+#include "evdi_drm_drv.h"
+
+
+struct evdi_fbdev {
+	struct drm_fb_helper helper;
+	struct evdi_framebuffer efb;
+	struct list_head fbdev_list;
+	struct fb_ops fb_ops;
+	int fb_count;
+};
+
+struct drm_clip_rect evdi_framebuffer_sanitize_rect(
+				const struct evdi_framebuffer *fb,
+				const struct drm_clip_rect *dirty_rect)
+{
+	struct drm_clip_rect rect = *dirty_rect;
+
+	if (rect.x1 > rect.x2) {
+		unsigned short tmp = rect.x2;
+
+		EVDI_WARN("Wrong clip rect: x1 > x2\n");
+		rect.x2 = rect.x1;
+		rect.x1 = tmp;
+	}
+
+	if (rect.y1 > rect.y2) {
+		unsigned short tmp = rect.y2;
+
+		EVDI_WARN("Wrong clip rect: y1 > y2\n");
+		rect.y2 = rect.y1;
+		rect.y1 = tmp;
+	}
+
+
+	if (rect.x1 > fb->base.width) {
+		EVDI_WARN("Wrong clip rect: x1 > fb.width\n");
+		rect.x1 = fb->base.width;
+	}
+
+	if (rect.y1 > fb->base.height) {
+		EVDI_WARN("Wrong clip rect: y1 > fb.height\n");
+		rect.y1 = fb->base.height;
+	}
+
+	if (rect.x2 > fb->base.width) {
+		EVDI_WARN("Wrong clip rect: x2 > fb.width\n");
+		rect.x2 = fb->base.width;
+	}
+
+	if (rect.y2 > fb->base.height) {
+		EVDI_WARN("Wrong clip rect: y2 > fb.height\n");
+		rect.y2 = fb->base.height;
+	}
+
+	return rect;
+}
+
+#ifdef CONFIG_FB
+static int evdi_handle_damage(struct evdi_framebuffer *fb,
+		       int x, int y, int width, int height)
+{
+	const struct drm_clip_rect dirty_rect = { x, y, x + width, y + height };
+	const struct drm_clip_rect rect =
+		evdi_framebuffer_sanitize_rect(fb, &dirty_rect);
+	struct drm_device *dev = fb->base.dev;
+	struct evdi_device *evdi = dev->dev_private;
+
+	EVDI_CHECKPT();
+
+	if (!fb->active)
+		return 0;
+	evdi_painter_set_scanout_buffer(evdi->painter, fb);
+	evdi_painter_mark_dirty(evdi, &rect);
+
+	return 0;
+}
+
+static int evdi_fb_mmap(struct fb_info *info, struct vm_area_struct *vma)
+{
+	unsigned long start = vma->vm_start;
+	unsigned long size = vma->vm_end - vma->vm_start;
+	unsigned long offset = vma->vm_pgoff << PAGE_SHIFT;
+	unsigned long page, pos;
+
+	if (vma->vm_pgoff > (~0UL >> PAGE_SHIFT))
+		return -EINVAL;
+
+	if (offset > info->fix.smem_len ||
+	    size > info->fix.smem_len - offset)
+		return -EINVAL;
+
+	pos = (unsigned long)info->fix.smem_start + offset;
+
+	pr_notice("mmap() framebuffer addr:%lu size:%lu\n", pos, size);
+
+	while (size > 0) {
+		page = vmalloc_to_pfn((void *)pos);
+		if (remap_pfn_range(vma, start, page, PAGE_SIZE, PAGE_SHARED))
+			return -EAGAIN;
+
+		start += PAGE_SIZE;
+		pos += PAGE_SIZE;
+		if (size > PAGE_SIZE)
+			size -= PAGE_SIZE;
+		else
+			size = 0;
+	}
+
+	return 0;
+}
+
+static void evdi_fb_fillrect(struct fb_info *info,
+			     const struct fb_fillrect *rect)
+{
+	struct evdi_fbdev *efbdev = info->par;
+
+	EVDI_CHECKPT();
+	sys_fillrect(info, rect);
+	evdi_handle_damage(&efbdev->efb, rect->dx, rect->dy, rect->width,
+			   rect->height);
+}
+
+static void evdi_fb_copyarea(struct fb_info *info,
+			     const struct fb_copyarea *region)
+{
+	struct evdi_fbdev *efbdev = info->par;
+
+	EVDI_CHECKPT();
+	sys_copyarea(info, region);
+	evdi_handle_damage(&efbdev->efb, region->dx, region->dy, region->width,
+			   region->height);
+}
+
+static void evdi_fb_imageblit(struct fb_info *info,
+			      const struct fb_image *image)
+{
+	struct evdi_fbdev *efbdev = info->par;
+
+	EVDI_CHECKPT();
+	sys_imageblit(info, image);
+	evdi_handle_damage(&efbdev->efb, image->dx, image->dy, image->width,
+			   image->height);
+}
+
+/*
+ * It's common for several clients to have framebuffer open simultaneously.
+ * e.g. both fbcon and X. Makes things interesting.
+ * Assumes caller is holding info->lock (for open and release at least)
+ */
+static int evdi_fb_open(struct fb_info *info, int user)
+{
+	struct evdi_fbdev *efbdev = info->par;
+
+	efbdev->fb_count++;
+	pr_notice("open /dev/fb%d user=%d fb_info=%p count=%d\n",
+		  info->node, user, info, efbdev->fb_count);
+
+	return 0;
+}
+
+/*
+ * Assumes caller is holding info->lock mutex (for open and release at least)
+ */
+static int evdi_fb_release(struct fb_info *info, int user)
+{
+	struct evdi_fbdev *efbdev = info->par;
+
+	efbdev->fb_count--;
+
+	pr_warn("released /dev/fb%d user=%d count=%d\n",
+		info->node, user, efbdev->fb_count);
+
+	return 0;
+}
+
+static struct fb_ops evdifb_ops = {
+	.owner = THIS_MODULE,
+	.fb_check_var = drm_fb_helper_check_var,
+	.fb_set_par = drm_fb_helper_set_par,
+	.fb_fillrect = evdi_fb_fillrect,
+	.fb_copyarea = evdi_fb_copyarea,
+	.fb_imageblit = evdi_fb_imageblit,
+	.fb_pan_display = drm_fb_helper_pan_display,
+	.fb_blank = drm_fb_helper_blank,
+	.fb_setcmap = drm_fb_helper_setcmap,
+	.fb_debug_enter = drm_fb_helper_debug_enter,
+	.fb_debug_leave = drm_fb_helper_debug_leave,
+	.fb_mmap = evdi_fb_mmap,
+	.fb_open = evdi_fb_open,
+	.fb_release = evdi_fb_release,
+};
+#endif /* CONFIG_FB */
+
+#if KERNEL_VERSION(5, 0, 0) <= LINUX_VERSION_CODE
+#else
+/*
+ * Function taken from
+ * https://lists.freedesktop.org/archives/dri-devel/2018-September/188716.html
+ */
+static int evdi_user_framebuffer_dirty(
+		struct drm_framebuffer *fb,
+		__maybe_unused struct drm_file *file_priv,
+		__always_unused unsigned int flags,
+		__always_unused unsigned int color,
+		__always_unused struct drm_clip_rect *clips,
+		__always_unused unsigned int num_clips)
+{
+	struct evdi_framebuffer *efb = to_evdi_fb(fb);
+	struct drm_device *dev = efb->base.dev;
+	struct evdi_device *evdi = dev->dev_private;
+
+	struct drm_modeset_acquire_ctx ctx;
+	struct drm_atomic_state *state;
+	struct drm_plane *plane;
+	int ret = 0;
+	int i;
+
+	EVDI_CHECKPT();
+
+	drm_modeset_acquire_init(&ctx,
+		/*
+		 * When called from ioctl, we are interruptable,
+		 * but not when called internally (ie. defio worker)
+		 */
+		file_priv ? DRM_MODESET_ACQUIRE_INTERRUPTIBLE :	0);
+
+	state = drm_atomic_state_alloc(fb->dev);
+	if (!state) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	state->acquire_ctx = &ctx;
+
+	for (i = 0; i < num_clips; ++i)
+		evdi_painter_mark_dirty(evdi, &clips[i]);
+
+retry:
+
+	drm_for_each_plane(plane, fb->dev) {
+		struct drm_plane_state *plane_state;
+
+		if (plane->state->fb != fb)
+			continue;
+
+		/*
+		 * Even if it says 'get state' this function will create and
+		 * initialize state if it does not exists. We use this property
+		 * to force create state.
+		 */
+		plane_state = drm_atomic_get_plane_state(state, plane);
+		if (IS_ERR(plane_state)) {
+			ret = PTR_ERR(plane_state);
+			goto out;
+		}
+	}
+
+	ret = drm_atomic_commit(state);
+
+out:
+	if (ret == -EDEADLK) {
+		drm_atomic_state_clear(state);
+		ret = drm_modeset_backoff(&ctx);
+		if (!ret)
+			goto retry;
+	}
+
+	if (state)
+		drm_atomic_state_put(state);
+
+	drm_modeset_drop_locks(&ctx);
+	drm_modeset_acquire_fini(&ctx);
+
+	return ret;
+}
+#endif
+
+static int evdi_user_framebuffer_create_handle(struct drm_framebuffer *fb,
+					       struct drm_file *file_priv,
+					       unsigned int *handle)
+{
+	struct evdi_framebuffer *efb = to_evdi_fb(fb);
+
+	return drm_gem_handle_create(file_priv, &efb->obj->base, handle);
+}
+
+static void evdi_user_framebuffer_destroy(struct drm_framebuffer *fb)
+{
+	struct evdi_framebuffer *efb = to_evdi_fb(fb);
+
+	EVDI_CHECKPT();
+	if (efb->obj)
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE || defined(EL8)
+		drm_gem_object_put(&efb->obj->base);
+#else
+		drm_gem_object_put_unlocked(&efb->obj->base);
+#endif
+	drm_framebuffer_cleanup(fb);
+	kfree(efb);
+}
+
+static const struct drm_framebuffer_funcs evdifb_funcs = {
+	.create_handle = evdi_user_framebuffer_create_handle,
+	.destroy = evdi_user_framebuffer_destroy,
+#if KERNEL_VERSION(5, 0, 0) <= LINUX_VERSION_CODE
+	.dirty = drm_atomic_helper_dirtyfb,
+#else
+	.dirty = evdi_user_framebuffer_dirty,
+#endif
+};
+
+static int
+evdi_framebuffer_init(struct drm_device *dev,
+		      struct evdi_framebuffer *efb,
+		      const struct drm_mode_fb_cmd2 *mode_cmd,
+		      struct evdi_gem_object *obj)
+{
+	efb->obj = obj;
+	drm_helper_mode_fill_fb_struct(dev, &efb->base, mode_cmd);
+	return drm_framebuffer_init(dev, &efb->base, &evdifb_funcs);
+}
+
+#ifdef CONFIG_FB
+static int evdifb_create(struct drm_fb_helper *helper,
+			 struct drm_fb_helper_surface_size *sizes)
+{
+	struct evdi_fbdev *efbdev = (struct evdi_fbdev *)helper;
+	struct drm_device *dev = efbdev->helper.dev;
+	struct fb_info *info;
+	struct device *device = dev->dev;
+	struct drm_framebuffer *fb;
+	struct drm_mode_fb_cmd2 mode_cmd;
+	struct evdi_gem_object *obj;
+	uint32_t size;
+	int ret = 0;
+
+	if (sizes->surface_bpp == 24) {
+		sizes->surface_bpp = 32;
+	} else if (sizes->surface_bpp != 32) {
+		EVDI_ERROR("Not supported pixel format (bpp=%d)\n",
+			   sizes->surface_bpp);
+		return -EINVAL;
+	}
+
+	mode_cmd.width = sizes->surface_width;
+	mode_cmd.height = sizes->surface_height;
+	mode_cmd.pitches[0] = mode_cmd.width * ((sizes->surface_bpp + 7) / 8);
+
+	mode_cmd.pixel_format = drm_mode_legacy_fb_format(sizes->surface_bpp,
+							  sizes->surface_depth);
+
+	size = mode_cmd.pitches[0] * mode_cmd.height;
+	size = ALIGN(size, PAGE_SIZE);
+
+	obj = evdi_gem_alloc_object(dev, size);
+	if (!obj)
+		goto out;
+
+	ret = evdi_gem_vmap(obj);
+	if (ret) {
+		DRM_ERROR("failed to vmap fb\n");
+		goto out_gfree;
+	}
+
+	info = framebuffer_alloc(0, device);
+	if (!info) {
+		ret = -ENOMEM;
+		goto out_gfree;
+	}
+	info->par = efbdev;
+
+	ret = evdi_framebuffer_init(dev, &efbdev->efb, &mode_cmd, obj);
+	if (ret)
+		goto out_gfree;
+
+	fb = &efbdev->efb.base;
+
+	efbdev->helper.fb = fb;
+	efbdev->helper.fbdev = info;
+
+	strcpy(info->fix.id, "evdidrmfb");
+
+	info->screen_base = efbdev->efb.obj->vmapping;
+	info->fix.smem_len = size;
+	info->fix.smem_start = (unsigned long)efbdev->efb.obj->vmapping;
+
+#if KERNEL_VERSION(4, 20, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	info->flags = FBINFO_DEFAULT;
+#else
+	info->flags = FBINFO_DEFAULT | FBINFO_CAN_FORCE_OUTPUT;
+#endif
+
+	efbdev->fb_ops = evdifb_ops;
+	info->fbops = &efbdev->fb_ops;
+
+#if KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	drm_fb_helper_fill_info(info, &efbdev->helper, sizes);
+#else
+	drm_fb_helper_fill_fix(info, fb->pitches[0], fb->format->depth);
+	drm_fb_helper_fill_var(info, &efbdev->helper, sizes->fb_width,
+			       sizes->fb_height);
+#endif
+
+	ret = fb_alloc_cmap(&info->cmap, 256, 0);
+	if (ret) {
+		ret = -ENOMEM;
+		goto out_gfree;
+	}
+
+	DRM_DEBUG_KMS("allocated %dx%d vmal %p\n",
+		      fb->width, fb->height, efbdev->efb.obj->vmapping);
+
+	return ret;
+ out_gfree:
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	drm_gem_object_put(&efbdev->efb.obj->base);
+#else
+	drm_gem_object_put_unlocked(&efbdev->efb.obj->base);
+#endif
+ out:
+	return ret;
+}
+
+static struct drm_fb_helper_funcs evdi_fb_helper_funcs = {
+	.fb_probe = evdifb_create,
+};
+
+static void evdi_fbdev_destroy(__always_unused struct drm_device *dev,
+			       struct evdi_fbdev *efbdev)
+{
+	struct fb_info *info;
+
+	if (efbdev->helper.fbdev) {
+		info = efbdev->helper.fbdev;
+		unregister_framebuffer(info);
+		if (info->cmap.len)
+			fb_dealloc_cmap(&info->cmap);
+
+		framebuffer_release(info);
+	}
+	drm_fb_helper_fini(&efbdev->helper);
+	if (efbdev->efb.obj) {
+		drm_framebuffer_unregister_private(&efbdev->efb.base);
+		drm_framebuffer_cleanup(&efbdev->efb.base);
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE || defined(EL8)
+		drm_gem_object_put(&efbdev->efb.obj->base);
+#else
+		drm_gem_object_put_unlocked(&efbdev->efb.obj->base);
+#endif
+	}
+}
+
+int evdi_fbdev_init(struct drm_device *dev)
+{
+	struct evdi_device *evdi;
+	struct evdi_fbdev *efbdev;
+	int ret;
+
+	evdi = dev->dev_private;
+	efbdev = kzalloc(sizeof(struct evdi_fbdev), GFP_KERNEL);
+	if (!efbdev)
+		return -ENOMEM;
+
+	evdi->fbdev = efbdev;
+	drm_fb_helper_prepare(dev, &efbdev->helper, &evdi_fb_helper_funcs);
+
+#if KERNEL_VERSION(5, 7, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	ret = drm_fb_helper_init(dev, &efbdev->helper);
+#else
+	ret = drm_fb_helper_init(dev, &efbdev->helper, 1);
+#endif
+	if (ret) {
+		kfree(efbdev);
+		return ret;
+	}
+
+#if KERNEL_VERSION(5, 7, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#else
+	drm_fb_helper_single_add_all_connectors(&efbdev->helper);
+#endif
+
+	ret = drm_fb_helper_initial_config(&efbdev->helper, 32);
+	if (ret) {
+		drm_fb_helper_fini(&efbdev->helper);
+		kfree(efbdev);
+	}
+	return ret;
+}
+
+void evdi_fbdev_cleanup(struct drm_device *dev)
+{
+	struct evdi_device *evdi = dev->dev_private;
+
+	if (!evdi->fbdev)
+		return;
+
+	evdi_fbdev_destroy(dev, evdi->fbdev);
+	kfree(evdi->fbdev);
+	evdi->fbdev = NULL;
+}
+
+void evdi_fbdev_unplug(struct drm_device *dev)
+{
+	struct evdi_device *evdi = dev->dev_private;
+	struct evdi_fbdev *efbdev;
+
+	if (!evdi->fbdev)
+		return;
+
+	efbdev = evdi->fbdev;
+	if (efbdev->helper.fbdev) {
+		struct fb_info *info;
+
+		info = efbdev->helper.fbdev;
+#if KERNEL_VERSION(5, 6, 0) <= LINUX_VERSION_CODE
+		unregister_framebuffer(info);
+#else
+		unlink_framebuffer(info);
+#endif
+	}
+}
+#endif /* CONFIG_FB */
+
+int evdi_fb_get_bpp(uint32_t format)
+{
+	const struct drm_format_info *info = drm_format_info(format);
+
+	if (!info)
+		return 0;
+	return info->cpp[0] * 8;
+}
+
+struct drm_framebuffer *evdi_fb_user_fb_create(
+					struct drm_device *dev,
+					struct drm_file *file,
+					const struct drm_mode_fb_cmd2 *mode_cmd)
+{
+	struct drm_gem_object *obj;
+	struct evdi_framebuffer *efb;
+	int ret;
+	uint32_t size;
+	int bpp = evdi_fb_get_bpp(mode_cmd->pixel_format);
+
+	if (bpp != 32) {
+		EVDI_ERROR("Unsupported bpp (%d)\n", bpp);
+		return ERR_PTR(-EINVAL);
+	}
+
+	obj = drm_gem_object_lookup(file, mode_cmd->handles[0]);
+	if (obj == NULL)
+		return ERR_PTR(-ENOENT);
+
+	size = mode_cmd->offsets[0] + mode_cmd->pitches[0] * mode_cmd->height;
+	size = ALIGN(size, PAGE_SIZE);
+
+	if (size > obj->size) {
+		DRM_ERROR("object size not sufficient for fb %d %zu %u %d %d\n",
+			  size, obj->size, mode_cmd->offsets[0],
+			  mode_cmd->pitches[0], mode_cmd->height);
+		goto err_no_mem;
+	}
+
+	efb = kzalloc(sizeof(*efb), GFP_KERNEL);
+	if (efb == NULL)
+		goto err_no_mem;
+	efb->base.obj[0] = obj;
+
+	ret = evdi_framebuffer_init(dev, efb, mode_cmd, to_evdi_bo(obj));
+	if (ret)
+		goto err_inval;
+	return &efb->base;
+
+ err_no_mem:
+	drm_gem_object_put(obj);
+	return ERR_PTR(-ENOMEM);
+ err_inval:
+	kfree(efb);
+	drm_gem_object_put(obj);
+	return ERR_PTR(-EINVAL);
+}
diff -ruN a/drivers/gpu/drm/evdi/evdi_gem.c b/drivers/gpu/drm/evdi/evdi_gem.c
--- a/drivers/gpu/drm/evdi/evdi_gem.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_gem.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,431 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2012 Red Hat
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include <linux/version.h>
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#else
+#include <drm/drmP.h>
+#endif
+#include "evdi_drm_drv.h"
+#include "evdi_params.h"
+#include <linux/shmem_fs.h>
+#include <linux/dma-buf.h>
+#include <linux/vmalloc.h>
+#include <drm/drm_cache.h>
+
+static int evdi_prime_pin(struct drm_gem_object *obj);
+static void evdi_prime_unpin(struct drm_gem_object *obj);
+
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+static const struct vm_operations_struct evdi_gem_vm_ops = {
+	.fault = evdi_gem_fault,
+	.open = drm_gem_vm_open,
+	.close = drm_gem_vm_close,
+};
+
+static struct drm_gem_object_funcs gem_obj_funcs = {
+	.free = evdi_gem_free_object,
+	.pin = evdi_prime_pin,
+	.unpin = evdi_prime_unpin,
+	.vm_ops = &evdi_gem_vm_ops,
+	.export = drm_gem_prime_export,
+	.get_sg_table = evdi_prime_get_sg_table,
+};
+#endif
+
+uint32_t evdi_gem_object_handle_lookup(struct drm_file *filp,
+				       struct drm_gem_object *obj)
+{
+	uint32_t it_handle = 0;
+	struct drm_gem_object *it_obj = NULL;
+
+	spin_lock(&filp->table_lock);
+	idr_for_each_entry(&filp->object_idr, it_obj, it_handle) {
+		if (it_obj == obj)
+			break;
+	}
+	spin_unlock(&filp->table_lock);
+
+	if (!it_obj)
+		it_handle = 0;
+
+	return it_handle;
+}
+
+struct evdi_gem_object *evdi_gem_alloc_object(struct drm_device *dev,
+					      size_t size)
+{
+	struct evdi_gem_object *obj;
+
+	obj = kzalloc(sizeof(*obj), GFP_KERNEL);
+	if (obj == NULL)
+		return NULL;
+
+	if (drm_gem_object_init(dev, &obj->base, size) != 0) {
+		kfree(obj);
+		return NULL;
+	}
+
+#if KERNEL_VERSION(5, 4, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	dma_resv_init(&obj->_resv);
+#else
+	reservation_object_init(&obj->_resv);
+#endif
+	obj->resv = &obj->_resv;
+
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+	obj->base.funcs = &gem_obj_funcs;
+#endif
+
+	mutex_init(&obj->pages_lock);
+
+	return obj;
+}
+
+static int
+evdi_gem_create(struct drm_file *file,
+		struct drm_device *dev, uint64_t size, uint32_t *handle_p)
+{
+	struct evdi_gem_object *obj;
+	int ret;
+	u32 handle;
+
+	size = roundup(size, PAGE_SIZE);
+
+	obj = evdi_gem_alloc_object(dev, size);
+	if (obj == NULL)
+		return -ENOMEM;
+
+	ret = drm_gem_handle_create(file, &obj->base, &handle);
+	if (ret) {
+		drm_gem_object_release(&obj->base);
+		kfree(obj);
+		return ret;
+	}
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	drm_gem_object_put(&obj->base);
+#else
+	drm_gem_object_put_unlocked(&obj->base);
+#endif
+	*handle_p = handle;
+	return 0;
+}
+
+static int evdi_align_pitch(int width, int cpp)
+{
+	int aligned = width;
+	int pitch_mask = 0;
+
+	switch (cpp) {
+	case 1:
+		pitch_mask = 255;
+		break;
+	case 2:
+		pitch_mask = 127;
+		break;
+	case 3:
+	case 4:
+		pitch_mask = 63;
+		break;
+	}
+
+	aligned += pitch_mask;
+	aligned &= ~pitch_mask;
+	return aligned * cpp;
+}
+
+int evdi_dumb_create(struct drm_file *file,
+		     struct drm_device *dev, struct drm_mode_create_dumb *args)
+{
+	args->pitch = evdi_align_pitch(args->width, DIV_ROUND_UP(args->bpp, 8));
+
+	args->size = args->pitch * args->height;
+	return evdi_gem_create(file, dev, args->size, &args->handle);
+}
+
+int evdi_drm_gem_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	int ret;
+
+	ret = drm_gem_mmap(filp, vma);
+	if (ret)
+		return ret;
+
+	vma->vm_flags &= ~VM_PFNMAP;
+	vma->vm_flags |= VM_MIXEDMAP;
+
+	return ret;
+}
+
+#if KERNEL_VERSION(4, 17, 0) <= LINUX_VERSION_CODE
+vm_fault_t evdi_gem_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+#else
+int evdi_gem_fault(struct vm_fault *vmf)
+{
+	struct vm_area_struct *vma = vmf->vma;
+#endif
+	struct evdi_gem_object *obj = to_evdi_bo(vma->vm_private_data);
+	struct page *page;
+	unsigned int page_offset;
+	int ret = 0;
+
+	page_offset = (vmf->address - vma->vm_start) >> PAGE_SHIFT;
+
+	if (!obj->pages)
+		return VM_FAULT_SIGBUS;
+
+	page = obj->pages[page_offset];
+	ret = vm_insert_page(vma, vmf->address, page);
+	switch (ret) {
+	case -EAGAIN:
+	case 0:
+	case -ERESTARTSYS:
+		return VM_FAULT_NOPAGE;
+	case -ENOMEM:
+		return VM_FAULT_OOM;
+	default:
+		return VM_FAULT_SIGBUS;
+	}
+	return VM_FAULT_SIGBUS;
+}
+
+static int evdi_gem_get_pages(struct evdi_gem_object *obj,
+			      __always_unused gfp_t gfpmask)
+{
+	struct page **pages;
+
+	if (obj->pages)
+		return 0;
+
+	pages = drm_gem_get_pages(&obj->base);
+
+	if (IS_ERR(pages))
+		return PTR_ERR(pages);
+
+	obj->pages = pages;
+
+#if defined(CONFIG_X86)
+	drm_clflush_pages(obj->pages, obj->base.size / PAGE_SIZE);
+#endif
+
+	return 0;
+}
+
+static void evdi_gem_put_pages(struct evdi_gem_object *obj)
+{
+	if (obj->base.import_attach) {
+		kvfree(obj->pages);
+		obj->pages = NULL;
+		return;
+	}
+
+	drm_gem_put_pages(&obj->base, obj->pages, false, false);
+	obj->pages = NULL;
+}
+
+static int evdi_pin_pages(struct evdi_gem_object *obj)
+{
+	int ret = 0;
+
+	mutex_lock(&obj->pages_lock);
+	if (obj->pages_pin_count++ == 0) {
+		ret = evdi_gem_get_pages(obj, GFP_KERNEL);
+		if (ret)
+			obj->pages_pin_count--;
+	}
+	mutex_unlock(&obj->pages_lock);
+
+	return ret;
+}
+
+static void evdi_unpin_pages(struct evdi_gem_object *obj)
+{
+	mutex_lock(&obj->pages_lock);
+	if (--obj->pages_pin_count == 0)
+		evdi_gem_put_pages(obj);
+	mutex_unlock(&obj->pages_lock);
+}
+
+int evdi_gem_vmap(struct evdi_gem_object *obj)
+{
+	int page_count = obj->base.size / PAGE_SIZE;
+	int ret;
+
+	if (obj->base.import_attach) {
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+		struct dma_buf_map map;
+
+		ret = dma_buf_vmap(obj->base.import_attach->dmabuf, &map);
+		if (ret)
+			return -ENOMEM;
+		obj->vmapping = map.vaddr;
+		obj->vmap_is_iomem = map.is_iomem;
+#else
+		obj->vmapping = dma_buf_vmap(obj->base.import_attach->dmabuf);
+		if (!obj->vmapping)
+			return -ENOMEM;
+#endif
+		return 0;
+	}
+
+	ret = evdi_pin_pages(obj);
+	if (ret)
+		return ret;
+
+	obj->vmapping = vmap(obj->pages, page_count, 0, PAGE_KERNEL);
+	if (!obj->vmapping)
+		return -ENOMEM;
+	return 0;
+}
+
+void evdi_gem_vunmap(struct evdi_gem_object *obj)
+{
+	if (obj->base.import_attach) {
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+		struct dma_buf_map map;
+
+		if (obj->vmap_is_iomem)
+			dma_buf_map_set_vaddr_iomem(&map, obj->vmapping);
+		else
+			dma_buf_map_set_vaddr(&map, obj->vmapping);
+
+		dma_buf_vunmap(obj->base.import_attach->dmabuf, &map);
+#else
+		dma_buf_vunmap(obj->base.import_attach->dmabuf, obj->vmapping);
+#endif
+		obj->vmapping = NULL;
+		return;
+	}
+
+	if (obj->vmapping) {
+		vunmap(obj->vmapping);
+		obj->vmapping = NULL;
+	}
+
+	evdi_unpin_pages(obj);
+}
+
+void evdi_gem_free_object(struct drm_gem_object *gem_obj)
+{
+	struct evdi_gem_object *obj = to_evdi_bo(gem_obj);
+
+	if (obj->vmapping)
+		evdi_gem_vunmap(obj);
+
+	if (gem_obj->import_attach)
+		drm_prime_gem_destroy(gem_obj, obj->sg);
+
+	if (obj->pages)
+		evdi_gem_put_pages(obj);
+
+	if (gem_obj->dev->vma_offset_manager)
+		drm_gem_free_mmap_offset(gem_obj);
+#if KERNEL_VERSION(5, 4, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	dma_resv_fini(&obj->_resv);
+#else
+	reservation_object_fini(&obj->_resv);
+#endif
+	obj->resv = NULL;
+	mutex_destroy(&obj->pages_lock);
+}
+
+/*
+ * the dumb interface doesn't work with the GEM straight MMAP
+ * interface, it expects to do MMAP on the drm fd, like normal
+ */
+int evdi_gem_mmap(struct drm_file *file,
+		  struct drm_device *dev, uint32_t handle, uint64_t *offset)
+{
+	struct evdi_gem_object *gobj;
+	struct drm_gem_object *obj;
+	int ret = 0;
+
+	mutex_lock(&dev->struct_mutex);
+	obj = drm_gem_object_lookup(file, handle);
+	if (obj == NULL) {
+		ret = -ENOENT;
+		goto unlock;
+	}
+	gobj = to_evdi_bo(obj);
+
+	ret = evdi_pin_pages(gobj);
+	if (ret)
+		goto out;
+
+	ret = drm_gem_create_mmap_offset(obj);
+	if (ret)
+		goto out;
+
+	*offset = drm_vma_node_offset_addr(&gobj->base.vma_node);
+
+ out:
+	drm_gem_object_put(&gobj->base);
+ unlock:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+struct drm_gem_object *
+evdi_prime_import_sg_table(struct drm_device *dev,
+			   struct dma_buf_attachment *attach,
+			   struct sg_table *sg)
+{
+	struct evdi_gem_object *obj;
+	int npages;
+
+	if (evdi_disable_texture_import)
+		return ERR_PTR(-ENOMEM);
+
+	obj = evdi_gem_alloc_object(dev, attach->dmabuf->size);
+	if (IS_ERR(obj))
+		return ERR_CAST(obj);
+
+	npages = PAGE_ALIGN(attach->dmabuf->size) / PAGE_SIZE;
+	DRM_DEBUG_PRIME("Importing %d pages\n", npages);
+	obj->pages = kvmalloc_array(npages, sizeof(struct page *), GFP_KERNEL);
+	if (!obj->pages) {
+		evdi_gem_free_object(&obj->base);
+		return ERR_PTR(-ENOMEM);
+	}
+
+#if KERNEL_VERSION(5, 12, 0) <= LINUX_VERSION_CODE
+	drm_prime_sg_to_page_array(sg, obj->pages, npages);
+#else
+	drm_prime_sg_to_page_addr_arrays(sg, obj->pages, NULL, npages);
+#endif
+	obj->sg = sg;
+	return &obj->base;
+}
+
+static int evdi_prime_pin(struct drm_gem_object *obj)
+{
+	struct evdi_gem_object *bo = to_evdi_bo(obj);
+
+	return evdi_pin_pages(bo);
+}
+
+static void evdi_prime_unpin(struct drm_gem_object *obj)
+{
+	struct evdi_gem_object *bo = to_evdi_bo(obj);
+
+	evdi_unpin_pages(bo);
+}
+
+struct sg_table *evdi_prime_get_sg_table(struct drm_gem_object *obj)
+{
+	struct evdi_gem_object *bo = to_evdi_bo(obj);
+	#if KERNEL_VERSION(5, 10, 0) <= LINUX_VERSION_CODE || defined(EL8)
+		return drm_prime_pages_to_sg(obj->dev, bo->pages, bo->base.size >> PAGE_SHIFT);
+	#else
+		return drm_prime_pages_to_sg(bo->pages, bo->base.size >> PAGE_SHIFT);
+	#endif
+}
diff -ruN a/drivers/gpu/drm/evdi/evdi_i2c.c b/drivers/gpu/drm/evdi/evdi_i2c.c
--- a/drivers/gpu/drm/evdi/evdi_i2c.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_i2c.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,55 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2020 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include "evdi_i2c.h"
+#include "evdi_debug.h"
+#include "evdi_drm_drv.h"
+
+static int dli2c_access_master(struct i2c_adapter *adapter,
+	struct i2c_msg *msgs, int num)
+{
+	int i = 0, result = 0;
+	struct evdi_device *evdi = adapter->algo_data;
+	struct evdi_painter *painter = evdi->painter;
+
+	for (i = 0; i < num; i++) {
+		if (evdi_painter_i2c_data_notify(painter, &msgs[i]))
+			result++;
+	}
+
+	return result;
+}
+
+static u32 dli2c_func(__always_unused struct i2c_adapter *adapter)
+{
+	return I2C_FUNC_I2C;
+}
+
+static struct i2c_algorithm dli2c_algorithm = {
+	.master_xfer = dli2c_access_master,
+	.functionality = dli2c_func,
+};
+
+int evdi_i2c_add(struct i2c_adapter *adapter, struct device *parent,
+	void *ddev)
+{
+	adapter->owner  = THIS_MODULE;
+	adapter->class  = I2C_CLASS_DDC;
+	adapter->algo   = &dli2c_algorithm;
+	strcpy(adapter->name, "DisplayLink I2C Adapter");
+	adapter->dev.parent = parent;
+	adapter->algo_data = ddev;
+
+	return i2c_add_adapter(adapter);
+}
+
+void evdi_i2c_remove(struct i2c_adapter *adapter)
+{
+	i2c_del_adapter(adapter);
+}
diff -ruN a/drivers/gpu/drm/evdi/evdi_i2c.h b/drivers/gpu/drm/evdi/evdi_i2c.h
--- a/drivers/gpu/drm/evdi/evdi_i2c.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_i2c.h	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,20 @@
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (c) 2020 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#ifndef EVDI_I2C_H
+#define EVDI_I2C_H
+
+#include <linux/module.h>
+#include <linux/i2c.h>
+
+int evdi_i2c_add(struct i2c_adapter *adapter,
+		struct device *parent,
+		void *ddev);
+void evdi_i2c_remove(struct i2c_adapter *adapter);
+
+#endif  /* EVDI_I2C_H */
diff -ruN a/drivers/gpu/drm/evdi/evdi_modeset.c b/drivers/gpu/drm/evdi/evdi_modeset.c
--- a/drivers/gpu/drm/evdi/evdi_modeset.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_modeset.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,537 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2012 Red Hat
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * Based on parts on udlfb.c:
+ * Copyright (C) 2009 its respective authors
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include <linux/version.h>
+#if KERNEL_VERSION(5, 0, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#include <drm/drm_damage_helper.h>
+#else
+#include <drm/drmP.h>
+#endif
+#include <drm/drm_atomic.h>
+#include <drm/drm_crtc.h>
+#include <drm/drm_crtc_helper.h>
+#include <drm/drm_plane_helper.h>
+#include <drm/drm_atomic_helper.h>
+#include "evdi_drm.h"
+#include "evdi_drm_drv.h"
+#include "evdi_cursor.h"
+#include "evdi_params.h"
+#if KERNEL_VERSION(5, 13, 0) <= LINUX_VERSION_CODE
+#include <drm/drm_gem_atomic_helper.h>
+#else
+#include <drm/drm_gem_framebuffer_helper.h>
+#endif
+
+static void evdi_crtc_dpms(__always_unused struct drm_crtc *crtc,
+			   __always_unused int mode)
+{
+	EVDI_CHECKPT();
+}
+
+static void evdi_crtc_disable(__always_unused struct drm_crtc *crtc)
+{
+	EVDI_CHECKPT();
+	drm_crtc_vblank_off(crtc);
+}
+
+static void evdi_crtc_destroy(struct drm_crtc *crtc)
+{
+	EVDI_CHECKPT();
+	drm_crtc_cleanup(crtc);
+	kfree(crtc);
+}
+
+static void evdi_crtc_commit(__always_unused struct drm_crtc *crtc)
+{
+	EVDI_CHECKPT();
+}
+
+static void evdi_crtc_set_nofb(__always_unused struct drm_crtc *crtc)
+{
+}
+
+static void evdi_crtc_atomic_flush(
+	struct drm_crtc *crtc
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+	, struct drm_atomic_state *state
+#else
+	, __always_unused struct drm_crtc_state *old_state
+#endif
+	)
+{
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+	struct drm_crtc_state *crtc_state = drm_atomic_get_new_crtc_state(state, crtc);
+#else
+	struct drm_crtc_state *crtc_state = crtc->state;
+#endif
+	struct evdi_device *evdi = crtc->dev->dev_private;
+
+
+	if (crtc_state->mode_changed && crtc_state->active)
+		evdi_painter_mode_changed_notify(evdi, &crtc_state->adjusted_mode);
+
+	if (crtc_state->active_changed)
+		evdi_painter_dpms_notify(evdi,
+			crtc_state->active ? DRM_MODE_DPMS_ON : DRM_MODE_DPMS_OFF);
+
+	evdi_painter_set_vblank(evdi->painter, crtc, crtc_state->event);
+	evdi_painter_send_update_ready_if_needed(evdi->painter);
+	crtc_state->event = NULL;
+}
+
+static void evdi_mark_full_screen_dirty(struct evdi_device *evdi)
+{
+	const struct drm_clip_rect rect =
+		evdi_painter_framebuffer_size(evdi->painter);
+
+	evdi_painter_mark_dirty(evdi, &rect);
+	evdi_painter_send_update_ready_if_needed(evdi->painter);
+}
+
+static int evdi_crtc_cursor_set(struct drm_crtc *crtc,
+				struct drm_file *file,
+				uint32_t handle,
+				uint32_t width,
+				uint32_t height,
+				int32_t hot_x,
+				int32_t hot_y)
+{
+	struct drm_device *dev = crtc->dev;
+	struct evdi_device *evdi = dev->dev_private;
+	struct drm_gem_object *obj = NULL;
+	struct evdi_gem_object *eobj = NULL;
+	/*
+	 * evdi_crtc_cursor_set is callback function using
+	 * deprecated cursor entry point.
+	 * There is no info about underlaying pixel format.
+	 * Hence we are assuming that it is in ARGB 32bpp format.
+	 * This format it the only one supported in cursor composition
+	 * function.
+	 * This format is also enforced during framebuffer creation.
+	 *
+	 * Proper format will be available when driver start support
+	 * universal planes for cursor.
+	 */
+	uint32_t format = DRM_FORMAT_ARGB8888;
+	uint32_t stride = 4 * width;
+
+	EVDI_CHECKPT();
+	if (handle) {
+		mutex_lock(&dev->struct_mutex);
+		obj = drm_gem_object_lookup(file, handle);
+		if (obj)
+			eobj = to_evdi_bo(obj);
+		else
+			EVDI_ERROR("Failed to lookup gem object.\n");
+		mutex_unlock(&dev->struct_mutex);
+	}
+
+	evdi_cursor_set(evdi->cursor,
+			eobj, width, height, hot_x, hot_y,
+			format, stride);
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	drm_gem_object_put(obj);
+#else
+	drm_gem_object_put_unlocked(obj);
+#endif
+
+	/*
+	 * For now we don't care whether the application wanted the mouse set,
+	 * or not.
+	 */
+	if (evdi->cursor_events_enabled)
+		evdi_painter_send_cursor_set(evdi->painter, evdi->cursor);
+	else
+		evdi_mark_full_screen_dirty(evdi);
+	return 0;
+}
+
+static int evdi_crtc_cursor_move(struct drm_crtc *crtc, int x, int y)
+{
+	struct drm_device *dev = crtc->dev;
+	struct evdi_device *evdi = dev->dev_private;
+
+	EVDI_CHECKPT();
+	evdi_cursor_move(evdi->cursor, x, y);
+
+	if (evdi->cursor_events_enabled)
+		evdi_painter_send_cursor_move(evdi->painter, evdi->cursor);
+	else
+		evdi_mark_full_screen_dirty(evdi);
+
+	return 0;
+}
+
+static struct drm_crtc_helper_funcs evdi_helper_funcs = {
+	.mode_set_nofb  = evdi_crtc_set_nofb,
+	.atomic_flush   = evdi_crtc_atomic_flush,
+
+	.dpms           = evdi_crtc_dpms,
+	.commit         = evdi_crtc_commit,
+	.disable        = evdi_crtc_disable
+};
+
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+static int evdi_enable_vblank(__always_unused struct drm_crtc *crtc)
+{
+	return 1;
+}
+
+static void evdi_disable_vblank(__always_unused struct drm_crtc *crtc)
+{
+}
+#endif
+
+static const struct drm_crtc_funcs evdi_crtc_funcs = {
+	.reset                  = drm_atomic_helper_crtc_reset,
+	.destroy                = evdi_crtc_destroy,
+	.set_config             = drm_atomic_helper_set_config,
+	.page_flip              = drm_atomic_helper_page_flip,
+	.atomic_duplicate_state = drm_atomic_helper_crtc_duplicate_state,
+	.atomic_destroy_state   = drm_atomic_helper_crtc_destroy_state,
+
+	.cursor_set2            = evdi_crtc_cursor_set,
+	.cursor_move            = evdi_crtc_cursor_move,
+#if KERNEL_VERSION(5, 11, 0) <= LINUX_VERSION_CODE
+	.enable_vblank          = evdi_enable_vblank,
+	.disable_vblank         = evdi_disable_vblank,
+#endif
+};
+
+static void evdi_plane_atomic_update(struct drm_plane *plane,
+#if KERNEL_VERSION(5, 13, 0) <= LINUX_VERSION_CODE
+				     struct drm_atomic_state *atom_state
+#else
+				     struct drm_plane_state *old_state
+#endif
+		)
+{
+#if KERNEL_VERSION(5, 13, 0) <= LINUX_VERSION_CODE
+	struct drm_plane_state *old_state = drm_atomic_get_old_plane_state(atom_state, plane);
+#else
+#endif
+	struct drm_plane_state *state;
+	struct evdi_device *evdi;
+	struct evdi_painter *painter;
+	struct drm_crtc *crtc;
+
+#if KERNEL_VERSION(5, 0, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	struct drm_atomic_helper_damage_iter iter;
+	struct drm_rect rect;
+	struct drm_clip_rect clip_rect;
+#endif
+
+	if (!plane || !plane->state) {
+		EVDI_WARN("Plane state is null\n");
+		return;
+	}
+
+	if (!plane->dev || !plane->dev->dev_private) {
+		EVDI_WARN("Plane device is null\n");
+		return;
+	}
+
+	state = plane->state;
+	evdi = plane->dev->dev_private;
+	painter = evdi->painter;
+	crtc = state->crtc;
+
+	if (!old_state->crtc && state->crtc)
+		evdi_painter_dpms_notify(evdi, DRM_MODE_DPMS_ON);
+	else if (old_state->crtc && !state->crtc)
+		evdi_painter_dpms_notify(evdi, DRM_MODE_DPMS_OFF);
+
+	if (state->fb) {
+		struct drm_framebuffer *fb = state->fb;
+		struct drm_framebuffer *old_fb = old_state->fb;
+		struct evdi_framebuffer *efb = to_evdi_fb(fb);
+
+		const struct drm_clip_rect fullscreen_rect = {
+			0, 0, fb->width, fb->height
+		};
+
+		if (!old_fb && crtc)
+			evdi_painter_force_full_modeset(painter);
+
+		if (old_fb &&
+		    fb->format && old_fb->format &&
+		    fb->format->format != old_fb->format->format)
+			evdi_painter_force_full_modeset(painter);
+
+		if (fb != old_fb ||
+		    evdi_painter_needs_full_modeset(painter)) {
+
+			evdi_painter_set_scanout_buffer(painter, efb);
+
+#if KERNEL_VERSION(5, 0, 0) <= LINUX_VERSION_CODE || defined(EL8)
+			state->visible = true;
+			state->src.x1 = 0;
+			state->src.y1 = 0;
+			state->src.x2 = fb->width << 16;
+			state->src.y2 = fb->height << 16;
+
+			drm_atomic_helper_damage_iter_init(&iter, old_state, state);
+			while (drm_atomic_helper_damage_iter_next(&iter, &rect)) {
+				clip_rect.x1 = rect.x1;
+				clip_rect.y1 = rect.y1;
+				clip_rect.x2 = rect.x2;
+				clip_rect.y2 = rect.y2;
+				evdi_painter_mark_dirty(evdi, &clip_rect);
+			}
+#endif
+
+		};
+
+		if (evdi_painter_get_num_dirts(painter) == 0)
+			evdi_painter_mark_dirty(evdi, &fullscreen_rect);
+	}
+}
+
+static void evdi_cursor_atomic_get_rect(struct drm_clip_rect *rect,
+					struct drm_plane_state *state)
+{
+	rect->x1 = (state->crtc_x < 0) ? 0 : state->crtc_x;
+	rect->y1 = (state->crtc_y < 0) ? 0 : state->crtc_y;
+	rect->x2 = state->crtc_x + state->crtc_w;
+	rect->y2 = state->crtc_y + state->crtc_h;
+}
+
+static void evdi_cursor_atomic_update(struct drm_plane *plane,
+#if KERNEL_VERSION(5, 13, 0) <= LINUX_VERSION_CODE
+				     struct drm_atomic_state *atom_state
+#else
+				     struct drm_plane_state *old_state
+#endif
+		)
+{
+#if KERNEL_VERSION(5, 13, 0) <= LINUX_VERSION_CODE
+	struct drm_plane_state *old_state = drm_atomic_get_old_plane_state(atom_state, plane);
+#else
+#endif
+	if (plane && plane->state && plane->dev && plane->dev->dev_private) {
+		struct drm_plane_state *state = plane->state;
+		struct evdi_device *evdi = plane->dev->dev_private;
+		struct drm_framebuffer *fb = state->fb;
+		struct evdi_framebuffer *efb = to_evdi_fb(fb);
+
+		struct drm_clip_rect old_rect;
+		struct drm_clip_rect rect;
+		bool cursor_changed = false;
+		bool cursor_position_changed = false;
+		int32_t cursor_position_x = 0;
+		int32_t cursor_position_y = 0;
+
+		mutex_lock(&plane->dev->struct_mutex);
+
+		evdi_cursor_position(evdi->cursor, &cursor_position_x,
+		&cursor_position_y);
+		evdi_cursor_move(evdi->cursor, state->crtc_x, state->crtc_y);
+		cursor_position_changed = cursor_position_x != state->crtc_x ||
+					  cursor_position_y != state->crtc_y;
+
+		if (fb != old_state->fb) {
+			if (fb != NULL) {
+				uint32_t stride = 4 * fb->width;
+
+				evdi_cursor_set(evdi->cursor,
+						efb->obj,
+						fb->width,
+						fb->height,
+						0,
+						0,
+						fb->format->format,
+						stride);
+			}
+
+			evdi_cursor_enable(evdi->cursor, fb != NULL);
+			cursor_changed = true;
+		}
+
+		mutex_unlock(&plane->dev->struct_mutex);
+		if (!evdi->cursor_events_enabled) {
+			evdi_cursor_atomic_get_rect(&old_rect, old_state);
+			evdi_cursor_atomic_get_rect(&rect, state);
+
+			evdi_painter_mark_dirty(evdi, &old_rect);
+			evdi_painter_mark_dirty(evdi, &rect);
+			return;
+		}
+		if (cursor_changed)
+			evdi_painter_send_cursor_set(evdi->painter,
+						     evdi->cursor);
+		if (cursor_position_changed)
+			evdi_painter_send_cursor_move(evdi->painter,
+						      evdi->cursor);
+	}
+}
+
+static const struct drm_plane_helper_funcs evdi_plane_helper_funcs = {
+	.atomic_update = evdi_plane_atomic_update,
+#if KERNEL_VERSION(5, 13, 0) <= LINUX_VERSION_CODE
+	.prepare_fb = drm_gem_plane_helper_prepare_fb
+#else
+	.prepare_fb = drm_gem_fb_prepare_fb
+#endif
+};
+
+static const struct drm_plane_helper_funcs evdi_cursor_helper_funcs = {
+	.atomic_update = evdi_cursor_atomic_update,
+#if KERNEL_VERSION(5, 13, 0) <= LINUX_VERSION_CODE
+	.prepare_fb = drm_gem_plane_helper_prepare_fb
+#else
+	.prepare_fb = drm_gem_fb_prepare_fb
+#endif
+};
+
+static const struct drm_plane_funcs evdi_plane_funcs = {
+	.update_plane = drm_atomic_helper_update_plane,
+	.disable_plane = drm_atomic_helper_disable_plane,
+	.destroy = drm_plane_cleanup,
+	.reset = drm_atomic_helper_plane_reset,
+	.atomic_duplicate_state = drm_atomic_helper_plane_duplicate_state,
+	.atomic_destroy_state = drm_atomic_helper_plane_destroy_state,
+};
+
+static const uint32_t formats[] = {
+	DRM_FORMAT_XRGB8888,
+	DRM_FORMAT_ARGB8888,
+	DRM_FORMAT_XBGR8888,
+	DRM_FORMAT_ABGR8888,
+};
+
+static struct drm_plane *evdi_create_plane(
+		struct drm_device *dev,
+		enum drm_plane_type type,
+		const struct drm_plane_helper_funcs *helper_funcs)
+{
+	struct drm_plane *plane;
+	int ret;
+
+	plane = kzalloc(sizeof(*plane), GFP_KERNEL);
+	if (plane == NULL) {
+		EVDI_ERROR("Failed to allocate primary plane\n");
+		return NULL;
+	}
+	plane->format_default = true;
+
+	ret = drm_universal_plane_init(dev,
+				       plane,
+				       0xFF,
+				       &evdi_plane_funcs,
+				       formats,
+				       ARRAY_SIZE(formats),
+				       NULL,
+				       type,
+				       NULL
+				       );
+
+	if (ret) {
+		EVDI_ERROR("Failed to initialize primary plane\n");
+		kfree(plane);
+		return NULL;
+	}
+
+	drm_plane_helper_add(plane, helper_funcs);
+
+	return plane;
+}
+
+static int evdi_crtc_init(struct drm_device *dev)
+{
+	struct drm_crtc *crtc = NULL;
+	struct drm_plane *primary_plane = NULL;
+	struct drm_plane *cursor_plane = NULL;
+	int status = 0;
+
+	EVDI_CHECKPT();
+	crtc = kzalloc(sizeof(struct drm_crtc), GFP_KERNEL);
+	if (crtc == NULL)
+		return -ENOMEM;
+
+	primary_plane = evdi_create_plane(dev, DRM_PLANE_TYPE_PRIMARY,
+					  &evdi_plane_helper_funcs);
+
+#if KERNEL_VERSION(5, 0, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	drm_plane_enable_fb_damage_clips(primary_plane);
+#endif
+
+	status = drm_crtc_init_with_planes(dev, crtc,
+					   primary_plane, cursor_plane,
+					   &evdi_crtc_funcs,
+					   NULL
+					   );
+
+	EVDI_DEBUG("drm_crtc_init: %d p%p\n", status, primary_plane);
+	drm_crtc_helper_add(crtc, &evdi_helper_funcs);
+
+	return 0;
+}
+
+static int evdi_atomic_check(struct drm_device *dev,
+			     struct drm_atomic_state *state)
+{
+	struct drm_crtc *crtc;
+	struct drm_crtc_state *crtc_state;
+	int i;
+	struct evdi_device *evdi = dev->dev_private;
+
+	if (state->allow_modeset && evdi_painter_needs_full_modeset(evdi->painter)) {
+		for_each_new_crtc_in_state(state, crtc, crtc_state, i) {
+			crtc_state->active_changed = true;
+			crtc_state->mode_changed = true;
+		}
+	}
+
+	return drm_atomic_helper_check(dev, state);
+}
+
+static const struct drm_mode_config_funcs evdi_mode_funcs = {
+	.fb_create = evdi_fb_user_fb_create,
+	.output_poll_changed = NULL,
+	.atomic_commit = drm_atomic_helper_commit,
+	.atomic_check = evdi_atomic_check
+};
+
+void evdi_modeset_init(struct drm_device *dev)
+{
+	struct drm_encoder *encoder;
+
+	EVDI_CHECKPT();
+
+	drm_mode_config_init(dev);
+
+	dev->mode_config.min_width = 64;
+	dev->mode_config.min_height = 64;
+
+	dev->mode_config.max_width = 3840;
+	dev->mode_config.max_height = 2160;
+
+	dev->mode_config.prefer_shadow = 0;
+	dev->mode_config.preferred_depth = 24;
+
+	dev->mode_config.funcs = &evdi_mode_funcs;
+
+	evdi_crtc_init(dev);
+
+	encoder = evdi_encoder_init(dev);
+
+	evdi_connector_init(dev, encoder);
+
+	drm_mode_config_reset(dev);
+}
+
+void evdi_modeset_cleanup(struct drm_device *dev)
+{
+	EVDI_CHECKPT();
+	drm_mode_config_cleanup(dev);
+}
diff -ruN a/drivers/gpu/drm/evdi/evdi_painter.c b/drivers/gpu/drm/evdi/evdi_painter.c
--- a/drivers/gpu/drm/evdi/evdi_painter.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_painter.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,1358 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2013 - 2020 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include "linux/thread_info.h"
+#include "linux/mm.h"
+#include <linux/version.h>
+#if KERNEL_VERSION(5, 5, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#else
+#include <drm/drmP.h>
+#endif
+#include <drm/drm_edid.h>
+#include "evdi_drm.h"
+#include "evdi_drm_drv.h"
+#include "evdi_cursor.h"
+#include "evdi_params.h"
+#include "evdi_i2c.h"
+#include <linux/mutex.h>
+#include <linux/compiler.h>
+#include <linux/platform_device.h>
+#include <linux/completion.h>
+
+#include <linux/dma-buf.h>
+
+#if KERNEL_VERSION(5, 1, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#include <drm/drm_probe_helper.h>
+#endif
+
+struct evdi_event_cursor_set_pending {
+	struct drm_pending_event base;
+	struct drm_evdi_event_cursor_set cursor_set;
+};
+
+struct evdi_event_cursor_move_pending {
+	struct drm_pending_event base;
+	struct drm_evdi_event_cursor_move cursor_move;
+};
+
+struct evdi_event_update_ready_pending {
+	struct drm_pending_event base;
+	struct drm_evdi_event_update_ready update_ready;
+};
+
+struct evdi_event_dpms_pending {
+	struct drm_pending_event base;
+	struct drm_evdi_event_dpms dpms;
+};
+
+struct evdi_event_mode_changed_pending {
+	struct drm_pending_event base;
+	struct drm_evdi_event_mode_changed mode_changed;
+};
+
+struct evdi_event_crtc_state_pending {
+	struct drm_pending_event base;
+	struct drm_evdi_event_crtc_state crtc_state;
+};
+
+struct evdi_event_ddcci_data_pending {
+	struct drm_pending_event base;
+	struct drm_evdi_event_ddcci_data ddcci_data;
+};
+
+#define MAX_DIRTS 16
+#define EDID_EXT_BLOCK_SIZE 128
+#define MAX_EDID_SIZE (255 * EDID_EXT_BLOCK_SIZE + sizeof(struct edid))
+#define I2C_ADDRESS_DDCCI 0x37
+#define DDCCI_TIMEOUT_MS 50
+
+struct evdi_painter {
+	bool is_connected;
+	struct edid *edid;
+	unsigned int edid_length;
+
+	struct mutex lock;
+	struct drm_clip_rect dirty_rects[MAX_DIRTS];
+	int num_dirts;
+	struct evdi_framebuffer *scanout_fb;
+
+	struct drm_file *drm_filp;
+	struct drm_device *drm_device;
+
+	bool was_update_requested;
+	bool needs_full_modeset;
+	struct drm_crtc *crtc;
+	struct drm_pending_vblank_event *vblank;
+
+	struct list_head pending_events;
+	struct delayed_work send_events_work;
+
+	struct completion ddcci_response_received;
+	char *ddcci_buffer;
+	unsigned int ddcci_buffer_length;
+};
+
+static void expand_rect(struct drm_clip_rect *a, const struct drm_clip_rect *b)
+{
+	a->x1 = min(a->x1, b->x1);
+	a->y1 = min(a->y1, b->y1);
+	a->x2 = max(a->x2, b->x2);
+	a->y2 = max(a->y2, b->y2);
+}
+
+static int rect_area(const struct drm_clip_rect *r)
+{
+	return (r->x2 - r->x1) * (r->y2 - r->y1);
+}
+
+static void merge_dirty_rects(struct drm_clip_rect *rects, int *count)
+{
+	int a, b;
+
+	for (a = 0; a < *count - 1; ++a) {
+		for (b = a + 1; b < *count;) {
+			/* collapse to bounding rect if it is fewer pixels */
+			const int area_a = rect_area(&rects[a]);
+			const int area_b = rect_area(&rects[b]);
+			struct drm_clip_rect bounding_rect = rects[a];
+
+			expand_rect(&bounding_rect, &rects[b]);
+
+			if (rect_area(&bounding_rect) <= area_a + area_b) {
+				rects[a] = bounding_rect;
+				rects[b] = rects[*count - 1];
+				/* repass */
+				b = a + 1;
+				--*count;
+			} else {
+				++b;
+			}
+		}
+	}
+}
+
+static void collapse_dirty_rects(struct drm_clip_rect *rects, int *count)
+{
+	int i;
+
+	EVDI_VERBOSE("Not enough space for rects. They will be collapsed");
+
+	for (i = 1; i < *count; ++i)
+		expand_rect(&rects[0], &rects[i]);
+
+	*count = 1;
+}
+
+static int copy_primary_pixels(struct evdi_framebuffer *efb,
+			       char __user *buffer,
+			       int buf_byte_stride,
+			       int num_rects, struct drm_clip_rect *rects,
+			       int const max_x,
+			       int const max_y)
+{
+	struct drm_framebuffer *fb = &efb->base;
+	struct drm_clip_rect *r;
+
+	EVDI_CHECKPT();
+
+	for (r = rects; r != rects + num_rects; ++r) {
+		const int byte_offset = r->x1 * 4;
+		const int byte_span = (r->x2 - r->x1) * 4;
+		const int src_offset = fb->offsets[0] +
+				       fb->pitches[0] * r->y1 + byte_offset;
+		const char *src = (char *)efb->obj->vmapping + src_offset;
+		const int dst_offset = buf_byte_stride * r->y1 + byte_offset;
+		char __user *dst = buffer + dst_offset;
+		int y = r->y2 - r->y1;
+
+		/* rect size may correspond to previous resolution */
+		if (max_x < r->x2 || max_y < r->y2) {
+			EVDI_WARN("Rect size beyond expected dimensions\n");
+			return -EFAULT;
+		}
+
+		EVDI_VERBOSE("copy rect %d,%d-%d,%d\n", r->x1, r->y1, r->x2,
+			     r->y2);
+
+		for (; y > 0; --y) {
+			if (copy_to_user(dst, src, byte_span))
+				return -EFAULT;
+
+			src += fb->pitches[0];
+			dst += buf_byte_stride;
+		}
+	}
+
+	return 0;
+}
+
+static void copy_cursor_pixels(struct evdi_framebuffer *efb,
+			       char __user *buffer,
+			       int buf_byte_stride,
+			       struct evdi_cursor *cursor)
+{
+	evdi_cursor_lock(cursor);
+	if (evdi_cursor_compose_and_copy(cursor,
+					 efb,
+					 buffer,
+					 buf_byte_stride))
+		EVDI_ERROR("Failed to blend cursor\n");
+
+	evdi_cursor_unlock(cursor);
+}
+
+#define painter_lock(painter)                           \
+	do {                                            \
+		EVDI_VERBOSE("Painter lock\n");         \
+		mutex_lock(&painter->lock);             \
+	} while (0)
+
+#define painter_unlock(painter)                         \
+	do {                                            \
+		EVDI_VERBOSE("Painter unlock\n");       \
+		mutex_unlock(&painter->lock);           \
+	} while (0)
+
+bool evdi_painter_is_connected(struct evdi_painter *painter)
+{
+	return painter ? painter->is_connected : false;
+}
+
+u8 *evdi_painter_get_edid_copy(struct evdi_device *evdi)
+{
+	u8 *block = NULL;
+
+	EVDI_CHECKPT();
+
+	painter_lock(evdi->painter);
+	if (evdi_painter_is_connected(evdi->painter) &&
+		evdi->painter->edid &&
+		evdi->painter->edid_length) {
+		block = kmalloc(evdi->painter->edid_length, GFP_KERNEL);
+		if (block) {
+			memcpy(block,
+			       evdi->painter->edid,
+			       evdi->painter->edid_length);
+		}
+	}
+	painter_unlock(evdi->painter);
+	return block;
+}
+
+static bool is_evdi_event_squashable(struct drm_pending_event *event)
+{
+	return event->event->type == DRM_EVDI_EVENT_CURSOR_SET ||
+	       event->event->type == DRM_EVDI_EVENT_CURSOR_MOVE;
+}
+
+static void evdi_painter_add_event_to_pending_list(
+	struct evdi_painter *painter,
+	struct drm_pending_event *event)
+{
+	unsigned long flags;
+	struct drm_pending_event *last_event = NULL;
+	struct list_head *list = NULL;
+
+	spin_lock_irqsave(&painter->drm_device->event_lock, flags);
+
+	list = &painter->pending_events;
+	if (!list_empty(list)) {
+		last_event =
+		  list_last_entry(list, struct drm_pending_event, link);
+	}
+
+	if (last_event &&
+	    event->event->type == last_event->event->type &&
+	    is_evdi_event_squashable(event)) {
+		list_replace(&last_event->link, &event->link);
+		kfree(last_event);
+	} else
+		list_add_tail(&event->link, list);
+
+	spin_unlock_irqrestore(&painter->drm_device->event_lock, flags);
+}
+
+static bool evdi_painter_flush_pending_events(struct evdi_painter *painter)
+{
+	unsigned long flags;
+	struct drm_pending_event *event_to_be_sent = NULL;
+	struct list_head *list = NULL;
+	bool has_space = false;
+	bool flushed_all = false;
+
+	spin_lock_irqsave(&painter->drm_device->event_lock, flags);
+
+	list = &painter->pending_events;
+	while ((event_to_be_sent = list_first_entry_or_null(
+			list, struct drm_pending_event, link))) {
+		has_space = drm_event_reserve_init_locked(painter->drm_device,
+		    painter->drm_filp, event_to_be_sent,
+		    event_to_be_sent->event) == 0;
+		if (has_space) {
+			list_del_init(&event_to_be_sent->link);
+			drm_send_event_locked(painter->drm_device,
+					      event_to_be_sent);
+		} else
+			break;
+	}
+
+	flushed_all = list_empty(&painter->pending_events);
+	spin_unlock_irqrestore(&painter->drm_device->event_lock, flags);
+
+	return flushed_all;
+}
+
+static void evdi_painter_send_event(struct evdi_painter *painter,
+				    struct drm_pending_event *event)
+{
+	if (!event) {
+		EVDI_ERROR("Null drm event!");
+		return;
+	}
+
+	if (!painter->drm_filp) {
+		EVDI_VERBOSE("Painter is not connected!");
+		drm_event_cancel_free(painter->drm_device, event);
+		return;
+	}
+
+	if (!painter->drm_device) {
+		EVDI_WARN("Painter is not connected to drm device!");
+		drm_event_cancel_free(painter->drm_device, event);
+		return;
+	}
+
+	if (!painter->is_connected) {
+		EVDI_WARN("Painter is not connected!");
+		drm_event_cancel_free(painter->drm_device, event);
+		return;
+	}
+
+	evdi_painter_add_event_to_pending_list(painter, event);
+	if (delayed_work_pending(&painter->send_events_work))
+		return;
+
+	if (evdi_painter_flush_pending_events(painter))
+		return;
+
+	schedule_delayed_work(&painter->send_events_work, msecs_to_jiffies(5));
+}
+
+static struct drm_pending_event *create_update_ready_event(void)
+{
+	struct evdi_event_update_ready_pending *event;
+
+	event = kzalloc(sizeof(*event), GFP_KERNEL);
+	if (!event) {
+		EVDI_ERROR("Failed to create update ready event");
+		return NULL;
+	}
+
+	event->update_ready.base.type = DRM_EVDI_EVENT_UPDATE_READY;
+	event->update_ready.base.length = sizeof(event->update_ready);
+	event->base.event = &event->update_ready.base;
+	return &event->base;
+}
+
+static void evdi_painter_send_update_ready(struct evdi_painter *painter)
+{
+	struct drm_pending_event *event = create_update_ready_event();
+
+	evdi_painter_send_event(painter, event);
+}
+
+static uint32_t evdi_painter_get_gem_handle(struct evdi_painter *painter,
+					   struct evdi_gem_object *obj)
+{
+	uint32_t handle = 0;
+
+	if (!obj)
+		return 0;
+
+	handle = evdi_gem_object_handle_lookup(painter->drm_filp, &obj->base);
+
+	if (handle)
+		return handle;
+
+	if (drm_gem_handle_create(painter->drm_filp,
+			      &obj->base, &handle)) {
+		EVDI_ERROR("Failed to create gem handle for %p\n",
+			painter->drm_filp);
+	}
+
+	return handle;
+}
+
+static struct drm_pending_event *create_cursor_set_event(
+		struct evdi_painter *painter,
+		struct evdi_cursor *cursor)
+{
+	struct evdi_event_cursor_set_pending *event;
+	struct evdi_gem_object *eobj = NULL;
+
+	event = kzalloc(sizeof(*event), GFP_KERNEL);
+	if (!event) {
+		EVDI_ERROR("Failed to create cursor set event");
+		return NULL;
+	}
+
+	event->cursor_set.base.type = DRM_EVDI_EVENT_CURSOR_SET;
+	event->cursor_set.base.length = sizeof(event->cursor_set);
+
+	evdi_cursor_lock(cursor);
+	event->cursor_set.enabled = evdi_cursor_enabled(cursor);
+	evdi_cursor_hotpoint(cursor, &event->cursor_set.hot_x,
+				     &event->cursor_set.hot_y);
+	evdi_cursor_size(cursor,
+		&event->cursor_set.width,
+		&event->cursor_set.height);
+	evdi_cursor_format(cursor, &event->cursor_set.pixel_format);
+	evdi_cursor_stride(cursor, &event->cursor_set.stride);
+	eobj = evdi_cursor_gem(cursor);
+	event->cursor_set.buffer_handle =
+		evdi_painter_get_gem_handle(painter, eobj);
+	if (eobj)
+		event->cursor_set.buffer_length = eobj->base.size;
+	if (!event->cursor_set.buffer_handle) {
+		event->cursor_set.enabled = false;
+		event->cursor_set.buffer_length = 0;
+	}
+	evdi_cursor_unlock(cursor);
+
+	event->base.event = &event->cursor_set.base;
+	return &event->base;
+}
+
+void evdi_painter_send_cursor_set(struct evdi_painter *painter,
+				  struct evdi_cursor *cursor)
+{
+	struct drm_pending_event *event =
+		create_cursor_set_event(painter, cursor);
+
+	evdi_painter_send_event(painter, event);
+}
+
+static struct drm_pending_event *create_cursor_move_event(
+		struct evdi_cursor *cursor)
+{
+	struct evdi_event_cursor_move_pending *event;
+
+	event = kzalloc(sizeof(*event), GFP_KERNEL);
+	if (!event) {
+		EVDI_ERROR("Failed to create cursor move event");
+		return NULL;
+	}
+
+	event->cursor_move.base.type = DRM_EVDI_EVENT_CURSOR_MOVE;
+	event->cursor_move.base.length = sizeof(event->cursor_move);
+
+	evdi_cursor_lock(cursor);
+	evdi_cursor_position(
+		cursor,
+		&event->cursor_move.x,
+		&event->cursor_move.y);
+	evdi_cursor_unlock(cursor);
+
+	event->base.event = &event->cursor_move.base;
+	return &event->base;
+}
+
+void evdi_painter_send_cursor_move(struct evdi_painter *painter,
+				   struct evdi_cursor *cursor)
+{
+	struct drm_pending_event *event = create_cursor_move_event(cursor);
+
+	evdi_painter_send_event(painter, event);
+}
+
+static struct drm_pending_event *create_dpms_event(int mode)
+{
+	struct evdi_event_dpms_pending *event;
+
+	event = kzalloc(sizeof(*event), GFP_KERNEL);
+	if (!event) {
+		EVDI_ERROR("Failed to create dpms event");
+		return NULL;
+	}
+
+	event->dpms.base.type = DRM_EVDI_EVENT_DPMS;
+	event->dpms.base.length = sizeof(event->dpms);
+	event->dpms.mode = mode;
+	event->base.event = &event->dpms.base;
+	return &event->base;
+}
+
+static void evdi_painter_send_dpms(struct evdi_painter *painter, int mode)
+{
+	struct drm_pending_event *event = create_dpms_event(mode);
+
+	evdi_painter_send_event(painter, event);
+}
+
+static struct drm_pending_event *create_mode_changed_event(
+	struct drm_display_mode *current_mode,
+	int32_t bits_per_pixel,
+	uint32_t pixel_format)
+{
+	struct evdi_event_mode_changed_pending *event;
+
+	event = kzalloc(sizeof(*event), GFP_KERNEL);
+	if (!event) {
+		EVDI_ERROR("Failed to create mode changed event");
+		return NULL;
+	}
+
+	event->mode_changed.base.type = DRM_EVDI_EVENT_MODE_CHANGED;
+	event->mode_changed.base.length = sizeof(event->mode_changed);
+
+	event->mode_changed.hdisplay = current_mode->hdisplay;
+	event->mode_changed.vdisplay = current_mode->vdisplay;
+	event->mode_changed.vrefresh = drm_mode_vrefresh(current_mode);
+	event->mode_changed.bits_per_pixel = bits_per_pixel;
+	event->mode_changed.pixel_format = pixel_format;
+
+	event->base.event = &event->mode_changed.base;
+	return &event->base;
+}
+
+static void evdi_painter_send_mode_changed(
+	struct evdi_painter *painter,
+	struct drm_display_mode *current_mode,
+	int32_t bits_per_pixel,
+	uint32_t pixel_format)
+{
+	struct drm_pending_event *event = create_mode_changed_event(
+		current_mode, bits_per_pixel, pixel_format);
+
+	evdi_painter_send_event(painter, event);
+}
+
+int evdi_painter_get_num_dirts(struct evdi_painter *painter)
+{
+	int num_dirts;
+
+	if (painter == NULL) {
+		EVDI_WARN("Painter is not connected!");
+		return 0;
+	}
+
+	painter_lock(painter);
+
+	num_dirts = painter->num_dirts;
+
+	painter_unlock(painter);
+
+	return num_dirts;
+}
+
+struct drm_clip_rect evdi_painter_framebuffer_size(
+	struct evdi_painter *painter)
+{
+	struct drm_clip_rect rect = {0, 0, 0, 0};
+	struct evdi_framebuffer *efb = NULL;
+
+	if (painter == NULL) {
+		EVDI_WARN("Painter is not connected!");
+		return rect;
+	}
+
+	painter_lock(painter);
+	efb = painter->scanout_fb;
+	if (!efb) {
+		if (painter->is_connected)
+			EVDI_DEBUG("Scanout buffer not set.");
+		goto unlock;
+	}
+	rect.x1 = 0;
+	rect.y1 = 0;
+	rect.x2 = efb->base.width;
+	rect.y2 = efb->base.height;
+unlock:
+	painter_unlock(painter);
+	return rect;
+}
+
+void evdi_painter_mark_dirty(struct evdi_device *evdi,
+			     const struct drm_clip_rect *dirty_rect)
+{
+	struct drm_clip_rect rect;
+	struct evdi_framebuffer *efb = NULL;
+	struct evdi_painter *painter = evdi->painter;
+
+	if (painter == NULL) {
+		EVDI_WARN("Painter is not connected!");
+		return;
+	}
+
+	painter_lock(painter);
+	efb = painter->scanout_fb;
+	if (!efb) {
+		if (painter->is_connected)
+			EVDI_DEBUG("(card%d) Skip clip rect. Scanout buffer not set.\n",
+			   evdi->dev_index);
+		goto unlock;
+	}
+
+	rect = evdi_framebuffer_sanitize_rect(efb, dirty_rect);
+
+	EVDI_VERBOSE("(card%d) %d,%d-%d,%d\n", evdi->dev_index, rect.x1,
+		     rect.y1, rect.x2, rect.y2);
+
+	if (painter->num_dirts == MAX_DIRTS)
+		merge_dirty_rects(&painter->dirty_rects[0],
+				  &painter->num_dirts);
+
+	if (painter->num_dirts == MAX_DIRTS)
+		collapse_dirty_rects(&painter->dirty_rects[0],
+				     &painter->num_dirts);
+
+	memcpy(&painter->dirty_rects[painter->num_dirts], &rect, sizeof(rect));
+	painter->num_dirts++;
+
+unlock:
+	painter_unlock(painter);
+}
+
+static void evdi_send_vblank(struct drm_crtc *crtc,
+			     struct drm_pending_vblank_event *vblank)
+{
+	if (crtc && vblank) {
+		unsigned long flags = 0;
+
+		spin_lock_irqsave(&crtc->dev->event_lock, flags);
+		drm_crtc_send_vblank_event(crtc, vblank);
+		spin_unlock_irqrestore(&crtc->dev->event_lock, flags);
+	}
+}
+
+static void evdi_painter_send_vblank(struct evdi_painter *painter)
+{
+	EVDI_CHECKPT();
+
+	evdi_send_vblank(painter->crtc, painter->vblank);
+
+	painter->crtc = NULL;
+	painter->vblank = NULL;
+}
+
+void evdi_painter_set_vblank(
+	struct evdi_painter *painter,
+	struct drm_crtc *crtc,
+	struct drm_pending_vblank_event *vblank)
+{
+	EVDI_CHECKPT();
+
+	if (painter) {
+		painter_lock(painter);
+
+		evdi_painter_send_vblank(painter);
+
+		if (painter->num_dirts > 0 && painter->is_connected) {
+			painter->crtc = crtc;
+			painter->vblank = vblank;
+		} else {
+			evdi_send_vblank(crtc, vblank);
+		}
+
+		painter_unlock(painter);
+	} else {
+		evdi_send_vblank(crtc, vblank);
+	}
+}
+
+void evdi_painter_send_update_ready_if_needed(struct evdi_painter *painter)
+{
+	EVDI_CHECKPT();
+	if (painter) {
+		painter_lock(painter);
+
+		if (painter->was_update_requested) {
+			evdi_painter_send_update_ready(painter);
+			painter->was_update_requested = false;
+		}
+
+		painter_unlock(painter);
+	} else {
+		EVDI_WARN("Painter does not exist!");
+	}
+}
+
+static const char *dpms_str[] = { "on", "standby", "suspend", "off" };
+
+void evdi_painter_dpms_notify(struct evdi_device *evdi, int mode)
+{
+	struct evdi_painter *painter = evdi->painter;
+	const char *mode_str;
+
+	if (!painter) {
+		EVDI_WARN("(card%d) Painter does not exist!", evdi->dev_index);
+		return;
+	}
+
+	if (!painter->is_connected)
+		return;
+
+	switch (mode) {
+	case DRM_MODE_DPMS_ON:
+	case DRM_MODE_DPMS_STANDBY:
+	case DRM_MODE_DPMS_SUSPEND:
+	case DRM_MODE_DPMS_OFF:
+		mode_str = dpms_str[mode];
+		break;
+	default:
+		mode_str = "unknown";
+	}
+	EVDI_INFO("(card%d) Notifying display power state: %s",
+		   evdi->dev_index, mode_str);
+	evdi_painter_send_dpms(painter, mode);
+}
+
+static void evdi_log_pixel_format(uint32_t pixel_format,
+		char *buf, size_t size)
+{
+#if KERNEL_VERSION(5, 14, 0) <= LINUX_VERSION_CODE
+	snprintf(buf, size, "pixel format %p4cc", &pixel_format);
+#else
+	struct drm_format_name_buf format_name;
+
+	drm_get_format_name(pixel_format, &format_name);
+	snprintf(buf, size, "pixel format %s", format_name.str);
+#endif
+}
+
+void evdi_painter_mode_changed_notify(struct evdi_device *evdi,
+				      struct drm_display_mode *new_mode)
+{
+	struct evdi_painter *painter = evdi->painter;
+	struct drm_framebuffer *fb;
+	int bits_per_pixel;
+	uint32_t pixel_format;
+	char buf[100];
+
+	if (painter == NULL)
+		return;
+
+	fb = &painter->scanout_fb->base;
+	if (fb == NULL)
+		return;
+
+	bits_per_pixel = fb->format->cpp[0] * 8;
+	pixel_format = fb->format->format;
+
+
+	evdi_log_pixel_format(pixel_format, buf, sizeof(buf));
+	EVDI_INFO("(card%d) Notifying mode changed: %dx%d@%d; bpp %d; %s",
+		   evdi->dev_index, new_mode->hdisplay, new_mode->vdisplay,
+		   drm_mode_vrefresh(new_mode), bits_per_pixel, buf);
+
+	evdi_painter_send_mode_changed(painter,
+				       new_mode,
+				       bits_per_pixel,
+				       pixel_format);
+	painter->needs_full_modeset = false;
+}
+
+static void evdi_painter_events_cleanup(struct evdi_painter *painter)
+{
+	struct drm_pending_event *event, *temp;
+	unsigned long flags;
+
+	spin_lock_irqsave(&painter->drm_device->event_lock, flags);
+	list_for_each_entry_safe(event, temp, &painter->pending_events, link) {
+		list_del(&event->link);
+		kfree(event);
+	}
+	spin_unlock_irqrestore(&painter->drm_device->event_lock, flags);
+
+	cancel_delayed_work_sync(&painter->send_events_work);
+}
+
+static void evdi_add_i2c_adapter(struct evdi_device *evdi)
+{
+	struct drm_device *ddev = evdi->ddev;
+	struct platform_device *platdev = to_platform_device(ddev->dev);
+	int result = 0;
+
+	evdi->i2c_adapter = kzalloc(sizeof(*evdi->i2c_adapter), GFP_KERNEL);
+
+	if (!evdi->i2c_adapter) {
+		EVDI_ERROR("(card%d) Failed to allocate for i2c adapter",
+			evdi->dev_index);
+		return;
+	}
+
+	result = evdi_i2c_add(evdi->i2c_adapter, &platdev->dev, ddev->dev_private);
+
+	if (result) {
+		kfree(evdi->i2c_adapter);
+		evdi->i2c_adapter = NULL;
+		EVDI_ERROR("(card%d) Failed to add i2c adapter, error %d",
+			evdi->dev_index, result);
+		return;
+	}
+
+	EVDI_INFO("(card%d) Added i2c adapter bus number %d",
+		evdi->dev_index, evdi->i2c_adapter->nr);
+
+	result = sysfs_create_link(&evdi->conn->kdev->kobj,
+			&evdi->i2c_adapter->dev.kobj, "ddc");
+
+	if (result) {
+		EVDI_ERROR("(card%d) Failed to create sysfs link, error %d",
+			evdi->dev_index, result);
+		return;
+	}
+}
+
+static void evdi_remove_i2c_adapter(struct evdi_device *evdi)
+{
+	if (evdi->i2c_adapter) {
+		EVDI_INFO("(card%d) Removing i2c adapter bus number %d",
+			evdi->dev_index, evdi->i2c_adapter->nr);
+
+		sysfs_remove_link(&evdi->conn->kdev->kobj, "ddc");
+
+		evdi_i2c_remove(evdi->i2c_adapter);
+
+		kfree(evdi->i2c_adapter);
+		evdi->i2c_adapter = NULL;
+	}
+}
+
+static int
+evdi_painter_connect(struct evdi_device *evdi,
+		     void const __user *edid_data, unsigned int edid_length,
+		     uint32_t sku_area_limit,
+		     struct drm_file *file, __always_unused int dev_index)
+{
+	struct evdi_painter *painter = evdi->painter;
+	struct edid *new_edid = NULL;
+	int expected_edid_size = 0;
+	char buf[100];
+
+	evdi_log_process(buf, sizeof(buf));
+
+	if (edid_length < sizeof(struct edid)) {
+		EVDI_ERROR("Edid length too small\n");
+		return -EINVAL;
+	}
+
+	if (edid_length > MAX_EDID_SIZE) {
+		EVDI_ERROR("Edid length too large\n");
+		return -EINVAL;
+	}
+
+	new_edid = kzalloc(edid_length, GFP_KERNEL);
+	if (!new_edid)
+		return -ENOMEM;
+
+	if (copy_from_user(new_edid, edid_data, edid_length)) {
+		EVDI_ERROR("(card%d) Failed to read edid\n", evdi->dev_index);
+		kfree(new_edid);
+		return -EFAULT;
+	}
+
+	expected_edid_size = sizeof(struct edid) +
+			     new_edid->extensions * EDID_EXT_BLOCK_SIZE;
+	if (expected_edid_size != edid_length) {
+		EVDI_ERROR("Wrong edid size. Expected %d but is %d\n",
+			   expected_edid_size, edid_length);
+		kfree(new_edid);
+		return -EINVAL;
+	}
+
+	if (painter->drm_filp)
+		EVDI_WARN("(card%d) Double connect - replacing %p with %p\n",
+			  evdi->dev_index, painter->drm_filp, file);
+
+	painter_lock(painter);
+
+	evdi->sku_area_limit = sku_area_limit;
+	painter->drm_filp = file;
+	kfree(painter->edid);
+	painter->edid_length = edid_length;
+	painter->edid = new_edid;
+	painter->is_connected = true;
+	painter->needs_full_modeset = true;
+
+	evdi_add_i2c_adapter(evdi);
+
+	painter_unlock(painter);
+
+	EVDI_INFO("(card%d) Connected with %s\n", evdi->dev_index, buf);
+
+	drm_helper_hpd_irq_event(evdi->ddev);
+
+	return 0;
+}
+
+static int evdi_painter_disconnect(struct evdi_device *evdi,
+	struct drm_file *file)
+{
+	struct evdi_painter *painter = evdi->painter;
+	char buf[100];
+
+	EVDI_CHECKPT();
+
+	painter_lock(painter);
+
+	if (file != painter->drm_filp) {
+		painter_unlock(painter);
+		return -EFAULT;
+	}
+
+	if (painter->scanout_fb) {
+		drm_framebuffer_put(&painter->scanout_fb->base);
+		painter->scanout_fb = NULL;
+	}
+
+	painter->is_connected = false;
+
+	evdi_log_process(buf, sizeof(buf));
+	EVDI_INFO("(card%d) Disconnected from %s\n", evdi->dev_index, buf);
+	evdi_painter_events_cleanup(painter);
+
+	evdi_painter_send_vblank(painter);
+
+	evdi_cursor_enable(evdi->cursor, false);
+
+	kfree(painter->ddcci_buffer);
+	painter->ddcci_buffer = NULL;
+	painter->ddcci_buffer_length = 0;
+
+	evdi_remove_i2c_adapter(evdi);
+
+	painter->drm_filp = NULL;
+
+	painter->was_update_requested = false;
+	evdi->cursor_events_enabled = false;
+
+	painter_unlock(painter);
+
+	// Signal anything waiting for ddc/ci response with NULL buffer
+	complete(&painter->ddcci_response_received);
+
+	drm_helper_hpd_irq_event(evdi->ddev);
+	return 0;
+}
+
+void evdi_painter_close(struct evdi_device *evdi, struct drm_file *file)
+{
+	EVDI_CHECKPT();
+
+	if (evdi->painter && file == evdi->painter->drm_filp)
+		evdi_painter_disconnect(evdi, file);
+}
+
+int evdi_painter_connect_ioctl(struct drm_device *drm_dev, void *data,
+			       struct drm_file *file)
+{
+	struct evdi_device *evdi = drm_dev->dev_private;
+	struct evdi_painter *painter = evdi->painter;
+	struct drm_evdi_connect *cmd = data;
+	int ret;
+
+	EVDI_CHECKPT();
+	if (painter) {
+		if (cmd->connected)
+			ret = evdi_painter_connect(evdi,
+					     cmd->edid,
+					     cmd->edid_length,
+					     cmd->sku_area_limit,
+					     file,
+					     cmd->dev_index);
+		else
+			ret = evdi_painter_disconnect(evdi, file);
+
+		if (ret) {
+			EVDI_WARN("(card%d)(pid=%d) disconnect failed\n",
+				  evdi->dev_index, (int)task_pid_nr(current));
+		}
+		return ret;
+	}
+	EVDI_WARN("(card%d) Painter does not exist!", evdi->dev_index);
+	return -ENODEV;
+}
+
+int evdi_painter_grabpix_ioctl(struct drm_device *drm_dev, void *data,
+			       __always_unused struct drm_file *file)
+{
+	struct evdi_device *evdi = drm_dev->dev_private;
+	struct evdi_painter *painter = evdi->painter;
+	struct drm_evdi_grabpix *cmd = data;
+	struct evdi_framebuffer *efb = NULL;
+	struct drm_clip_rect dirty_rects[MAX_DIRTS];
+	struct drm_crtc *crtc = NULL;
+	struct drm_pending_vblank_event *vblank = NULL;
+	int err;
+	int ret;
+	struct dma_buf_attachment *import_attach;
+
+	EVDI_CHECKPT();
+
+	if (cmd->mode != EVDI_GRABPIX_MODE_DIRTY) {
+		EVDI_ERROR("Unknown command mode\n");
+		return -EINVAL;
+	}
+
+	if (cmd->num_rects < 1) {
+		EVDI_ERROR("No space for clip rects\n");
+		return -EINVAL;
+	}
+
+	if (!painter)
+		return -ENODEV;
+
+	painter_lock(painter);
+
+	if (painter->was_update_requested) {
+		EVDI_WARN("(card%d) Update ready not sent,",
+			  evdi->dev_index);
+		EVDI_WARN(" but pixels are grabbed.\n");
+	}
+
+	if (painter->num_dirts < 0) {
+		err = -EAGAIN;
+		goto err_painter;
+	}
+
+	merge_dirty_rects(&painter->dirty_rects[0],
+			  &painter->num_dirts);
+	if (painter->num_dirts > cmd->num_rects)
+		collapse_dirty_rects(&painter->dirty_rects[0],
+				     &painter->num_dirts);
+
+	cmd->num_rects = painter->num_dirts;
+	memcpy(dirty_rects, painter->dirty_rects,
+	       painter->num_dirts * sizeof(painter->dirty_rects[0]));
+
+	efb = painter->scanout_fb;
+
+	if (!efb) {
+		EVDI_ERROR("Scanout buffer not set\n");
+		err = -EAGAIN;
+		goto err_painter;
+	}
+
+	painter->num_dirts = 0;
+
+	drm_framebuffer_get(&efb->base);
+
+	crtc = painter->crtc;
+	painter->crtc = NULL;
+
+	vblank = painter->vblank;
+	painter->vblank = NULL;
+
+
+	painter_unlock(painter);
+
+	if (!efb->obj->vmapping) {
+		if (evdi_gem_vmap(efb->obj) == -ENOMEM) {
+			EVDI_ERROR("Failed to map scanout buffer\n");
+			err = -EFAULT;
+			goto err_fb;
+		}
+		if (!efb->obj->vmapping) {
+			EVDI_ERROR("Inexistent vmapping\n");
+			err = -EFAULT;
+			goto err_fb;
+		}
+	}
+
+	if (cmd->buf_width != efb->base.width ||
+		cmd->buf_height != efb->base.height) {
+		EVDI_ERROR("Invalid buffer dimension\n");
+		err = -EINVAL;
+		goto err_fb;
+	}
+
+	if (copy_to_user(cmd->rects, dirty_rects,
+		cmd->num_rects * sizeof(cmd->rects[0]))) {
+		err = -EFAULT;
+		goto err_fb;
+	}
+
+	import_attach = efb->obj->base.import_attach;
+	if (import_attach) {
+		ret = dma_buf_begin_cpu_access(import_attach->dmabuf,
+					       DMA_FROM_DEVICE);
+		if (ret) {
+			err = -EFAULT;
+			goto err_fb;
+		}
+	}
+
+	err = copy_primary_pixels(efb,
+				  cmd->buffer,
+				  cmd->buf_byte_stride,
+				  cmd->num_rects,
+				  dirty_rects,
+				  cmd->buf_width,
+				  cmd->buf_height);
+	if (err == 0 && !evdi->cursor_events_enabled)
+		copy_cursor_pixels(efb,
+				   cmd->buffer,
+				   cmd->buf_byte_stride,
+				   evdi->cursor);
+
+	if (import_attach)
+		dma_buf_end_cpu_access(import_attach->dmabuf,
+				       DMA_FROM_DEVICE);
+
+err_fb:
+	evdi_send_vblank(crtc, vblank);
+
+	drm_framebuffer_put(&efb->base);
+
+	return err;
+
+err_painter:
+	painter_unlock(painter);
+	return err;
+}
+
+int evdi_painter_request_update_ioctl(struct drm_device *drm_dev,
+				      __always_unused void *data,
+				      __always_unused struct drm_file *file)
+{
+	struct evdi_device *evdi = drm_dev->dev_private;
+	struct evdi_painter *painter = evdi->painter;
+	int result = 0;
+
+	if (painter) {
+		painter_lock(painter);
+
+		if (painter->was_update_requested) {
+			EVDI_WARN
+			  ("(card%d) Update was already requested - ignoring\n",
+			   evdi->dev_index);
+		} else {
+			if (painter->num_dirts > 0)
+				result = 1;
+			else
+				painter->was_update_requested = true;
+		}
+
+		painter_unlock(painter);
+
+		return result;
+	} else {
+		return -ENODEV;
+	}
+}
+
+static void evdi_send_events_work(struct work_struct *work)
+{
+	struct evdi_painter *painter =
+		container_of(work, struct evdi_painter,	send_events_work.work);
+
+	if (evdi_painter_flush_pending_events(painter))
+		return;
+
+	schedule_delayed_work(&painter->send_events_work, msecs_to_jiffies(5));
+}
+
+int evdi_painter_init(struct evdi_device *dev)
+{
+	EVDI_CHECKPT();
+	dev->painter = kzalloc(sizeof(*dev->painter), GFP_KERNEL);
+	if (dev->painter) {
+		mutex_init(&dev->painter->lock);
+		dev->painter->edid = NULL;
+		dev->painter->edid_length = 0;
+		dev->painter->needs_full_modeset = true;
+		dev->painter->crtc = NULL;
+		dev->painter->vblank = NULL;
+		dev->painter->drm_device = dev->ddev;
+		INIT_LIST_HEAD(&dev->painter->pending_events);
+		INIT_DELAYED_WORK(&dev->painter->send_events_work,
+			evdi_send_events_work);
+		init_completion(&dev->painter->ddcci_response_received);
+		return 0;
+	}
+	return -ENOMEM;
+}
+
+void evdi_painter_cleanup(struct evdi_painter *painter)
+{
+	EVDI_CHECKPT();
+	if (!painter) {
+		EVDI_WARN("Painter does not exist\n");
+		return;
+	}
+
+	painter_lock(painter);
+	kfree(painter->edid);
+	painter->edid_length = 0;
+	painter->edid = NULL;
+
+	evdi_painter_send_vblank(painter);
+
+	evdi_painter_events_cleanup(painter);
+
+	painter->drm_device = NULL;
+	painter_unlock(painter);
+}
+
+void evdi_painter_set_scanout_buffer(struct evdi_painter *painter,
+				     struct evdi_framebuffer *newfb)
+{
+	struct evdi_framebuffer *oldfb = NULL;
+
+	if (newfb)
+		drm_framebuffer_get(&newfb->base);
+
+	painter_lock(painter);
+
+	oldfb = painter->scanout_fb;
+	painter->scanout_fb = newfb;
+
+	painter_unlock(painter);
+
+	if (oldfb)
+		drm_framebuffer_put(&oldfb->base);
+}
+
+bool evdi_painter_needs_full_modeset(struct evdi_painter *painter)
+{
+	return painter ? painter->needs_full_modeset : false;
+}
+
+
+void evdi_painter_force_full_modeset(struct evdi_painter *painter)
+{
+	if (painter)
+		painter->needs_full_modeset = true;
+}
+
+static struct drm_pending_event *create_ddcci_data_event(struct i2c_msg *msg)
+{
+	struct evdi_event_ddcci_data_pending *event;
+
+	event = kzalloc(sizeof(*event), GFP_KERNEL);
+	if (!event || !msg) {
+		EVDI_ERROR("Failed to create ddcci data event");
+		return NULL;
+	}
+
+	event->ddcci_data.base.type = DRM_EVDI_EVENT_DDCCI_DATA;
+	event->ddcci_data.base.length = sizeof(event->ddcci_data);
+	// Truncate buffers to a maximum of 64 bytes
+	event->ddcci_data.buffer_length = min_t(__u16, msg->len,
+		sizeof(event->ddcci_data.buffer));
+	memcpy(event->ddcci_data.buffer, msg->buf,
+		event->ddcci_data.buffer_length);
+	event->ddcci_data.flags = msg->flags;
+	event->ddcci_data.address = msg->addr;
+
+	event->base.event = &event->ddcci_data.base;
+	return &event->base;
+}
+
+static void evdi_painter_ddcci_data(struct evdi_painter *painter, struct i2c_msg *msg)
+{
+	struct drm_pending_event *event = create_ddcci_data_event(msg);
+
+	reinit_completion(&painter->ddcci_response_received);
+	evdi_painter_send_event(painter, event);
+
+	if (wait_for_completion_interruptible_timeout(
+		&painter->ddcci_response_received,
+		msecs_to_jiffies(DDCCI_TIMEOUT_MS)) > 0) {
+
+		// Match expected buffer length including any truncation
+		const uint32_t expected_response_length = min_t(__u16, msg->len,
+								DDCCI_BUFFER_SIZE);
+
+		painter_lock(painter);
+
+		if (expected_response_length != painter->ddcci_buffer_length)
+			EVDI_WARN("DDCCI buffer length mismatch");
+		else if (painter->ddcci_buffer)
+			memcpy(msg->buf, painter->ddcci_buffer,
+			       painter->ddcci_buffer_length);
+		else
+			EVDI_WARN("Ignoring NULL DDCCI buffer");
+
+		painter_unlock(painter);
+	} else {
+		EVDI_WARN("DDCCI response timeout");
+	}
+}
+
+bool evdi_painter_i2c_data_notify(struct evdi_painter *painter, struct i2c_msg *msg)
+{
+	if (!evdi_painter_is_connected(painter)) {
+		EVDI_WARN("Painter not connected");
+		return false;
+	}
+
+	if (!msg) {
+		EVDI_WARN("Ignored NULL ddc/ci message");
+		return false;
+	}
+
+	if (msg->addr != I2C_ADDRESS_DDCCI) {
+		EVDI_DEBUG("Ignored ddc/ci data for address 0x%x\n", msg->addr);
+		return false;
+	}
+
+	evdi_painter_ddcci_data(painter, msg);
+	return true;
+}
+
+int evdi_painter_ddcci_response_ioctl(struct drm_device *drm_dev, void *data,
+				__always_unused struct drm_file *file)
+{
+	struct evdi_device *evdi = drm_dev->dev_private;
+	struct evdi_painter *painter = evdi->painter;
+	struct drm_evdi_ddcci_response *cmd = data;
+	int result = 0;
+
+	painter_lock(painter);
+
+	// Truncate any read to 64 bytes
+	painter->ddcci_buffer_length = min_t(uint32_t, cmd->buffer_length,
+					     DDCCI_BUFFER_SIZE);
+
+	kfree(painter->ddcci_buffer);
+	painter->ddcci_buffer = kzalloc(painter->ddcci_buffer_length, GFP_KERNEL);
+	if (!painter->ddcci_buffer) {
+		EVDI_ERROR("DDC buffer allocation failed\n");
+		result = -ENOMEM;
+		goto unlock;
+	}
+
+	if (copy_from_user(painter->ddcci_buffer, cmd->buffer,
+		painter->ddcci_buffer_length)) {
+		EVDI_ERROR("Failed to read ddcci_buffer\n");
+		kfree(painter->ddcci_buffer);
+		painter->ddcci_buffer = NULL;
+		result = -EFAULT;
+		goto unlock;
+	}
+
+	complete(&painter->ddcci_response_received);
+
+unlock:
+	painter_unlock(painter);
+	return result;
+}
+
+int evdi_painter_enable_cursor_events_ioctl(struct drm_device *drm_dev, void *data,
+					__always_unused struct drm_file *file)
+{
+	struct evdi_device *evdi = drm_dev->dev_private;
+	struct drm_evdi_enable_cursor_events *cmd = data;
+
+	evdi->cursor_events_enabled = cmd->enable;
+
+	return 0;
+}
diff -ruN a/drivers/gpu/drm/evdi/evdi_params.c b/drivers/gpu/drm/evdi/evdi_params.c
--- a/drivers/gpu/drm/evdi/evdi_params.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_params.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,29 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+
+#include "evdi_params.h"
+#include "evdi_debug.h"
+
+unsigned int evdi_loglevel __read_mostly = EVDI_LOGLEVEL_DEBUG;
+unsigned short int evdi_initial_device_count __read_mostly;
+unsigned short int evdi_disable_texture_import __read_mostly;
+
+module_param_named(initial_loglevel, evdi_loglevel, int, 0400);
+MODULE_PARM_DESC(initial_loglevel, "Initial log level");
+
+module_param_named(initial_device_count,
+		   evdi_initial_device_count, ushort, 0644);
+MODULE_PARM_DESC(initial_device_count, "Initial DRM device count (default: 0)");
+
+module_param_named(disable_texture_import,
+		   evdi_disable_texture_import, ushort, 0644);
+MODULE_PARM_DESC(disable_texture_import, "Disable fast path GPU texture import (default: 0, set 1 to disable)");
diff -ruN a/drivers/gpu/drm/evdi/evdi_params.h b/drivers/gpu/drm/evdi/evdi_params.h
--- a/drivers/gpu/drm/evdi/evdi_params.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_params.h	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,16 @@
+/* SPDX-License-Identifier: GPL-2.0-only
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#ifndef EVDI_PARAMS_H
+#define EVDI_PARAMS_H
+
+extern unsigned int evdi_loglevel;
+extern unsigned short int evdi_initial_device_count;
+extern unsigned short int evdi_disable_texture_import;
+
+#endif /* EVDI_PARAMS_H */
diff -ruN a/drivers/gpu/drm/evdi/evdi_platform_dev.c b/drivers/gpu/drm/evdi/evdi_platform_dev.c
--- a/drivers/gpu/drm/evdi/evdi_platform_dev.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_platform_dev.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,157 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (c) 2020 DisplayLink (UK) Ltd.
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program. If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include "evdi_platform_dev.h"
+#include <linux/version.h>
+#include <linux/dma-mapping.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#include <linux/iommu.h>
+#endif
+
+#include "evdi_platform_drv.h"
+#include "evdi_debug.h"
+#include "evdi_drm_drv.h"
+
+struct evdi_platform_device_data {
+	struct drm_device *drm_dev;
+	struct device *parent;
+	bool symlinked;
+};
+
+struct platform_device *evdi_platform_dev_create(struct platform_device_info *info)
+{
+	struct platform_device *platform_dev = NULL;
+
+	platform_dev = platform_device_register_full(info);
+	if (dma_set_mask(&platform_dev->dev, DMA_BIT_MASK(64))) {
+		EVDI_DEBUG("Unable to change dma mask to 64 bit. ");
+		EVDI_DEBUG("Sticking with 32 bit\n");
+	}
+
+	EVDI_INFO("Evdi platform_device create\n");
+
+	return platform_dev;
+}
+
+void evdi_platform_dev_destroy(struct platform_device *dev)
+{
+	platform_device_unregister(dev);
+	EVDI_INFO("Evdi platform_device destroy\n");
+}
+
+int evdi_platform_device_probe(struct platform_device *pdev)
+{
+	struct drm_device *dev;
+	struct evdi_platform_device_data *data;
+
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE || defined(EL8)
+#if IS_ENABLED(CONFIG_IOMMU_API) && defined(CONFIG_INTEL_IOMMU)
+	struct dev_iommu iommu;
+#endif
+#endif
+	EVDI_CHECKPT();
+
+	data = kzalloc(sizeof(struct evdi_platform_device_data), GFP_KERNEL);
+	if (!data)
+		return -ENOMEM;
+/* Intel-IOMMU workaround: platform-bus unsupported, force ID-mapping */
+#if IS_ENABLED(CONFIG_IOMMU_API) && defined(CONFIG_INTEL_IOMMU)
+#if KERNEL_VERSION(5, 9, 0) <= LINUX_VERSION_CODE || defined(EL8)
+	memset(&iommu, 0, sizeof(iommu));
+	iommu.priv = (void *)-1;
+	pdev->dev.iommu = &iommu;
+#else
+#define INTEL_IOMMU_DUMMY_DOMAIN                ((void *)-1)
+	pdev->dev.archdata.iommu = INTEL_IOMMU_DUMMY_DOMAIN;
+#endif
+#endif
+
+	dev = evdi_drm_device_create(&pdev->dev);
+	if (IS_ERR_OR_NULL(dev))
+		goto err_free;
+
+	data->drm_dev = dev;
+	data->symlinked = false;
+	platform_set_drvdata(pdev, data);
+	return PTR_ERR_OR_ZERO(dev);
+
+err_free:
+	kfree(data);
+	return PTR_ERR_OR_ZERO(dev);
+}
+
+int evdi_platform_device_remove(struct platform_device *pdev)
+{
+	struct evdi_platform_device_data *data = platform_get_drvdata(pdev);
+
+	EVDI_CHECKPT();
+
+	evdi_drm_device_remove(data->drm_dev);
+	kfree(data);
+	return 0;
+}
+
+bool evdi_platform_device_is_free(struct platform_device *pdev)
+{
+	struct evdi_platform_device_data *data = platform_get_drvdata(pdev);
+	struct evdi_device *evdi = data->drm_dev->dev_private;
+
+	if (evdi && !evdi_painter_is_connected(evdi->painter) &&
+	    !data->symlinked)
+		return true;
+	return false;
+}
+
+void evdi_platform_device_link(struct platform_device *pdev,
+				      struct device *parent)
+{
+	struct evdi_platform_device_data *data = NULL;
+	int ret = 0;
+
+	if (!parent || !pdev)
+		return;
+
+	data = platform_get_drvdata(pdev);
+	if (!evdi_platform_device_is_free(pdev)) {
+		EVDI_FATAL("Device is already attached can't symlink again\n");
+		return;
+	}
+
+	ret = sysfs_create_link(&pdev->dev.kobj, &parent->kobj, "device");
+	if (ret) {
+		EVDI_FATAL("Failed to create sysfs link from evdi to parent device\n");
+	} else {
+		data->symlinked = true;
+		data->parent = parent;
+	}
+}
+
+void evdi_platform_device_unlink_if_linked_with(struct platform_device *pdev,
+				struct device *parent)
+{
+	struct evdi_platform_device_data *data = platform_get_drvdata(pdev);
+
+	if (parent && data->parent == parent) {
+		sysfs_remove_link(&pdev->dev.kobj, "device");
+		data->symlinked = false;
+		data->parent = NULL;
+		EVDI_INFO("Detached from parent device\n");
+	}
+}
diff -ruN a/drivers/gpu/drm/evdi/evdi_platform_dev.h b/drivers/gpu/drm/evdi/evdi_platform_dev.h
--- a/drivers/gpu/drm/evdi/evdi_platform_dev.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_platform_dev.h	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,42 @@
+/* SPDX-License-Identifier: GPL-2.0-only
+ * evdi_platform_dev.h
+ *
+ * Copyright (c) 2020 DisplayLink (UK) Ltd.
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program. If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef _EVDI_PLATFORM_DEV_H_
+#define _EVDI_PLATFORM_DEV_H_
+
+#include <linux/types.h>
+
+struct platform_device_info;
+struct platform_device;
+struct drm_driver;
+struct device;
+
+struct platform_device *evdi_platform_dev_create(struct platform_device_info *info);
+void evdi_platform_dev_destroy(struct platform_device *dev);
+
+int evdi_platform_device_probe(struct platform_device *pdev);
+int evdi_platform_device_remove(struct platform_device *pdev);
+bool evdi_platform_device_is_free(struct platform_device *pdev);
+void evdi_platform_device_link(struct platform_device *pdev,
+				struct device *parent);
+void evdi_platform_device_unlink_if_linked_with(struct platform_device *pdev,
+				struct device *parent);
+
+#endif
+
diff -ruN a/drivers/gpu/drm/evdi/evdi_platform_drv.c b/drivers/gpu/drm/evdi/evdi_platform_drv.c
--- a/drivers/gpu/drm/evdi/evdi_platform_drv.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_platform_drv.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,249 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Copyright (C) 2012 Red Hat
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/usb.h>
+
+#include "evdi_params.h"
+#include "evdi_debug.h"
+#include "evdi_platform_drv.h"
+#include "evdi_platform_dev.h"
+#include "evdi_sysfs.h"
+
+MODULE_AUTHOR("DisplayLink (UK) Ltd.");
+MODULE_DESCRIPTION("Extensible Virtual Display Interface");
+MODULE_LICENSE("GPL");
+
+#define EVDI_DEVICE_COUNT_MAX 16
+
+static struct evdi_platform_drv_context {
+	struct device *root_dev;
+	unsigned int dev_count;
+	struct platform_device *devices[EVDI_DEVICE_COUNT_MAX];
+	struct notifier_block usb_notifier;
+	struct mutex lock;
+} g_ctx;
+
+#define evdi_platform_drv_context_lock(ctx) \
+		mutex_lock(&ctx->lock)
+
+#define evdi_platform_drv_context_unlock(ctx) \
+		mutex_unlock(&ctx->lock)
+
+static int evdi_platform_drv_usb(__always_unused struct notifier_block *nb,
+		unsigned long action,
+		void *data)
+{
+	struct usb_device *usb_dev = (struct usb_device *)(data);
+	struct platform_device *pdev;
+	int i = 0;
+
+	if (!usb_dev)
+		return 0;
+	if (action != BUS_NOTIFY_DEL_DEVICE)
+		return 0;
+
+	for (i = 0; i < EVDI_DEVICE_COUNT_MAX; ++i) {
+		pdev = g_ctx.devices[i];
+		if (!pdev)
+			continue;
+		evdi_platform_device_unlink_if_linked_with(pdev, &usb_dev->dev);
+		if (pdev->dev.parent == &usb_dev->dev) {
+			EVDI_INFO("Parent USB removed. Removing evdi.%d\n", i);
+			evdi_platform_dev_destroy(pdev);
+			evdi_platform_drv_context_lock((&g_ctx));
+			g_ctx.dev_count--;
+			g_ctx.devices[i] = NULL;
+			evdi_platform_drv_context_unlock((&g_ctx));
+		}
+	}
+	return 0;
+}
+
+static int evdi_platform_drv_get_free_idx(struct evdi_platform_drv_context *ctx)
+{
+	int i;
+
+	for (i = 0; i < EVDI_DEVICE_COUNT_MAX; ++i) {
+		if (ctx->devices[i] == NULL)
+			return i;
+	}
+	return -ENOMEM;
+}
+
+static struct platform_device *evdi_platform_drv_get_free_device(struct evdi_platform_drv_context *ctx)
+{
+	int i;
+	struct platform_device *pdev = NULL;
+
+	for (i = 0; i < EVDI_DEVICE_COUNT_MAX; ++i) {
+		pdev = ctx->devices[i];
+		if (pdev && evdi_platform_device_is_free(pdev))
+			return pdev;
+	}
+	return NULL;
+}
+
+
+static struct platform_device *evdi_platform_drv_create_new_device(struct evdi_platform_drv_context *ctx)
+{
+	struct platform_device *pdev = NULL;
+	struct platform_device_info pdevinfo = {
+		.parent = NULL,
+		.name = DRIVER_NAME,
+		.id = evdi_platform_drv_get_free_idx(ctx),
+		.res = NULL,
+		.num_res = 0,
+		.data = NULL,
+		.size_data = 0,
+		.dma_mask = DMA_BIT_MASK(32),
+	};
+
+	if (pdevinfo.id < 0 || ctx->dev_count >= EVDI_DEVICE_COUNT_MAX) {
+		EVDI_ERROR("Evdi device add failed. Too many devices.\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	pdev = evdi_platform_dev_create(&pdevinfo);
+	ctx->devices[pdevinfo.id] = pdev;
+	ctx->dev_count++;
+
+	return pdev;
+}
+
+int evdi_platform_device_add(struct device *device, struct device *parent)
+{
+	struct evdi_platform_drv_context *ctx =
+		(struct evdi_platform_drv_context *)dev_get_drvdata(device);
+	struct platform_device *pdev = NULL;
+
+	evdi_platform_drv_context_lock(ctx);
+	if (parent)
+		pdev = evdi_platform_drv_get_free_device(ctx);
+
+	if (IS_ERR_OR_NULL(pdev))
+		pdev = evdi_platform_drv_create_new_device(ctx);
+	evdi_platform_drv_context_unlock(ctx);
+
+	if (IS_ERR_OR_NULL(pdev))
+		return -EINVAL;
+
+	evdi_platform_device_link(pdev, parent);
+	return 0;
+}
+
+int evdi_platform_add_devices(struct device *device, unsigned int val)
+{
+	int dev_count = evdi_platform_device_count(device);
+
+	if (val == 0) {
+		EVDI_WARN("Adding 0 devices has no effect\n");
+		return 0;
+	}
+	if (val > EVDI_DEVICE_COUNT_MAX - dev_count) {
+		EVDI_ERROR("Evdi device add failed. Too many devices.\n");
+		return -EINVAL;
+	}
+
+	EVDI_DEBUG("Increasing device count to %u\n", dev_count + val);
+	while (val-- && evdi_platform_device_add(device, NULL) == 0)
+		;
+	return 0;
+}
+
+void evdi_platform_remove_all_devices(struct device *device)
+{
+	int i;
+	struct evdi_platform_drv_context *ctx =
+		(struct evdi_platform_drv_context *)dev_get_drvdata(device);
+
+	evdi_platform_drv_context_lock(ctx);
+	for (i = 0; i < EVDI_DEVICE_COUNT_MAX; ++i) {
+		if (ctx->devices[i]) {
+			EVDI_INFO("Removing evdi %d\n", i);
+			evdi_platform_dev_destroy(ctx->devices[i]);
+			g_ctx.dev_count--;
+			g_ctx.devices[i] = NULL;
+		}
+	}
+	ctx->dev_count = 0;
+	evdi_platform_drv_context_unlock(ctx);
+}
+
+int evdi_platform_device_count(struct device *device)
+{
+	int count = 0;
+	struct evdi_platform_drv_context *ctx = NULL;
+
+	ctx = (struct evdi_platform_drv_context *)dev_get_drvdata(device);
+	evdi_platform_drv_context_lock(ctx);
+	count = ctx->dev_count;
+	evdi_platform_drv_context_unlock(ctx);
+
+	return count;
+
+}
+
+static struct platform_driver evdi_platform_driver = {
+	.probe = evdi_platform_device_probe,
+	.remove = evdi_platform_device_remove,
+	.driver = {
+		   .name = DRIVER_NAME,
+		   .mod_name = KBUILD_MODNAME,
+		   .owner = THIS_MODULE,
+	}
+};
+
+static int __init evdi_init(void)
+{
+	int ret;
+
+	EVDI_INFO("Initialising logging on level %u\n", evdi_loglevel);
+	EVDI_INFO("Atomic driver: yes");
+
+	memset(&g_ctx, 0, sizeof(g_ctx));
+	g_ctx.root_dev = root_device_register(DRIVER_NAME);
+	g_ctx.usb_notifier.notifier_call = evdi_platform_drv_usb;
+	mutex_init(&g_ctx.lock);
+	dev_set_drvdata(g_ctx.root_dev, &g_ctx);
+
+	usb_register_notify(&g_ctx.usb_notifier);
+	evdi_sysfs_init(g_ctx.root_dev);
+	ret = platform_driver_register(&evdi_platform_driver);
+	if (ret)
+		return ret;
+
+	if (evdi_initial_device_count)
+		return evdi_platform_add_devices(
+			g_ctx.root_dev, evdi_initial_device_count);
+
+	return 0;
+}
+
+static void __exit evdi_exit(void)
+{
+	EVDI_CHECKPT();
+	evdi_platform_remove_all_devices(g_ctx.root_dev);
+	platform_driver_unregister(&evdi_platform_driver);
+
+	if (!PTR_ERR_OR_ZERO(g_ctx.root_dev)) {
+		evdi_sysfs_exit(g_ctx.root_dev);
+		usb_unregister_notify(&g_ctx.usb_notifier);
+		dev_set_drvdata(g_ctx.root_dev, NULL);
+		root_device_unregister(g_ctx.root_dev);
+	}
+	EVDI_INFO("Exit %s driver\n", DRIVER_NAME);
+}
+
+module_init(evdi_init);
+module_exit(evdi_exit);
diff -ruN a/drivers/gpu/drm/evdi/evdi_platform_drv.h b/drivers/gpu/drm/evdi/evdi_platform_drv.h
--- a/drivers/gpu/drm/evdi/evdi_platform_drv.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_platform_drv.h	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,40 @@
+/* SPDX-License-Identifier: GPL-2.0-only
+ * evdi_platform_drv.h
+ *
+ * Copyright (c) 2020 DisplayLink (UK) Ltd.
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program. If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef _EVDI_PLATFORM_DRV_H_
+#define _EVDI_PLATFORM_DRV_H_
+
+struct device;
+struct platform_device_info;
+
+#define DRIVER_NAME   "evdi"
+#define DRIVER_DESC   "Extensible Virtual Display Interface"
+#define DRIVER_DATE   "20210126"
+
+#define DRIVER_MAJOR 1
+#define DRIVER_MINOR 9
+#define DRIVER_PATCH 1
+
+void evdi_platform_remove_all_devices(struct device *device);
+int evdi_platform_device_count(struct device *device);
+int evdi_platform_add_devices(struct device *device, unsigned int val);
+int evdi_platform_device_add(struct device *device, struct device *parent);
+
+#endif
+
diff -ruN a/drivers/gpu/drm/evdi/evdi_sysfs.c b/drivers/gpu/drm/evdi/evdi_sysfs.c
--- a/drivers/gpu/drm/evdi/evdi_sysfs.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_sysfs.c	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,256 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * evdi_sysfs.c
+ *
+ * Copyright (c) 2020 DisplayLink (UK) Ltd.
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program. If not, see <http://www.gnu.org/licenses/>.
+ */
+
+
+#include <linux/device.h>
+#include <linux/slab.h>
+#include <linux/usb.h>
+
+#include "evdi_sysfs.h"
+#include "evdi_params.h"
+#include "evdi_debug.h"
+#include "evdi_platform_drv.h"
+
+#define MAX_EVDI_USB_ADDR 10
+
+static ssize_t version_show(__always_unused struct device *dev,
+			    __always_unused struct device_attribute *attr,
+			    char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u.%u.%u\n", DRIVER_MAJOR,
+			DRIVER_MINOR, DRIVER_PATCH);
+}
+
+static ssize_t count_show(__always_unused struct device *dev,
+			  __always_unused struct device_attribute *attr,
+			  char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", evdi_platform_device_count(dev));
+}
+
+struct evdi_usb_addr {
+	int addr[MAX_EVDI_USB_ADDR];
+	int len;
+	struct usb_device *usb;
+};
+
+static int evdi_platform_device_attach(struct device *device,
+		struct evdi_usb_addr *parent_addr);
+
+static ssize_t add_device_with_usb_path(struct device *dev,
+			 const char *buf, size_t count)
+{
+	char *usb_path = kstrdup(buf, GFP_KERNEL);
+	char *temp_path = usb_path;
+	char *bus_token;
+	char *usb_token;
+	char *usb_token_copy = NULL;
+	char *token;
+	char *bus;
+	char *port;
+	struct evdi_usb_addr usb_addr;
+
+	if (!usb_path)
+		return -ENOMEM;
+
+	memset(&usb_addr, 0, sizeof(usb_addr));
+	temp_path = strnstr(temp_path, "usb:", count);
+	if (!temp_path)
+		goto err_parse_usb_path;
+
+
+	temp_path = strim(temp_path);
+
+	bus_token = strsep(&temp_path, ":");
+	if (!bus_token)
+		goto err_parse_usb_path;
+
+	usb_token = strsep(&temp_path, ":");
+	if (!usb_token)
+		goto err_parse_usb_path;
+
+	/* Separate trailing ':*' from usb_token */
+	strsep(&temp_path, ":");
+
+	token = usb_token_copy = kstrdup(usb_token, GFP_KERNEL);
+	bus = strsep(&token, "-");
+	if (!bus)
+		goto err_parse_usb_path;
+	if (kstrtouint(bus, 10, &usb_addr.addr[usb_addr.len++]))
+		goto err_parse_usb_path;
+
+	do {
+		port = strsep(&token, ".");
+		if (!port)
+			goto err_parse_usb_path;
+		if (kstrtouint(port, 10, &usb_addr.addr[usb_addr.len++]))
+			goto err_parse_usb_path;
+	} while (token && port && usb_addr.len < MAX_EVDI_USB_ADDR);
+
+	if (evdi_platform_device_attach(dev, &usb_addr) != 0) {
+		EVDI_ERROR("Unable to attach to: %s\n", buf);
+		kfree(usb_path);
+		kfree(usb_token_copy);
+		return -EINVAL;
+	}
+
+	EVDI_INFO("Attaching to %s:%s\n", bus_token, usb_token);
+	kfree(usb_path);
+	kfree(usb_token_copy);
+	return count;
+
+err_parse_usb_path:
+	EVDI_ERROR("Unable to parse usb path: %s", buf);
+	kfree(usb_path);
+	kfree(usb_token_copy);
+	return -EINVAL;
+}
+
+static int find_usb_device_at_path(struct usb_device *usb, void *data)
+{
+	struct evdi_usb_addr *find_path = (struct evdi_usb_addr *)(data);
+	struct usb_device *pdev = usb;
+	int port = 0;
+	int i;
+
+	i = find_path->len - 1;
+	while (pdev != NULL && i >= 0 && i < MAX_EVDI_USB_ADDR) {
+		port = pdev->portnum;
+		if (port == 0)
+			port = pdev->bus->busnum;
+
+		if (port != find_path->addr[i])
+			return 0;
+
+		if (pdev->parent == NULL && i == 0) {
+			find_path->usb = usb;
+			return 1;
+		}
+		pdev = pdev->parent;
+		i--;
+	}
+
+	return 0;
+}
+
+static int evdi_platform_device_attach(struct device *device,
+		struct evdi_usb_addr *parent_addr)
+{
+	struct device *parent = NULL;
+
+	if (!parent_addr)
+		return -EINVAL;
+
+	if (!usb_for_each_dev(parent_addr, find_usb_device_at_path) ||
+	    !parent_addr->usb)
+		return -EINVAL;
+
+	parent = &parent_addr->usb->dev;
+	return evdi_platform_device_add(device, parent);
+}
+
+static ssize_t add_store(struct device *dev,
+			 __always_unused struct device_attribute *attr,
+			 const char *buf, size_t count)
+{
+	unsigned int val;
+	int ret;
+
+	if (strnstr(buf, "usb:", count))
+		return add_device_with_usb_path(dev, buf, count);
+
+	if (kstrtouint(buf, 10, &val)) {
+		EVDI_ERROR("Invalid device count \"%s\"\n", buf);
+		return -EINVAL;
+	}
+
+	ret = evdi_platform_add_devices(dev, val);
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+static ssize_t remove_all_store(struct device *dev,
+				__always_unused struct device_attribute *attr,
+				__always_unused const char *buf,
+				size_t count)
+{
+	evdi_platform_remove_all_devices(dev);
+	return count;
+}
+
+static ssize_t loglevel_show(__always_unused struct device *dev,
+			     __always_unused struct device_attribute *attr,
+			     char *buf)
+{
+	return snprintf(buf, PAGE_SIZE, "%u\n", evdi_loglevel);
+}
+
+static ssize_t loglevel_store(__always_unused struct device *dev,
+			      __always_unused struct device_attribute *attr,
+			      const char *buf,
+			      size_t count)
+{
+	unsigned int val;
+
+	if (kstrtouint(buf, 10, &val)) {
+		EVDI_ERROR("Unable to parse %u\n", val);
+		return -EINVAL;
+	}
+	if (val > EVDI_LOGLEVEL_VERBOSE) {
+		EVDI_ERROR("Invalid loglevel %u\n", val);
+		return -EINVAL;
+	}
+
+	EVDI_INFO("Setting loglevel to %u\n", val);
+	evdi_loglevel = val;
+	return count;
+}
+
+static struct device_attribute evdi_device_attributes[] = {
+	__ATTR_RO(count),
+	__ATTR_RO(version),
+	__ATTR_RW(loglevel),
+	__ATTR_WO(add),
+	__ATTR_WO(remove_all)
+};
+
+void evdi_sysfs_init(struct device *root)
+{
+	int i;
+
+	if (!PTR_ERR_OR_ZERO(root))
+		for (i = 0; i < ARRAY_SIZE(evdi_device_attributes); i++)
+			device_create_file(root, &evdi_device_attributes[i]);
+}
+
+void evdi_sysfs_exit(struct device *root)
+{
+	int i;
+
+	if (PTR_ERR_OR_ZERO(root)) {
+		EVDI_ERROR("root device is null");
+		return;
+	}
+	for (i = 0; i < ARRAY_SIZE(evdi_device_attributes); i++)
+		device_remove_file(root, &evdi_device_attributes[i]);
+}
+
diff -ruN a/drivers/gpu/drm/evdi/evdi_sysfs.h b/drivers/gpu/drm/evdi/evdi_sysfs.h
--- a/drivers/gpu/drm/evdi/evdi_sysfs.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/evdi_sysfs.h	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,28 @@
+/* SPDX-License-Identifier: GPL-2.0-only
+ * evdi_sysfs.h
+ *
+ * Copyright (c) 2020 DisplayLink (UK) Ltd.
+ *
+ * This program is free software; you can redistribute  it and/or modify it
+ * under  the terms of  the GNU General  Public License as published by the
+ * Free Software Foundation;  either version 2 of the  License, or (at your
+ * option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program. If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef _EVDI_SYSFS_H_
+#define _EVDI_SYSFS_H_
+
+struct device;
+
+void evdi_sysfs_init(struct device *root);
+void evdi_sysfs_exit(struct device *root);
+
+#endif
diff -ruN a/drivers/gpu/drm/evdi/Kconfig b/drivers/gpu/drm/evdi/Kconfig
--- a/drivers/gpu/drm/evdi/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/Kconfig	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,21 @@
+#
+# Copyright (c) 2015 - 2019 DisplayLink (UK) Ltd.
+#
+# This file is subject to the terms and conditions of the GNU General Public
+# License v2. See the file COPYING in the main directory of this archive for
+# more details.
+#
+
+config DRM_EVDI
+	tristate "Extensible Virtual Display Interface"
+	depends on DRM
+	depends on USB_SUPPORT
+	depends on USB_ARCH_HAS_HCD
+	select USB
+	select DRM_KMS_HELPER
+	help
+		This is a KMS interface driver allowing user-space programs to
+		register a virtual display (that imitates physical monitor) and
+		retrieve contents (as a frame buffer) that system renders on it.
+		Say M/Y to add support for these devices via DRM/KMS interfaces.
+
diff -ruN a/drivers/gpu/drm/evdi/LICENSE b/drivers/gpu/drm/evdi/LICENSE
--- a/drivers/gpu/drm/evdi/LICENSE	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/LICENSE	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,339 @@
+                    GNU GENERAL PUBLIC LICENSE
+                       Version 2, June 1991
+
+ Copyright (C) 1989, 1991 Free Software Foundation, Inc.,
+ 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+                            Preamble
+
+  The licenses for most software are designed to take away your
+freedom to share and change it.  By contrast, the GNU General Public
+License is intended to guarantee your freedom to share and change free
+software--to make sure the software is free for all its users.  This
+General Public License applies to most of the Free Software
+Foundation's software and to any other program whose authors commit to
+using it.  (Some other Free Software Foundation software is covered by
+the GNU Lesser General Public License instead.)  You can apply it to
+your programs, too.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+this service if you wish), that you receive source code or can get it
+if you want it, that you can change the software or use pieces of it
+in new free programs; and that you know you can do these things.
+
+  To protect your rights, we need to make restrictions that forbid
+anyone to deny you these rights or to ask you to surrender the rights.
+These restrictions translate to certain responsibilities for you if you
+distribute copies of the software, or if you modify it.
+
+  For example, if you distribute copies of such a program, whether
+gratis or for a fee, you must give the recipients all the rights that
+you have.  You must make sure that they, too, receive or can get the
+source code.  And you must show them these terms so they know their
+rights.
+
+  We protect your rights with two steps: (1) copyright the software, and
+(2) offer you this license which gives you legal permission to copy,
+distribute and/or modify the software.
+
+  Also, for each author's protection and ours, we want to make certain
+that everyone understands that there is no warranty for this free
+software.  If the software is modified by someone else and passed on, we
+want its recipients to know that what they have is not the original, so
+that any problems introduced by others will not reflect on the original
+authors' reputations.
+
+  Finally, any free program is threatened constantly by software
+patents.  We wish to avoid the danger that redistributors of a free
+program will individually obtain patent licenses, in effect making the
+program proprietary.  To prevent this, we have made it clear that any
+patent must be licensed for everyone's free use or not licensed at all.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+                    GNU GENERAL PUBLIC LICENSE
+   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
+
+  0. This License applies to any program or other work which contains
+a notice placed by the copyright holder saying it may be distributed
+under the terms of this General Public License.  The "Program", below,
+refers to any such program or work, and a "work based on the Program"
+means either the Program or any derivative work under copyright law:
+that is to say, a work containing the Program or a portion of it,
+either verbatim or with modifications and/or translated into another
+language.  (Hereinafter, translation is included without limitation in
+the term "modification".)  Each licensee is addressed as "you".
+
+Activities other than copying, distribution and modification are not
+covered by this License; they are outside its scope.  The act of
+running the Program is not restricted, and the output from the Program
+is covered only if its contents constitute a work based on the
+Program (independent of having been made by running the Program).
+Whether that is true depends on what the Program does.
+
+  1. You may copy and distribute verbatim copies of the Program's
+source code as you receive it, in any medium, provided that you
+conspicuously and appropriately publish on each copy an appropriate
+copyright notice and disclaimer of warranty; keep intact all the
+notices that refer to this License and to the absence of any warranty;
+and give any other recipients of the Program a copy of this License
+along with the Program.
+
+You may charge a fee for the physical act of transferring a copy, and
+you may at your option offer warranty protection in exchange for a fee.
+
+  2. You may modify your copy or copies of the Program or any portion
+of it, thus forming a work based on the Program, and copy and
+distribute such modifications or work under the terms of Section 1
+above, provided that you also meet all of these conditions:
+
+    a) You must cause the modified files to carry prominent notices
+    stating that you changed the files and the date of any change.
+
+    b) You must cause any work that you distribute or publish, that in
+    whole or in part contains or is derived from the Program or any
+    part thereof, to be licensed as a whole at no charge to all third
+    parties under the terms of this License.
+
+    c) If the modified program normally reads commands interactively
+    when run, you must cause it, when started running for such
+    interactive use in the most ordinary way, to print or display an
+    announcement including an appropriate copyright notice and a
+    notice that there is no warranty (or else, saying that you provide
+    a warranty) and that users may redistribute the program under
+    these conditions, and telling the user how to view a copy of this
+    License.  (Exception: if the Program itself is interactive but
+    does not normally print such an announcement, your work based on
+    the Program is not required to print an announcement.)
+
+These requirements apply to the modified work as a whole.  If
+identifiable sections of that work are not derived from the Program,
+and can be reasonably considered independent and separate works in
+themselves, then this License, and its terms, do not apply to those
+sections when you distribute them as separate works.  But when you
+distribute the same sections as part of a whole which is a work based
+on the Program, the distribution of the whole must be on the terms of
+this License, whose permissions for other licensees extend to the
+entire whole, and thus to each and every part regardless of who wrote it.
+
+Thus, it is not the intent of this section to claim rights or contest
+your rights to work written entirely by you; rather, the intent is to
+exercise the right to control the distribution of derivative or
+collective works based on the Program.
+
+In addition, mere aggregation of another work not based on the Program
+with the Program (or with a work based on the Program) on a volume of
+a storage or distribution medium does not bring the other work under
+the scope of this License.
+
+  3. You may copy and distribute the Program (or a work based on it,
+under Section 2) in object code or executable form under the terms of
+Sections 1 and 2 above provided that you also do one of the following:
+
+    a) Accompany it with the complete corresponding machine-readable
+    source code, which must be distributed under the terms of Sections
+    1 and 2 above on a medium customarily used for software interchange; or,
+
+    b) Accompany it with a written offer, valid for at least three
+    years, to give any third party, for a charge no more than your
+    cost of physically performing source distribution, a complete
+    machine-readable copy of the corresponding source code, to be
+    distributed under the terms of Sections 1 and 2 above on a medium
+    customarily used for software interchange; or,
+
+    c) Accompany it with the information you received as to the offer
+    to distribute corresponding source code.  (This alternative is
+    allowed only for noncommercial distribution and only if you
+    received the program in object code or executable form with such
+    an offer, in accord with Subsection b above.)
+
+The source code for a work means the preferred form of the work for
+making modifications to it.  For an executable work, complete source
+code means all the source code for all modules it contains, plus any
+associated interface definition files, plus the scripts used to
+control compilation and installation of the executable.  However, as a
+special exception, the source code distributed need not include
+anything that is normally distributed (in either source or binary
+form) with the major components (compiler, kernel, and so on) of the
+operating system on which the executable runs, unless that component
+itself accompanies the executable.
+
+If distribution of executable or object code is made by offering
+access to copy from a designated place, then offering equivalent
+access to copy the source code from the same place counts as
+distribution of the source code, even though third parties are not
+compelled to copy the source along with the object code.
+
+  4. You may not copy, modify, sublicense, or distribute the Program
+except as expressly provided under this License.  Any attempt
+otherwise to copy, modify, sublicense or distribute the Program is
+void, and will automatically terminate your rights under this License.
+However, parties who have received copies, or rights, from you under
+this License will not have their licenses terminated so long as such
+parties remain in full compliance.
+
+  5. You are not required to accept this License, since you have not
+signed it.  However, nothing else grants you permission to modify or
+distribute the Program or its derivative works.  These actions are
+prohibited by law if you do not accept this License.  Therefore, by
+modifying or distributing the Program (or any work based on the
+Program), you indicate your acceptance of this License to do so, and
+all its terms and conditions for copying, distributing or modifying
+the Program or works based on it.
+
+  6. Each time you redistribute the Program (or any work based on the
+Program), the recipient automatically receives a license from the
+original licensor to copy, distribute or modify the Program subject to
+these terms and conditions.  You may not impose any further
+restrictions on the recipients' exercise of the rights granted herein.
+You are not responsible for enforcing compliance by third parties to
+this License.
+
+  7. If, as a consequence of a court judgment or allegation of patent
+infringement or for any other reason (not limited to patent issues),
+conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot
+distribute so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you
+may not distribute the Program at all.  For example, if a patent
+license would not permit royalty-free redistribution of the Program by
+all those who receive copies directly or indirectly through you, then
+the only way you could satisfy both it and this License would be to
+refrain entirely from distribution of the Program.
+
+If any portion of this section is held invalid or unenforceable under
+any particular circumstance, the balance of the section is intended to
+apply and the section as a whole is intended to apply in other
+circumstances.
+
+It is not the purpose of this section to induce you to infringe any
+patents or other property right claims or to contest validity of any
+such claims; this section has the sole purpose of protecting the
+integrity of the free software distribution system, which is
+implemented by public license practices.  Many people have made
+generous contributions to the wide range of software distributed
+through that system in reliance on consistent application of that
+system; it is up to the author/donor to decide if he or she is willing
+to distribute software through any other system and a licensee cannot
+impose that choice.
+
+This section is intended to make thoroughly clear what is believed to
+be a consequence of the rest of this License.
+
+  8. If the distribution and/or use of the Program is restricted in
+certain countries either by patents or by copyrighted interfaces, the
+original copyright holder who places the Program under this License
+may add an explicit geographical distribution limitation excluding
+those countries, so that distribution is permitted only in or among
+countries not thus excluded.  In such case, this License incorporates
+the limitation as if written in the body of this License.
+
+  9. The Free Software Foundation may publish revised and/or new versions
+of the General Public License from time to time.  Such new versions will
+be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+Each version is given a distinguishing version number.  If the Program
+specifies a version number of this License which applies to it and "any
+later version", you have the option of following the terms and conditions
+either of that version or of any later version published by the Free
+Software Foundation.  If the Program does not specify a version number of
+this License, you may choose any version ever published by the Free Software
+Foundation.
+
+  10. If you wish to incorporate parts of the Program into other free
+programs whose distribution conditions are different, write to the author
+to ask for permission.  For software which is copyrighted by the Free
+Software Foundation, write to the Free Software Foundation; we sometimes
+make exceptions for this.  Our decision will be guided by the two goals
+of preserving the free status of all derivatives of our free software and
+of promoting the sharing and reuse of software generally.
+
+                            NO WARRANTY
+
+  11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY
+FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN
+OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
+PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED
+OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS
+TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE
+PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,
+REPAIR OR CORRECTION.
+
+  12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR
+REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,
+INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING
+OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED
+TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY
+YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER
+PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE
+POSSIBILITY OF SUCH DAMAGES.
+
+                     END OF TERMS AND CONDITIONS
+
+            How to Apply These Terms to Your New Programs
+
+  If you develop a new program, and you want it to be of the greatest
+possible use to the public, the best way to achieve this is to make it
+free software which everyone can redistribute and change under these terms.
+
+  To do so, attach the following notices to the program.  It is safest
+to attach them to the start of each source file to most effectively
+convey the exclusion of warranty; and each file should have at least
+the "copyright" line and a pointer to where the full notice is found.
+
+    <one line to give the program's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This program is free software; you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation; either version 2 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License along
+    with this program; if not, write to the Free Software Foundation, Inc.,
+    51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+
+Also add information on how to contact you by electronic and paper mail.
+
+If the program is interactive, make it output a short notice like this
+when it starts in an interactive mode:
+
+    Gnomovision version 69, Copyright (C) year name of author
+    Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
+    This is free software, and you are welcome to redistribute it
+    under certain conditions; type `show c' for details.
+
+The hypothetical commands `show w' and `show c' should show the appropriate
+parts of the General Public License.  Of course, the commands you use may
+be called something other than `show w' and `show c'; they could even be
+mouse-clicks or menu items--whatever suits your program.
+
+You should also get your employer (if you work as a programmer) or your
+school, if any, to sign a "copyright disclaimer" for the program, if
+necessary.  Here is a sample; alter the names:
+
+  Yoyodyne, Inc., hereby disclaims all copyright interest in the program
+  `Gnomovision' (which makes passes at compilers) written by James Hacker.
+
+  <signature of Ty Coon>, 1 April 1989
+  Ty Coon, President of Vice
+
+This General Public License does not permit incorporating your program into
+proprietary programs.  If your program is a subroutine library, you may
+consider it more useful to permit linking proprietary applications with the
+library.  If this is what you want to do, use the GNU Lesser General
+Public License instead of this License.
diff -ruN a/drivers/gpu/drm/evdi/Makefile b/drivers/gpu/drm/evdi/Makefile
--- a/drivers/gpu/drm/evdi/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/evdi/Makefile	2021-12-23 08:35:27.000000000 +0100
@@ -0,0 +1,2 @@
+evdi-y := evdi_platform_drv.o evdi_platform_dev.o evdi_sysfs.o evdi_modeset.o evdi_connector.o evdi_encoder.o evdi_drm_drv.o evdi_fb.o evdi_gem.o evdi_painter.o evdi_params.o evdi_cursor.o evdi_debug.o evdi_i2c.o
+obj-$(CONFIG_DRM_EVDI) := evdi.o
diff -ruN a/drivers/gpu/drm/i2c/sil164_drv.c b/drivers/gpu/drm/i2c/sil164_drv.c
--- a/drivers/gpu/drm/i2c/sil164_drv.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/i2c/sil164_drv.c	2021-12-23 08:35:27.000000000 +0100
@@ -43,11 +43,6 @@
 #define to_sil164_priv(x) \
 	((struct sil164_priv *)to_encoder_slave(x)->slave_priv)
 
-#define sil164_dbg(client, format, ...) do {				\
-		if (drm_debug_enabled(DRM_UT_KMS))			\
-			dev_printk(KERN_DEBUG, &client->dev,		\
-				   "%s: " format, __func__, ## __VA_ARGS__); \
-	} while (0)
 #define sil164_info(client, format, ...)		\
 	dev_info(&client->dev, format, __VA_ARGS__)
 #define sil164_err(client, format, ...)			\
@@ -359,8 +354,8 @@
 	int rev = sil164_read(client, SIL164_REVISION);
 
 	if (vendor != 0x1 || device != 0x6) {
-		sil164_dbg(client, "Unknown device %x:%x.%x\n",
-			   vendor, device, rev);
+		drm_dev_dbg(&client->dev, DRM_UT_KMS,
+			    "Unknown device %x:%x.%x\n", vendor, device, rev);
 		return -ENODEV;
 	}
 
@@ -389,7 +384,8 @@
 	};
 
 	if (i2c_transfer(adap, &msg, 1) != 1) {
-		sil164_dbg(adap, "No dual-link slave found.");
+		drm_dev_dbg(&adap->dev, DRM_UT_KMS,
+			    "No dual-link slave found.");
 		return NULL;
 	}
 
diff -ruN a/drivers/gpu/drm/Kconfig b/drivers/gpu/drm/Kconfig
--- a/drivers/gpu/drm/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/Kconfig	2021-12-23 08:35:19.000000000 +0100
@@ -196,6 +196,8 @@
 	help
 	  Helpers for ttm-based gem objects
 
+source "drivers/gpu/drm/ttm/Kconfig"
+
 config DRM_GEM_CMA_HELPER
 	bool
 	depends on DRM
@@ -291,6 +293,8 @@
 
 	  If M is selected the module will be called vkms.
 
+source "drivers/gpu/drm/evdi/Kconfig"
+
 source "drivers/gpu/drm/exynos/Kconfig"
 
 source "drivers/gpu/drm/rockchip/Kconfig"
diff -ruN a/drivers/gpu/drm/Makefile b/drivers/gpu/drm/Makefile
--- a/drivers/gpu/drm/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/Makefile	2021-12-23 08:35:19.000000000 +0100
@@ -85,6 +85,7 @@
 obj-$(CONFIG_DRM_VGEM)	+= vgem/
 obj-$(CONFIG_DRM_VKMS)	+= vkms/
 obj-$(CONFIG_DRM_NOUVEAU) +=nouveau/
+obj-$(CONFIG_DRM_EVDI)	+= evdi/
 obj-$(CONFIG_DRM_EXYNOS) +=exynos/
 obj-$(CONFIG_DRM_ROCKCHIP) +=rockchip/
 obj-$(CONFIG_DRM_GMA500) += gma500/
diff -ruN a/drivers/gpu/drm/ttm/Kconfig b/drivers/gpu/drm/ttm/Kconfig
--- a/drivers/gpu/drm/ttm/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/gpu/drm/ttm/Kconfig	2021-12-23 08:35:28.000000000 +0100
@@ -0,0 +1,7 @@
+config DRM_TTM_BO_WAIT_TIMEOUT
+	int "Default timeout for ttm bo wait (in seconds)"
+	depends on DRM_TTM
+	default 15
+	help
+	  This option controls the default timeout (in seconds) used in
+	  ttm_bo_wait
diff -ruN a/drivers/gpu/drm/ttm/ttm_bo.c b/drivers/gpu/drm/ttm/ttm_bo.c
--- a/drivers/gpu/drm/ttm/ttm_bo.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/ttm/ttm_bo.c	2021-12-23 08:35:28.000000000 +0100
@@ -1110,7 +1110,7 @@
 int ttm_bo_wait(struct ttm_buffer_object *bo,
 		bool interruptible, bool no_wait)
 {
-	long timeout = 15 * HZ;
+	long timeout = CONFIG_DRM_TTM_BO_WAIT_TIMEOUT * HZ;
 
 	if (no_wait) {
 		if (dma_resv_test_signaled(bo->base.resv, true))
diff -ruN a/drivers/gpu/drm/ttm/ttm_bo_vm.c b/drivers/gpu/drm/ttm/ttm_bo_vm.c
--- a/drivers/gpu/drm/ttm/ttm_bo_vm.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/ttm/ttm_bo_vm.c	2021-12-23 08:35:28.000000000 +0100
@@ -357,8 +357,6 @@
 {
 	struct ttm_buffer_object *bo = vma->vm_private_data;
 
-	WARN_ON(bo->bdev->dev_mapping != vma->vm_file->f_mapping);
-
 	ttm_bo_get(bo);
 }
 EXPORT_SYMBOL(ttm_bo_vm_open);
diff -ruN a/drivers/gpu/drm/udl/udl_connector.c b/drivers/gpu/drm/udl/udl_connector.c
--- a/drivers/gpu/drm/udl/udl_connector.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/udl/udl_connector.c	2021-12-23 08:35:28.000000000 +0100
@@ -60,6 +60,14 @@
 			  struct drm_display_mode *mode)
 {
 	struct udl_device *udl = to_udl(connector->dev);
+	int con_type = connector->connector_type;
+
+	if ((con_type == DRM_MODE_CONNECTOR_DVII ||
+	     con_type == DRM_MODE_CONNECTOR_DVID ||
+	     con_type == DRM_MODE_CONNECTOR_DVIA) &&
+	    mode->clock > 165000)
+		return MODE_CLOCK_HIGH;
+
 	if (!udl->sku_pixel_limit)
 		return 0;
 
diff -ruN a/drivers/gpu/drm/virtio/virtgpu_debugfs.c b/drivers/gpu/drm/virtio/virtgpu_debugfs.c
--- a/drivers/gpu/drm/virtio/virtgpu_debugfs.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/virtio/virtgpu_debugfs.c	2021-12-23 08:35:29.000000000 +0100
@@ -52,6 +52,7 @@
 			    vgdev->has_resource_assign_uuid);
 
 	virtio_gpu_add_bool(m, "blob resources", vgdev->has_resource_blob);
+	virtio_gpu_add_bool(m, "context init", vgdev->has_context_init);
 	virtio_gpu_add_int(m, "cap sets", vgdev->num_capsets);
 	virtio_gpu_add_int(m, "scanouts", vgdev->num_scanouts);
 	if (vgdev->host_visible_region.len) {
diff -ruN a/drivers/gpu/drm/virtio/virtgpu_drv.c b/drivers/gpu/drm/virtio/virtgpu_drv.c
--- a/drivers/gpu/drm/virtio/virtgpu_drv.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/virtio/virtgpu_drv.c	2021-12-23 08:35:29.000000000 +0100
@@ -172,6 +172,7 @@
 	VIRTIO_GPU_F_EDID,
 	VIRTIO_GPU_F_RESOURCE_UUID,
 	VIRTIO_GPU_F_RESOURCE_BLOB,
+	VIRTIO_GPU_F_CONTEXT_INIT,
 };
 static struct virtio_driver virtio_gpu_driver = {
 	.feature_table = features,
diff -ruN a/drivers/gpu/drm/virtio/virtgpu_drv.h b/drivers/gpu/drm/virtio/virtgpu_drv.h
--- a/drivers/gpu/drm/virtio/virtgpu_drv.h	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/virtio/virtgpu_drv.h	2021-12-23 08:35:29.000000000 +0100
@@ -54,6 +54,8 @@
 #define STATE_OK 1
 #define STATE_ERR 2
 
+#define MAX_CAPSET_ID 63
+
 struct virtio_gpu_object_params {
 	unsigned long size;
 	bool dumb;
@@ -83,6 +85,14 @@
 struct virtio_gpu_object {
 	struct drm_gem_shmem_object base;
 	uint32_t hw_res_handle;
+
+	bool create_callback_done;
+	/* These variables are only valid if create_callback_done is true */
+	uint32_t num_planes;
+	uint64_t format_modifier;
+	uint32_t strides[4];
+	uint32_t offsets[4];
+
 	bool dumb;
 	bool created;
 	bool host3d_blob, guest_blob;
@@ -233,6 +243,7 @@
 	bool has_resource_assign_uuid;
 	bool has_resource_blob;
 	bool has_host_visible;
+	bool has_context_init;
 	struct virtio_shm_region host_visible_region;
 	struct drm_mm host_visible_mm;
 
@@ -244,6 +255,7 @@
 
 	struct virtio_gpu_drv_capset *capsets;
 	uint32_t num_capsets;
+	uint64_t capset_id_mask;
 	struct list_head cap_cache;
 
 	/* protects uuid state when exporting */
@@ -254,12 +266,13 @@
 
 struct virtio_gpu_fpriv {
 	uint32_t ctx_id;
+	uint32_t context_init;
 	bool context_created;
 	struct mutex context_lock;
 };
 
 /* virtgpu_ioctl.c */
-#define DRM_VIRTIO_NUM_IOCTLS 11
+#define DRM_VIRTIO_NUM_IOCTLS 12
 extern struct drm_ioctl_desc virtio_gpu_ioctls[DRM_VIRTIO_NUM_IOCTLS];
 void virtio_gpu_create_context(struct drm_device *dev, struct drm_file *file);
 
@@ -337,7 +350,8 @@
 			      struct virtio_gpu_drv_cap_cache **cache_p);
 int virtio_gpu_cmd_get_edids(struct virtio_gpu_device *vgdev);
 void virtio_gpu_cmd_context_create(struct virtio_gpu_device *vgdev, uint32_t id,
-				   uint32_t nlen, const char *name);
+				   uint32_t context_init, uint32_t nlen,
+				   const char *name);
 void virtio_gpu_cmd_context_destroy(struct virtio_gpu_device *vgdev,
 				    uint32_t id);
 void virtio_gpu_cmd_context_attach_resource(struct virtio_gpu_device *vgdev,
@@ -367,7 +381,7 @@
 					struct drm_virtgpu_3d_box *box,
 					struct virtio_gpu_object_array *objs,
 					struct virtio_gpu_fence *fence);
-void
+int
 virtio_gpu_cmd_resource_create_3d(struct virtio_gpu_device *vgdev,
 				  struct virtio_gpu_object *bo,
 				  struct virtio_gpu_object_params *params,
diff -ruN a/drivers/gpu/drm/virtio/virtgpu_ioctl.c b/drivers/gpu/drm/virtio/virtgpu_ioctl.c
--- a/drivers/gpu/drm/virtio/virtgpu_ioctl.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/virtio/virtgpu_ioctl.c	2021-12-23 08:35:29.000000000 +0100
@@ -38,20 +38,30 @@
 				    VIRTGPU_BLOB_FLAG_USE_SHAREABLE | \
 				    VIRTGPU_BLOB_FLAG_USE_CROSS_DEVICE)
 
+/* Must be called with &virtio_gpu_fpriv.struct_mutex held. */
+static void virtio_gpu_create_context_locked(struct virtio_gpu_device *vgdev,
+					     struct virtio_gpu_fpriv *vfpriv)
+{
+	char dbgname[TASK_COMM_LEN];
+
+	get_task_comm(dbgname, current);
+	virtio_gpu_cmd_context_create(vgdev, vfpriv->ctx_id,
+				      vfpriv->context_init, strlen(dbgname),
+				      dbgname);
+
+	vfpriv->context_created = true;
+}
+
 void virtio_gpu_create_context(struct drm_device *dev, struct drm_file *file)
 {
 	struct virtio_gpu_device *vgdev = dev->dev_private;
 	struct virtio_gpu_fpriv *vfpriv = file->driver_priv;
-	char dbgname[TASK_COMM_LEN];
 
 	mutex_lock(&vfpriv->context_lock);
 	if (vfpriv->context_created)
 		goto out_unlock;
 
-	get_task_comm(dbgname, current);
-	virtio_gpu_cmd_context_create(vgdev, vfpriv->ctx_id,
-				      strlen(dbgname), dbgname);
-	vfpriv->context_created = true;
+	virtio_gpu_create_context_locked(vgdev, vfpriv);
 
 out_unlock:
 	mutex_unlock(&vfpriv->context_lock);
@@ -299,12 +309,20 @@
 	return 0;
 }
 
-static int virtio_gpu_resource_info_ioctl(struct drm_device *dev, void *data,
-					  struct drm_file *file)
+static int virtio_gpu_resource_info_cros_ioctl(struct drm_device *dev,
+					       void *data,
+					       struct drm_file *file)
 {
-	struct drm_virtgpu_resource_info *ri = data;
+	struct virtio_gpu_device *vgdev = dev->dev_private;
+	struct drm_virtgpu_resource_info_cros *ri = data;
+	const u32 type = ri->type;
 	struct drm_gem_object *gobj = NULL;
 	struct virtio_gpu_object *qobj = NULL;
+	int ret = 0;
+
+	if (type != VIRTGPU_RESOURCE_INFO_TYPE_DEFAULT &&
+	    type != VIRTGPU_RESOURCE_INFO_TYPE_EXTENDED)
+		return -EINVAL;
 
 	gobj = drm_gem_object_lookup(file, ri->bo_handle);
 	if (gobj == NULL)
@@ -314,11 +332,41 @@
 
 	ri->size = qobj->base.base.size;
 	ri->res_handle = qobj->hw_res_handle;
-	if (qobj->host3d_blob || qobj->guest_blob)
+
+	if (type == VIRTGPU_RESOURCE_INFO_TYPE_DEFAULT) {
 		ri->blob_mem = qobj->blob_mem;
+		goto out;
+	} else if (qobj->blob_mem == VIRTGPU_BLOB_MEM_GUEST) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	ri->stride = 0;
+	if (qobj->blob_mem)
+		goto out;
+
+	if (!qobj->create_callback_done) {
+		ret = wait_event_interruptible(vgdev->resp_wq,
+					       qobj->create_callback_done);
+		if (ret)
+			goto out;
+	}
 
+	if (qobj->num_planes) {
+		int i;
+
+		ri->num_planes = qobj->num_planes;
+		for (i = 0; i < qobj->num_planes; i++) {
+			ri->strides[i] = qobj->strides[i];
+			ri->offsets[i] = qobj->offsets[i];
+		}
+	}
+
+	ri->format_modifier = qobj->format_modifier;
+
+out:
 	drm_gem_object_put(gobj);
-	return 0;
+	return ret;
 }
 
 static int virtio_gpu_transfer_from_host_ioctl(struct drm_device *dev,
@@ -662,6 +710,79 @@
 	return 0;
 }
 
+static int virtio_gpu_context_init_ioctl(struct drm_device *dev,
+					 void *data, struct drm_file *file)
+{
+	int ret = 0;
+	uint32_t num_params, i, param, value;
+	size_t len;
+	struct drm_virtgpu_context_set_param *ctx_set_params = NULL;
+	struct virtio_gpu_device *vgdev = dev->dev_private;
+	struct virtio_gpu_fpriv *vfpriv = file->driver_priv;
+	struct drm_virtgpu_context_init *args = data;
+
+	num_params = args->num_params;
+	len = num_params * sizeof(struct drm_virtgpu_context_set_param);
+
+	if (!vgdev->has_context_init || !vgdev->has_virgl_3d)
+		return -EINVAL;
+
+	/* Number of unique parameters supported at this time. */
+	if (num_params > 1)
+		return -EINVAL;
+
+	ctx_set_params = memdup_user(u64_to_user_ptr(args->ctx_set_params),
+				     len);
+
+	if (IS_ERR(ctx_set_params))
+		return PTR_ERR(ctx_set_params);
+
+	mutex_lock(&vfpriv->context_lock);
+	if (vfpriv->context_created) {
+		ret = -EEXIST;
+		goto out_unlock;
+	}
+
+	for (i = 0; i < num_params; i++) {
+		param = ctx_set_params[i].param;
+		value = ctx_set_params[i].value;
+
+		switch (param) {
+		case VIRTGPU_CONTEXT_PARAM_CAPSET_ID:
+			if (value > MAX_CAPSET_ID) {
+				ret = -EINVAL;
+				goto out_unlock;
+			}
+
+			if ((vgdev->capset_id_mask & (1 << value)) == 0) {
+				ret = -EINVAL;
+				goto out_unlock;
+			}
+
+			/* Context capset ID already set */
+			if (vfpriv->context_init &
+			    VIRTIO_GPU_CONTEXT_INIT_CAPSET_ID_MASK) {
+				ret = -EINVAL;
+				goto out_unlock;
+			}
+
+			vfpriv->context_init |= value;
+			break;
+		default:
+			ret = -EINVAL;
+			goto out_unlock;
+		}
+	}
+
+	virtio_gpu_create_context_locked(vgdev, vfpriv);
+	virtio_gpu_notify(vgdev);
+
+out_unlock:
+	mutex_unlock(&vfpriv->context_lock);
+	kfree(ctx_set_params);
+	return ret;
+}
+
 struct drm_ioctl_desc virtio_gpu_ioctls[DRM_VIRTIO_NUM_IOCTLS] = {
 	DRM_IOCTL_DEF_DRV(VIRTGPU_MAP, virtio_gpu_map_ioctl,
 			  DRM_RENDER_ALLOW),
@@ -676,7 +797,8 @@
 			  virtio_gpu_resource_create_ioctl,
 			  DRM_RENDER_ALLOW),
 
-	DRM_IOCTL_DEF_DRV(VIRTGPU_RESOURCE_INFO, virtio_gpu_resource_info_ioctl,
+	DRM_IOCTL_DEF_DRV(VIRTGPU_RESOURCE_INFO_CROS,
+			  virtio_gpu_resource_info_cros_ioctl,
 			  DRM_RENDER_ALLOW),
 
 	/* make transfer async to the main ring? - no sure, can we
@@ -698,4 +820,7 @@
 	DRM_IOCTL_DEF_DRV(VIRTGPU_RESOURCE_CREATE_BLOB,
 			  virtio_gpu_resource_create_blob_ioctl,
 			  DRM_RENDER_ALLOW),
+
+	DRM_IOCTL_DEF_DRV(VIRTGPU_CONTEXT_INIT, virtio_gpu_context_init_ioctl,
+			  DRM_RENDER_ALLOW),
 };
diff -ruN a/drivers/gpu/drm/virtio/virtgpu_kms.c b/drivers/gpu/drm/virtio/virtgpu_kms.c
--- a/drivers/gpu/drm/virtio/virtgpu_kms.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/virtio/virtgpu_kms.c	2021-12-23 08:35:29.000000000 +0100
@@ -65,6 +65,7 @@
 				   int num_capsets)
 {
 	int i, ret;
+	bool invalid_capset_id = false;
 
 	vgdev->capsets = kcalloc(num_capsets,
 				 sizeof(struct virtio_gpu_drv_capset),
@@ -78,19 +79,34 @@
 		virtio_gpu_notify(vgdev);
 		ret = wait_event_timeout(vgdev->resp_wq,
 					 vgdev->capsets[i].id > 0, 5 * HZ);
-		if (ret == 0) {
+		/*
+		 * Capability ids are defined in the virtio-gpu spec and are
+		 * between 1 to 63, inclusive.
+		 */
+		if (!vgdev->capsets[i].id ||
+		    vgdev->capsets[i].id > MAX_CAPSET_ID)
+			invalid_capset_id = true;
+
+		if (ret == 0)
 			DRM_ERROR("timed out waiting for cap set %d\n", i);
+		else if (invalid_capset_id)
+			DRM_ERROR("invalid capset id %u", vgdev->capsets[i].id);
+
+		if (ret == 0 || invalid_capset_id) {
 			spin_lock(&vgdev->display_info_lock);
 			kfree(vgdev->capsets);
 			vgdev->capsets = NULL;
 			spin_unlock(&vgdev->display_info_lock);
 			return;
 		}
+
+		vgdev->capset_id_mask |= 1 << vgdev->capsets[i].id;
 		DRM_INFO("cap set %d: id %d, max-version %d, max-size %d\n",
 			 i, vgdev->capsets[i].id,
 			 vgdev->capsets[i].max_version,
 			 vgdev->capsets[i].max_size);
 	}
+
 	vgdev->num_capsets = num_capsets;
 }
 
@@ -175,13 +191,19 @@
 			    (unsigned long)vgdev->host_visible_region.addr,
 			    (unsigned long)vgdev->host_visible_region.len);
 	}
+	if (virtio_has_feature(vgdev->vdev, VIRTIO_GPU_F_CONTEXT_INIT)) {
+		vgdev->has_context_init = true;
+	}
 
-	DRM_INFO("features: %cvirgl %cedid %cresource_blob %chost_visible\n",
+	DRM_INFO("features: %cvirgl %cedid %cresource_blob %chost_visible",
 		 vgdev->has_virgl_3d    ? '+' : '-',
 		 vgdev->has_edid        ? '+' : '-',
 		 vgdev->has_resource_blob ? '+' : '-',
 		 vgdev->has_host_visible ? '+' : '-');
 
+	DRM_INFO("features: %ccontext_init\n",
+		 vgdev->has_context_init ? '+' : '-');
+
 	ret = virtio_find_vqs(vgdev->vdev, 2, vqs, callbacks, names, NULL);
 	if (ret) {
 		DRM_ERROR("failed to find virt queues\n");
diff -ruN a/drivers/gpu/drm/virtio/virtgpu_vq.c b/drivers/gpu/drm/virtio/virtgpu_vq.c
--- a/drivers/gpu/drm/virtio/virtgpu_vq.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/gpu/drm/virtio/virtgpu_vq.c	2021-12-23 08:35:29.000000000 +0100
@@ -911,7 +911,8 @@
 }
 
 void virtio_gpu_cmd_context_create(struct virtio_gpu_device *vgdev, uint32_t id,
-				   uint32_t nlen, const char *name)
+				   uint32_t context_init, uint32_t nlen,
+				   const char *name)
 {
 	struct virtio_gpu_ctx_create *cmd_p;
 	struct virtio_gpu_vbuffer *vbuf;
@@ -922,6 +923,7 @@
 	cmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_CTX_CREATE);
 	cmd_p->hdr.ctx_id = cpu_to_le32(id);
 	cmd_p->nlen = cpu_to_le32(nlen);
+	cmd_p->context_init = cpu_to_le32(context_init);
 	strncpy(cmd_p->debug_name, name, sizeof(cmd_p->debug_name) - 1);
 	cmd_p->debug_name[sizeof(cmd_p->debug_name) - 1] = 0;
 	virtio_gpu_queue_ctrl_buffer(vgdev, vbuf);
@@ -977,7 +979,45 @@
 	virtio_gpu_queue_ctrl_buffer(vgdev, vbuf);
 }
 
-void
+static void virtio_gpu_cmd_resource_create_cb(struct virtio_gpu_device *vgdev,
+					      struct virtio_gpu_vbuffer *vbuf)
+{
+	struct virtio_gpu_resp_resource_plane_info *resp =
+		(struct virtio_gpu_resp_resource_plane_info *)vbuf->resp_buf;
+	struct virtio_gpu_object *obj =
+		(struct virtio_gpu_object *)vbuf->data_buf;
+	uint32_t resp_type = le32_to_cpu(resp->hdr.type);
+	int i;
+
+	/*
+	 * Keeps the data_buf, which points to this virtio_gpu_object, from
+	 * getting kfree'd after this cb returns.
+	 */
+	vbuf->data_buf = NULL;
+
+	switch (resp_type) {
+	case VIRTIO_GPU_RESP_OK_RESOURCE_PLANE_INFO:
+	case VIRTIO_GPU_RESP_OK_RESOURCE_PLANE_INFO_LEGACY:
+		break;
+	default:
+		goto finish_pending;
+	}
+
+	obj->num_planes = le32_to_cpu(resp->num_planes);
+	obj->format_modifier = le64_to_cpu(resp->format_modifier);
+
+	for (i = 0; i < obj->num_planes; i++) {
+		obj->strides[i] = le32_to_cpu(resp->strides[i]);
+		obj->offsets[i] = le32_to_cpu(resp->offsets[i]);
+	}
+
+finish_pending:
+	obj->create_callback_done = true;
+	drm_gem_object_put(&obj->base.base);
+	wake_up_all(&vgdev->resp_wq);
+}
+
+int
 virtio_gpu_cmd_resource_create_3d(struct virtio_gpu_device *vgdev,
 				  struct virtio_gpu_object *bo,
 				  struct virtio_gpu_object_params *params,
@@ -986,8 +1026,15 @@
 {
 	struct virtio_gpu_resource_create_3d *cmd_p;
 	struct virtio_gpu_vbuffer *vbuf;
+	struct virtio_gpu_resp_resource_plane_info *resp_buf;
 
-	cmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));
+	resp_buf = kzalloc(sizeof(*resp_buf), GFP_KERNEL);
+	if (!resp_buf)
+		return -ENOMEM;
+
+	cmd_p = virtio_gpu_alloc_cmd_resp(vgdev,
+		virtio_gpu_cmd_resource_create_cb, &vbuf, sizeof(*cmd_p),
+		sizeof(struct virtio_gpu_resp_resource_plane_info), resp_buf);
 	memset(cmd_p, 0, sizeof(*cmd_p));
 	vbuf->objs = objs;
 
@@ -1005,9 +1052,16 @@
 	cmd_p->nr_samples = cpu_to_le32(params->nr_samples);
 	cmd_p->flags = cpu_to_le32(params->flags);
 
+	/* Reuse the data_buf pointer for the object pointer. */
+	vbuf->data_buf = bo;
+	bo->create_callback_done = false;
+	drm_gem_object_get(&bo->base.base);
+
 	virtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, fence);
 
 	bo->created = true;
+
+	return 0;
 }
 
 void virtio_gpu_cmd_transfer_to_host_3d(struct virtio_gpu_device *vgdev,
diff -ruN a/drivers/hid/hid-core.c b/drivers/hid/hid-core.c
--- a/drivers/hid/hid-core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/hid/hid-core.c	2021-12-23 08:35:29.000000000 +0100
@@ -262,6 +262,7 @@
 	unsigned int offset;
 	unsigned int i;
 	unsigned int application;
+	bool dg_tsn_large_fixup = false;
 
 	application = hid_lookup_collection(parser, HID_COLLECTION_APPLICATION);
 
@@ -300,6 +301,18 @@
 	usages = max_t(unsigned, parser->local.usage_index,
 				 parser->global.report_count);
 
+	/* Recognize a Usage(Digitizers.Transducer Serial Number) of 64 bits,
+	 * for special processing later; we need to reserve two usages now.
+	 */
+	if (usages == 1 &&
+	    parser->global.report_size == 64 &&
+	    parser->local.usage[0] == (HID_UP_DIGITIZER | 0x005b) &&
+	    parser->global.report_count == 1 &&
+	    parser->local.usage_index == 1) {
+		dg_tsn_large_fixup = true;
+		usages = 2;
+	}
+
 	field = hid_register_field(report, usages);
 	if (!field)
 		return 0;
@@ -333,11 +346,21 @@
 	field->unit_exponent = parser->global.unit_exponent;
 	field->unit = parser->global.unit;
 
+	/* Fix up that particular report, split it into two distinct 32-bit fields.
+	 */
+	if (dg_tsn_large_fixup) {
+		/* Convert second half into Usage(Digitizers.Transducer Serial Number Second 32 Bits) */
+		field->usage[1].hid = (HID_UP_DIGITIZER | 0x006e);
+		field->report_size = 32;
+		field->report_count = 2;
+	}
+
 	return 0;
 }
 
 /*
- * Read data value from item.
+ * Read data value from global items, which are
+ * a maximum of 32 bits in size.
  */
 
 static u32 item_udata(struct hid_item *item)
@@ -709,7 +732,7 @@
 
 /*
  * Fetch a report description item from the data stream. We support long
- * items, though they are not used yet.
+ * items, though there are not yet any defined uses for them.
  */
 
 static u8 *fetch_item(__u8 *start, __u8 *end, struct hid_item *item)
@@ -745,6 +768,7 @@
 	item->format = HID_ITEM_FORMAT_SHORT;
 	item->size = b & 3;
 
+	/* Map size values 0,1,2,3 to actual sizes 0,1,2,4 */
 	switch (item->size) {
 	case 0:
 		return start;
@@ -763,7 +787,7 @@
 		return start;
 
 	case 3:
-		item->size++;
+		item->size = 4;
 		if ((end - start) < 4)
 			return NULL;
 		item->data.u32 = get_unaligned_le32(start);
@@ -1300,9 +1324,7 @@
 EXPORT_SYMBOL_GPL(hid_open_report);
 
 /*
- * Convert a signed n-bit integer to signed 32-bit integer. Common
- * cases are done through the compiler, the screwed things has to be
- * done by hand.
+ * Convert a signed n-bit integer to signed 32-bit integer.
  */
 
 static s32 snto32(__u32 value, unsigned n)
@@ -1310,12 +1332,7 @@
 	if (!value || !n)
 		return 0;
 
-	switch (n) {
-	case 8:  return ((__s8)value);
-	case 16: return ((__s16)value);
-	case 32: return ((__s32)value);
-	}
-	return value & (1 << (n - 1)) ? value | (~0U << n) : value;
+	return sign_extend32(value, n - 1);
 }
 
 s32 hid_snto32(__u32 value, unsigned n)
diff -ruN a/drivers/hid/hid-generic.c b/drivers/hid/hid-generic.c
--- a/drivers/hid/hid-generic.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/hid/hid-generic.c	2021-12-23 08:35:29.000000000 +0100
@@ -20,6 +20,7 @@
 #include <asm/byteorder.h>
 
 #include <linux/hid.h>
+#include "hid-ids.h"
 
 static struct hid_driver hid_generic;
 
@@ -58,7 +59,11 @@
 {
 	int ret;
 
-	hdev->quirks |= HID_QUIRK_INPUT_PER_APP;
+	/* FIXME(b/157067041) : Remove this unquirk for the Logi K580. */
+	if (hdev->vendor != USB_VENDOR_ID_LOGITECH ||
+	    hdev->product != USB_DEVICE_ID_LOGITECH_K580_CHROME ||
+	    hdev->bus != BUS_BLUETOOTH)
+		hdev->quirks |= HID_QUIRK_INPUT_PER_APP;
 
 	ret = hid_parse(hdev);
 	if (ret)
diff -ruN a/drivers/hid/hid-google-hammer.c b/drivers/hid/hid-google-hammer.c
--- a/drivers/hid/hid-google-hammer.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/hid/hid-google-hammer.c	2021-12-23 08:35:29.000000000 +0100
@@ -96,8 +96,9 @@
 	struct cros_ec_device *ec = _notify;
 	unsigned long flags;
 	bool base_present;
+	const u8 event_type = ec->event_data.event_type & EC_MKBP_EVENT_TYPE_MASK;
 
-	if (ec->event_data.event_type == EC_MKBP_EVENT_SWITCH) {
+	if (event_type == EC_MKBP_EVENT_SWITCH) {
 		base_present = cbas_parse_base_state(
 					&ec->event_data.data.switches);
 		dev_dbg(cbas_ec.dev,
diff -ruN a/drivers/hid/hid-ids.h b/drivers/hid/hid-ids.h
--- a/drivers/hid/hid-ids.h	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/hid/hid-ids.h	2021-12-23 08:35:29.000000000 +0100
@@ -487,6 +487,7 @@
 #define USB_DEVICE_ID_GOODTOUCH_000f	0x000f
 
 #define USB_VENDOR_ID_GOOGLE		0x18d1
+#define USB_DEVICE_ID_GOOGLE_QUICKSTEP	0x0477
 #define USB_DEVICE_ID_GOOGLE_HAMMER	0x5022
 #define USB_DEVICE_ID_GOOGLE_TOUCH_ROSE	0x5028
 #define USB_DEVICE_ID_GOOGLE_STAFF	0x502b
@@ -767,6 +768,7 @@
 #define USB_DEVICE_ID_LOGITECH_AUDIOHUB 0x0a0e
 #define USB_DEVICE_ID_LOGITECH_T651	0xb00c
 #define USB_DEVICE_ID_LOGITECH_DINOVO_EDGE_KBD	0xb309
+#define USB_DEVICE_ID_LOGITECH_K580_CHROME	0xb35d
 #define USB_DEVICE_ID_LOGITECH_C007	0xc007
 #define USB_DEVICE_ID_LOGITECH_C077	0xc077
 #define USB_DEVICE_ID_LOGITECH_RECEIVER	0xc101
@@ -917,6 +919,9 @@
 #define USB_VENDOR_ID_NINTENDO		0x057e
 #define USB_DEVICE_ID_NINTENDO_WIIMOTE	0x0306
 #define USB_DEVICE_ID_NINTENDO_WIIMOTE2	0x0330
+#define USB_DEVICE_ID_NINTENDO_JOYCONL	0x2006
+#define USB_DEVICE_ID_NINTENDO_JOYCONR	0x2007
+#define USB_DEVICE_ID_NINTENDO_PROCON	0x2009
 
 #define USB_VENDOR_ID_NOVATEK		0x0603
 #define USB_DEVICE_ID_NOVATEK_PCT	0x0600
diff -ruN a/drivers/hid/hid-input.c b/drivers/hid/hid-input.c
--- a/drivers/hid/hid-input.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/hid/hid-input.c	2021-12-23 08:35:29.000000000 +0100
@@ -287,6 +287,7 @@
 	POWER_SUPPLY_PROP_ONLINE,
 	POWER_SUPPLY_PROP_CAPACITY,
 	POWER_SUPPLY_PROP_MODEL_NAME,
+	POWER_SUPPLY_PROP_SERIAL_NUMBER,
 	POWER_SUPPLY_PROP_STATUS,
 	POWER_SUPPLY_PROP_SCOPE,
 };
@@ -407,6 +408,33 @@
 		val->strval = dev->name;
 		break;
 
+	case POWER_SUPPLY_PROP_SERIAL_NUMBER:
+		/*
+		 * Serial number does not have an active HID query
+		 * mechanism like hidinput_query_battery_capacity, as the
+		 * only devices expected to have serial numbers are digitizers,
+		 * which are unlikely to be able to pull the serial number from
+		 * an untethered pen on demand.
+		 */
+		if (dev->battery_serial_number == 0) {
+			/* Make no claims about S/N format if we haven't actually seen a value yet. */
+			strcpy(dev->battery_serial_number_str, "");
+		} else {
+			if (!dev->battery_sn_64bit) {
+				snprintf(dev->battery_serial_number_str,
+					sizeof(dev->battery_serial_number_str),
+					"%08llX",
+					dev->battery_serial_number);
+			} else {
+				snprintf(dev->battery_serial_number_str,
+					sizeof(dev->battery_serial_number_str),
+					"%016llX",
+					dev->battery_serial_number);
+			}
+		}
+		val->strval = dev->battery_serial_number_str;
+		break;
+
 	case POWER_SUPPLY_PROP_STATUS:
 		if (dev->battery_status != HID_BATTERY_REPORTED &&
 		    !dev->battery_avoid_query) {
@@ -489,6 +517,9 @@
 	dev->battery_max = max;
 	dev->battery_report_type = report_type;
 	dev->battery_report_id = field->report->id;
+	dev->battery_state_changed = false;
+	dev->battery_reported = false;
+	dev->battery_sn_64bit = false;
 
 	/*
 	 * Stylus is normally not connected to the device and thus we
@@ -530,7 +561,13 @@
 	dev->battery = NULL;
 }
 
-static void hidinput_update_battery(struct hid_device *dev, int value)
+static void hidinput_set_battery_sn_64bit(struct hid_device *dev)
+{
+	dev->battery_sn_64bit = true;
+}
+
+static void hidinput_update_battery_capacity(struct hid_device *dev,
+					     __s32 value)
 {
 	int capacity;
 
@@ -542,15 +579,70 @@
 
 	capacity = hidinput_scale_battery_capacity(dev, value);
 
+	if (capacity != dev->battery_capacity) {
+		dev->battery_capacity = capacity;
+		dev->battery_state_changed = true;
+	}
+	dev->battery_reported = true;
+}
+
+static void hidinput_update_battery_serial(struct hid_device *dev,
+					   const __s32 value, bool top_32_bits)
+{
+	__u64 sn;
+	__u32 sn_hi, sn_lo;
+
+	if (!dev->battery)
+		return;
+
+	if (!top_32_bits) {
+		sn_lo = (__u32)value;
+		sn_hi = (__u32)(dev->battery_new_serial_number >> 32);
+	} else {
+		sn_lo = (__u32)dev->battery_new_serial_number;
+		sn_hi = (__u32)value;
+	}
+
+	sn = (((__u64)sn_hi) << 32) | (__u64)sn_lo;
+
+	dev->battery_new_serial_number = sn;
+	dev->battery_reported = true;
+}
+
+static void hidinput_flush_battery(struct hid_device *dev)
+{
+	if (!dev->battery)
+		return;
+
+	/* Only consider pushing a battery change if there is a
+	 * battery field in this report.
+	 */
+	if (!dev->battery_reported)
+		return;
+
+	/* As we have the entire S/N now, check if it changed, and is non-zero.
+	 * We do want to ignore actual updates of zero, as they are expected to
+	 * convey 'no information', instead of 'no stylus present'.
+	 */
+	if (dev->battery_new_serial_number != 0 &&
+	    dev->battery_new_serial_number != dev->battery_serial_number) {
+		dev->battery_serial_number = dev->battery_new_serial_number;
+		dev->battery_state_changed = true;
+	}
+
 	if (dev->battery_status != HID_BATTERY_REPORTED ||
-	    capacity != dev->battery_capacity ||
+	    dev->battery_state_changed ||
 	    ktime_after(ktime_get_coarse(), dev->battery_ratelimit_time)) {
-		dev->battery_capacity = capacity;
 		dev->battery_status = HID_BATTERY_REPORTED;
+		dev->battery_state_changed = false;
 		dev->battery_ratelimit_time =
 			ktime_add_ms(ktime_get_coarse(), 30 * 1000);
 		power_supply_changed(dev->battery);
 	}
+
+	/* Clean up for next report */
+	dev->battery_reported = false;
+	dev->battery_new_serial_number = 0;
 }
 #else  /* !CONFIG_HID_BATTERY_STRENGTH */
 static int hidinput_setup_battery(struct hid_device *dev, unsigned report_type,
@@ -563,7 +655,21 @@
 {
 }
 
-static void hidinput_update_battery(struct hid_device *dev, int value)
+static void hidinput_set_battery_sn_64bit(struct hid_device *dev)
+{
+}
+
+static void hidinput_update_battery_capacity(struct hid_device *dev,
+					     __s32 value)
+{
+}
+
+static void hidinput_update_battery_serial(struct hid_device *dev,
+					   const __s32 value, bool top_32_bits)
+{
+}
+
+static void hidinput_flush_battery(struct hid_device *dev)
 {
 }
 #endif	/* CONFIG_HID_BATTERY_STRENGTH */
@@ -802,7 +908,8 @@
 		break;
 
 	case HID_UP_DIGITIZER:
-		if ((field->application & 0xff) == 0x01) /* Digitizer */
+		if (((field->application & 0xff) == 0x01) ||
+			(device->quirks & HID_QUIRK_DEVICE_IS_DIGITIZER)) /* Digitizer */
 			__set_bit(INPUT_PROP_POINTER, input->propbit);
 		else if ((field->application & 0xff) == 0x02) /* Pen */
 			__set_bit(INPUT_PROP_DIRECT, input->propbit);
@@ -877,6 +984,11 @@
 			max = MSC_MAX;
 			break;
 
+		case 0x6e: /* TransducerSerialNumberSecond32Bits */
+			hidinput_set_battery_sn_64bit(device);
+			usage->type = EV_MSC;
+			return;
+
 		default:  goto unknown;
 		}
 		break;
@@ -1319,9 +1431,17 @@
 		return;
 
 	if (usage->type == EV_PWR) {
-		hidinput_update_battery(hid, value);
+		hidinput_update_battery_capacity(hid, value);
+		return;
+	}
+	if (usage->type == EV_MSC && usage->hid == (HID_UP_DIGITIZER | 0x006e)) { /* TransducerSerialNumberSecond32Bits */
+		hidinput_update_battery_serial(hid, value, true);
 		return;
 	}
+	if (usage->type == EV_MSC && usage->code == MSC_SERIAL) {
+		hidinput_update_battery_serial(hid, value, false);
+		/* fall through to standard MSC_SERIAL processing */
+	}
 
 	if (!field->hidinput)
 		return;
@@ -1452,6 +1572,8 @@
 {
 	struct hid_input *hidinput;
 
+	hidinput_flush_battery(hid);
+
 	if (hid->quirks & HID_QUIRK_NO_INPUT_SYNC)
 		return;
 
diff -ruN a/drivers/hid/hid-nintendo.c b/drivers/hid/hid-nintendo.c
--- a/drivers/hid/hid-nintendo.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/hid/hid-nintendo.c	2021-12-23 08:35:29.000000000 +0100
@@ -0,0 +1,869 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * HID driver for Nintendo Switch Joy-Cons and Pro Controllers
+ *
+ * Copyright (c) 2019 Daniel J. Ogorchock <djogorchock@gmail.com>
+ *
+ * The following resources/projects were referenced for this driver:
+ *   https://github.com/dekuNukem/Nintendo_Switch_Reverse_Engineering
+ *   https://gitlab.com/pjranki/joycon-linux-kernel (Peter Rankin)
+ *   https://github.com/FrotBot/SwitchProConLinuxUSB
+ *   https://github.com/MTCKC/ProconXInput
+ *   hid-wiimote kernel hid driver
+ *   hid-logitech-hidpp driver
+ *
+ * This driver supports the Nintendo Switch Joy-Cons and Pro Controllers. The
+ * Pro Controllers can either be used over USB or Bluetooth.
+ *
+ * The driver will retrieve the factory calibration info from the controllers,
+ * so little to no user calibration should be required.
+ *
+ */
+
+#include "hid-ids.h"
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/hid.h>
+#include <linux/input.h>
+#include <linux/module.h>
+#include <linux/spinlock.h>
+
+/*
+ * Reference the url below for the following HID report defines:
+ * https://github.com/dekuNukem/Nintendo_Switch_Reverse_Engineering
+ */
+
+/* Output Reports */
+static const u8 JC_OUTPUT_RUMBLE_AND_SUBCMD	= 0x01;
+static const u8 JC_OUTPUT_FW_UPDATE_PKT		= 0x03;
+static const u8 JC_OUTPUT_RUMBLE_ONLY		= 0x10;
+static const u8 JC_OUTPUT_MCU_DATA		= 0x11;
+static const u8 JC_OUTPUT_USB_CMD		= 0x80;
+
+/* Subcommand IDs */
+static const u8 JC_SUBCMD_STATE			/*= 0x00*/;
+static const u8 JC_SUBCMD_MANUAL_BT_PAIRING	= 0x01;
+static const u8 JC_SUBCMD_REQ_DEV_INFO		= 0x02;
+static const u8 JC_SUBCMD_SET_REPORT_MODE	= 0x03;
+static const u8 JC_SUBCMD_TRIGGERS_ELAPSED	= 0x04;
+static const u8 JC_SUBCMD_GET_PAGE_LIST_STATE	= 0x05;
+static const u8 JC_SUBCMD_SET_HCI_STATE		= 0x06;
+static const u8 JC_SUBCMD_RESET_PAIRING_INFO	= 0x07;
+static const u8 JC_SUBCMD_LOW_POWER_MODE	= 0x08;
+static const u8 JC_SUBCMD_SPI_FLASH_READ	= 0x10;
+static const u8 JC_SUBCMD_SPI_FLASH_WRITE	= 0x11;
+static const u8 JC_SUBCMD_RESET_MCU		= 0x20;
+static const u8 JC_SUBCMD_SET_MCU_CONFIG	= 0x21;
+static const u8 JC_SUBCMD_SET_MCU_STATE		= 0x22;
+static const u8 JC_SUBCMD_SET_PLAYER_LIGHTS	= 0x30;
+static const u8 JC_SUBCMD_GET_PLAYER_LIGHTS	= 0x31;
+static const u8 JC_SUBCMD_SET_HOME_LIGHT	= 0x38;
+static const u8 JC_SUBCMD_ENABLE_IMU		= 0x40;
+static const u8 JC_SUBCMD_SET_IMU_SENSITIVITY	= 0x41;
+static const u8 JC_SUBCMD_WRITE_IMU_REG		= 0x42;
+static const u8 JC_SUBCMD_READ_IMU_REG		= 0x43;
+static const u8 JC_SUBCMD_ENABLE_VIBRATION	= 0x48;
+static const u8 JC_SUBCMD_GET_REGULATED_VOLTAGE	= 0x50;
+
+/* Input Reports */
+static const u8 JC_INPUT_BUTTON_EVENT		= 0x3F;
+static const u8 JC_INPUT_SUBCMD_REPLY		= 0x21;
+static const u8 JC_INPUT_IMU_DATA		= 0x30;
+static const u8 JC_INPUT_MCU_DATA		= 0x31;
+static const u8 JC_INPUT_USB_RESPONSE		= 0x81;
+
+/* Feature Reports */
+static const u8 JC_FEATURE_LAST_SUBCMD		= 0x02;
+static const u8 JC_FEATURE_OTA_FW_UPGRADE	= 0x70;
+static const u8 JC_FEATURE_SETUP_MEM_READ	= 0x71;
+static const u8 JC_FEATURE_MEM_READ		= 0x72;
+static const u8 JC_FEATURE_ERASE_MEM_SECTOR	= 0x73;
+static const u8 JC_FEATURE_MEM_WRITE		= 0x74;
+static const u8 JC_FEATURE_LAUNCH		= 0x75;
+
+/* USB Commands */
+static const u8 JC_USB_CMD_CONN_STATUS		= 0x01;
+static const u8 JC_USB_CMD_HANDSHAKE		= 0x02;
+static const u8 JC_USB_CMD_BAUDRATE_3M		= 0x03;
+static const u8 JC_USB_CMD_NO_TIMEOUT		= 0x04;
+static const u8 JC_USB_CMD_EN_TIMEOUT		= 0x05;
+static const u8 JC_USB_RESET			= 0x06;
+static const u8 JC_USB_PRE_HANDSHAKE		= 0x91;
+static const u8 JC_USB_SEND_UART		= 0x92;
+
+/* SPI storage addresses of factory calibration data */
+static const u16 JC_CAL_DATA_START		= 0x603d;
+static const u16 JC_CAL_DATA_END		= 0x604e;
+#define JC_CAL_DATA_SIZE	(JC_CAL_DATA_END - JC_CAL_DATA_START + 1)
+
+
+/* The raw analog joystick values will be mapped in terms of this magnitude */
+static const u16 JC_MAX_STICK_MAG		= 32767;
+static const u16 JC_STICK_FUZZ			= 250;
+static const u16 JC_STICK_FLAT			= 500;
+
+/* Hat values for pro controller's d-pad */
+static const u16 JC_MAX_DPAD_MAG		= 1;
+static const u16 JC_DPAD_FUZZ			/*= 0*/;
+static const u16 JC_DPAD_FLAT			/*= 0*/;
+
+/* States for controller state machine */
+enum joycon_ctlr_state {
+	JOYCON_CTLR_STATE_INIT,
+	JOYCON_CTLR_STATE_READ,
+};
+
+struct joycon_stick_cal {
+	s32 max;
+	s32 min;
+	s32 center;
+};
+
+/*
+ * All the controller's button values are stored in a u32.
+ * They can be accessed with bitwise ANDs.
+ */
+static const u32 JC_BTN_Y	= BIT(0);
+static const u32 JC_BTN_X	= BIT(1);
+static const u32 JC_BTN_B	= BIT(2);
+static const u32 JC_BTN_A	= BIT(3);
+static const u32 JC_BTN_SR_R	= BIT(4);
+static const u32 JC_BTN_SL_R	= BIT(5);
+static const u32 JC_BTN_R	= BIT(6);
+static const u32 JC_BTN_ZR	= BIT(7);
+static const u32 JC_BTN_MINUS	= BIT(8);
+static const u32 JC_BTN_PLUS	= BIT(9);
+static const u32 JC_BTN_RSTICK	= BIT(10);
+static const u32 JC_BTN_LSTICK	= BIT(11);
+static const u32 JC_BTN_HOME	= BIT(12);
+static const u32 JC_BTN_CAP	= BIT(13); /* capture button */
+static const u32 JC_BTN_DOWN	= BIT(16);
+static const u32 JC_BTN_UP	= BIT(17);
+static const u32 JC_BTN_RIGHT	= BIT(18);
+static const u32 JC_BTN_LEFT	= BIT(19);
+static const u32 JC_BTN_SR_L	= BIT(20);
+static const u32 JC_BTN_SL_L	= BIT(21);
+static const u32 JC_BTN_L	= BIT(22);
+static const u32 JC_BTN_ZL	= BIT(23);
+
+enum joycon_msg_type {
+	JOYCON_MSG_TYPE_NONE,
+	JOYCON_MSG_TYPE_USB,
+	JOYCON_MSG_TYPE_SUBCMD,
+};
+
+struct joycon_subcmd_request {
+	u8 output_id; /* must be 0x01 for subcommand, 0x10 for rumble only */
+	u8 packet_num; /* incremented every send */
+	u8 rumble_data[8];
+	u8 subcmd_id;
+	u8 data[]; /* length depends on the subcommand */
+} __packed;
+
+struct joycon_subcmd_reply {
+	u8 ack; /* MSB 1 for ACK, 0 for NACK */
+	u8 id; /* id of requested subcmd */
+	u8 data[]; /* will be at most 35 bytes */
+} __packed;
+
+struct joycon_input_report {
+	u8 id;
+	u8 timer;
+	u8 bat_con; /* battery and connection info */
+	u8 button_status[3];
+	u8 left_stick[3];
+	u8 right_stick[3];
+	u8 vibrator_report;
+
+	/*
+	 * If support for firmware updates, gyroscope data, and/or NFC/IR
+	 * are added in the future, this can be swapped for a union.
+	 */
+	struct joycon_subcmd_reply reply;
+} __packed;
+
+#define JC_MAX_RESP_SIZE	(sizeof(struct joycon_input_report) + 35)
+
+/* Each physical controller is associated with a joycon_ctlr struct */
+struct joycon_ctlr {
+	struct hid_device *hdev;
+	struct input_dev *input;
+	enum joycon_ctlr_state ctlr_state;
+
+	/* The following members are used for synchronous sends/receives */
+	enum joycon_msg_type msg_type;
+	u8 subcmd_num;
+	struct mutex output_mutex;
+	u8 input_buf[JC_MAX_RESP_SIZE];
+	wait_queue_head_t wait;
+	bool received_resp;
+	u8 usb_ack_match;
+	u8 subcmd_ack_match;
+
+	/* factory calibration data */
+	struct joycon_stick_cal left_stick_cal_x;
+	struct joycon_stick_cal left_stick_cal_y;
+	struct joycon_stick_cal right_stick_cal_x;
+	struct joycon_stick_cal right_stick_cal_y;
+
+};
+
+static int __joycon_hid_send(struct hid_device *hdev, u8 *data, size_t len)
+{
+	u8 *buf;
+	int ret;
+
+	buf = kmemdup(data, len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+	ret = hid_hw_output_report(hdev, buf, len);
+	kfree(buf);
+	if (ret < 0)
+		hid_dbg(hdev, "Failed to send output report ret=%d\n", ret);
+	return ret;
+}
+
+static int joycon_hid_send_sync(struct joycon_ctlr *ctlr, u8 *data, size_t len)
+{
+	int ret;
+
+	ret = __joycon_hid_send(ctlr->hdev, data, len);
+	if (ret < 0) {
+		memset(ctlr->input_buf, 0, JC_MAX_RESP_SIZE);
+		return ret;
+	}
+
+	if (!wait_event_timeout(ctlr->wait, ctlr->received_resp, HZ)) {
+		hid_dbg(ctlr->hdev, "synchronous send/receive timed out\n");
+		memset(ctlr->input_buf, 0, JC_MAX_RESP_SIZE);
+		return -ETIMEDOUT;
+	}
+
+	ctlr->received_resp = false;
+	return 0;
+}
+
+static int joycon_send_usb(struct joycon_ctlr *ctlr, u8 cmd)
+{
+	int ret;
+	u8 buf[2] = {JC_OUTPUT_USB_CMD};
+
+	buf[1] = cmd;
+	ctlr->usb_ack_match = cmd;
+	ctlr->msg_type = JOYCON_MSG_TYPE_USB;
+	ret = joycon_hid_send_sync(ctlr, buf, sizeof(buf));
+	if (ret)
+		hid_dbg(ctlr->hdev, "send usb command failed; ret=%d\n", ret);
+	return ret;
+}
+
+static int joycon_send_subcmd(struct joycon_ctlr *ctlr,
+			      struct joycon_subcmd_request *subcmd,
+			      size_t data_len)
+{
+	int ret;
+
+	subcmd->output_id = JC_OUTPUT_RUMBLE_AND_SUBCMD;
+	subcmd->packet_num = ctlr->subcmd_num;
+	if (++ctlr->subcmd_num > 0xF)
+		ctlr->subcmd_num = 0;
+	ctlr->subcmd_ack_match = subcmd->subcmd_id;
+	ctlr->msg_type = JOYCON_MSG_TYPE_SUBCMD;
+
+	ret = joycon_hid_send_sync(ctlr, (u8 *)subcmd,
+				   sizeof(*subcmd) + data_len);
+	if (ret < 0)
+		hid_dbg(ctlr->hdev, "send subcommand failed; ret=%d\n", ret);
+	else
+		ret = 0;
+	return ret;
+}
+
+/* Supply nibbles for flash and on. Ones correspond to active */
+static int joycon_set_player_leds(struct joycon_ctlr *ctlr, u8 flash, u8 on)
+{
+	struct joycon_subcmd_request *req;
+	u8 buffer[sizeof(*req) + 1] = { 0 };
+
+	req = (struct joycon_subcmd_request *)buffer;
+	req->subcmd_id = JC_SUBCMD_SET_PLAYER_LIGHTS;
+	req->data[0] = (flash << 4) | on;
+
+	hid_dbg(ctlr->hdev, "setting player leds\n");
+	return joycon_send_subcmd(ctlr, req, 1);
+}
+
+static const u16 DFLT_STICK_CAL_CEN = 2000;
+static const u16 DFLT_STICK_CAL_MAX = 3500;
+static const u16 DFLT_STICK_CAL_MIN = 500;
+static int joycon_request_calibration(struct joycon_ctlr *ctlr)
+{
+	struct joycon_subcmd_request *req;
+	u8 buffer[sizeof(*req) + 5] = { 0 };
+	struct joycon_input_report *report;
+	struct joycon_stick_cal *cal_x;
+	struct joycon_stick_cal *cal_y;
+	s32 x_max_above;
+	s32 x_min_below;
+	s32 y_max_above;
+	s32 y_min_below;
+	u8 *data;
+	u8 *raw_cal;
+	int ret;
+
+	req = (struct joycon_subcmd_request *)buffer;
+	req->subcmd_id = JC_SUBCMD_SPI_FLASH_READ;
+	data = req->data;
+	data[0] = 0xFF & JC_CAL_DATA_START;
+	data[1] = 0xFF & (JC_CAL_DATA_START >> 8);
+	data[2] = 0xFF & (JC_CAL_DATA_START >> 16);
+	data[3] = 0xFF & (JC_CAL_DATA_START >> 24);
+	data[4] = JC_CAL_DATA_SIZE;
+
+	hid_dbg(ctlr->hdev, "requesting cal data\n");
+	ret = joycon_send_subcmd(ctlr, req, 5);
+	if (ret) {
+		hid_warn(ctlr->hdev,
+			 "Failed to read stick cal, using defaults; ret=%d\n",
+			 ret);
+
+		ctlr->left_stick_cal_x.center = DFLT_STICK_CAL_CEN;
+		ctlr->left_stick_cal_x.max = DFLT_STICK_CAL_MAX;
+		ctlr->left_stick_cal_x.min = DFLT_STICK_CAL_MIN;
+
+		ctlr->left_stick_cal_y.center = DFLT_STICK_CAL_CEN;
+		ctlr->left_stick_cal_y.max = DFLT_STICK_CAL_MAX;
+		ctlr->left_stick_cal_y.min = DFLT_STICK_CAL_MIN;
+
+		ctlr->right_stick_cal_x.center = DFLT_STICK_CAL_CEN;
+		ctlr->right_stick_cal_x.max = DFLT_STICK_CAL_MAX;
+		ctlr->right_stick_cal_x.min = DFLT_STICK_CAL_MIN;
+
+		ctlr->right_stick_cal_y.center = DFLT_STICK_CAL_CEN;
+		ctlr->right_stick_cal_y.max = DFLT_STICK_CAL_MAX;
+		ctlr->right_stick_cal_y.min = DFLT_STICK_CAL_MIN;
+
+		return ret;
+	}
+
+	report = (struct joycon_input_report *)ctlr->input_buf;
+	raw_cal = &report->reply.data[5];
+
+	/* left stick calibration parsing */
+	cal_x = &ctlr->left_stick_cal_x;
+	cal_y = &ctlr->left_stick_cal_y;
+
+	x_max_above = hid_field_extract(ctlr->hdev, (raw_cal + 0), 0, 12);
+	y_max_above = hid_field_extract(ctlr->hdev, (raw_cal + 1), 4, 12);
+	cal_x->center = hid_field_extract(ctlr->hdev, (raw_cal + 3), 0, 12);
+	cal_y->center = hid_field_extract(ctlr->hdev, (raw_cal + 4), 4, 12);
+	x_min_below = hid_field_extract(ctlr->hdev, (raw_cal + 6), 0, 12);
+	y_min_below = hid_field_extract(ctlr->hdev, (raw_cal + 7), 4, 12);
+	cal_x->max = cal_x->center + x_max_above;
+	cal_x->min = cal_x->center - x_min_below;
+	cal_y->max = cal_y->center + y_max_above;
+	cal_y->min = cal_y->center - y_min_below;
+
+	/* right stick calibration parsing */
+	raw_cal += 9;
+	cal_x = &ctlr->right_stick_cal_x;
+	cal_y = &ctlr->right_stick_cal_y;
+
+	cal_x->center = hid_field_extract(ctlr->hdev, (raw_cal + 0), 0, 12);
+	cal_y->center = hid_field_extract(ctlr->hdev, (raw_cal + 1), 4, 12);
+	x_min_below = hid_field_extract(ctlr->hdev, (raw_cal + 3), 0, 12);
+	y_min_below = hid_field_extract(ctlr->hdev, (raw_cal + 4), 4, 12);
+	x_max_above = hid_field_extract(ctlr->hdev, (raw_cal + 6), 0, 12);
+	y_max_above = hid_field_extract(ctlr->hdev, (raw_cal + 7), 4, 12);
+	cal_x->max = cal_x->center + x_max_above;
+	cal_x->min = cal_x->center - x_min_below;
+	cal_y->max = cal_y->center + y_max_above;
+	cal_y->min = cal_y->center - y_min_below;
+
+	hid_dbg(ctlr->hdev, "calibration:\n"
+			    "l_x_c=%d l_x_max=%d l_x_min=%d\n"
+			    "l_y_c=%d l_y_max=%d l_y_min=%d\n"
+			    "r_x_c=%d r_x_max=%d r_x_min=%d\n"
+			    "r_y_c=%d r_y_max=%d r_y_min=%d\n",
+			    ctlr->left_stick_cal_x.center,
+			    ctlr->left_stick_cal_x.max,
+			    ctlr->left_stick_cal_x.min,
+			    ctlr->left_stick_cal_y.center,
+			    ctlr->left_stick_cal_y.max,
+			    ctlr->left_stick_cal_y.min,
+			    ctlr->right_stick_cal_x.center,
+			    ctlr->right_stick_cal_x.max,
+			    ctlr->right_stick_cal_x.min,
+			    ctlr->right_stick_cal_y.center,
+			    ctlr->right_stick_cal_y.max,
+			    ctlr->right_stick_cal_y.min);
+
+	return 0;
+}
+
+static int joycon_set_report_mode(struct joycon_ctlr *ctlr)
+{
+	struct joycon_subcmd_request *req;
+	u8 buffer[sizeof(*req) + 1] = { 0 };
+
+	req = (struct joycon_subcmd_request *)buffer;
+	req->subcmd_id = JC_SUBCMD_SET_REPORT_MODE;
+	req->data[0] = 0x30; /* standard, full report mode */
+
+	hid_dbg(ctlr->hdev, "setting controller report mode\n");
+	return joycon_send_subcmd(ctlr, req, 1);
+}
+
+static s32 joycon_map_stick_val(struct joycon_stick_cal *cal, s32 val)
+{
+	s32 center = cal->center;
+	s32 min = cal->min;
+	s32 max = cal->max;
+	s32 new_val;
+
+	if (val > center) {
+		new_val = (val - center) * JC_MAX_STICK_MAG;
+		new_val /= (max - center);
+	} else {
+		new_val = (center - val) * -JC_MAX_STICK_MAG;
+		new_val /= (center - min);
+	}
+	new_val = clamp(new_val, (s32)-JC_MAX_STICK_MAG, (s32)JC_MAX_STICK_MAG);
+	return new_val;
+}
+
+static void joycon_parse_report(struct joycon_ctlr *ctlr,
+				struct joycon_input_report *rep)
+{
+	struct input_dev *dev = ctlr->input;
+	u32 btns;
+	u32 id = ctlr->hdev->product;
+
+	btns = hid_field_extract(ctlr->hdev, rep->button_status, 0, 24);
+
+	if (id != USB_DEVICE_ID_NINTENDO_JOYCONR) {
+		u16 raw_x;
+		u16 raw_y;
+		s32 x;
+		s32 y;
+
+		/* get raw stick values */
+		raw_x = hid_field_extract(ctlr->hdev, rep->left_stick, 0, 12);
+		raw_y = hid_field_extract(ctlr->hdev,
+					  rep->left_stick + 1, 4, 12);
+		/* map the stick values */
+		x = joycon_map_stick_val(&ctlr->left_stick_cal_x, raw_x);
+		y = -joycon_map_stick_val(&ctlr->left_stick_cal_y, raw_y);
+		/* report sticks */
+		input_report_abs(dev, ABS_X, x);
+		input_report_abs(dev, ABS_Y, y);
+
+		/* report buttons */
+		input_report_key(dev, BTN_TL, btns & JC_BTN_L);
+		input_report_key(dev, BTN_TL2, btns & JC_BTN_ZL);
+		input_report_key(dev, BTN_SELECT, btns & JC_BTN_MINUS);
+		input_report_key(dev, BTN_THUMBL, btns & JC_BTN_LSTICK);
+		input_report_key(dev, BTN_Z, btns & JC_BTN_CAP);
+
+		if (id != USB_DEVICE_ID_NINTENDO_PROCON) {
+			/* Report the S buttons as the non-existent triggers */
+			input_report_key(dev, BTN_TR, btns & JC_BTN_SL_L);
+			input_report_key(dev, BTN_TR2, btns & JC_BTN_SR_L);
+
+			/* Report d-pad as digital buttons for the joy-cons */
+			input_report_key(dev, BTN_DPAD_DOWN,
+					 btns & JC_BTN_DOWN);
+			input_report_key(dev, BTN_DPAD_UP, btns & JC_BTN_UP);
+			input_report_key(dev, BTN_DPAD_RIGHT,
+					 btns & JC_BTN_RIGHT);
+			input_report_key(dev, BTN_DPAD_LEFT,
+					 btns & JC_BTN_LEFT);
+		} else {
+			int hatx = 0;
+			int haty = 0;
+
+			/* d-pad x */
+			if (btns & JC_BTN_LEFT)
+				hatx = -1;
+			else if (btns & JC_BTN_RIGHT)
+				hatx = 1;
+			input_report_abs(dev, ABS_HAT0X, hatx);
+
+			/* d-pad y */
+			if (btns & JC_BTN_UP)
+				haty = -1;
+			else if (btns & JC_BTN_DOWN)
+				haty = 1;
+			input_report_abs(dev, ABS_HAT0Y, haty);
+		}
+	}
+	if (id != USB_DEVICE_ID_NINTENDO_JOYCONL) {
+		u16 raw_x;
+		u16 raw_y;
+		s32 x;
+		s32 y;
+
+		/* get raw stick values */
+		raw_x = hid_field_extract(ctlr->hdev, rep->right_stick, 0, 12);
+		raw_y = hid_field_extract(ctlr->hdev,
+					  rep->right_stick + 1, 4, 12);
+		/* map stick values */
+		x = joycon_map_stick_val(&ctlr->right_stick_cal_x, raw_x);
+		y = -joycon_map_stick_val(&ctlr->right_stick_cal_y, raw_y);
+		/* report sticks */
+		input_report_abs(dev, ABS_RX, x);
+		input_report_abs(dev, ABS_RY, y);
+
+		/* report buttons */
+		input_report_key(dev, BTN_TR, btns & JC_BTN_R);
+		input_report_key(dev, BTN_TR2, btns & JC_BTN_ZR);
+		if (id != USB_DEVICE_ID_NINTENDO_PROCON) {
+			/* Report the S buttons as the non-existent triggers */
+			input_report_key(dev, BTN_TL, btns & JC_BTN_SL_R);
+			input_report_key(dev, BTN_TL2, btns & JC_BTN_SR_R);
+		}
+		input_report_key(dev, BTN_START, btns & JC_BTN_PLUS);
+		input_report_key(dev, BTN_THUMBR, btns & JC_BTN_RSTICK);
+		input_report_key(dev, BTN_MODE, btns & JC_BTN_HOME);
+		input_report_key(dev, BTN_WEST, btns & JC_BTN_Y);
+		input_report_key(dev, BTN_NORTH, btns & JC_BTN_X);
+		input_report_key(dev, BTN_EAST, btns & JC_BTN_A);
+		input_report_key(dev, BTN_SOUTH, btns & JC_BTN_B);
+	}
+
+	input_sync(dev);
+}
+
+
+static const unsigned int joycon_button_inputs_l[] = {
+	BTN_SELECT, BTN_Z, BTN_THUMBL,
+	BTN_TL, BTN_TL2,
+	0 /* 0 signals end of array */
+};
+
+static const unsigned int joycon_button_inputs_r[] = {
+	BTN_START, BTN_MODE, BTN_THUMBR,
+	BTN_SOUTH, BTN_EAST, BTN_NORTH, BTN_WEST,
+	BTN_TR, BTN_TR2,
+	0 /* 0 signals end of array */
+};
+
+/* We report joy-con d-pad inputs as buttons and pro controller as a hat. */
+static const unsigned int joycon_dpad_inputs_jc[] = {
+	BTN_DPAD_UP, BTN_DPAD_DOWN, BTN_DPAD_LEFT, BTN_DPAD_RIGHT,
+};
+
+static DEFINE_MUTEX(joycon_input_num_mutex);
+static int joycon_input_create(struct joycon_ctlr *ctlr)
+{
+	struct hid_device *hdev;
+	static int input_num = 1;
+	const char *name;
+	int ret;
+	int i;
+
+	hdev = ctlr->hdev;
+
+	switch (hdev->product) {
+	case USB_DEVICE_ID_NINTENDO_PROCON:
+		name = "Nintendo Switch Pro Controller";
+		break;
+	case USB_DEVICE_ID_NINTENDO_JOYCONL:
+		name = "Nintendo Switch Left Joy-Con";
+		break;
+	case USB_DEVICE_ID_NINTENDO_JOYCONR:
+		name = "Nintendo Switch Right Joy-Con";
+		break;
+	default: /* Should be impossible */
+		hid_err(hdev, "Invalid hid product\n");
+		return -EINVAL;
+	}
+
+	ctlr->input = devm_input_allocate_device(&hdev->dev);
+	if (!ctlr->input)
+		return -ENOMEM;
+	ctlr->input->id.bustype = hdev->bus;
+	ctlr->input->id.vendor = hdev->vendor;
+	ctlr->input->id.product = hdev->product;
+	ctlr->input->id.version = hdev->version;
+	ctlr->input->name = name;
+	input_set_drvdata(ctlr->input, ctlr);
+
+
+	/* set up sticks and buttons */
+	if (hdev->product != USB_DEVICE_ID_NINTENDO_JOYCONR) {
+		input_set_abs_params(ctlr->input, ABS_X,
+				     -JC_MAX_STICK_MAG, JC_MAX_STICK_MAG,
+				     JC_STICK_FUZZ, JC_STICK_FLAT);
+		input_set_abs_params(ctlr->input, ABS_Y,
+				     -JC_MAX_STICK_MAG, JC_MAX_STICK_MAG,
+				     JC_STICK_FUZZ, JC_STICK_FLAT);
+
+		for (i = 0; joycon_button_inputs_l[i] > 0; i++)
+			input_set_capability(ctlr->input, EV_KEY,
+					     joycon_button_inputs_l[i]);
+
+		/* configure d-pad differently for joy-con vs pro controller */
+		if (hdev->product != USB_DEVICE_ID_NINTENDO_PROCON) {
+			for (i = 0; joycon_dpad_inputs_jc[i] > 0; i++)
+				input_set_capability(ctlr->input, EV_KEY,
+						     joycon_dpad_inputs_jc[i]);
+		} else {
+			input_set_abs_params(ctlr->input, ABS_HAT0X,
+					     -JC_MAX_DPAD_MAG, JC_MAX_DPAD_MAG,
+					     JC_DPAD_FUZZ, JC_DPAD_FLAT);
+			input_set_abs_params(ctlr->input, ABS_HAT0Y,
+					     -JC_MAX_DPAD_MAG, JC_MAX_DPAD_MAG,
+					     JC_DPAD_FUZZ, JC_DPAD_FLAT);
+		}
+	}
+	if (hdev->product != USB_DEVICE_ID_NINTENDO_JOYCONL) {
+		input_set_abs_params(ctlr->input, ABS_RX,
+				     -JC_MAX_STICK_MAG, JC_MAX_STICK_MAG,
+				     JC_STICK_FUZZ, JC_STICK_FLAT);
+		input_set_abs_params(ctlr->input, ABS_RY,
+				     -JC_MAX_STICK_MAG, JC_MAX_STICK_MAG,
+				     JC_STICK_FUZZ, JC_STICK_FLAT);
+
+		for (i = 0; joycon_button_inputs_r[i] > 0; i++)
+			input_set_capability(ctlr->input, EV_KEY,
+					     joycon_button_inputs_r[i]);
+	}
+
+	/* Let's report joy-con S triggers separately */
+	if (hdev->product == USB_DEVICE_ID_NINTENDO_JOYCONL) {
+		input_set_capability(ctlr->input, EV_KEY, BTN_TR);
+		input_set_capability(ctlr->input, EV_KEY, BTN_TR2);
+	} else if (hdev->product == USB_DEVICE_ID_NINTENDO_JOYCONR) {
+		input_set_capability(ctlr->input, EV_KEY, BTN_TL);
+		input_set_capability(ctlr->input, EV_KEY, BTN_TL2);
+	}
+
+	ret = input_register_device(ctlr->input);
+	if (ret)
+		return ret;
+
+	/* Set the default controller player leds based on controller number */
+	mutex_lock(&joycon_input_num_mutex);
+	mutex_lock(&ctlr->output_mutex);
+	ret = joycon_set_player_leds(ctlr, 0, 0xF >> (4 - input_num));
+	if (ret)
+		hid_warn(ctlr->hdev, "Failed to set leds; ret=%d\n", ret);
+	mutex_unlock(&ctlr->output_mutex);
+	if (++input_num > 4)
+		input_num = 1;
+	mutex_unlock(&joycon_input_num_mutex);
+
+	return 0;
+}
+
+/* Common handler for parsing inputs */
+static int joycon_ctlr_read_handler(struct joycon_ctlr *ctlr, u8 *data,
+							      int size)
+{
+	if (data[0] == JC_INPUT_SUBCMD_REPLY || data[0] == JC_INPUT_IMU_DATA ||
+	    data[0] == JC_INPUT_MCU_DATA) {
+		if (size >= 12) /* make sure it contains the input report */
+			joycon_parse_report(ctlr,
+					    (struct joycon_input_report *)data);
+	}
+
+	return 0;
+}
+
+static int joycon_ctlr_handle_event(struct joycon_ctlr *ctlr, u8 *data,
+							      int size)
+{
+	int ret = 0;
+	bool match = false;
+	struct joycon_input_report *report;
+
+	if (unlikely(mutex_is_locked(&ctlr->output_mutex)) &&
+	    ctlr->msg_type != JOYCON_MSG_TYPE_NONE) {
+		switch (ctlr->msg_type) {
+		case JOYCON_MSG_TYPE_USB:
+			if (size < 2)
+				break;
+			if (data[0] == JC_INPUT_USB_RESPONSE &&
+			    data[1] == ctlr->usb_ack_match)
+				match = true;
+			break;
+		case JOYCON_MSG_TYPE_SUBCMD:
+			if (size < sizeof(struct joycon_input_report) ||
+			    data[0] != JC_INPUT_SUBCMD_REPLY)
+				break;
+			report = (struct joycon_input_report *)data;
+			if (report->reply.id == ctlr->subcmd_ack_match)
+				match = true;
+			break;
+		default:
+			break;
+		}
+
+		if (match) {
+			memcpy(ctlr->input_buf, data,
+			       min(size, (int)JC_MAX_RESP_SIZE));
+			ctlr->msg_type = JOYCON_MSG_TYPE_NONE;
+			ctlr->received_resp = true;
+			wake_up(&ctlr->wait);
+
+			/* This message has been handled */
+			return 1;
+		}
+	}
+
+	if (ctlr->ctlr_state == JOYCON_CTLR_STATE_READ)
+		ret = joycon_ctlr_read_handler(ctlr, data, size);
+
+	return ret;
+}
+
+static int nintendo_hid_event(struct hid_device *hdev,
+			      struct hid_report *report, u8 *raw_data, int size)
+{
+	struct joycon_ctlr *ctlr = hid_get_drvdata(hdev);
+
+	if (size < 1)
+		return -EINVAL;
+
+	return joycon_ctlr_handle_event(ctlr, raw_data, size);
+}
+
+static int nintendo_hid_probe(struct hid_device *hdev,
+			    const struct hid_device_id *id)
+{
+	int ret;
+	struct joycon_ctlr *ctlr;
+
+	hid_dbg(hdev, "probe - start\n");
+
+	ctlr = devm_kzalloc(&hdev->dev, sizeof(*ctlr), GFP_KERNEL);
+	if (!ctlr) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	ctlr->hdev = hdev;
+	ctlr->ctlr_state = JOYCON_CTLR_STATE_INIT;
+	hid_set_drvdata(hdev, ctlr);
+	mutex_init(&ctlr->output_mutex);
+	init_waitqueue_head(&ctlr->wait);
+
+	ret = hid_parse(hdev);
+	if (ret) {
+		hid_err(hdev, "HID parse failed\n");
+		goto err;
+	}
+
+	ret = hid_hw_start(hdev, HID_CONNECT_HIDRAW);
+	if (ret) {
+		hid_err(hdev, "HW start failed\n");
+		goto err;
+	}
+
+	ret = hid_hw_open(hdev);
+	if (ret) {
+		hid_err(hdev, "cannot start hardware I/O\n");
+		goto err_stop;
+	}
+
+	hid_device_io_start(hdev);
+
+	/* Initialize the controller */
+	mutex_lock(&ctlr->output_mutex);
+	/* if handshake command fails, assume ble pro controller */
+	if (hdev->product == USB_DEVICE_ID_NINTENDO_PROCON &&
+	    !joycon_send_usb(ctlr, JC_USB_CMD_HANDSHAKE)) {
+		hid_dbg(hdev, "detected USB controller\n");
+		/* set baudrate for improved latency */
+		ret = joycon_send_usb(ctlr, JC_USB_CMD_BAUDRATE_3M);
+		if (ret) {
+			hid_err(hdev, "Failed to set baudrate; ret=%d\n", ret);
+			goto err_mutex;
+		}
+		/* handshake */
+		ret = joycon_send_usb(ctlr, JC_USB_CMD_HANDSHAKE);
+		if (ret) {
+			hid_err(hdev, "Failed handshake; ret=%d\n", ret);
+			goto err_mutex;
+		}
+		/*
+		 * Set no timeout (to keep controller in USB mode).
+		 * This doesn't send a response, so ignore the timeout.
+		 */
+		joycon_send_usb(ctlr, JC_USB_CMD_NO_TIMEOUT);
+	}
+
+	/* get controller calibration data, and parse it */
+	ret = joycon_request_calibration(ctlr);
+	if (ret) {
+		/*
+		 * We can function with default calibration, but it may be
+		 * inaccurate. Provide a warning, and continue on.
+		 */
+		hid_warn(hdev, "Analog stick positions may be inaccurate\n");
+	}
+
+	/* Set the reporting mode to 0x30, which is the full report mode */
+	ret = joycon_set_report_mode(ctlr);
+	if (ret) {
+		hid_err(hdev, "Failed to set report mode; ret=%d\n", ret);
+		goto err_mutex;
+	}
+
+	mutex_unlock(&ctlr->output_mutex);
+
+	ret = joycon_input_create(ctlr);
+	if (ret) {
+		hid_err(hdev, "Failed to create input device; ret=%d\n", ret);
+		goto err_close;
+	}
+
+	ctlr->ctlr_state = JOYCON_CTLR_STATE_READ;
+
+	hid_dbg(hdev, "probe - success\n");
+	return 0;
+
+err_mutex:
+	mutex_unlock(&ctlr->output_mutex);
+err_close:
+	hid_hw_close(hdev);
+err_stop:
+	hid_hw_stop(hdev);
+err:
+	hid_err(hdev, "probe - fail = %d\n", ret);
+	return ret;
+}
+
+static void nintendo_hid_remove(struct hid_device *hdev)
+{
+	hid_dbg(hdev, "remove\n");
+	hid_hw_close(hdev);
+	hid_hw_stop(hdev);
+}
+
+static const struct hid_device_id nintendo_hid_devices[] = {
+	{ HID_USB_DEVICE(USB_VENDOR_ID_NINTENDO,
+			 USB_DEVICE_ID_NINTENDO_PROCON) },
+	{ HID_BLUETOOTH_DEVICE(USB_VENDOR_ID_NINTENDO,
+			 USB_DEVICE_ID_NINTENDO_PROCON) },
+	{ HID_BLUETOOTH_DEVICE(USB_VENDOR_ID_NINTENDO,
+			 USB_DEVICE_ID_NINTENDO_JOYCONL) },
+	{ HID_BLUETOOTH_DEVICE(USB_VENDOR_ID_NINTENDO,
+			 USB_DEVICE_ID_NINTENDO_JOYCONR) },
+	{ }
+};
+MODULE_DEVICE_TABLE(hid, nintendo_hid_devices);
+
+static struct hid_driver nintendo_hid_driver = {
+	.name		= "nintendo",
+	.id_table	= nintendo_hid_devices,
+	.probe		= nintendo_hid_probe,
+	.remove		= nintendo_hid_remove,
+	.raw_event	= nintendo_hid_event,
+};
+module_hid_driver(nintendo_hid_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Daniel J. Ogorchock <djogorchock@gmail.com>");
+MODULE_DESCRIPTION("Driver for Nintendo Switch Controllers");
diff -ruN a/drivers/hid/hid-quickstep.c b/drivers/hid/hid-quickstep.c
--- a/drivers/hid/hid-quickstep.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/hid/hid-quickstep.c	2021-12-23 08:35:29.000000000 +0100
@@ -0,0 +1,173 @@
+/*
+ *  HID driver for Quickstep, ChromeOS's Latency Measurement Gadget
+ *
+ *  The device is connected via USB and transmits a byte each time a
+ *  laster is crossed.  The job of the driver is to record when those events
+ *  happen and then make that information availible to the user via sysfs
+ *  entries.
+ */
+
+#include <linux/device.h>
+#include <linux/hid.h>
+#include <linux/module.h>
+#include <linux/string.h>
+#include <linux/time.h>
+
+#include "hid-ids.h"
+
+#define MAX_CROSSINGS 64
+
+enum change_type { OFF, ON };
+
+struct qs_event {
+	struct timespec64 time;
+	enum change_type direction;
+};
+
+struct qs_data {
+	unsigned int head;
+	struct qs_event events[MAX_CROSSINGS];
+};
+
+static ssize_t append_event(struct qs_event *event, char *buf, ssize_t len)
+{
+	return snprintf(buf, len, "%010lld.%09ld\t%d\n", event->time.tv_sec,
+			event->time.tv_nsec, event->direction);
+
+}
+
+static ssize_t show_log(struct device *dev, struct device_attribute *attr,
+		char *buf)
+{
+	int i, str_len;
+	struct qs_data *data = dev_get_drvdata(dev);
+
+	str_len = snprintf(buf, PAGE_SIZE,
+			"Laser Crossings:\ntime\t\t\tdirection\n");
+
+	if (data->head >= MAX_CROSSINGS) {
+		for (i = data->head % MAX_CROSSINGS; i < MAX_CROSSINGS; i++) {
+			str_len += append_event(&data->events[i], buf + str_len,
+						PAGE_SIZE - str_len);
+		}
+	}
+
+	for (i = 0; i < data->head % MAX_CROSSINGS; i++) {
+		str_len += append_event(&data->events[i], buf + str_len,
+					PAGE_SIZE - str_len);
+	}
+
+	return str_len;
+}
+
+static void empty_quickstep_data(struct qs_data *data)
+{
+	if (data == NULL)
+		return;
+	data->head = 0;
+}
+
+static ssize_t clear_log(struct device *dev, struct device_attribute *attr,
+		const char *buf, size_t len)
+{
+	empty_quickstep_data(dev_get_drvdata(dev));
+	return len;
+}
+
+static DEVICE_ATTR(laser, 0444, show_log, NULL);
+static DEVICE_ATTR(clear, 0220, NULL, clear_log);
+static struct attribute *dev_attrs[] = {
+	&dev_attr_laser.attr,
+	&dev_attr_clear.attr,
+	NULL,
+};
+static struct attribute_group dev_attr_group = {.attrs = dev_attrs};
+
+static int quickstep_probe(struct hid_device *hdev,
+		const struct hid_device_id *id)
+{
+	int ret;
+	struct qs_data *data;
+
+	ret = hid_parse(hdev);
+	if (ret) {
+		hid_err(hdev, "parse failed\n");
+		return ret;
+	}
+
+	ret = hid_hw_start(hdev, HID_CONNECT_DEFAULT);
+	if (ret) {
+		hid_err(hdev, "hw start failed\n");
+		return ret;
+	}
+
+	ret = hid_hw_open(hdev);
+	if (ret) {
+		hid_err(hdev, "hw open failed\n");
+		hid_hw_stop(hdev);
+		return ret;
+	}
+
+	data = kmalloc(sizeof(struct qs_data), GFP_KERNEL);
+	empty_quickstep_data(data);
+	hid_set_drvdata(hdev, data);
+
+	ret = sysfs_create_group(&hdev->dev.kobj, &dev_attr_group);
+
+	return ret;
+}
+
+static void quickstep_remove(struct hid_device *hdev)
+{
+	sysfs_remove_group(&hdev->dev.kobj, &dev_attr_group);
+	hid_hw_stop(hdev);
+	kfree(hid_get_drvdata(hdev));
+}
+
+static int quickstep_raw_event(struct hid_device *hdev,
+	struct hid_report *report, u8 *msg, int size)
+{
+	struct timespec64 time;
+	struct qs_data *data = hid_get_drvdata(hdev);
+
+	ktime_get_real_ts64(&time);
+
+	data->events[data->head % MAX_CROSSINGS].time = time;
+	data->events[data->head % MAX_CROSSINGS].direction = msg[0] ? ON : OFF;
+
+	data->head++;
+	if (data->head >= MAX_CROSSINGS * 2)
+		data->head = MAX_CROSSINGS + data->head % MAX_CROSSINGS;
+
+	return 0;
+}
+
+static const struct hid_device_id quickstep_devices[] = {
+	{ HID_USB_DEVICE(USB_VENDOR_ID_GOOGLE,
+		USB_DEVICE_ID_GOOGLE_QUICKSTEP) },
+	{ }
+};
+MODULE_DEVICE_TABLE(hid, quickstep_devices);
+
+static struct hid_driver quickstep_driver = {
+	.name = "quickstep",
+	.id_table = quickstep_devices,
+	.probe = quickstep_probe,
+	.remove = quickstep_remove,
+	.raw_event = quickstep_raw_event,
+};
+
+static int __init quickstep_init(void)
+{
+	return hid_register_driver(&quickstep_driver);
+}
+
+static void __exit quickstep_exit(void)
+{
+	hid_unregister_driver(&quickstep_driver);
+}
+
+module_init(quickstep_init);
+module_exit(quickstep_exit);
+MODULE_AUTHOR("Charlie Mooney <charliemooney@google.com>");
+MODULE_LICENSE("GPL");
diff -ruN a/drivers/hid/hid-quirks.c b/drivers/hid/hid-quirks.c
--- a/drivers/hid/hid-quirks.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/hid/hid-quirks.c	2021-12-23 08:35:29.000000000 +0100
@@ -102,6 +102,8 @@
 	{ HID_USB_DEVICE(USB_VENDOR_ID_HP, USB_PRODUCT_ID_HP_PIXART_OEM_USB_OPTICAL_MOUSE_0941), HID_QUIRK_ALWAYS_POLL },
 	{ HID_USB_DEVICE(USB_VENDOR_ID_HP, USB_PRODUCT_ID_HP_PIXART_OEM_USB_OPTICAL_MOUSE_0641), HID_QUIRK_ALWAYS_POLL },
 	{ HID_USB_DEVICE(USB_VENDOR_ID_HP, USB_PRODUCT_ID_HP_PIXART_OEM_USB_OPTICAL_MOUSE_1f4a), HID_QUIRK_ALWAYS_POLL },
+	{ HID_USB_DEVICE(USB_VENDOR_ID_HUION, USB_DEVICE_ID_HUION_HS64), HID_QUIRK_DEVICE_IS_DIGITIZER },
+	{ HID_USB_DEVICE(USB_VENDOR_ID_HUION, USB_DEVICE_ID_HUION_TABLET), HID_QUIRK_DEVICE_IS_DIGITIZER },
 	{ HID_USB_DEVICE(USB_VENDOR_ID_IDEACOM, USB_DEVICE_ID_IDEACOM_IDC6680), HID_QUIRK_MULTI_INPUT },
 	{ HID_USB_DEVICE(USB_VENDOR_ID_INNOMEDIA, USB_DEVICE_ID_INNEX_GENESIS_ATARI), HID_QUIRK_MULTI_INPUT },
 	{ HID_USB_DEVICE(USB_VENDOR_ID_KYE, USB_DEVICE_ID_KYE_EASYPEN_M610X), HID_QUIRK_MULTI_INPUT },
@@ -572,6 +574,9 @@
 #if IS_ENABLED(CONFIG_HID_PRODIKEYS)
 	{ HID_USB_DEVICE(USB_VENDOR_ID_CREATIVELABS, USB_DEVICE_ID_PRODIKEYS_PCMIDI) },
 #endif
+#if IS_ENABLED(CONFIG_HID_QUICKSTEP)
+	{ HID_USB_DEVICE(USB_VENDOR_ID_GOOGLE, USB_DEVICE_ID_GOOGLE_QUICKSTEP) },
+#endif
 #if IS_ENABLED(CONFIG_HID_RETRODE)
 	{ HID_USB_DEVICE(USB_VENDOR_ID_FUTURE_TECHNOLOGY, USB_DEVICE_ID_RETRODE2) },
 #endif
@@ -1272,6 +1277,13 @@
 		quirks = hid_gets_squirk(hdev);
 	mutex_unlock(&dquirks_lock);
 
+	/*
+	 * UGEE/XP-Pen HID Pen devices which have 0x0-0x9 as the low nibble
+	 * of the device ID are actually digitizers, not HID Pen devices
+	 */
+	if (hdev->vendor == USB_VENDOR_ID_UGEE && (hdev->product & 0x0F) <= 0x09)
+		quirks |= HID_QUIRK_DEVICE_IS_DIGITIZER;
+
 	return quirks;
 }
 EXPORT_SYMBOL_GPL(hid_lookup_quirk);
diff -ruN a/drivers/hid/Kconfig b/drivers/hid/Kconfig
--- a/drivers/hid/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/hid/Kconfig	2021-12-23 08:35:29.000000000 +0100
@@ -731,6 +731,17 @@
 	  To compile this driver as a module, choose M here: the
 	  module will be called hid-multitouch.
 
+config HID_NINTENDO
+	tristate "Nintendo Joy-Con and Pro Controller support"
+	depends on HID
+	help
+	Adds support for the Nintendo Switch Joy-Cons and Pro Controller.
+	All controllers support bluetooth, and the Pro Controller also supports
+	its USB mode.
+
+	To compile this driver as a module, choose M here: the
+	module will be called hid-nintendo.
+
 config HID_NTI
 	tristate "NTI keyboard adapters"
 	help
@@ -890,6 +901,13 @@
 	Support for Primax devices that are not fully compliant with the
 	HID standard.
 
+config HID_QUICKSTEP
+	tristate "ChromeOS Touch Latency Measurement Device -- Quickstep"
+	depends on USB_HID
+	help
+	This module is the driver for the ChromeOS Touch Latency Measurement
+	Device known as Quickstep.
+
 config HID_RETRODE
 	tristate "Retrode 2 USB adapter for vintage video games"
 	depends on USB_HID
diff -ruN a/drivers/hid/Makefile b/drivers/hid/Makefile
--- a/drivers/hid/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/hid/Makefile	2021-12-23 08:35:29.000000000 +0100
@@ -78,6 +78,7 @@
 obj-$(CONFIG_HID_MICROSOFT)	+= hid-microsoft.o
 obj-$(CONFIG_HID_MONTEREY)	+= hid-monterey.o
 obj-$(CONFIG_HID_MULTITOUCH)	+= hid-multitouch.o
+obj-$(CONFIG_HID_NINTENDO)	+= hid-nintendo.o
 obj-$(CONFIG_HID_NTI)			+= hid-nti.o
 obj-$(CONFIG_HID_NTRIG)		+= hid-ntrig.o
 obj-$(CONFIG_HID_ORTEK)		+= hid-ortek.o
@@ -97,6 +98,7 @@
 obj-$(CONFIG_HID_PLANTRONICS)	+= hid-plantronics.o
 obj-$(CONFIG_HID_PLAYSTATION)	+= hid-playstation.o
 obj-$(CONFIG_HID_PRIMAX)	+= hid-primax.o
+obj-$(CONFIG_HID_QUICKSTEP)	+= hid-quickstep.o
 obj-$(CONFIG_HID_REDRAGON)	+= hid-redragon.o
 obj-$(CONFIG_HID_RETRODE)	+= hid-retrode.o
 obj-$(CONFIG_HID_ROCCAT)	+= hid-roccat.o hid-roccat-common.o \
diff -ruN a/drivers/hwtracing/coresight/coresight-cpu-debug.c b/drivers/hwtracing/coresight/coresight-cpu-debug.c
--- a/drivers/hwtracing/coresight/coresight-cpu-debug.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/hwtracing/coresight/coresight-cpu-debug.c	2021-12-23 08:35:29.000000000 +0100
@@ -105,7 +105,7 @@
 static int debug_count;
 static struct dentry *debug_debugfs_dir;
 
-static bool debug_enable;
+static bool debug_enable = IS_ENABLED(CONFIG_CORESIGHT_CPU_DEBUG_DEFAULT_ON);
 module_param_named(enable, debug_enable, bool, 0600);
 MODULE_PARM_DESC(enable, "Control to enable coresight CPU debug functionality");
 
diff -ruN a/drivers/hwtracing/coresight/Kconfig b/drivers/hwtracing/coresight/Kconfig
--- a/drivers/hwtracing/coresight/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/hwtracing/coresight/Kconfig	2021-12-23 08:35:29.000000000 +0100
@@ -150,6 +150,19 @@
 	  To compile this driver as a module, choose M here: the
 	  module will be called coresight-cpu-debug.
 
+config CORESIGHT_CPU_DEBUG_DEFAULT_ON
+	bool "Enable CoreSight CPU Debug by default"
+	depends on CORESIGHT_CPU_DEBUG
+	help
+	  Say Y here to enable the CoreSight Debug panic-debug by default. This
+	  can also be enabled via debugfs, but this ensures the debug feature
+	  is enabled as early as possible.
+
+	  Has the same effect as setting coresight_cpu_debug.enable=1 on the
+	  kernel command line.
+
+	  Say N if unsure.
+
 config CORESIGHT_CTI
 	tristate "CoreSight Cross Trigger Interface (CTI) driver"
 	depends on ARM || ARM64
diff -ruN a/drivers/iio/common/cros_ec_sensors/cros_ec_activity.c b/drivers/iio/common/cros_ec_sensors/cros_ec_activity.c
--- a/drivers/iio/common/cros_ec_sensors/cros_ec_activity.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/iio/common/cros_ec_sensors/cros_ec_activity.c	2021-12-23 08:35:30.000000000 +0100
@@ -0,0 +1,404 @@
+/*
+ * cros_ec_sensors_activity - Driver for activities/gesture recognition.
+ *
+ * Copyright (C) 2015 Google, Inc
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * This driver uses the cros-ec interface to communicate with the Chrome OS
+ * EC about accelerometer data. Accelerometer access is presented through
+ * iio sysfs.
+ */
+
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/iio/common/cros_ec_sensors_core.h>
+#include <linux/iio/events.h>
+#include <linux/iio/iio.h>
+#include <linux/iio/trigger_consumer.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_data/cros_ec_commands.h>
+#include <linux/platform_data/cros_ec_proto.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+
+#define DRV_NAME "cros-ec-activity"
+
+/* st data for ec_sensors iio driver. */
+struct cros_ec_sensors_state {
+	/* Shared by all sensors */
+	struct cros_ec_sensors_core_state core;
+
+	struct iio_chan_spec *channels;
+	unsigned nb_activities;
+
+	int body_detection_channel_index;
+	int sig_motion_channel_index;
+	int double_tap_channel_index;
+};
+
+static const struct iio_event_spec cros_ec_activity_single_shot[] = {
+	{
+		.type = IIO_EV_TYPE_CHANGE,
+		/* significant motion trigger when we get out of still. */
+		.dir = IIO_EV_DIR_FALLING,
+		.mask_separate = BIT(IIO_EV_INFO_ENABLE),
+	 },
+};
+static const struct iio_event_spec cros_ec_body_detect_events[] = {
+	{
+		.type = IIO_EV_TYPE_CHANGE,
+		.dir = IIO_EV_DIR_EITHER,
+		.mask_separate = BIT(IIO_EV_INFO_ENABLE),
+	 },
+};
+
+static int ec_sensors_read(struct iio_dev *indio_dev,
+			  struct iio_chan_spec const *chan,
+			  int *val, int *val2, long mask)
+{
+	struct cros_ec_sensors_state *st = iio_priv(indio_dev);
+	int ret;
+
+	mutex_lock(&st->core.cmd_lock);
+	switch (chan->type) {
+	case IIO_PROXIMITY:
+		switch (mask) {
+		case IIO_CHAN_INFO_RAW:
+			st->core.param.cmd = MOTIONSENSE_CMD_GET_ACTIVITY;
+			st->core.param.get_activity.activity =
+					MOTIONSENSE_ACTIVITY_BODY_DETECTION;
+			if (cros_ec_motion_send_host_cmd(&st->core, 0) !=
+			    EC_RES_SUCCESS) {
+				ret = -EIO;
+			} else {
+				*val = st->core.resp->get_activity.state;
+				ret = IIO_VAL_INT;
+			}
+			break;
+		default:
+			ret = -EINVAL;
+		}
+		break;
+	case IIO_ACTIVITY:
+		dev_warn(&indio_dev->dev, "%s: Not Expected: %d\n", __func__,
+			 chan->channel2);
+		ret = -ENOSYS;
+		break;
+	default:
+		ret = -EINVAL;
+	}
+	mutex_unlock(&st->core.cmd_lock);
+	return ret;
+}
+
+static int ec_sensors_write(struct iio_dev *indio_dev,
+			       struct iio_chan_spec const *chan,
+			       int val, int val2, long mask)
+{
+	dev_warn(&indio_dev->dev, "%s: Not Expected: %d\n", __func__,
+		 chan->channel2);
+	return -ENOSYS;
+}
+
+static int cros_ec_read_event_config(struct iio_dev *indio_dev,
+				     const struct iio_chan_spec *chan,
+				     enum iio_event_type type,
+				     enum iio_event_direction dir)
+{
+	struct cros_ec_sensors_state *st = iio_priv(indio_dev);
+	int ret;
+
+	if (chan->type != IIO_ACTIVITY && chan->type != IIO_PROXIMITY)
+		return -EINVAL;
+
+	mutex_lock(&st->core.cmd_lock);
+	st->core.param.cmd = MOTIONSENSE_CMD_LIST_ACTIVITIES;
+	ret = cros_ec_motion_send_host_cmd(&st->core, 0);
+	if (ret)
+		goto done;
+	switch (chan->type) {
+	case IIO_PROXIMITY:
+		ret = !!(st->core.resp->list_activities.enabled &
+			 (1 << MOTIONSENSE_ACTIVITY_BODY_DETECTION));
+		break;
+	case IIO_ACTIVITY:
+		switch (chan->channel2) {
+		case IIO_MOD_STILL:
+			ret = !!(st->core.resp->list_activities.enabled &
+				 (1 << MOTIONSENSE_ACTIVITY_SIG_MOTION));
+			break;
+		case IIO_MOD_DOUBLE_TAP:
+			ret = !!(st->core.resp->list_activities.enabled &
+				 (1 << MOTIONSENSE_ACTIVITY_DOUBLE_TAP));
+			break;
+		default:
+			dev_warn(&indio_dev->dev, "Unknown activity: %d\n",
+				 chan->channel2);
+			ret = -EINVAL;
+		}
+		break;
+	default:
+		dev_warn(&indio_dev->dev, "Unknown channel type: %d\n",
+			 chan->type);
+		ret = -EINVAL;
+	}
+done:
+	mutex_unlock(&st->core.cmd_lock);
+	return ret;
+}
+
+static int cros_ec_write_event_config(struct iio_dev *indio_dev,
+				      const struct iio_chan_spec *chan,
+				      enum iio_event_type type,
+				      enum iio_event_direction dir, int state)
+{
+	struct cros_ec_sensors_state *st = iio_priv(indio_dev);
+	int ret;
+
+	if (chan->type != IIO_ACTIVITY && chan->type != IIO_PROXIMITY)
+		return -EINVAL;
+
+	mutex_lock(&st->core.cmd_lock);
+	st->core.param.cmd = MOTIONSENSE_CMD_SET_ACTIVITY;
+	switch (chan->type) {
+	case IIO_PROXIMITY:
+		st->core.param.set_activity.activity =
+			MOTIONSENSE_ACTIVITY_BODY_DETECTION;
+		break;
+	case IIO_ACTIVITY:
+		switch (chan->channel2) {
+		case IIO_MOD_STILL:
+			st->core.param.set_activity.activity =
+				MOTIONSENSE_ACTIVITY_SIG_MOTION;
+			break;
+		case IIO_MOD_DOUBLE_TAP:
+			st->core.param.set_activity.activity =
+				MOTIONSENSE_ACTIVITY_DOUBLE_TAP;
+			break;
+		default:
+			dev_warn(&indio_dev->dev, "Unknown activity: %d\n",
+				 chan->channel2);
+		}
+		break;
+	default:
+		dev_warn(&indio_dev->dev, "Unknown channel type: %d\n",
+			 chan->type);
+	}
+	st->core.param.set_activity.enable = state;
+
+	ret = cros_ec_motion_send_host_cmd(&st->core, 0);
+
+	mutex_unlock(&st->core.cmd_lock);
+	return ret;
+}
+
+static int cros_ec_activity_push_data(
+		struct iio_dev *indio_dev,
+		s16 *data,
+		s64 timestamp)
+{
+	struct ec_response_activity_data *activity_data =
+			(struct ec_response_activity_data *)data;
+	enum motionsensor_activity activity = activity_data->activity;
+	uint8_t state = activity_data->state;
+	const struct cros_ec_sensors_state *st = iio_priv(indio_dev);
+	const struct iio_chan_spec *chan;
+	const struct iio_event_spec *event;
+	enum iio_event_direction dir;
+	int index;
+	u64 ev;
+
+	switch (activity) {
+	case MOTIONSENSE_ACTIVITY_BODY_DETECTION:
+		index = st->body_detection_channel_index;
+		dir = state ? IIO_EV_DIR_FALLING : IIO_EV_DIR_RISING;
+		break;
+	case MOTIONSENSE_ACTIVITY_SIG_MOTION:
+		index = st->sig_motion_channel_index;
+		dir = IIO_EV_DIR_FALLING;
+		break;
+	case MOTIONSENSE_ACTIVITY_DOUBLE_TAP:
+		index = st->double_tap_channel_index;
+		dir = IIO_EV_DIR_FALLING;
+		break;
+	default:
+		dev_warn(&indio_dev->dev, "Unknown activity: %d\n", activity);
+		return 0;
+	}
+	chan = &st->channels[index];
+	event = &chan->event_spec[0];
+
+	ev = IIO_UNMOD_EVENT_CODE(chan->type, index, event->type, dir);
+	iio_push_event(indio_dev, ev, timestamp);
+	return 0;
+}
+
+static irqreturn_t cros_ec_activity_capture(int irq, void *p)
+{
+	struct iio_poll_func *pf = p;
+	struct iio_dev *indio_dev = pf->indio_dev;
+
+	dev_warn(&indio_dev->dev, "%s: Not Expected\n", __func__);
+	return IRQ_NONE;
+}
+
+/* Not implemented */
+static int cros_ec_read_event_value(struct iio_dev *indio_dev,
+				    const struct iio_chan_spec *chan,
+				    enum iio_event_type type,
+				    enum iio_event_direction dir,
+				    enum iio_event_info info,
+				    int *val, int *val2)
+{
+	dev_warn(&indio_dev->dev, "%s: Not Expected: %d\n", __func__,
+		 chan->channel2);
+	return -ENOSYS;
+}
+
+static int cros_ec_write_event_value(struct iio_dev *indio_dev,
+				     const struct iio_chan_spec *chan,
+				     enum iio_event_type type,
+				     enum iio_event_direction dir,
+				     enum iio_event_info info,
+				     int val, int val2)
+{
+	dev_warn(&indio_dev->dev, "%s: Not Expected: %d\n", __func__,
+		 chan->channel2);
+	return -ENOSYS;
+}
+
+static const struct iio_info ec_sensors_info = {
+	.read_raw = &ec_sensors_read,
+	.write_raw = &ec_sensors_write,
+	.read_event_config = cros_ec_read_event_config,
+	.write_event_config = cros_ec_write_event_config,
+	.read_event_value = cros_ec_read_event_value,
+	.write_event_value = cros_ec_write_event_value,
+};
+
+static int cros_ec_sensors_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct cros_ec_device *ec_device = dev_get_drvdata(dev->parent);
+	struct iio_dev *indio_dev;
+	struct cros_ec_sensors_state *st;
+	struct iio_chan_spec *channel;
+	unsigned long activities;
+	int i, index, ret, nb_activities;
+
+	if (!ec_device) {
+		dev_warn(&pdev->dev, "No CROS EC device found.\n");
+		return -EINVAL;
+	}
+
+	indio_dev = devm_iio_device_alloc(&pdev->dev, sizeof(*st));
+	if (!indio_dev)
+		return -ENOMEM;
+
+	ret = cros_ec_sensors_core_init(pdev, indio_dev, true,
+			cros_ec_activity_capture);
+	if (ret)
+		return ret;
+
+	indio_dev->info = &ec_sensors_info;
+	st = iio_priv(indio_dev);
+	st->core.type = st->core.resp->info.type;
+	st->core.loc = st->core.resp->info.location;
+
+	/*
+	 * List all available activities
+	 */
+	st->core.param.cmd = MOTIONSENSE_CMD_LIST_ACTIVITIES;
+	ret = cros_ec_motion_send_host_cmd(&st->core, 0);
+	if (ret)
+		return ret;
+	activities = st->core.resp->list_activities.enabled |
+		     st->core.resp->list_activities.disabled;
+	nb_activities = hweight_long(activities) + 1;
+
+	if (!activities)
+		return -ENODEV;
+
+	/* Allocate a channel per activity and one for timestamp */
+	st->channels = devm_kcalloc(&pdev->dev, nb_activities,
+				    sizeof(*st->channels), GFP_KERNEL);
+	if (!st->channels)
+		return -ENOMEM;
+
+	channel = &st->channels[0];
+	index = 0;
+	for_each_set_bit(i, &activities, BITS_PER_LONG) {
+		channel->scan_index = index;
+
+		/* List all available activities */
+		if (i == MOTIONSENSE_ACTIVITY_BODY_DETECTION) {
+			channel->type = IIO_PROXIMITY;
+			channel->info_mask_separate = BIT(IIO_CHAN_INFO_RAW);
+			channel->modified = 0;
+			channel->event_spec = cros_ec_body_detect_events;
+			channel->num_event_specs =
+					ARRAY_SIZE(cros_ec_body_detect_events);
+			st->body_detection_channel_index = index;
+		} else {
+			channel->type = IIO_ACTIVITY;
+			channel->modified = 1;
+			channel->event_spec = cros_ec_activity_single_shot;
+			channel->num_event_specs = ARRAY_SIZE(
+					cros_ec_activity_single_shot);
+			switch (i) {
+			case MOTIONSENSE_ACTIVITY_SIG_MOTION:
+				channel->channel2 = IIO_MOD_STILL;
+				st->sig_motion_channel_index = index;
+				break;
+			case MOTIONSENSE_ACTIVITY_DOUBLE_TAP:
+				channel->channel2 = IIO_MOD_DOUBLE_TAP;
+				st->double_tap_channel_index = index;
+				break;
+			default:
+				dev_warn(&pdev->dev,
+					 "Unknown activity: %d\n", i);
+				continue;
+			}
+		}
+		channel->ext_info = cros_ec_sensors_limited_info;
+		channel++;
+		index++;
+	}
+
+	/* Timestamp */
+	channel->scan_index = index;
+	channel->type = IIO_TIMESTAMP;
+	channel->channel = -1;
+	channel->scan_type.sign = 's';
+	channel->scan_type.realbits = 64;
+	channel->scan_type.storagebits = 64;
+
+	indio_dev->channels = st->channels;
+	indio_dev->num_channels = index + 1;
+
+	st->core.read_ec_sensors_data = cros_ec_sensors_read_cmd;
+
+	ret = cros_ec_sensors_core_register(dev, indio_dev, cros_ec_activity_push_data);
+	return ret;
+}
+
+static struct platform_driver cros_ec_sensors_platform_driver = {
+	.driver = {
+		.name	= DRV_NAME,
+	},
+	.probe		= cros_ec_sensors_probe,
+};
+module_platform_driver(cros_ec_sensors_platform_driver);
+
+MODULE_DESCRIPTION("ChromeOS EC activity sensors driver");
+MODULE_ALIAS("platform:" DRV_NAME);
+MODULE_LICENSE("GPL v2");
diff -ruN a/drivers/iio/common/cros_ec_sensors/cros_ec_sensors.c b/drivers/iio/common/cros_ec_sensors/cros_ec_sensors.c
--- a/drivers/iio/common/cros_ec_sensors/cros_ec_sensors.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/iio/common/cros_ec_sensors/cros_ec_sensors.c	2021-12-23 08:35:30.000000000 +0100
@@ -8,6 +8,7 @@
  * EC about sensors data. Data access is presented through iio sysfs.
  */
 
+#include <linux/delay.h>
 #include <linux/device.h>
 #include <linux/iio/buffer.h>
 #include <linux/iio/common/cros_ec_sensors_core.h>
@@ -213,6 +214,17 @@
 
 	mutex_unlock(&st->core.cmd_lock);
 
+	if ((ret == 0) &&
+	    ((mask == IIO_CHAN_INFO_FREQUENCY) ||
+	     (mask == IIO_CHAN_INFO_SAMP_FREQ))) {
+		/*
+		 * Add a delay to allow the EC to flush older datum.
+		 * Assuming 1Mb link to the EC and 20 bytes per event, with 200
+		 * elements in the FIFO, we need 4ms. Add time for interrupt
+		 * handling and waking up requestor.
+		 */
+		usleep_range(10000, 15000);
+	}
 	return ret;
 }
 
diff -ruN a/drivers/iio/common/cros_ec_sensors/cros_ec_sensors_core.c b/drivers/iio/common/cros_ec_sensors/cros_ec_sensors_core.c
--- a/drivers/iio/common/cros_ec_sensors/cros_ec_sensors_core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/iio/common/cros_ec_sensors/cros_ec_sensors_core.c	2021-12-23 08:35:30.000000000 +0100
@@ -32,6 +32,7 @@
 static char *cros_ec_loc[] = {
 	[MOTIONSENSE_LOC_BASE] = "base",
 	[MOTIONSENSE_LOC_LID] = "lid",
+	[MOTIONSENSE_LOC_CAMERA] = "camera",
 	[MOTIONSENSE_LOC_MAX] = "unknown",
 };
 
@@ -93,6 +94,14 @@
 		*min_freq = 250;
 		*max_freq = 20000;
 		break;
+	case MOTIONSENSE_TYPE_SYNC:
+		/*
+		 * Frequency for sync/counter sensors is overloaded for
+		 * enable/disable.
+		 */
+		*min_freq = 0;
+		*max_freq = 1;
+		break;
 	case MOTIONSENSE_TYPE_ACTIVITY:
 	default:
 		*min_freq = 0;
@@ -117,6 +126,33 @@
 	return ret;
 }
 
+static ssize_t cros_ec_sensors_flush(struct device *dev,
+				     struct device_attribute *attr,
+				     const char *buf, size_t len)
+{
+	struct iio_dev *indio_dev = dev_to_iio_dev(dev);
+	struct cros_ec_sensors_core_state *st = iio_priv(indio_dev);
+	int ret = 0;
+	bool flush;
+
+	ret = strtobool(buf, &flush);
+	if (ret < 0)
+		return ret;
+	if (!flush)
+		return -EINVAL;
+
+	mutex_lock(&st->cmd_lock);
+	st->param.cmd = MOTIONSENSE_CMD_FIFO_FLUSH;
+	ret = cros_ec_motion_send_host_cmd(st, 0);
+	if (ret != 0)
+		dev_warn(&indio_dev->dev, "Unable to flush sensor\n");
+	mutex_unlock(&st->cmd_lock);
+	return ret ? ret : len;
+}
+
+static IIO_DEVICE_ATTR(hwfifo_flush, 0644, NULL,
+		       cros_ec_sensors_flush, 0);
+
 static ssize_t cros_ec_sensor_set_report_latency(struct device *dev,
 						 struct device_attribute *attr,
 						 const char *buf, size_t len)
@@ -179,6 +215,7 @@
 static IIO_DEVICE_ATTR_RO(hwfifo_watermark_max, 0);
 
 static const struct attribute *cros_ec_sensor_fifo_attributes[] = {
+	&iio_dev_attr_hwfifo_flush.dev_attr.attr,
 	&iio_dev_attr_hwfifo_timeout.dev_attr.attr,
 	&iio_dev_attr_hwfifo_watermark_max.dev_attr.attr,
 	NULL,
@@ -416,14 +453,14 @@
 	ret = strtobool(buf, &calibrate);
 	if (ret < 0)
 		return ret;
-	if (!calibrate)
-		return -EINVAL;
 
 	mutex_lock(&st->cmd_lock);
 	st->param.cmd = MOTIONSENSE_CMD_PERFORM_CALIB;
+	st->param.perform_calib.enable = calibrate;
 	ret = cros_ec_motion_send_host_cmd(st, 0);
 	if (ret != 0) {
-		dev_warn(&indio_dev->dev, "Unable to calibrate sensor\n");
+		dev_warn(&indio_dev->dev, "Unable to calibrate sensor: %d\n",
+			 ret);
 	} else {
 		/* Save values */
 		for (i = CROS_EC_SENSOR_X; i < CROS_EC_SENSOR_MAX_AXIS; i++)
@@ -472,6 +509,21 @@
 };
 EXPORT_SYMBOL_GPL(cros_ec_sensors_ext_info);
 
+const struct iio_chan_spec_ext_info cros_ec_sensors_limited_info[] = {
+	{
+		.name = "id",
+		.shared = IIO_SHARED_BY_ALL,
+		.read = cros_ec_sensors_id
+	},
+	{
+		.name = "location",
+		.shared = IIO_SHARED_BY_ALL,
+		.read = cros_ec_sensors_loc
+	},
+	{ },
+};
+EXPORT_SYMBOL_GPL(cros_ec_sensors_limited_info);
+
 /**
  * cros_ec_sensors_idx_to_reg - convert index into offset in shared memory
  * @st:		pointer to state information for device
diff -ruN a/drivers/iio/common/cros_ec_sensors/cros_ec_sensors_sync.c b/drivers/iio/common/cros_ec_sensors/cros_ec_sensors_sync.c
--- a/drivers/iio/common/cros_ec_sensors/cros_ec_sensors_sync.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/iio/common/cros_ec_sensors/cros_ec_sensors_sync.c	2021-12-23 08:35:30.000000000 +0100
@@ -0,0 +1,150 @@
+/*
+ * cros_ec_sensors_sync - Driver for synchronisation sensor behind CrOS EC.
+ *
+ * Copyright 2018 Google, Inc
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * This driver uses the cros-ec interface to communicate with the Chrome OS
+ * EC about counter sensors. Counters are presented through
+ * iio sysfs.
+ */
+
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/iio/buffer.h>
+#include <linux/iio/common/cros_ec_sensors_core.h>
+#include <linux/iio/iio.h>
+#include <linux/iio/kfifo_buf.h>
+#include <linux/iio/trigger.h>
+#include <linux/iio/triggered_buffer.h>
+#include <linux/iio/trigger_consumer.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/platform_data/cros_ec_commands.h>
+#include <linux/platform_data/cros_ec_proto.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+
+/*
+ * One channel for timestamp.
+ */
+#define MAX_CHANNELS 1
+
+/* State data for ec_sensors iio driver. */
+struct cros_ec_sensors_sync_state {
+	/* Shared by all sensors */
+	struct cros_ec_sensors_core_state core;
+
+	struct iio_chan_spec channels[MAX_CHANNELS];
+};
+
+static int cros_ec_sensors_sync_read(struct iio_dev *indio_dev,
+				    struct iio_chan_spec const *chan,
+				    int *val, int *val2, long mask)
+{
+	struct cros_ec_sensors_sync_state *st = iio_priv(indio_dev);
+	int ret;
+
+	mutex_lock(&st->core.cmd_lock);
+	ret = cros_ec_sensors_core_read(&st->core, chan, val, val2, mask);
+	mutex_unlock(&st->core.cmd_lock);
+	return ret;
+}
+
+static int cros_ec_sensors_write(struct iio_dev *indio_dev,
+				 struct iio_chan_spec const *chan,
+				 int val, int val2, long mask)
+{
+	struct cros_ec_sensors_sync_state *st = iio_priv(indio_dev);
+	int ret;
+
+	mutex_lock(&st->core.cmd_lock);
+
+	ret = cros_ec_sensors_core_write(
+			&st->core, chan, val, val2, mask);
+
+	mutex_unlock(&st->core.cmd_lock);
+	return ret;
+}
+
+static const struct iio_info cros_ec_sensors_sync_info = {
+	.read_raw = &cros_ec_sensors_sync_read,
+	.write_raw = &cros_ec_sensors_write,
+	.read_avail = &cros_ec_sensors_core_read_avail,
+};
+
+static int cros_ec_sensors_sync_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct iio_dev *indio_dev;
+	struct cros_ec_sensors_sync_state *state;
+	struct iio_chan_spec *channel;
+	int ret;
+
+	indio_dev = devm_iio_device_alloc(dev, sizeof(*state));
+	if (!indio_dev)
+		return -ENOMEM;
+
+	ret = cros_ec_sensors_core_init(pdev, indio_dev, true,
+			cros_ec_sensors_capture);
+	if (ret)
+		return ret;
+
+	indio_dev->info = &cros_ec_sensors_sync_info;
+	state = iio_priv(indio_dev);
+	/*
+	 * Sync sensor notion of frequencies is either on or off.
+	 * EC reports min and max as 1, that would translate in 1 mHz.
+	 * Force it to 1 (..HZ), more readable.
+	 * For the EC, any frequencies different from 0 means the sync sensor is
+	 * enabled.
+	 */
+	state->core.frequencies[2] = state->core.frequencies[4] = 1;
+	state->core.frequencies[3] = state->core.frequencies[5] = 0;
+
+	channel = state->channels;
+	channel->info_mask_shared_by_all = BIT(IIO_CHAN_INFO_SAMP_FREQ);
+	channel->info_mask_shared_by_all_available =
+		BIT(IIO_CHAN_INFO_SAMP_FREQ);
+	channel->type = IIO_TIMESTAMP;
+	channel->channel = -1;
+	channel->scan_index = 1;
+	channel->scan_type.sign = 's';
+	channel->scan_type.realbits = 64;
+	channel->scan_type.storagebits = 64;
+
+	indio_dev->channels = state->channels;
+	indio_dev->num_channels = MAX_CHANNELS;
+
+	state->core.read_ec_sensors_data = cros_ec_sensors_read_cmd;
+
+	return cros_ec_sensors_core_register(dev, indio_dev, cros_ec_sensors_push_data);
+}
+
+static const struct platform_device_id cros_ec_sensors_sync_ids[] = {
+	{
+		.name = "cros-ec-sync",
+	},
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(platform, cros_ec_sensors_sync_ids);
+
+static struct platform_driver cros_ec_sensors_sync_platform_driver = {
+	.driver = {
+		.name	= "cros-ec-sync",
+	},
+	.probe		= cros_ec_sensors_sync_probe,
+	.id_table	= cros_ec_sensors_sync_ids,
+};
+module_platform_driver(cros_ec_sensors_sync_platform_driver);
+
+MODULE_DESCRIPTION("ChromeOS EC synchronisation sensor driver");
+MODULE_LICENSE("GPL v2");
diff -ruN a/drivers/iio/common/cros_ec_sensors/Kconfig b/drivers/iio/common/cros_ec_sensors/Kconfig
--- a/drivers/iio/common/cros_ec_sensors/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/iio/common/cros_ec_sensors/Kconfig	2021-12-23 08:35:30.000000000 +0100
@@ -30,3 +30,23 @@
 	  convertible devices.
 	  This module is loaded when the EC can calculate the angle between the base
 	  and the lid.
+
+config IIO_CROS_EC_ACTIVITY
+	tristate "ChromeOS EC Activity Sensors"
+	depends on IIO_CROS_EC_SENSORS_CORE
+	help
+	  Module to handle activity events detections presented by the ChromeOS
+	  EC Sensor hub.
+	  Activities can be simple (low/no motion) or more complex (riding train).
+	  They are being reported by physical devices or the EC itself.
+	  Creates an IIO device to manage all activities.
+
+config IIO_CROS_EC_SENSORS_SYNC
+	tristate "ChromeOS EC Counter Sensors"
+	depends on IIO_CROS_EC_SENSORS_CORE
+	help
+	  Module to handle synchronisation sensors presented by the ChromeOS EC
+	  Sensor hub.
+	  Synchronisation sensors are counter sensors that are triggered when
+	  events occurs from other subsystems. They are use to synchronised
+	  those subsystem with existing MEMS sensors, like gyroscope.
diff -ruN a/drivers/iio/common/cros_ec_sensors/Makefile b/drivers/iio/common/cros_ec_sensors/Makefile
--- a/drivers/iio/common/cros_ec_sensors/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/iio/common/cros_ec_sensors/Makefile	2021-12-23 08:35:30.000000000 +0100
@@ -3,6 +3,8 @@
 # Makefile for sensors seen through the ChromeOS EC sensor hub.
 #
 
+obj-$(CONFIG_IIO_CROS_EC_ACTIVITY) += cros_ec_activity.o
 obj-$(CONFIG_IIO_CROS_EC_SENSORS_CORE) += cros_ec_sensors_core.o
 obj-$(CONFIG_IIO_CROS_EC_SENSORS) += cros_ec_sensors.o
 obj-$(CONFIG_IIO_CROS_EC_SENSORS_LID_ANGLE) += cros_ec_lid_angle.o
+obj-$(CONFIG_IIO_CROS_EC_SENSORS_SYNC) += cros_ec_sensors_sync.o
diff -ruN a/drivers/iio/industrialio-core.c b/drivers/iio/industrialio-core.c
--- a/drivers/iio/industrialio-core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/iio/industrialio-core.c	2021-12-23 08:35:30.000000000 +0100
@@ -134,6 +134,7 @@
 	[IIO_MOD_ETHANOL] = "ethanol",
 	[IIO_MOD_H2] = "h2",
 	[IIO_MOD_O2] = "o2",
+	[IIO_MOD_DOUBLE_TAP] = "double_tap",
 };
 
 /* relies on pairs of these shared then separate */
diff -ruN a/drivers/iio/light/cros_ec_light_prox.c b/drivers/iio/light/cros_ec_light_prox.c
--- a/drivers/iio/light/cros_ec_light_prox.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/iio/light/cros_ec_light_prox.c	2021-12-23 08:35:30.000000000 +0100
@@ -17,82 +17,174 @@
 #include <linux/module.h>
 #include <linux/platform_data/cros_ec_commands.h>
 #include <linux/platform_data/cros_ec_proto.h>
+#include <linux/platform_data/cros_ec_sensorhub.h>
 #include <linux/platform_device.h>
 #include <linux/slab.h>
 
 /*
- * We only represent one entry for light or proximity. EC is merging different
- * light sensors to return the what the eye would see. For proximity, we
- * currently support only one light source.
+ * At least We only represent one entry for light or  proximity.
+ * For proximity, we currently support only one light source.
+ * For light we support single sensor or 4 channels (C + RGB).
  */
-#define CROS_EC_LIGHT_PROX_MAX_CHANNELS (1 + 1)
+#define CROS_EC_LIGHT_PROX_MIN_CHANNELS (1 + 1)
 
 /* State data for ec_sensors iio driver. */
 struct cros_ec_light_prox_state {
 	/* Shared by all sensors */
 	struct cros_ec_sensors_core_state core;
+	struct iio_chan_spec *channel;
 
-	struct iio_chan_spec channels[CROS_EC_LIGHT_PROX_MAX_CHANNELS];
+	u16 rgb_space[CROS_EC_SENSOR_MAX_AXIS];
+	struct calib_data rgb_calib[CROS_EC_SENSOR_MAX_AXIS];
 };
 
+static void cros_ec_light_channel_common(struct iio_chan_spec *channel)
+{
+	channel->info_mask_shared_by_all =
+		BIT(IIO_CHAN_INFO_SAMP_FREQ);
+	channel->info_mask_separate =
+		BIT(IIO_CHAN_INFO_RAW) |
+		BIT(IIO_CHAN_INFO_CALIBBIAS) |
+		BIT(IIO_CHAN_INFO_CALIBSCALE);
+	channel->info_mask_shared_by_all_available =
+		BIT(IIO_CHAN_INFO_SAMP_FREQ);
+	channel->scan_type.realbits = CROS_EC_SENSOR_BITS;
+	channel->scan_type.storagebits = CROS_EC_SENSOR_BITS;
+	channel->scan_type.shift = 0;
+	channel->scan_index = 0;
+	channel->ext_info = cros_ec_sensors_ext_info;
+	channel->scan_type.sign = 'u';
+}
+
+static int cros_ec_light_extra_send_host_cmd(
+		struct cros_ec_sensors_core_state *state,
+		int increment,
+		u16 opt_length)
+{
+	uint8_t save_sensor_num = state->param.info.sensor_num;
+	int ret;
+
+	state->param.info.sensor_num += increment;
+	ret = cros_ec_motion_send_host_cmd(state, opt_length);
+	state->param.info.sensor_num = save_sensor_num;
+	return ret;
+}
+
+
+
+static int cros_ec_light_prox_read_data(
+		struct iio_dev *indio_dev,
+		struct iio_chan_spec const *chan,
+		int *val)
+{
+	struct cros_ec_light_prox_state *st = iio_priv(indio_dev);
+	int i, ret;
+	int idx = chan->scan_index;
+
+	st->core.param.cmd = MOTIONSENSE_CMD_DATA;
+
+	/*
+	 * The data coming from the light sensor is
+	 * pre-processed and represents the ambient light
+	 * illuminance reading expressed in lux.
+	 */
+	if (idx == 0) {
+		ret = cros_ec_motion_send_host_cmd(
+				&st->core, sizeof(st->core.resp->data));
+		if (ret)
+			return ret;
+		*val = (u16)st->core.resp->data.data[0];
+	} else {
+		ret = cros_ec_light_extra_send_host_cmd(
+				&st->core, 1, sizeof(st->core.resp->data));
+		if (ret)
+			return ret;
+
+		for (i = 0; i < CROS_EC_SENSOR_MAX_AXIS; i++)
+			st->rgb_space[i] =
+				st->core.resp->data.data[i];
+		*val = st->rgb_space[idx - 1];
+	}
+
+	return IIO_VAL_INT;
+}
+
 static int cros_ec_light_prox_read(struct iio_dev *indio_dev,
 				   struct iio_chan_spec const *chan,
 				   int *val, int *val2, long mask)
 {
 	struct cros_ec_light_prox_state *st = iio_priv(indio_dev);
-	u16 data = 0;
-	s64 val64;
-	int ret;
+	int i, ret = IIO_VAL_INT;
 	int idx = chan->scan_index;
+	s64 val64;
 
 	mutex_lock(&st->core.cmd_lock);
-
 	switch (mask) {
 	case IIO_CHAN_INFO_RAW:
-		if (chan->type == IIO_PROXIMITY) {
-			ret = cros_ec_sensors_read_cmd(indio_dev, 1 << idx,
-						     (s16 *)&data);
-			if (ret)
-				break;
-			*val = data;
-			ret = IIO_VAL_INT;
-		} else {
-			ret = -EINVAL;
-		}
-		break;
 	case IIO_CHAN_INFO_PROCESSED:
-		if (chan->type == IIO_LIGHT) {
-			ret = cros_ec_sensors_read_cmd(indio_dev, 1 << idx,
-						     (s16 *)&data);
-			if (ret)
-				break;
-			/*
-			 * The data coming from the light sensor is
-			 * pre-processed and represents the ambient light
-			 * illuminance reading expressed in lux.
-			 */
-			*val = data;
-			ret = IIO_VAL_INT;
-		} else {
-			ret = -EINVAL;
-		}
+		ret = cros_ec_light_prox_read_data(indio_dev, chan, val);
 		break;
 	case IIO_CHAN_INFO_CALIBBIAS:
 		st->core.param.cmd = MOTIONSENSE_CMD_SENSOR_OFFSET;
 		st->core.param.sensor_offset.flags = 0;
 
-		ret = cros_ec_motion_send_host_cmd(&st->core, 0);
+		if (idx == 0)
+			ret = cros_ec_motion_send_host_cmd(&st->core, 0);
+		else
+			ret = cros_ec_light_extra_send_host_cmd(
+					&st->core, 1, 0);
 		if (ret)
 			break;
-
-		/* Save values */
-		st->core.calib[0].offset =
-			st->core.resp->sensor_offset.offset[0];
-
-		*val = st->core.calib[idx].offset;
+		if (idx == 0) {
+			*val = st->core.calib[0].offset =
+				st->core.resp->sensor_offset.offset[0];
+		} else {
+			for (i = CROS_EC_SENSOR_X; i < CROS_EC_SENSOR_MAX_AXIS;
+			     i++)
+				st->rgb_calib[i].offset =
+					st->core.resp->sensor_offset.offset[i];
+			*val = st->rgb_calib[idx - 1].offset;
+		}
 		ret = IIO_VAL_INT;
 		break;
 	case IIO_CHAN_INFO_CALIBSCALE:
+		if (indio_dev->num_channels > CROS_EC_LIGHT_PROX_MIN_CHANNELS) {
+			u16 scale;
+
+			st->core.param.cmd = MOTIONSENSE_CMD_SENSOR_SCALE;
+			st->core.param.sensor_scale.flags = 0;
+			if (idx == 0)
+				ret = cros_ec_motion_send_host_cmd(
+						&st->core, 0);
+			else
+				ret = cros_ec_light_extra_send_host_cmd(
+						&st->core, 1, 0);
+			if (ret)
+				break;
+			if (idx == 0) {
+				scale = st->core.calib[0].scale =
+					st->core.resp->sensor_scale.scale[0];
+			} else {
+				for (i = CROS_EC_SENSOR_X;
+				     i < CROS_EC_SENSOR_MAX_AXIS;
+				     i++)
+					st->rgb_calib[i].scale =
+					  st->core.resp->sensor_scale.scale[i];
+				scale = st->rgb_calib[idx - 1].scale;
+			}
+			/*
+			 * scale is a number x.y, where x is coded on 1 bit,
+			 * y coded on 15 bits, between 0 and 9999.
+			 */
+			*val = scale >> 15;
+			*val2 = ((scale & 0x7FFF) * 1000000LL) /
+				MOTION_SENSE_DEFAULT_SCALE;
+			ret = IIO_VAL_INT_PLUS_MICRO;
+			break;
+		}
+		/* RANGE is used for calibration in 1 channel sensors. */
+		fallthrough;
+	case IIO_CHAN_INFO_SCALE:
 		/*
 		 * RANGE is used for calibration
 		 * scale is a number x.y, where x is coded on 16 bits,
@@ -126,24 +218,64 @@
 			       int val, int val2, long mask)
 {
 	struct cros_ec_light_prox_state *st = iio_priv(indio_dev);
-	int ret;
+	int ret, i;
 	int idx = chan->scan_index;
 
 	mutex_lock(&st->core.cmd_lock);
 
 	switch (mask) {
 	case IIO_CHAN_INFO_CALIBBIAS:
-		st->core.calib[idx].offset = val;
 		/* Send to EC for each axis, even if not complete */
 		st->core.param.cmd = MOTIONSENSE_CMD_SENSOR_OFFSET;
 		st->core.param.sensor_offset.flags = MOTION_SENSE_SET_OFFSET;
-		st->core.param.sensor_offset.offset[0] =
-			st->core.calib[0].offset;
 		st->core.param.sensor_offset.temp =
 					EC_MOTION_SENSE_INVALID_CALIB_TEMP;
-		ret = cros_ec_motion_send_host_cmd(&st->core, 0);
+		if (idx == 0) {
+			st->core.calib[0].offset = val;
+			st->core.param.sensor_offset.offset[0] = val;
+			ret = cros_ec_motion_send_host_cmd(&st->core, 0);
+		} else {
+			st->rgb_calib[idx - 1].offset = val;
+			for (i = CROS_EC_SENSOR_X;
+			     i < CROS_EC_SENSOR_MAX_AXIS;
+			     i++)
+				st->core.param.sensor_offset.offset[i] =
+					st->rgb_calib[i].offset;
+			ret = cros_ec_light_extra_send_host_cmd(
+					&st->core, 1, 0);
+		}
 		break;
 	case IIO_CHAN_INFO_CALIBSCALE:
+		if (indio_dev->num_channels >
+				CROS_EC_LIGHT_PROX_MIN_CHANNELS) {
+			st->core.param.cmd = MOTIONSENSE_CMD_SENSOR_SCALE;
+			st->core.param.sensor_offset.flags =
+				MOTION_SENSE_SET_OFFSET;
+			st->core.param.sensor_offset.temp =
+				EC_MOTION_SENSE_INVALID_CALIB_TEMP;
+			if (idx == 0) {
+				st->core.calib[0].scale = val;
+				st->core.param.sensor_scale.scale[0] = val;
+				ret = cros_ec_motion_send_host_cmd(
+						&st->core, 0);
+			} else {
+				st->rgb_calib[idx - 1].scale = val;
+				for (i = CROS_EC_SENSOR_X;
+				     i < CROS_EC_SENSOR_MAX_AXIS;
+				     i++)
+					st->core.param.sensor_scale.scale[i] =
+						st->rgb_calib[i].scale;
+				ret = cros_ec_light_extra_send_host_cmd(
+						&st->core, 1, 0);
+			}
+			break;
+		}
+		/*
+		 * For sensors with only one channel, _RANGE is used
+		 * instead of _SCALE.
+		 */
+		fallthrough;
+	case IIO_CHAN_INFO_SCALE:
 		st->core.param.cmd = MOTIONSENSE_CMD_SENSOR_RANGE;
 		st->core.curr_range = (val << 16) | (val2 / 100);
 		st->core.param.sensor_range.data = st->core.curr_range;
@@ -151,6 +283,16 @@
 		if (ret == 0)
 			st->core.range_updated = true;
 		break;
+	case IIO_CHAN_INFO_SAMP_FREQ:
+		ret = cros_ec_sensors_core_write(&st->core, chan, val, val2,
+						 mask);
+		/* Repeat the same command to the RGB sensor. */
+		if (!ret && indio_dev->num_channels >
+			    CROS_EC_LIGHT_PROX_MIN_CHANNELS)
+			ret = cros_ec_light_extra_send_host_cmd(
+					&st->core, 1, 0);
+
+		break;
 	default:
 		ret = cros_ec_sensors_core_write(&st->core, chan, val, val2,
 						 mask);
@@ -162,26 +304,132 @@
 	return ret;
 }
 
+static int cros_ec_light_push_data(
+		struct iio_dev *indio_dev,
+		s16 *data,
+		s64 timestamp)
+{
+	struct cros_ec_sensors_core_state *st = iio_priv(indio_dev);
+	s16 *out = (s16 *)st->samples;
+
+	if (!st || !indio_dev->active_scan_mask)
+		return 0;
+
+	/* Save clear channel, will be used when RGB data arrives. */
+	if (test_bit(0, indio_dev->active_scan_mask))
+		*out = data[0];
+
+	/* Wait for RGB callback to send samples upstream. */
+	return 0;
+}
+
+static int cros_ec_light_push_data_rgb(
+		struct iio_dev *indio_dev,
+		s16 *data,
+		s64 timestamp)
+{
+	struct cros_ec_sensors_core_state *st = iio_priv(indio_dev);
+	s16 *out;
+	s64 delta;
+	unsigned int i = 1;
+
+	if (!st || !indio_dev->active_scan_mask)
+		return 0;
+
+	/*
+	 * Send all data needed.
+	 */
+	out = (s16 *)st->samples;
+	if (test_bit(0, indio_dev->active_scan_mask))
+		out++;
+
+	for_each_set_bit_from(i,
+			 indio_dev->active_scan_mask,
+			 indio_dev->masklength) {
+		*out = data[i - 1];
+		out++;
+	}
+
+	if (iio_device_get_clock(indio_dev) != CLOCK_BOOTTIME)
+		delta = iio_get_time_ns(indio_dev) - cros_ec_get_time_ns();
+	else
+		delta = 0;
+
+	iio_push_to_buffers_with_timestamp(indio_dev, st->samples,
+					   timestamp + delta);
+	return 0;
+}
+
+static irqreturn_t cros_ec_light_capture(int irq, void *p)
+{
+	struct iio_poll_func *pf = p;
+	struct iio_dev *indio_dev = pf->indio_dev;
+	struct cros_ec_sensors_core_state *st = iio_priv(indio_dev);
+	int ret, i, idx = 0;
+	s16 data = 0;
+	const unsigned long scan_mask = *(indio_dev->active_scan_mask);
+
+	mutex_lock(&st->cmd_lock);
+
+	/* Clear capture data. */
+	memset(st->samples, 0, indio_dev->scan_bytes);
+
+	/* Read first channel. */
+	ret = cros_ec_sensors_read_cmd(indio_dev, 1, &data);
+	if (ret < 0)
+		goto done;
+	if (test_bit(0, indio_dev->active_scan_mask))
+		((s16 *)st->samples)[idx++] = data;
+
+	/* Read remaining channels. */
+	if (scan_mask & ((1 << indio_dev->num_channels) - 2)) {
+		ret = cros_ec_light_extra_send_host_cmd(
+				st, 1, sizeof(st->resp->data));
+		if (ret < 0)
+			goto done;
+		for (i = 0; i < CROS_EC_SENSOR_MAX_AXIS; i++)
+			if (test_bit(i + 1, indio_dev->active_scan_mask))
+				((s16 *)st->samples)[idx++] =
+					st->resp->data.data[i];
+	}
+
+	iio_push_to_buffers_with_timestamp(indio_dev, st->samples,
+					   iio_get_time_ns(indio_dev));
+
+done:
+	/*
+	 * Tell the core we are done with this trigger and ready for the
+	 * next one.
+	 */
+	iio_trigger_notify_done(indio_dev->trig);
+
+	mutex_unlock(&st->cmd_lock);
+
+	return IRQ_HANDLED;
+}
+
 static const struct iio_info cros_ec_light_prox_info = {
 	.read_raw = &cros_ec_light_prox_read,
 	.write_raw = &cros_ec_light_prox_write,
 	.read_avail = &cros_ec_sensors_core_read_avail,
 };
 
+
 static int cros_ec_light_prox_probe(struct platform_device *pdev)
 {
 	struct device *dev = &pdev->dev;
+	struct cros_ec_sensorhub *sensor_hub = dev_get_drvdata(dev->parent);
 	struct iio_dev *indio_dev;
 	struct cros_ec_light_prox_state *state;
 	struct iio_chan_spec *channel;
-	int ret;
+	int ret, i, num_channels = CROS_EC_LIGHT_PROX_MIN_CHANNELS;
 
 	indio_dev = devm_iio_device_alloc(dev, sizeof(*state));
 	if (!indio_dev)
 		return -ENOMEM;
 
 	ret = cros_ec_sensors_core_init(pdev, indio_dev, true,
-					cros_ec_sensors_capture);
+					cros_ec_light_capture);
 	if (ret)
 		return ret;
 
@@ -190,54 +438,80 @@
 	state = iio_priv(indio_dev);
 	state->core.type = state->core.resp->info.type;
 	state->core.loc = state->core.resp->info.location;
-	channel = state->channels;
 
-	/* Common part */
-	channel->info_mask_shared_by_all =
-		BIT(IIO_CHAN_INFO_SAMP_FREQ);
-	channel->info_mask_shared_by_all_available =
-		BIT(IIO_CHAN_INFO_SAMP_FREQ);
-	channel->scan_type.realbits = CROS_EC_SENSOR_BITS;
-	channel->scan_type.storagebits = CROS_EC_SENSOR_BITS;
-	channel->scan_type.shift = 0;
-	channel->scan_index = 0;
-	channel->ext_info = cros_ec_sensors_ext_info;
-	channel->scan_type.sign = 'u';
+	/* Check if we need more sensors for RGB (or XYZ). */
+	state->core.param.cmd = MOTIONSENSE_CMD_INFO;
+	if (cros_ec_light_extra_send_host_cmd(&state->core, 1, 0) == 0 &&
+	    state->core.resp->info.type == MOTIONSENSE_TYPE_LIGHT_RGB)
+		num_channels += CROS_EC_SENSOR_MAX_AXIS;
 
+	channel = devm_kcalloc(dev, num_channels, sizeof(*channel), 0);
+	if (channel == NULL)
+		return -ENOMEM;
+
+	indio_dev->channels = channel;
+	indio_dev->num_channels = num_channels;
+
+	cros_ec_light_channel_common(channel);
 	/* Sensor specific */
 	switch (state->core.type) {
 	case MOTIONSENSE_TYPE_LIGHT:
 		channel->type = IIO_LIGHT;
-		channel->info_mask_separate =
-			BIT(IIO_CHAN_INFO_PROCESSED) |
-			BIT(IIO_CHAN_INFO_CALIBBIAS) |
-			BIT(IIO_CHAN_INFO_CALIBSCALE);
+		if (num_channels < CROS_EC_LIGHT_PROX_MIN_CHANNELS +
+				CROS_EC_SENSOR_MAX_AXIS) {
+			/* For backward compatibility. */
+			channel->info_mask_separate =
+				BIT(IIO_CHAN_INFO_PROCESSED) |
+				BIT(IIO_CHAN_INFO_CALIBBIAS) |
+				BIT(IIO_CHAN_INFO_CALIBSCALE);
+		} else {
+			/*
+			 * To set a global scale, as CALIB_SCALE for RGB sensor
+			 * is limited between 0 and 2.
+			 */
+			channel->info_mask_shared_by_all |=
+				BIT(IIO_CHAN_INFO_SCALE);
+		}
 		break;
 	case MOTIONSENSE_TYPE_PROX:
 		channel->type = IIO_PROXIMITY;
-		channel->info_mask_separate =
-			BIT(IIO_CHAN_INFO_RAW) |
-			BIT(IIO_CHAN_INFO_CALIBBIAS) |
-			BIT(IIO_CHAN_INFO_CALIBSCALE);
 		break;
 	default:
 		dev_warn(dev, "Unknown motion sensor\n");
 		return -EINVAL;
 	}
+	channel++;
+
+	if (num_channels > CROS_EC_LIGHT_PROX_MIN_CHANNELS) {
+		u8 sensor_num = state->core.param.info.sensor_num;
+
+		for (i = CROS_EC_SENSOR_X; i < CROS_EC_SENSOR_MAX_AXIS;
+				i++, channel++) {
+			cros_ec_light_channel_common(channel);
+			channel->scan_index = i + 1;
+			channel->modified = 1;
+			channel->channel2 = IIO_MOD_LIGHT_RED + i;
+			channel->type = IIO_LIGHT;
+		}
+		cros_ec_sensorhub_unregister_push_data(sensor_hub, sensor_num);
+		cros_ec_sensorhub_register_push_data(
+				sensor_hub, sensor_num,
+				indio_dev,
+				cros_ec_light_push_data);
+		cros_ec_sensorhub_register_push_data(
+				sensor_hub, sensor_num + 1,
+				indio_dev,
+				cros_ec_light_push_data_rgb);
+	}
 
 	/* Timestamp */
-	channel++;
 	channel->type = IIO_TIMESTAMP;
 	channel->channel = -1;
-	channel->scan_index = 1;
+	channel->scan_index = num_channels - 1;
 	channel->scan_type.sign = 's';
 	channel->scan_type.realbits = 64;
 	channel->scan_type.storagebits = 64;
 
-	indio_dev->channels = state->channels;
-
-	indio_dev->num_channels = CROS_EC_LIGHT_PROX_MAX_CHANNELS;
-
 	state->core.read_ec_sensors_data = cros_ec_sensors_read_cmd;
 
 	return devm_iio_device_register(dev, indio_dev);
diff -ruN a/drivers/input/keyboard/cros_ec_keyb.c b/drivers/input/keyboard/cros_ec_keyb.c
--- a/drivers/input/keyboard/cros_ec_keyb.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/input/keyboard/cros_ec_keyb.c	2021-12-23 08:35:31.000000000 +0100
@@ -12,6 +12,7 @@
 // expensive.
 
 #include <linux/module.h>
+#include <linux/acpi.h>
 #include <linux/bitops.h>
 #include <linux/i2c.h>
 #include <linux/input.h>
@@ -60,8 +61,12 @@
 	struct device *dev;
 	struct cros_ec_device *ec;
 
+	/* Keyboard input device */
 	struct input_dev *idev;
+	/* BUttons and switches input device */
 	struct input_dev *bs_idev;
+	u32 switch_map;
+	u32 button_map;
 	struct notifier_block notifier;
 
 	u16 function_row_physmap[MAX_NUM_TOP_ROW_KEYS];
@@ -118,6 +123,10 @@
 	},
 };
 
+typedef void (*cros_ec_keyb_work_fn)(struct cros_ec_keyb *ckdev,
+				     const struct cros_ec_bs_map *map,
+				     u32 mask);
+
 /*
  * Returns true when there is at least one combination of pressed keys that
  * results in ghosting.
@@ -203,7 +212,30 @@
 }
 
 /**
- * cros_ec_keyb_report_bs - Report non-matrixed buttons or switches
+ * cros_ec_keyb_bs_event - Report a given button switch event
+ */
+void cros_ec_keyb_bs_event(struct cros_ec_keyb *ckdev,
+			   const struct cros_ec_bs_map *map,
+			   u32 mask)
+{
+	input_event(ckdev->bs_idev, map->ev_type, map->code,
+		    !!(mask & BIT(map->bit)) ^ map->inverted);
+}
+
+
+/**
+ * cros_ec_keyb_bs_set - Set capability for a given button switch
+ */
+void cros_ec_keyb_bs_set(struct cros_ec_keyb *ckdev,
+			 const struct cros_ec_bs_map *map,
+			 u32 mask)
+{
+	if (mask & BIT(map->bit))
+		input_set_capability(ckdev->bs_idev, map->ev_type, map->code);
+}
+
+/**
+ * cros_ec_keyb_walker_bs - Report non-matrixed buttons or switches
  *
  * This takes a bitmap of buttons or switches from the EC and reports events,
  * syncing at the end.
@@ -212,11 +244,11 @@
  * @ev_type: The input event type (e.g., EV_KEY).
  * @mask: A bitmap of buttons from the EC.
  */
-static void cros_ec_keyb_report_bs(struct cros_ec_keyb *ckdev,
-				   unsigned int ev_type, u32 mask)
+static void cros_ec_keyb_walker_bs(struct cros_ec_keyb *ckdev,
+				   unsigned int ev_type, u32 mask,
+				   cros_ec_keyb_work_fn work_fn)
 
 {
-	struct input_dev *idev = ckdev->bs_idev;
 	int i;
 
 	for (i = 0; i < ARRAY_SIZE(cros_ec_keyb_bs); i++) {
@@ -224,11 +256,9 @@
 
 		if (map->ev_type != ev_type)
 			continue;
-
-		input_event(idev, ev_type, map->code,
-			    !!(mask & BIT(map->bit)) ^ map->inverted);
+		work_fn(ckdev, map, mask);
 	}
-	input_sync(idev);
+	input_sync(ckdev->bs_idev);
 }
 
 static int cros_ec_keyb_work(struct notifier_block *nb,
@@ -283,7 +313,8 @@
 					&ckdev->ec->event_data.data.switches);
 			ev_type = EV_SW;
 		}
-		cros_ec_keyb_report_bs(ckdev, ev_type, val);
+		cros_ec_keyb_walker_bs(ckdev, ev_type, val,
+				       cros_ec_keyb_bs_event);
 		break;
 
 	default:
@@ -397,14 +428,18 @@
 	union ec_response_get_next_data event_data = {};
 	int ret;
 
+	if (!ckdev->switch_map)
+		return 0;
+
 	ret = cros_ec_keyb_info(ec_dev, EC_MKBP_INFO_CURRENT,
 				EC_MKBP_EVENT_SWITCH, &event_data,
 				sizeof(event_data.switches));
 	if (ret)
 		return ret;
 
-	cros_ec_keyb_report_bs(ckdev, EV_SW,
-			       get_unaligned_le32(&event_data.switches));
+	cros_ec_keyb_walker_bs(ckdev, EV_SW,
+			       get_unaligned_le32(&event_data.switches),
+			       cros_ec_keyb_bs_event);
 
 	return 0;
 }
@@ -420,12 +455,7 @@
  */
 static __maybe_unused int cros_ec_keyb_resume(struct device *dev)
 {
-	struct cros_ec_keyb *ckdev = dev_get_drvdata(dev);
-
-	if (ckdev->bs_idev)
-		return cros_ec_keyb_query_switches(ckdev);
-
-	return 0;
+	return cros_ec_keyb_query_switches(dev_get_drvdata(dev));
 }
 
 /**
@@ -449,26 +479,23 @@
 	struct input_dev *idev;
 	union ec_response_get_next_data event_data = {};
 	const char *phys;
-	u32 buttons;
-	u32 switches;
 	int ret;
-	int i;
 
 	ret = cros_ec_keyb_info(ec_dev, EC_MKBP_INFO_SUPPORTED,
 				EC_MKBP_EVENT_BUTTON, &event_data,
 				sizeof(event_data.buttons));
 	if (ret)
 		return ret;
-	buttons = get_unaligned_le32(&event_data.buttons);
+	ckdev->button_map = get_unaligned_le32(&event_data.buttons);
 
 	ret = cros_ec_keyb_info(ec_dev, EC_MKBP_INFO_SUPPORTED,
 				EC_MKBP_EVENT_SWITCH, &event_data,
 				sizeof(event_data.switches));
 	if (ret)
 		return ret;
-	switches = get_unaligned_le32(&event_data.switches);
+	ckdev->switch_map = get_unaligned_le32(&event_data.switches);
 
-	if (!buttons && !switches)
+	if (!ckdev->button_map && !ckdev->switch_map)
 		return 0;
 
 	/*
@@ -496,31 +523,27 @@
 	input_set_drvdata(idev, ckdev);
 	ckdev->bs_idev = idev;
 
-	for (i = 0; i < ARRAY_SIZE(cros_ec_keyb_bs); i++) {
-		const struct cros_ec_bs_map *map = &cros_ec_keyb_bs[i];
-
-		if ((map->ev_type == EV_KEY && (buttons & BIT(map->bit))) ||
-		    (map->ev_type == EV_SW && (switches & BIT(map->bit))))
-			input_set_capability(idev, map->ev_type, map->code);
-	}
+	cros_ec_keyb_walker_bs(ckdev, EV_KEY, ckdev->button_map,
+			cros_ec_keyb_bs_set);
+	cros_ec_keyb_walker_bs(ckdev, EV_SW, ckdev->switch_map,
+			cros_ec_keyb_bs_set);
 
-	ret = cros_ec_keyb_query_switches(ckdev);
+	ret = input_register_device(ckdev->bs_idev);
 	if (ret) {
-		dev_err(dev, "cannot query switches\n");
+		dev_err(dev, "cannot register input device\n");
 		return ret;
 	}
 
-	ret = input_register_device(ckdev->bs_idev);
+	ret = cros_ec_keyb_query_switches(ckdev);
 	if (ret) {
-		dev_err(dev, "cannot register input device\n");
+		dev_err(dev, "cannot query switches\n");
 		return ret;
 	}
-
 	return 0;
 }
 
 /**
- * cros_ec_keyb_register_bs - Register matrix keys
+ * cros_ec_keyb_register_matrix - Register matrix keys
  *
  * Handles all the bits of the keyboard driver related to matrix keys.
  *
@@ -667,8 +690,12 @@
 	struct cros_ec_keyb *ckdev;
 	int err;
 
-	if (!dev->of_node)
-		return -ENODEV;
+	/*
+	 * If the parent ec device has not been probed yet, defer the probe of
+	 * this keyboard/button driver until later.
+	 */
+	if (ec == NULL)
+		return -EPROBE_DEFER;
 
 	ckdev = devm_kzalloc(dev, sizeof(*ckdev), GFP_KERNEL);
 	if (!ckdev)
@@ -678,10 +705,12 @@
 	ckdev->dev = dev;
 	dev_set_drvdata(dev, ckdev);
 
-	err = cros_ec_keyb_register_matrix(ckdev);
-	if (err) {
-		dev_err(dev, "cannot register matrix inputs: %d\n", err);
-		return err;
+	if (dev->of_node) {
+		err = cros_ec_keyb_register_matrix(ckdev);
+		if (err) {
+			dev_err(dev, "register matrix inputs error: %d\n", err);
+			return err;
+		}
 	}
 
 	err = cros_ec_keyb_register_bs(ckdev);
@@ -718,6 +747,14 @@
 	return 0;
 }
 
+#ifdef CONFIG_ACPI
+static const struct acpi_device_id cros_ec_keyb_acpi_match[] = {
+	{ "GOOG0007", 0 },
+	{ }
+};
+MODULE_DEVICE_TABLE(acpi, cros_ec_keyb_acpi_match);
+#endif
+
 #ifdef CONFIG_OF
 static const struct of_device_id cros_ec_keyb_of_match[] = {
 	{ .compatible = "google,cros-ec-keyb" },
@@ -734,6 +771,7 @@
 	.driver = {
 		.name = "cros-ec-keyb",
 		.of_match_table = of_match_ptr(cros_ec_keyb_of_match),
+		.acpi_match_table = ACPI_PTR(cros_ec_keyb_acpi_match),
 		.pm = &cros_ec_keyb_pm_ops,
 	},
 };
diff -ruN a/drivers/input/touchscreen/elants_i2c.c b/drivers/input/touchscreen/elants_i2c.c
--- a/drivers/input/touchscreen/elants_i2c.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/input/touchscreen/elants_i2c.c	2021-12-23 08:35:31.000000000 +0100
@@ -172,6 +172,8 @@
 
 	/* Must be last to be used for DMA operations */
 	u8 buf[MAX_PACKET_SIZE] ____cacheline_aligned;
+
+	bool unbinding;
 };
 
 static int elants_i2c_send(struct i2c_client *client,
@@ -1324,6 +1326,12 @@
 {
 	struct elants_data *ts = _data;
 
+	if (ts->unbinding) {
+		dev_info(&ts->client->dev,
+			 "Not disabling regulators to continue allowing userspace i2c-dev access\n");
+		return;
+	}
+
 	if (!IS_ERR_OR_NULL(ts->reset_gpio)) {
 		/*
 		 * Activate reset gpio to prevent leakage through the
@@ -1540,6 +1548,19 @@
 	return 0;
 }
 
+static int elants_i2c_remove(struct i2c_client *client)
+{
+	struct elants_data *ts = i2c_get_clientdata(client);
+
+	/*
+	 * Let elants_i2c_power_off know that it needs to keep
+	 * regulators on.
+	 */
+	ts->unbinding = true;
+
+	return 0;
+}
+
 static int __maybe_unused elants_i2c_suspend(struct device *dev)
 {
 	struct i2c_client *client = to_i2c_client(dev);
@@ -1644,6 +1665,7 @@
 
 static struct i2c_driver elants_i2c_driver = {
 	.probe_new = elants_i2c_probe,
+	.remove = elants_i2c_remove,
 	.id_table = elants_i2c_id,
 	.driver = {
 		.name = DEVICE_NAME,
diff -ruN a/drivers/irqchip/irq-gic-v3.c b/drivers/irqchip/irq-gic-v3.c
--- a/drivers/irqchip/irq-gic-v3.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/irqchip/irq-gic-v3.c	2021-12-23 08:35:31.000000000 +0100
@@ -18,6 +18,8 @@
 #include <linux/percpu.h>
 #include <linux/refcount.h>
 #include <linux/slab.h>
+#include <linux/wakeup_reason.h>
+
 
 #include <linux/irqchip.h>
 #include <linux/irqchip/arm-gic-common.h>
@@ -730,6 +732,7 @@
 
 	if (handle_domain_irq(gic_data.domain, irqnr, regs)) {
 		WARN_ONCE(true, "Unexpected interrupt received!\n");
+		log_abnormal_wakeup_reason("unexpected HW IRQ %u", irqnr);
 		gic_deactivate_unhandled(irqnr);
 	}
 }
diff -ruN a/drivers/Kconfig b/drivers/Kconfig
--- a/drivers/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/Kconfig	2021-12-23 08:35:15.000000000 +0100
@@ -236,4 +236,7 @@
 source "drivers/counter/Kconfig"
 
 source "drivers/most/Kconfig"
+
+source "drivers/pkglist/Kconfig"
+
 endmenu
diff -ruN a/drivers/mailbox/mtk-cmdq-mailbox.c b/drivers/mailbox/mtk-cmdq-mailbox.c
--- a/drivers/mailbox/mtk-cmdq-mailbox.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/mailbox/mtk-cmdq-mailbox.c	2021-12-23 08:35:31.000000000 +0100
@@ -36,6 +36,7 @@
 #define CMDQ_THR_END_ADDR		0x24
 #define CMDQ_THR_WAIT_TOKEN		0x30
 #define CMDQ_THR_PRIORITY		0x40
+#define CMDQ_THR_INSTN_TIMEOUT_CYCLES	0x50
 
 #define GCE_GCTL_VALUE			0x48
 
@@ -54,6 +55,15 @@
 #define CMDQ_JUMP_BY_OFFSET		0x10000000
 #define CMDQ_JUMP_BY_PA			0x10000001
 
+/*
+ * instruction time-out
+ * cycles to issue instruction time-out interrupt for wait and poll instructions
+ * GCE axi_clock 156MHz
+ * 1 cycle = 6.41ns
+ * instruction time out 2^22*2*6.41ns = 53ms
+ */
+#define CMDQ_INSTN_TIMEOUT_CYCLES	22
+
 struct cmdq_thread {
 	struct mbox_chan	*chan;
 	void __iomem		*base;
@@ -375,6 +385,7 @@
 		writel((task->pa_base + pkt->cmd_buf_size) >> cmdq->shift_pa,
 		       thread->base + CMDQ_THR_END_ADDR);
 
+		writel(CMDQ_INSTN_TIMEOUT_CYCLES, thread->base + CMDQ_THR_INSTN_TIMEOUT_CYCLES);
 		writel(thread->priority, thread->base + CMDQ_THR_PRIORITY);
 		writel(CMDQ_THR_IRQ_EN, thread->base + CMDQ_THR_IRQ_ENABLE);
 		writel(CMDQ_THR_ENABLED, thread->base + CMDQ_THR_ENABLE_TASK);
diff -ruN a/drivers/Makefile b/drivers/Makefile
--- a/drivers/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/Makefile	2021-12-23 08:35:15.000000000 +0100
@@ -188,3 +188,5 @@
 obj-$(CONFIG_INTERCONNECT)	+= interconnect/
 obj-$(CONFIG_COUNTER)		+= counter/
 obj-$(CONFIG_MOST)		+= most/
+
+obj-$(CONFIG_PKGLIST)		+= pkglist/
diff -ruN a/drivers/md/dm-init.c b/drivers/md/dm-init.c
--- a/drivers/md/dm-init.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/md/dm-init.c	2021-12-23 08:35:32.000000000 +0100
@@ -301,3 +301,254 @@
 
 module_param(create, charp, 0);
 MODULE_PARM_DESC(create, "Create a mapped device in early boot");
+
+/* ---------------------------------------------------------------
+ * ChromeOS shim - convert dm= format to dm-mod.create= format
+ * ---------------------------------------------------------------
+ */
+
+struct dm_chrome_target {
+	char *field[4];
+};
+
+struct dm_chrome_dev {
+	char *name, *uuid, *mode;
+	unsigned int num_targets;
+	struct dm_chrome_target targets[DM_MAX_TARGETS];
+};
+
+static char __init *dm_chrome_parse_target(char *str, struct dm_chrome_target *tgt)
+{
+	unsigned int i;
+
+	tgt->field[0] = str;
+	/* Delimit first 3 fields that are separated by space */
+	for (i = 0; i < ARRAY_SIZE(tgt->field) - 1; i++) {
+		tgt->field[i + 1] = str_field_delimit(&tgt->field[i], ' ');
+		if (!tgt->field[i + 1])
+			return NULL;
+	}
+	/* Delimit last field that can be terminated by comma */
+	return str_field_delimit(&tgt->field[i], ',');
+}
+
+static char __init *dm_chrome_parse_dev(char *str, struct dm_chrome_dev *dev)
+{
+	char *target, *num;
+	unsigned int i;
+
+	if (!str)
+		return ERR_PTR(-EINVAL);
+
+	target = str_field_delimit(&str, ',');
+	if (!target)
+		return ERR_PTR(-EINVAL);
+
+	/* Delimit first 3 fields that are separated by space */
+	dev->name = str;
+	dev->uuid = str_field_delimit(&dev->name, ' ');
+	if (!dev->uuid)
+		return ERR_PTR(-EINVAL);
+
+	dev->mode = str_field_delimit(&dev->uuid, ' ');
+	if (!dev->mode)
+		return ERR_PTR(-EINVAL);
+
+	/* num is optional */
+	num = str_field_delimit(&dev->mode, ' ');
+	if (!num)
+		dev->num_targets = 1;
+	else {
+		/* Delimit num and check if it the last field */
+		if(str_field_delimit(&num, ' '))
+			return ERR_PTR(-EINVAL);
+		if (kstrtouint(num, 0, &dev->num_targets))
+			return ERR_PTR(-EINVAL);
+	}
+
+	if (dev->num_targets > DM_MAX_TARGETS) {
+		DMERR("too many targets %u > %d",
+		      dev->num_targets, DM_MAX_TARGETS);
+		return ERR_PTR(-EINVAL);
+	}
+
+	for (i = 0; i < dev->num_targets - 1; i++) {
+		target = dm_chrome_parse_target(target, &dev->targets[i]);
+		if (!target)
+			return ERR_PTR(-EINVAL);
+	}
+	/* The last one can return NULL if it reaches the end of str */
+	return dm_chrome_parse_target(target, &dev->targets[i]);
+}
+
+static char __init *dm_chrome_convert(struct dm_chrome_dev *devs, unsigned int num_devs)
+{
+	char *str = kmalloc(DM_MAX_STR_SIZE, GFP_KERNEL);
+	char *p = str;
+	unsigned int i, j;
+	int ret;
+
+	if (!str)
+		return ERR_PTR(-ENOMEM);
+
+	for (i = 0; i < num_devs; i++) {
+		if (!strcmp(devs[i].uuid, "none"))
+			devs[i].uuid = "";
+		ret = snprintf(p, DM_MAX_STR_SIZE - (p - str),
+			       "%s,%s,,%s",
+			       devs[i].name,
+			       devs[i].uuid,
+			       devs[i].mode);
+		if (ret < 0)
+			goto out;
+		p += ret;
+
+		for (j = 0; j < devs[i].num_targets; j++) {
+			ret = snprintf(p, DM_MAX_STR_SIZE - (p - str),
+				       ",%s %s %s %s",
+				       devs[i].targets[j].field[0],
+				       devs[i].targets[j].field[1],
+				       devs[i].targets[j].field[2],
+				       devs[i].targets[j].field[3]);
+			if (ret < 0)
+				goto out;
+			p += ret;
+		}
+		if (i < num_devs - 1) {
+			ret = snprintf(p, DM_MAX_STR_SIZE - (p - str), ";");
+			if (ret < 0)
+				goto out;
+			p += ret;
+		}
+	}
+
+	return str;
+
+out:
+	kfree(str);
+	return ERR_PTR(ret);
+}
+
+/**
+ * dm_chrome_shim - convert old dm= format used in chromeos to the new
+ * upstream format.
+ *
+ * ChromeOS old format
+ * -------------------
+ * <device>        ::= [<num>] <device-mapper>+
+ * <device-mapper> ::= <head> "," <target>+
+ * <head>          ::= <name> <uuid> <mode> [<num>]
+ * <target>        ::= <start> <length> <type> <options> ","
+ * <mode>          ::= "ro" | "rw"
+ * <uuid>          ::= xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx | "none"
+ * <type>          ::= "verity" | "bootcache" | ...
+ *
+ * Example:
+ * 2 vboot none ro 1,
+ *     0 1768000 bootcache
+ *       device=aa55b119-2a47-8c45-946a-5ac57765011f+1
+ *       signature=76e9be054b15884a9fa85973e9cb274c93afadb6
+ *       cache_start=1768000 max_blocks=100000 size_limit=23 max_trace=20000,
+ *   vroot none ro 1,
+ *     0 1740800 verity payload=254:0 hashtree=254:0 hashstart=1740800 alg=sha1
+ *       root_hexdigest=76e9be054b15884a9fa85973e9cb274c93afadb6
+ *       salt=5b3549d54d6c7a3837b9b81ed72e49463a64c03680c47835bef94d768e5646fe
+ *
+ * Notes:
+ *  1. uuid is a label for the device and we set it to "none".
+ *  2. The <num> field will be optional initially and assumed to be 1.
+ *     Once all the scripts that set these fields have been set, it will
+ *     be made mandatory.
+ */
+
+static char *chrome_create;
+
+static int __init dm_chrome_shim(char *arg) {
+	if (!arg || create)
+		return -EINVAL;
+	chrome_create = arg;
+	return 0;
+}
+
+static int __init dm_chrome_parse_devices(void)
+{
+	struct dm_chrome_dev *devs;
+	unsigned int num_devs, i;
+	char *next, *base_str;
+	int ret = 0;
+
+	/* Verify if dm-mod.create was not used */
+	if (!chrome_create || create)
+		return -EINVAL;
+
+	if (strlen(chrome_create) >= DM_MAX_STR_SIZE) {
+		DMERR("Argument is too big. Limit is %d\n", DM_MAX_STR_SIZE);
+		return -EINVAL;
+	}
+
+	base_str = kstrdup(chrome_create, GFP_KERNEL);
+	if (!base_str)
+		return -ENOMEM;
+
+	next = str_field_delimit(&base_str, ' ');
+	if (!next) {
+		ret = -EINVAL;
+		goto out_str;
+	}
+
+	/* if first field is not the optional <num> field */
+	if (kstrtouint(base_str, 0, &num_devs)) {
+		num_devs = 1;
+		/* rewind next pointer */
+		next = base_str;
+	}
+
+	if (num_devs > DM_MAX_DEVICES) {
+		DMERR("too many devices %u > %d", num_devs, DM_MAX_DEVICES);
+		ret = -EINVAL;
+		goto out_str;
+	}
+
+	devs = kcalloc(num_devs, sizeof(*devs), GFP_KERNEL);
+	if (!devs)
+		return -ENOMEM;
+
+	/* restore string */
+	strcpy(base_str, chrome_create);
+
+	/* parse devices */
+	for (i = 0; i < num_devs; i++) {
+		next = dm_chrome_parse_dev(next, &devs[i]);
+		if (IS_ERR(next)) {
+			DMERR("couldn't parse device");
+			ret = PTR_ERR(next);
+			goto out_devs;
+		}
+	}
+
+	create = dm_chrome_convert(devs, num_devs);
+	if (IS_ERR(create)) {
+		ret = PTR_ERR(create);
+		goto out_devs;
+	}
+
+	DMDEBUG("Converting:\n\tdm=\"%s\"\n\tdm-mod.create=\"%s\"\n",
+		chrome_create, create);
+
+	/* Call upstream code */
+	dm_init_init();
+
+	kfree(create);
+
+out_devs:
+	create = NULL;
+	kfree(devs);
+out_str:
+	kfree(base_str);
+
+	return ret;
+}
+
+late_initcall(dm_chrome_parse_devices);
+
+__setup("dm=", dm_chrome_shim);
diff -ruN a/drivers/md/dm-verity-chromeos.c b/drivers/md/dm-verity-chromeos.c
--- a/drivers/md/dm-verity-chromeos.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/md/dm-verity-chromeos.c	2021-12-23 08:35:32.000000000 +0100
@@ -0,0 +1,304 @@
+/*
+ * Copyright (C) 2010 The Chromium OS Authors <chromium-os-dev@chromium.org>
+ *                    All Rights Reserved.
+ *
+ * This file is released under the GPL.
+ */
+/*
+ * Implements a Chrome OS platform specific error handler.
+ * When verity detects an invalid block, this error handling will
+ * attempt to corrupt the kernel boot image. On reboot, the bios will
+ * detect the kernel corruption and switch to the alternate kernel
+ * and root file system partitions.
+ *
+ * Assumptions:
+ * 1. Partitions are specified on the command line using uuid.
+ * 2. The kernel partition is the partition number is one less
+ *    than the root partition number.
+ */
+#include <linux/bio.h>
+#include <linux/blkdev.h>
+#include <linux/chromeos_platform.h>
+#include <linux/device.h>
+#include <linux/device-mapper.h>
+#include <linux/err.h>
+#include <linux/genhd.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mount.h>
+#include <linux/notifier.h>
+#include <linux/string.h>
+#include <asm/page.h>
+
+#include "dm-verity.h"
+
+#define DM_MSG_PREFIX "verity-chromeos"
+#define DMVERROR "DMVERROR"
+
+static void chromeos_invalidate_kernel_endio(struct bio *bio)
+{
+	if (bio->bi_status) {
+		DMERR("%s: bio operation failed (status=0x%x)", __func__,
+		      bio->bi_status);
+		chromeos_set_need_recovery();
+	}
+	complete(bio->bi_private);
+}
+
+static int chromeos_invalidate_kernel_submit(struct bio *bio,
+					     struct block_device *bdev,
+					     unsigned int op,
+					     unsigned int op_flags,
+					     struct page *page)
+{
+	DECLARE_COMPLETION_ONSTACK(wait);
+
+	bio->bi_private = &wait;
+	bio->bi_end_io = chromeos_invalidate_kernel_endio;
+	bio_set_dev(bio, bdev);
+
+	bio->bi_iter.bi_sector = 0;
+	bio->bi_vcnt = 1;
+	bio->bi_iter.bi_idx = 0;
+	bio->bi_iter.bi_size = 512;
+	bio->bi_iter.bi_bvec_done = 0;
+	bio_set_op_attrs(bio, op, op_flags);
+	bio->bi_io_vec[0].bv_page = page;
+	bio->bi_io_vec[0].bv_len = 512;
+	bio->bi_io_vec[0].bv_offset = 0;
+
+	submit_bio(bio);
+	/* Wait up to 2 seconds for completion or fail. */
+	if (!wait_for_completion_timeout(&wait, msecs_to_jiffies(2000)))
+		return -1;
+	return 0;
+}
+
+static dev_t get_boot_dev_from_root_dev(struct block_device *root_bdev)
+{
+	/* Very basic sanity checking. This should be better. */
+	if (!root_bdev || MAJOR(root_bdev->bd_dev) == 254 ||
+	    root_bdev->bd_partno <= 1) {
+		return 0;
+	}
+	return MKDEV(MAJOR(root_bdev->bd_dev), MINOR(root_bdev->bd_dev) - 1);
+}
+
+static char kern_guid[48];
+
+/* get_boot_dev is bassed on dm_get_device_by_uuid in dm_bootcache. */
+static dev_t get_boot_dev(void)
+{
+	const char partuuid[] = "PARTUUID=";
+	char uuid[sizeof(partuuid) + 36];
+	char *uuid_str;
+	dev_t devt = 0;
+
+	if (!strlen(kern_guid)) {
+		DMERR("Couldn't get uuid, try root dev");
+		return 0;
+	}
+
+	if (strncmp(kern_guid, partuuid, strlen(partuuid))) {
+		/* Not prefixed with "PARTUUID=", so add it */
+		strcpy(uuid, partuuid);
+		strlcat(uuid, kern_guid, sizeof(uuid));
+		uuid_str = uuid;
+	} else {
+		uuid_str = kern_guid;
+	}
+	devt = name_to_dev_t(uuid_str);
+	if (!devt)
+		goto found_nothing;
+	return devt;
+
+found_nothing:
+	DMDEBUG("No matching partition for GUID: %s", uuid_str);
+	return 0;
+}
+
+/* Replaces the first 8 bytes of a partition with DMVERROR */
+static int chromeos_invalidate_kernel_bio(struct block_device *root_bdev)
+{
+	int ret = 0;
+	struct block_device *bdev;
+	struct bio *bio;
+	struct page *page;
+	dev_t devt;
+	fmode_t dev_mode;
+
+	devt = get_boot_dev();
+	if (!devt) {
+		devt = get_boot_dev_from_root_dev(root_bdev);
+		if (!devt)
+			return -EINVAL;
+	}
+
+	/* First we open the device for reading. */
+	dev_mode = FMODE_READ | FMODE_EXCL;
+	bdev = blkdev_get_by_dev(devt, dev_mode,
+				 chromeos_invalidate_kernel_bio);
+	if (IS_ERR(bdev)) {
+		DMERR("invalidate_kernel: could not open device for reading");
+		dev_mode = 0;
+		ret = -1;
+		goto failed_to_read;
+	}
+
+	bio = bio_alloc(GFP_NOIO, 1);
+	if (!bio) {
+		ret = -1;
+		goto failed_bio_alloc;
+	}
+
+	page = alloc_page(GFP_NOIO);
+	if (!page) {
+		ret = -ENOMEM;
+		goto failed_to_alloc_page;
+	}
+
+	/*
+	 * Request read operation with REQ_PREFLUSH flag to ensure that the
+	 * cache of non-volatile storage device has been flushed before read is
+	 * started.
+	 */
+	if (chromeos_invalidate_kernel_submit(bio, bdev,
+					      REQ_OP_READ,
+					      REQ_SYNC | REQ_PREFLUSH,
+					      page)) {
+		ret = -1;
+		goto failed_to_submit_read;
+	}
+
+	/* We have a page. Let's make sure it looks right. */
+	if (memcmp("CHROMEOS", page_address(page), 8)) {
+		DMERR("invalidate_kernel called on non-kernel partition");
+		ret = -EINVAL;
+		goto invalid_header;
+	} else {
+		DMERR("invalidate_kernel: found CHROMEOS kernel partition");
+	}
+
+	/* Stamp it and rewrite */
+	memcpy(page_address(page), DMVERROR, strlen(DMVERROR));
+
+	/* The block dev was being changed on read. Let's reopen here. */
+	blkdev_put(bdev, dev_mode);
+	dev_mode = FMODE_WRITE | FMODE_EXCL;
+	bdev = blkdev_get_by_dev(devt, dev_mode,
+				 chromeos_invalidate_kernel_bio);
+	if (IS_ERR(bdev)) {
+		DMERR("invalidate_kernel: could not open device for writing");
+		dev_mode = 0;
+		ret = -1;
+		goto failed_to_write;
+	}
+
+	/* We re-use the same bio to do the write after the read. Need to reset
+	 * it to initialize bio->bi_remaining.
+	 */
+	bio_reset(bio);
+
+	/*
+	 * Request write operation with REQ_FUA flag to ensure that I/O
+	 * completion for the write is signaled only after the data has been
+	 * committed to non-volatile storage.
+	 */
+	if (chromeos_invalidate_kernel_submit(bio, bdev, REQ_OP_WRITE,
+					      REQ_SYNC | REQ_FUA, page)) {
+		ret = -1;
+		goto failed_to_submit_write;
+	}
+
+	DMERR("invalidate_kernel: completed.");
+	ret = 0;
+failed_to_submit_write:
+failed_to_write:
+invalid_header:
+	__free_page(page);
+failed_to_submit_read:
+	/* Technically, we'll leak a page with the pending bio, but
+	 *  we're about to panic so it's safer to do the panic() we expect.
+	 */
+failed_to_alloc_page:
+	bio_put(bio);
+failed_bio_alloc:
+	if (dev_mode)
+		blkdev_put(bdev, dev_mode);
+failed_to_read:
+	return ret;
+}
+
+/*
+ * Invalidate the kernel which corresponds to the root block device.
+ *
+ * This function stamps DMVERROR on the beginning of the kernel partition.
+ *
+ * The kernel partition is attempted to be found by subtracting 1 from
+ *  the root partition.
+ * If that fails, then the kernel_guid commandline parameter is used to
+ *  find the kernel partition number.
+ * The DMVERROR string is stamped over only the CHROMEOS string at the
+ *  beginning of the kernel blob, leaving the rest of it intact.
+ */
+static int chromeos_invalidate_kernel(struct block_device *root_bdev)
+{
+	return chromeos_invalidate_kernel_bio(root_bdev);
+}
+
+static int error_handler(struct notifier_block *nb, unsigned long transient,
+			 void *opaque_err)
+{
+	struct dm_verity_error_state *err =
+		(struct dm_verity_error_state *) opaque_err;
+	err->behavior = DM_VERITY_ERROR_BEHAVIOR_PANIC;
+	if (transient)
+		return 0;
+
+	/* TODO(wad) Implement phase 2:
+	 * - Attempt to read the dev_status_offset from the hash dev.
+	 * - If the status offset is 0, replace the first byte of the sector
+	 *   with 01 and panic().
+	 * - If the status offset is not 0, invalidate the associated kernel
+	 *   partition, then reboot.
+	 * - make user space tools clear the last sector
+	 */
+	if (chromeos_invalidate_kernel(err->dev))
+		chromeos_set_need_recovery();
+	return 0;
+}
+
+static struct notifier_block chromeos_nb = {
+	.notifier_call = &error_handler,
+	.next = NULL,
+	.priority = 1,
+};
+
+static int __init dm_verity_chromeos_init(void)
+{
+	int r;
+
+	r = dm_verity_register_error_notifier(&chromeos_nb);
+	if (r < 0)
+		DMERR("failed to register handler: %d", r);
+	else
+		DMINFO("dm-verity-chromeos registered");
+	return r;
+}
+
+static void __exit dm_verity_chromeos_exit(void)
+{
+	dm_verity_unregister_error_notifier(&chromeos_nb);
+}
+
+module_init(dm_verity_chromeos_init);
+module_exit(dm_verity_chromeos_exit);
+
+MODULE_AUTHOR("Will Drewry <wad@chromium.org>");
+MODULE_DESCRIPTION("chromeos-specific error handler for dm-verity");
+MODULE_LICENSE("GPL");
+
+/* Declare parameter with no module prefix */
+#undef MODULE_PARAM_PREFIX
+#define MODULE_PARAM_PREFIX	""
+module_param_string(kern_guid, kern_guid, sizeof(kern_guid), 0);
diff -ruN a/drivers/md/dm-verity.h b/drivers/md/dm-verity.h
--- a/drivers/md/dm-verity.h	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/md/dm-verity.h	2021-12-23 08:35:32.000000000 +0100
@@ -14,6 +14,7 @@
 #include <linux/dm-bufio.h>
 #include <linux/device-mapper.h>
 #include <crypto/hash.h>
+#include <linux/notifier.h>
 
 #define DM_VERITY_MAX_LEVELS		63
 
@@ -56,6 +57,7 @@
 	int hash_failed;	/* set to 1 if hash of any block failed */
 	enum verity_mode mode;	/* mode for handling verification errors */
 	unsigned corrupted_errs;/* Number of errors for corrupted blocks */
+	int error_behavior;	/* selects error behavior on io errors */
 
 	struct workqueue_struct *verify_wq;
 
@@ -93,6 +95,40 @@
 	 */
 };
 
+struct verity_result {
+	struct completion completion;
+	int err;
+};
+
+struct dm_verity_error_state {
+	int code;
+	int transient;  /* Likely to not happen after a reboot */
+	u64 block;
+	const char *message;
+
+	sector_t dev_start;
+	sector_t dev_len;
+	struct block_device *dev;
+
+	sector_t hash_dev_start;
+	sector_t hash_dev_len;
+	struct block_device *hash_dev;
+
+	/* Final behavior after all notifications are completed. */
+	int behavior;
+};
+
+/* This enum must be matched to allowed_error_behaviors in dm-verity.c */
+enum dm_verity_error_behavior {
+	DM_VERITY_ERROR_BEHAVIOR_EIO = 0,
+	DM_VERITY_ERROR_BEHAVIOR_PANIC,
+	DM_VERITY_ERROR_BEHAVIOR_NONE,
+	DM_VERITY_ERROR_BEHAVIOR_NOTIFY
+};
+
+int dm_verity_register_error_notifier(struct notifier_block *nb);
+int dm_verity_unregister_error_notifier(struct notifier_block *nb);
+
 static inline struct ahash_request *verity_io_hash_req(struct dm_verity *v,
 						     struct dm_verity_io *io)
 {
diff -ruN a/drivers/md/dm-verity-target.c b/drivers/md/dm-verity-target.c
--- a/drivers/md/dm-verity-target.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/md/dm-verity-target.c	2021-12-23 08:35:32.000000000 +0100
@@ -16,8 +16,10 @@
 #include "dm-verity.h"
 #include "dm-verity-fec.h"
 #include "dm-verity-verify-sig.h"
+#include <linux/delay.h>
 #include <linux/module.h>
 #include <linux/reboot.h>
+#include <crypto/hash.h>
 
 #define DM_MSG_PREFIX			"verity"
 
@@ -33,8 +35,9 @@
 #define DM_VERITY_OPT_PANIC		"panic_on_corruption"
 #define DM_VERITY_OPT_IGN_ZEROES	"ignore_zero_blocks"
 #define DM_VERITY_OPT_AT_MOST_ONCE	"check_at_most_once"
+#define DM_VERITY_OPT_ERROR_BEHAVIOR	"error_behavior"
 
-#define DM_VERITY_OPTS_MAX		(3 + DM_VERITY_OPTS_FEC + \
+#define DM_VERITY_OPTS_MAX		(4 + DM_VERITY_OPTS_FEC + \
 					 DM_VERITY_ROOT_HASH_VERIFICATION_OPTS)
 
 static unsigned dm_verity_prefetch_cluster = DM_VERITY_DEFAULT_PREFETCH_SIZE;
@@ -48,6 +51,122 @@
 	unsigned n_blocks;
 };
 
+/* Provide a lightweight means of specifying the global default for
+ * error behavior: eio, reboot, or none
+ * Legacy support for 0 = eio, 1 = reboot/panic, 2 = none, 3 = notify.
+ * This is matched to the enum in dm-verity.h.
+ */
+static char *error_behavior_istring[] = { "0", "1", "2", "3" };
+static const char *allowed_error_behaviors[] = { "eio", "panic", "none",
+						 "notify", NULL };
+static char *error_behavior = "eio";
+module_param(error_behavior, charp, 0644);
+MODULE_PARM_DESC(error_behavior, "Behavior on error "
+				 "(eio, panic, none, notify)");
+
+/* Controls whether verity_get_device will wait forever for a device. */
+static int dev_wait;
+module_param(dev_wait, int, 0444);
+MODULE_PARM_DESC(dev_wait, "Wait forever for a backing device");
+
+static BLOCKING_NOTIFIER_HEAD(verity_error_notifier);
+
+int dm_verity_register_error_notifier(struct notifier_block *nb)
+{
+	return blocking_notifier_chain_register(&verity_error_notifier, nb);
+}
+EXPORT_SYMBOL_GPL(dm_verity_register_error_notifier);
+
+int dm_verity_unregister_error_notifier(struct notifier_block *nb)
+{
+	return blocking_notifier_chain_unregister(&verity_error_notifier, nb);
+}
+EXPORT_SYMBOL_GPL(dm_verity_unregister_error_notifier);
+
+/* If the request is not successful, this handler takes action.
+ * TODO make this call a registered handler.
+ */
+static void verity_error(struct dm_verity *v, struct dm_verity_io *io,
+			 blk_status_t status)
+{
+	const char *message = v->hash_failed ? "integrity" : "block";
+	int error_behavior = DM_VERITY_ERROR_BEHAVIOR_PANIC;
+	dev_t devt = 0;
+	u64 block = ~0;
+	struct dm_verity_error_state error_state;
+	/* If the hash did not fail, then this is likely transient. */
+	int transient = !v->hash_failed;
+
+	devt = v->data_dev->bdev->bd_dev;
+	error_behavior = v->error_behavior;
+	if (io)
+		block = io->block;
+
+	DMERR_LIMIT("verification failure occurred: %s failure%s", message,
+		    transient ? " (transient)" : "");
+
+	if (error_behavior == DM_VERITY_ERROR_BEHAVIOR_NOTIFY) {
+		error_state.code = status;
+		error_state.transient = transient;
+		error_state.block = block;
+		error_state.message = message;
+		error_state.dev_start = v->data_start;
+		error_state.dev_len = v->data_blocks;
+		error_state.dev = v->data_dev->bdev;
+		error_state.hash_dev_start = v->hash_start;
+		error_state.hash_dev_len = v->hash_blocks;
+		error_state.hash_dev = v->hash_dev->bdev;
+
+		/* Set default fallthrough behavior. */
+		error_state.behavior = DM_VERITY_ERROR_BEHAVIOR_PANIC;
+		error_behavior = DM_VERITY_ERROR_BEHAVIOR_PANIC;
+
+		if (!blocking_notifier_call_chain(
+		    &verity_error_notifier, transient, &error_state)) {
+			error_behavior = error_state.behavior;
+		}
+	}
+
+	switch (error_behavior) {
+	case DM_VERITY_ERROR_BEHAVIOR_EIO:
+		break;
+	case DM_VERITY_ERROR_BEHAVIOR_NONE:
+		break;
+	default:
+		goto do_panic;
+	}
+	return;
+
+do_panic:
+	panic("dm-verity failure: "
+	      "device:%u:%u status:%d block:%llu message:%s",
+	      MAJOR(devt), MINOR(devt), status, (u64)block, message);
+}
+
+/**
+ * verity_parse_error_behavior - parse a behavior charp to the enum
+ * @behavior:	NUL-terminated char array
+ *
+ * Checks if the behavior is valid either as text or as an index digit
+ * and returns the proper enum value in string form or ERR_PTR(-EINVAL)
+ * on error.
+ */
+static char *verity_parse_error_behavior(const char *behavior)
+{
+	const char **allowed = allowed_error_behaviors;
+	int index;
+
+	for (index = 0; *allowed; allowed++, index++)
+		if (!strcmp(*allowed, behavior) || behavior[0] == index + '0')
+			break;
+
+	if (!*allowed)
+		return ERR_PTR(-EINVAL);
+
+	/* Convert to the integer index matching the enum. */
+	return error_behavior_istring[index];
+}
+
 /*
  * Auxiliary structure appended to each dm-bufio buffer. If the value
  * hash_verified is nonzero, hash of the block has been verified.
@@ -563,6 +682,8 @@
 	struct dm_verity *v = io->v;
 	struct bio *bio = dm_bio_from_per_bio_data(io, v->ti->per_io_data_size);
 
+	if (status && !verity_fec_is_enabled(io->v))
+		verity_error(v, io, status);
 	bio->bi_end_io = io->orig_bi_end_io;
 	bio->bi_status = status;
 
@@ -1012,6 +1133,22 @@
 				return r;
 			continue;
 
+		} else if (!strcasecmp(arg_name, DM_VERITY_OPT_ERROR_BEHAVIOR)) {
+			int behavior;
+
+			if (!argc) {
+				ti->error = "Missing error behavior parameter";
+				return -EINVAL;
+			}
+			if (kstrtoint(dm_shift_arg(as), 0, &behavior) ||
+			    behavior < 0) {
+				ti->error = "Bad error behavior parameter";
+				return -EINVAL;
+			}
+			v->error_behavior = behavior;
+			argc--;
+			continue;
+
 		} else if (verity_is_fec_opt_arg(arg_name)) {
 			r = verity_fec_parse_opt_args(as, v, &argc, arg_name);
 			if (r)
@@ -1034,6 +1171,132 @@
 	return r;
 }
 
+static int verity_get_device(struct dm_target *ti, const char *devname,
+			     struct dm_dev **dm_dev)
+{
+	do {
+		/* Try the normal path first since if everything is ready, it
+		 * will be the fastest.
+		 */
+		if (!dm_get_device(ti, devname,
+				   dm_table_get_mode(ti->table), dm_dev))
+			return 0;
+
+		if (!dev_wait)
+			break;
+
+		/* No need to be too aggressive since this is a slow path. */
+		msleep(500);
+	} while (dev_wait && (driver_probe_done() != 0 || *dm_dev == NULL));
+	return -1;
+}
+
+static void splitarg(char *arg, char **key, char **val)
+{
+	*key = strsep(&arg, "=");
+	*val = strsep(&arg, "");
+}
+
+/* Convert Chrome OS arguments into standard arguments */
+
+static char *chromeos_args(unsigned *pargc, char ***pargv)
+{
+	char *hashstart = NULL;
+	char **argv = *pargv;
+	int argc = *pargc;
+	char *key, *val;
+	int nargc = 10;
+	char **nargv;
+	char *errstr;
+	int i;
+
+	nargv = kcalloc(14, sizeof(char *), GFP_KERNEL);
+	if (!nargv)
+		return "Failed to allocate memory";
+
+	nargv[0] = "0";		/* version */
+	nargv[3] = "4096";	/* hash block size */
+	nargv[4] = "4096";	/* data block size */
+	nargv[9] = "-";		/* salt (optional) */
+
+	for (i = 0; i < argc; ++i) {
+		DMDEBUG("Argument %d: '%s'", i, argv[i]);
+		splitarg(argv[i], &key, &val);
+		if (!key) {
+			DMWARN("Bad argument %d: missing key?", i);
+			errstr = "Bad argument: missing key";
+			goto err;
+		}
+		if (!val) {
+			DMWARN("Bad argument %d='%s': missing value", i, key);
+			errstr = "Bad argument: missing value";
+			goto err;
+		}
+		if (!strcmp(key, "alg")) {
+			nargv[7] = val;
+		} else if (!strcmp(key, "payload")) {
+			nargv[1] = val;
+		} else if (!strcmp(key, "hashtree")) {
+			nargv[2] = val;
+		} else if (!strcmp(key, "root_hexdigest")) {
+			nargv[8] = val;
+		} else if (!strcmp(key, "hashstart")) {
+			unsigned long num;
+
+			if (kstrtoul(val, 10, &num)) {
+				errstr = "Invalid hashstart";
+				goto err;
+			}
+			num >>= (12 - SECTOR_SHIFT);
+			hashstart = kmalloc(24, GFP_KERNEL);
+			if (!hashstart) {
+				errstr = "Failed to allocate memory";
+				goto err;
+			}
+			scnprintf(hashstart, sizeof(hashstart), "%lu", num);
+			nargv[5] = hashstart;
+			nargv[6] = hashstart;
+		} else if (!strcmp(key, "salt")) {
+			nargv[9] = val;
+		} else if (!strcmp(key, DM_VERITY_OPT_ERROR_BEHAVIOR)) {
+			char *behavior = verity_parse_error_behavior(val);
+
+			if (IS_ERR(behavior)) {
+				errstr = "Invalid error behavior";
+				goto err;
+			}
+			nargv[10] = "2";
+			nargv[11] = key;
+			nargv[12] = behavior;
+			nargc = 13;
+		}
+	}
+
+	if (!nargv[1] || !nargv[2] || !nargv[5] || !nargv[7] || !nargv[8]) {
+		errstr = "Missing argument";
+		goto err;
+	}
+
+	*pargc = nargc;
+	*pargv = nargv;
+	return NULL;
+
+err:
+	kfree(nargv);
+	kfree(hashstart);
+	return errstr;
+}
+
+/* Release memory allocated for Chrome OS parameter conversion */
+
+static void free_chromeos_argv(char **argv)
+{
+	if (argv) {
+		kfree(argv[5]);
+		kfree(argv);
+	}
+}
+
 /*
  * Target parameters:
  *	<version>	The current format is version 1.
@@ -1060,10 +1323,19 @@
 	sector_t hash_position;
 	char dummy;
 	char *root_hash_digest_to_validate;
+	char **chromeos_argv = NULL;
+
+	if (argc < 10) {
+		ti->error = chromeos_args(&argc, &argv);
+		if (ti->error)
+			return -EINVAL;
+		chromeos_argv = argv;
+	}
 
 	v = kzalloc(sizeof(struct dm_verity), GFP_KERNEL);
 	if (!v) {
 		ti->error = "Cannot allocate verity structure";
+		free_chromeos_argv(chromeos_argv);
 		return -ENOMEM;
 	}
 	ti->private = v;
@@ -1093,13 +1365,13 @@
 	}
 	v->version = num;
 
-	r = dm_get_device(ti, argv[1], FMODE_READ, &v->data_dev);
+	r = verity_get_device(ti, argv[1], &v->data_dev);
 	if (r) {
 		ti->error = "Data device lookup failed";
 		goto bad;
 	}
 
-	r = dm_get_device(ti, argv[2], FMODE_READ, &v->hash_dev);
+	r = verity_get_device(ti, argv[2], &v->hash_dev);
 	if (r) {
 		ti->error = "Hash device lookup failed";
 		goto bad;
@@ -1299,14 +1571,14 @@
 				       __alignof__(struct dm_verity_io));
 
 	verity_verify_sig_opts_cleanup(&verify_args);
-
+	free_chromeos_argv(chromeos_argv);
 	return 0;
 
 bad:
 
 	verity_verify_sig_opts_cleanup(&verify_args);
 	verity_dtr(ti);
-
+	free_chromeos_argv(chromeos_argv);
 	return r;
 }
 
diff -ruN a/drivers/md/Kconfig b/drivers/md/Kconfig
--- a/drivers/md/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/md/Kconfig	2021-12-23 08:35:31.000000000 +0100
@@ -289,6 +289,19 @@
 
 	  If unsure, say N.
 
+config DM_VERITY_CHROMEOS
+	tristate "Support Chrome OS specific verity error behavior"
+	depends on DM_VERITY
+	help
+	  Enables Chrome OS platform-specific error behavior.  In particular,
+	  it will modify the partition preceding the verified block device
+	  when non-transient error occurs (followed by a panic).
+
+	  This module relies on linux/chromeos_platform.h and will behave
+	  reasonably if it only supplies the stubs.
+
+	  If unsure, say N.
+
 config DM_SNAPSHOT
        tristate "Snapshot target"
        depends on BLK_DEV_DM
diff -ruN a/drivers/md/Makefile b/drivers/md/Makefile
--- a/drivers/md/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/md/Makefile	2021-12-23 08:35:31.000000000 +0100
@@ -83,6 +83,7 @@
 obj-$(CONFIG_DM_INTEGRITY)	+= dm-integrity.o
 obj-$(CONFIG_DM_ZONED)		+= dm-zoned.o
 obj-$(CONFIG_DM_WRITECACHE)	+= dm-writecache.o
+obj-$(CONFIG_DM_VERITY_CHROMEOS)	+= dm-verity-chromeos.o
 
 ifeq ($(CONFIG_DM_INIT),y)
 dm-mod-objs			+= dm-init.o
diff -ruN a/drivers/media/common/videobuf2/videobuf2-core.c b/drivers/media/common/videobuf2/videobuf2-core.c
--- a/drivers/media/common/videobuf2/videobuf2-core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/media/common/videobuf2/videobuf2-core.c	2021-12-23 08:35:32.000000000 +0100
@@ -327,12 +327,9 @@
 	if (vb->synced)
 		return;
 
-	if (vb->need_cache_sync_on_prepare) {
-		for (plane = 0; plane < vb->num_planes; ++plane)
-			call_void_memop(vb, prepare,
-					vb->planes[plane].mem_priv);
-	}
 	vb->synced = 1;
+	for (plane = 0; plane < vb->num_planes; ++plane)
+		call_void_memop(vb, prepare, vb->planes[plane].mem_priv);
 }
 
 /*
@@ -346,12 +343,9 @@
 	if (!vb->synced)
 		return;
 
-	if (vb->need_cache_sync_on_finish) {
-		for (plane = 0; plane < vb->num_planes; ++plane)
-			call_void_memop(vb, finish,
-					vb->planes[plane].mem_priv);
-	}
 	vb->synced = 0;
+	for (plane = 0; plane < vb->num_planes; ++plane)
+		call_void_memop(vb, finish, vb->planes[plane].mem_priv);
 }
 
 /*
@@ -382,6 +376,27 @@
 	}
 }
 
+static void init_buffer_cache_hints(struct vb2_queue *q, struct vb2_buffer *vb)
+{
+	/*
+	 * DMA exporter should take care of cache syncs, so we can avoid
+	 * explicit ->prepare()/->finish() syncs. For other ->memory types
+	 * we always need ->prepare() or/and ->finish() cache sync.
+	 */
+	if (q->memory == VB2_MEMORY_DMABUF) {
+		vb->skip_cache_sync_on_finish = 1;
+		vb->skip_cache_sync_on_prepare = 1;
+		return;
+	}
+
+	/*
+	 * ->finish() cache sync can be avoided when queue direction is
+	 * TO_DEVICE.
+	 */
+	if (q->dma_dir == DMA_TO_DEVICE)
+		vb->skip_cache_sync_on_finish = 1;
+}
+
 /*
  * __vb2_queue_alloc() - allocate videobuf buffer structures and (for MMAP type)
  * video buffer memory for all buffers/planes on the queue and initializes the
@@ -415,17 +430,7 @@
 		vb->index = q->num_buffers + buffer;
 		vb->type = q->type;
 		vb->memory = memory;
-		/*
-		 * We need to set these flags here so that the videobuf2 core
-		 * will call ->prepare()/->finish() cache sync/flush on vb2
-		 * buffers when appropriate. However, we can avoid explicit
-		 * ->prepare() and ->finish() cache sync for DMABUF buffers,
-		 * because DMA exporter takes care of it.
-		 */
-		if (q->memory != VB2_MEMORY_DMABUF) {
-			vb->need_cache_sync_on_prepare = 1;
-			vb->need_cache_sync_on_finish = 1;
-		}
+		init_buffer_cache_hints(q, vb);
 		for (plane = 0; plane < num_planes; ++plane) {
 			vb->planes[plane].length = plane_sizes[plane];
 			vb->planes[plane].min_length = plane_sizes[plane];
@@ -733,11 +738,30 @@
 }
 EXPORT_SYMBOL(vb2_verify_memory_type);
 
+static void set_queue_coherency(struct vb2_queue *q, bool non_coherent_mem)
+{
+	q->non_coherent_mem = 0;
+
+	if (!vb2_queue_allows_cache_hints(q))
+		return;
+	q->non_coherent_mem = non_coherent_mem;
+}
+
+static bool verify_coherency_flags(struct vb2_queue *q, bool non_coherent_mem)
+{
+	if (non_coherent_mem != q->non_coherent_mem) {
+		dprintk(q, 1, "memory coherency model mismatch\n");
+		return false;
+	}
+	return true;
+}
+
 int vb2_core_reqbufs(struct vb2_queue *q, enum vb2_memory memory,
-		     unsigned int *count)
+		     unsigned int flags, unsigned int *count)
 {
 	unsigned int num_buffers, allocated_buffers, num_planes = 0;
 	unsigned plane_sizes[VB2_MAX_PLANES] = { };
+	bool non_coherent_mem = flags & V4L2_MEMORY_FLAG_NON_COHERENT;
 	unsigned int i;
 	int ret;
 
@@ -752,7 +776,8 @@
 	}
 
 	if (*count == 0 || q->num_buffers != 0 ||
-	    (q->memory != VB2_MEMORY_UNKNOWN && q->memory != memory)) {
+	    (q->memory != VB2_MEMORY_UNKNOWN && q->memory != memory) ||
+	    !verify_coherency_flags(q, non_coherent_mem)) {
 		/*
 		 * We already have buffers allocated, so first check if they
 		 * are not in use and can be freed.
@@ -789,6 +814,7 @@
 	 */
 	mutex_lock(&q->mmap_lock);
 	q->memory = memory;
+	set_queue_coherency(q, non_coherent_mem);
 	mutex_unlock(&q->mmap_lock);
 
 	/*
@@ -873,13 +899,14 @@
 EXPORT_SYMBOL_GPL(vb2_core_reqbufs);
 
 int vb2_core_create_bufs(struct vb2_queue *q, enum vb2_memory memory,
-			 unsigned int *count,
+			 unsigned int flags, unsigned int *count,
 			 unsigned int requested_planes,
 			 const unsigned int requested_sizes[])
 {
 	unsigned int num_planes = 0, num_buffers, allocated_buffers;
 	unsigned plane_sizes[VB2_MAX_PLANES] = { };
 	bool no_previous_buffers = !q->num_buffers;
+	bool non_coherent_mem = flags & V4L2_MEMORY_FLAG_NON_COHERENT;
 	int ret;
 
 	if (q->num_buffers == VB2_MAX_FRAME) {
@@ -894,11 +921,14 @@
 		memset(q->alloc_devs, 0, sizeof(q->alloc_devs));
 		q->memory = memory;
 		q->waiting_for_buffers = !q->is_output;
+		set_queue_coherency(q, non_coherent_mem);
 	} else {
 		if (q->memory != memory) {
 			dprintk(q, 1, "memory model mismatch\n");
 			return -EINVAL;
 		}
+		if (!verify_coherency_flags(q, non_coherent_mem))
+			return -EINVAL;
 	}
 
 	num_buffers = min(*count, VB2_MAX_FRAME - q->num_buffers);
@@ -1080,7 +1110,6 @@
 	void *mem_priv;
 	unsigned int plane;
 	int ret = 0;
-	bool reacquired = vb->planes[0].mem_priv == NULL;
 
 	memset(planes, 0, sizeof(planes[0]) * vb->num_planes);
 	/* Copy relevant information provided by the userspace */
@@ -1090,14 +1119,7 @@
 		return ret;
 
 	for (plane = 0; plane < vb->num_planes; ++plane) {
-		/* Skip the plane if already verified */
-		if (vb->planes[plane].m.userptr &&
-			vb->planes[plane].m.userptr == planes[plane].m.userptr
-			&& vb->planes[plane].length == planes[plane].length)
-			continue;
-
-		dprintk(q, 3, "userspace address for plane %d changed, reacquiring memory\n",
-			plane);
+		WARN_ON(vb->planes[plane].mem_priv != NULL);
 
 		/* Check if the provided plane buffer is large enough */
 		if (planes[plane].length < vb->planes[plane].min_length) {
@@ -1109,22 +1131,6 @@
 			goto err;
 		}
 
-		/* Release previously acquired memory if present */
-		if (vb->planes[plane].mem_priv) {
-			if (!reacquired) {
-				reacquired = true;
-				vb->copied_timestamp = 0;
-				call_void_vb_qop(vb, buf_cleanup, vb);
-			}
-			call_void_memop(vb, put_userptr, vb->planes[plane].mem_priv);
-		}
-
-		vb->planes[plane].mem_priv = NULL;
-		vb->planes[plane].bytesused = 0;
-		vb->planes[plane].length = 0;
-		vb->planes[plane].m.userptr = 0;
-		vb->planes[plane].data_offset = 0;
-
 		/* Acquire each plane's memory */
 		mem_priv = call_ptr_memop(get_userptr,
 					  vb,
@@ -1151,17 +1157,14 @@
 		vb->planes[plane].data_offset = planes[plane].data_offset;
 	}
 
-	if (reacquired) {
-		/*
-		 * One or more planes changed, so we must call buf_init to do
-		 * the driver-specific initialization on the newly acquired
-		 * buffer, if provided.
-		 */
-		ret = call_vb_qop(vb, buf_init, vb);
-		if (ret) {
-			dprintk(q, 1, "buffer initialization failed\n");
-			goto err;
-		}
+	/*
+	 * Call buf_init to do driver-specific initialization on the newly
+	 * acquired buffer, if provided.
+	 */
+	ret = call_vb_qop(vb, buf_init, vb);
+	if (ret) {
+		dprintk(q, 1, "buffer initialization failed\n");
+		goto err;
 	}
 
 	ret = call_vb_qop(vb, buf_prepare, vb);
@@ -1873,6 +1876,22 @@
 
 	vb->state = VB2_BUF_STATE_DEQUEUED;
 
+	if (q->memory == VB2_MEMORY_USERPTR) {
+		int i;
+
+		call_void_vb_qop(vb, buf_cleanup, vb);
+
+		for (i = 0; i < vb->num_planes; ++i) {
+			WARN_ON(vb->planes[i].mem_priv == NULL);
+			call_void_memop(vb, put_userptr,
+					vb->planes[i].mem_priv);
+			vb->planes[i].mem_priv = NULL;
+			vb->planes[i].bytesused = 0;
+			vb->planes[i].length = 0;
+			vb->planes[i].m.userptr = 0;
+			vb->planes[i].data_offset = 0;
+		}
+	}
 	call_void_bufop(q, init_buffer, vb);
 }
 
@@ -2582,7 +2601,7 @@
 	fileio->memory = VB2_MEMORY_MMAP;
 	fileio->type = q->type;
 	q->fileio = fileio;
-	ret = vb2_core_reqbufs(q, fileio->memory, &fileio->count);
+	ret = vb2_core_reqbufs(q, fileio->memory, 0, &fileio->count);
 	if (ret)
 		goto err_kfree;
 
@@ -2639,7 +2658,7 @@
 
 err_reqbufs:
 	fileio->count = 0;
-	vb2_core_reqbufs(q, fileio->memory, &fileio->count);
+	vb2_core_reqbufs(q, fileio->memory, 0, &fileio->count);
 
 err_kfree:
 	q->fileio = NULL;
@@ -2659,7 +2678,7 @@
 		vb2_core_streamoff(q, q->type);
 		q->fileio = NULL;
 		fileio->count = 0;
-		vb2_core_reqbufs(q, fileio->memory, &fileio->count);
+		vb2_core_reqbufs(q, fileio->memory, 0, &fileio->count);
 		kfree(fileio);
 		dprintk(q, 3, "file io emulator closed\n");
 	}
diff -ruN a/drivers/media/common/videobuf2/videobuf2-dma-contig.c b/drivers/media/common/videobuf2/videobuf2-dma-contig.c
--- a/drivers/media/common/videobuf2/videobuf2-dma-contig.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/media/common/videobuf2/videobuf2-dma-contig.c	2021-12-23 08:35:32.000000000 +0100
@@ -17,6 +17,7 @@
 #include <linux/sched.h>
 #include <linux/slab.h>
 #include <linux/dma-mapping.h>
+#include <linux/highmem.h>
 
 #include <media/videobuf2-v4l2.h>
 #include <media/videobuf2-dma-contig.h>
@@ -42,6 +43,7 @@
 	struct dma_buf_attachment	*db_attach;
 
 	struct vb2_buffer		*vb;
+	bool				non_coherent_mem;
 };
 
 /*********************************************/
@@ -75,17 +77,39 @@
 	return &buf->dma_addr;
 }
 
+/*
+ * This function may fail if:
+ *
+ * - dma_buf_vmap() fails
+ *   E.g. due to lack of virtual mapping address space, or due to
+ *   dmabuf->ops misconfiguration.
+ *
+ * - dma_vmap_noncontiguous() fails
+ *   For instance, when requested buffer size is larger than totalram_pages().
+ *   Relevant for buffers that use non-coherent memory.
+ *
+ * - Queue DMA attrs have DMA_ATTR_NO_KERNEL_MAPPING set
+ *   Relevant for buffers that use coherent memory.
+ */
 static void *vb2_dc_vaddr(struct vb2_buffer *vb, void *buf_priv)
 {
 	struct vb2_dc_buf *buf = buf_priv;
-	struct dma_buf_map map;
-	int ret;
 
-	if (!buf->vaddr && buf->db_attach) {
-		ret = dma_buf_vmap(buf->db_attach->dmabuf, &map);
-		buf->vaddr = ret ? NULL : map.vaddr;
+	if (buf->vaddr)
+		return buf->vaddr;
+
+	if (buf->db_attach) {
+		struct dma_buf_map map;
+
+		if (!dma_buf_vmap(buf->db_attach->dmabuf, &map))
+			buf->vaddr = map.vaddr;
+
+		return buf->vaddr;
 	}
 
+	if (buf->non_coherent_mem)
+		buf->vaddr = dma_vmap_noncontiguous(buf->dev, buf->size,
+						    buf->dma_sgt);
 	return buf->vaddr;
 }
 
@@ -101,10 +125,19 @@
 	struct vb2_dc_buf *buf = buf_priv;
 	struct sg_table *sgt = buf->dma_sgt;
 
-	if (!sgt)
+	/* This takes care of DMABUF and user-enforced cache sync hint */
+	if (buf->vb->skip_cache_sync_on_prepare)
 		return;
 
+	if (!buf->non_coherent_mem)
+		return;
+
+	/* For both USERPTR and non-coherent MMAP */
 	dma_sync_sgtable_for_device(buf->dev, sgt, buf->dma_dir);
+
+	/* Non-coherent MMAP only */
+	if (buf->vaddr)
+		flush_kernel_vmap_range(buf->vaddr, buf->size);
 }
 
 static void vb2_dc_finish(void *buf_priv)
@@ -112,10 +145,19 @@
 	struct vb2_dc_buf *buf = buf_priv;
 	struct sg_table *sgt = buf->dma_sgt;
 
-	if (!sgt)
+	/* This takes care of DMABUF and user-enforced cache sync hint */
+	if (buf->vb->skip_cache_sync_on_finish)
 		return;
 
+	if (!buf->non_coherent_mem)
+		return;
+
+	/* For both USERPTR and non-coherent MMAP */
 	dma_sync_sgtable_for_cpu(buf->dev, sgt, buf->dma_dir);
+
+	/* Non-coherent MMAP only */
+	if (buf->vaddr)
+		invalidate_kernel_vmap_range(buf->vaddr, buf->size);
 }
 
 /*********************************************/
@@ -129,21 +171,69 @@
 	if (!refcount_dec_and_test(&buf->refcount))
 		return;
 
-	if (buf->sgt_base) {
-		sg_free_table(buf->sgt_base);
-		kfree(buf->sgt_base);
+	if (buf->non_coherent_mem) {
+		if (buf->vaddr)
+			dma_vunmap_noncontiguous(buf->dev, buf->vaddr);
+		dma_free_noncontiguous(buf->dev, buf->size,
+				       buf->dma_sgt, buf->dma_dir);
+	} else {
+		if (buf->sgt_base) {
+			sg_free_table(buf->sgt_base);
+			kfree(buf->sgt_base);
+		}
+		dma_free_attrs(buf->dev, buf->size, buf->cookie,
+			       buf->dma_addr, buf->attrs);
 	}
-	dma_free_attrs(buf->dev, buf->size, buf->cookie, buf->dma_addr,
-		       buf->attrs);
 	put_device(buf->dev);
 	kfree(buf);
 }
 
+static int vb2_dc_alloc_coherent(struct vb2_dc_buf *buf)
+{
+	struct vb2_queue *q = buf->vb->vb2_queue;
+
+	buf->cookie = dma_alloc_attrs(buf->dev,
+				      buf->size,
+				      &buf->dma_addr,
+				      GFP_KERNEL | q->gfp_flags,
+				      buf->attrs);
+	if (!buf->cookie)
+		return -ENOMEM;
+
+	if (q->dma_attrs & DMA_ATTR_NO_KERNEL_MAPPING)
+		return 0;
+
+	buf->vaddr = buf->cookie;
+	return 0;
+}
+
+static int vb2_dc_alloc_non_coherent(struct vb2_dc_buf *buf)
+{
+	struct vb2_queue *q = buf->vb->vb2_queue;
+
+	buf->dma_sgt = dma_alloc_noncontiguous(buf->dev,
+					       buf->size,
+					       buf->dma_dir,
+					       GFP_KERNEL | q->gfp_flags,
+					       buf->attrs);
+	if (!buf->dma_sgt)
+		return -ENOMEM;
+
+	buf->dma_addr = sg_dma_address(buf->dma_sgt->sgl);
+
+	/*
+	 * For non-coherent buffers the kernel mapping is created on demand
+	 * in vb2_dc_vaddr().
+	 */
+	return 0;
+}
+
 static void *vb2_dc_alloc(struct vb2_buffer *vb,
 			  struct device *dev,
 			  unsigned long size)
 {
 	struct vb2_dc_buf *buf;
+	int ret;
 
 	if (WARN_ON(!dev))
 		return ERR_PTR(-EINVAL);
@@ -153,27 +243,28 @@
 		return ERR_PTR(-ENOMEM);
 
 	buf->attrs = vb->vb2_queue->dma_attrs;
-	buf->cookie = dma_alloc_attrs(dev, size, &buf->dma_addr,
-				      GFP_KERNEL | vb->vb2_queue->gfp_flags,
-				      buf->attrs);
-	if (!buf->cookie) {
-		dev_err(dev, "dma_alloc_coherent of size %lu failed\n", size);
-		kfree(buf);
-		return ERR_PTR(-ENOMEM);
-	}
-
-	if ((buf->attrs & DMA_ATTR_NO_KERNEL_MAPPING) == 0)
-		buf->vaddr = buf->cookie;
+	buf->dma_dir = vb->vb2_queue->dma_dir;
+	buf->vb = vb;
+	buf->non_coherent_mem = vb->vb2_queue->non_coherent_mem;
 
+	buf->size = size;
 	/* Prevent the device from being released while the buffer is used */
 	buf->dev = get_device(dev);
-	buf->size = size;
-	buf->dma_dir = vb->vb2_queue->dma_dir;
+
+	if (buf->non_coherent_mem)
+		ret = vb2_dc_alloc_non_coherent(buf);
+	else
+		ret = vb2_dc_alloc_coherent(buf);
+
+	if (ret) {
+		dev_err(dev, "dma_alloc_coherent of size %lu failed\n", size);
+		kfree(buf);
+		return ERR_PTR(-ENOMEM);
+	}
 
 	buf->handler.refcount = &buf->refcount;
 	buf->handler.put = vb2_dc_put;
 	buf->handler.arg = buf;
-	buf->vb = vb;
 
 	refcount_set(&buf->refcount, 1);
 
@@ -190,9 +281,12 @@
 		return -EINVAL;
 	}
 
-	ret = dma_mmap_attrs(buf->dev, vma, buf->cookie,
-		buf->dma_addr, buf->size, buf->attrs);
-
+	if (buf->non_coherent_mem)
+		ret = dma_mmap_noncontiguous(buf->dev, vma, buf->size,
+					     buf->dma_sgt);
+	else
+		ret = dma_mmap_attrs(buf->dev, vma, buf->cookie, buf->dma_addr,
+				     buf->size, buf->attrs);
 	if (ret) {
 		pr_err("Remapping memory failed, error: %d\n", ret);
 		return ret;
@@ -354,9 +448,15 @@
 
 static int vb2_dc_dmabuf_ops_vmap(struct dma_buf *dbuf, struct dma_buf_map *map)
 {
-	struct vb2_dc_buf *buf = dbuf->priv;
+	struct vb2_dc_buf *buf;
+	void *vaddr;
 
-	dma_buf_map_set_vaddr(map, buf->vaddr);
+	buf = dbuf->priv;
+	vaddr = vb2_dc_vaddr(buf->vb, buf);
+	if (!vaddr)
+		return -EINVAL;
+
+	dma_buf_map_set_vaddr(map, vaddr);
 
 	return 0;
 }
@@ -384,6 +484,9 @@
 	int ret;
 	struct sg_table *sgt;
 
+	if (buf->non_coherent_mem)
+		return buf->dma_sgt;
+
 	sgt = kmalloc(sizeof(*sgt), GFP_KERNEL);
 	if (!sgt) {
 		dev_err(buf->dev, "failed to alloc sg table\n");
@@ -562,6 +665,8 @@
 
 	buf->dma_addr = sg_dma_address(sgt->sgl);
 	buf->dma_sgt = sgt;
+	buf->non_coherent_mem = 1;
+
 out:
 	buf->size = size;
 
diff -ruN a/drivers/media/common/videobuf2/videobuf2-dma-sg.c b/drivers/media/common/videobuf2/videobuf2-dma-sg.c
--- a/drivers/media/common/videobuf2/videobuf2-dma-sg.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/media/common/videobuf2/videobuf2-dma-sg.c	2021-12-23 08:35:32.000000000 +0100
@@ -204,6 +204,9 @@
 	struct vb2_dma_sg_buf *buf = buf_priv;
 	struct sg_table *sgt = buf->dma_sgt;
 
+	if (buf->vb->skip_cache_sync_on_prepare)
+		return;
+
 	dma_sync_sgtable_for_device(buf->dev, sgt, buf->dma_dir);
 }
 
@@ -212,6 +215,9 @@
 	struct vb2_dma_sg_buf *buf = buf_priv;
 	struct sg_table *sgt = buf->dma_sgt;
 
+	if (buf->vb->skip_cache_sync_on_finish)
+		return;
+
 	dma_sync_sgtable_for_cpu(buf->dev, sgt, buf->dma_dir);
 }
 
diff -ruN a/drivers/media/common/videobuf2/videobuf2-v4l2.c b/drivers/media/common/videobuf2/videobuf2-v4l2.c
--- a/drivers/media/common/videobuf2/videobuf2-v4l2.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/media/common/videobuf2/videobuf2-v4l2.c	2021-12-23 08:35:32.000000000 +0100
@@ -219,6 +219,17 @@
 					b->m.planes[plane].m.fd;
 				planes[plane].length =
 					b->m.planes[plane].length;
+				/*
+				 * HACK(crbug/901264): This allows users to use
+				 * data_offset to pass an offset when importing
+				 * a DMA-buf that contains all color planes of
+				 * a multiplanar format.
+				 *
+				 * TODO(b/149113276): Remove this hack once
+				 * v4l2_buffer_ext API is supported.
+				 */
+				planes[plane].data_offset =
+					b->m.planes[plane].data_offset;
 			}
 			break;
 		default:
@@ -345,24 +356,6 @@
 				   struct vb2_buffer *vb,
 				   struct v4l2_buffer *b)
 {
-	/*
-	 * DMA exporter should take care of cache syncs, so we can avoid
-	 * explicit ->prepare()/->finish() syncs. For other ->memory types
-	 * we always need ->prepare() or/and ->finish() cache sync.
-	 */
-	if (q->memory == VB2_MEMORY_DMABUF) {
-		vb->need_cache_sync_on_finish = 0;
-		vb->need_cache_sync_on_prepare = 0;
-		return;
-	}
-
-	/*
-	 * Cache sync/invalidation flags are set by default in order to
-	 * preserve existing behaviour for old apps/drivers.
-	 */
-	vb->need_cache_sync_on_prepare = 1;
-	vb->need_cache_sync_on_finish = 1;
-
 	if (!vb2_queue_allows_cache_hints(q)) {
 		/*
 		 * Clear buffer cache flags if queue does not support user
@@ -374,18 +367,11 @@
 		return;
 	}
 
-	/*
-	 * ->finish() cache sync can be avoided when queue direction is
-	 * TO_DEVICE.
-	 */
-	if (q->dma_dir == DMA_TO_DEVICE)
-		vb->need_cache_sync_on_finish = 0;
-
 	if (b->flags & V4L2_BUF_FLAG_NO_CACHE_INVALIDATE)
-		vb->need_cache_sync_on_finish = 0;
+		vb->skip_cache_sync_on_finish = 1;
 
 	if (b->flags & V4L2_BUF_FLAG_NO_CACHE_CLEAN)
-		vb->need_cache_sync_on_prepare = 0;
+		vb->skip_cache_sync_on_prepare = 1;
 }
 
 static int vb2_queue_or_prepare_buf(struct vb2_queue *q, struct media_device *mdev,
@@ -717,12 +703,32 @@
 #endif
 }
 
+static void validate_memory_flags(struct vb2_queue *q,
+				  int memory,
+				  u32 *flags)
+{
+	if (!q->allow_cache_hints || memory != V4L2_MEMORY_MMAP) {
+		/*
+		 * This needs to clear V4L2_MEMORY_FLAG_NON_COHERENT only,
+		 * but in order to avoid bugs we zero out all bits.
+		 */
+		*flags = 0;
+	} else {
+		/* Clear all unknown flags. */
+		*flags &= V4L2_MEMORY_FLAG_NON_COHERENT;
+	}
+}
+
 int vb2_reqbufs(struct vb2_queue *q, struct v4l2_requestbuffers *req)
 {
 	int ret = vb2_verify_memory_type(q, req->memory, req->type);
+	u32 flags = req->flags;
 
 	fill_buf_caps(q, &req->capabilities);
-	return ret ? ret : vb2_core_reqbufs(q, req->memory, &req->count);
+	validate_memory_flags(q, req->memory, &flags);
+	req->flags = flags;
+	return ret ? ret : vb2_core_reqbufs(q, req->memory,
+					    req->flags, &req->count);
 }
 EXPORT_SYMBOL_GPL(vb2_reqbufs);
 
@@ -754,6 +760,7 @@
 	unsigned i;
 
 	fill_buf_caps(q, &create->capabilities);
+	validate_memory_flags(q, create->memory, &create->flags);
 	create->index = q->num_buffers;
 	if (create->count == 0)
 		return ret != -EBUSY ? ret : 0;
@@ -797,6 +804,7 @@
 		if (requested_sizes[i] == 0)
 			return -EINVAL;
 	return ret ? ret : vb2_core_create_bufs(q, create->memory,
+						create->flags,
 						&create->count,
 						requested_planes,
 						requested_sizes);
@@ -993,13 +1001,16 @@
 {
 	struct video_device *vdev = video_devdata(file);
 	int res = vb2_verify_memory_type(vdev->queue, p->memory, p->type);
+	u32 flags = p->flags;
 
 	fill_buf_caps(vdev->queue, &p->capabilities);
+	validate_memory_flags(vdev->queue, p->memory, &flags);
+	p->flags = flags;
 	if (res)
 		return res;
 	if (vb2_queue_is_busy(vdev, file))
 		return -EBUSY;
-	res = vb2_core_reqbufs(vdev->queue, p->memory, &p->count);
+	res = vb2_core_reqbufs(vdev->queue, p->memory, p->flags, &p->count);
 	/* If count == 0, then the owner has released all buffers and he
 	   is no longer owner of the queue. Otherwise we have a new owner. */
 	if (res == 0)
@@ -1017,6 +1028,7 @@
 
 	p->index = vdev->queue->num_buffers;
 	fill_buf_caps(vdev->queue, &p->capabilities);
+	validate_memory_flags(vdev->queue, p->memory, &p->flags);
 	/*
 	 * If count == 0, then just check if memory and type are valid.
 	 * Any -EBUSY result from vb2_verify_memory_type can be mapped to 0.
diff -ruN a/drivers/media/dvb-core/dvb_vb2.c b/drivers/media/dvb-core/dvb_vb2.c
--- a/drivers/media/dvb-core/dvb_vb2.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/media/dvb-core/dvb_vb2.c	2021-12-23 08:35:32.000000000 +0100
@@ -342,7 +342,7 @@
 
 	ctx->buf_siz = req->size;
 	ctx->buf_cnt = req->count;
-	ret = vb2_core_reqbufs(&ctx->vb_q, VB2_MEMORY_MMAP, &req->count);
+	ret = vb2_core_reqbufs(&ctx->vb_q, VB2_MEMORY_MMAP, 0, &req->count);
 	if (ret) {
 		ctx->state = DVB_VB2_STATE_NONE;
 		dprintk(1, "[%s] count=%d size=%d errno=%d\n", ctx->name,
diff -ruN a/drivers/media/i2c/imx208.c b/drivers/media/i2c/imx208.c
--- a/drivers/media/i2c/imx208.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/media/i2c/imx208.c	2021-12-23 08:35:32.000000000 +0100
@@ -277,6 +277,7 @@
 	struct v4l2_ctrl *pixel_rate;
 	struct v4l2_ctrl *vblank;
 	struct v4l2_ctrl *hblank;
+	struct v4l2_ctrl *exposure;
 	struct v4l2_ctrl *vflip;
 	struct v4l2_ctrl *hflip;
 
@@ -432,8 +433,17 @@
 	struct imx208 *imx208 =
 		container_of(ctrl->handler, struct imx208, ctrl_handler);
 	struct i2c_client *client = v4l2_get_subdevdata(&imx208->sd);
+	s64 max;
 	int ret;
 
+	if (ctrl->id == V4L2_CID_VBLANK) {
+		/* Update max exposure while meeting expected vblanking */
+		max = imx208->cur_mode->height + ctrl->val - 8;
+		__v4l2_ctrl_modify_range(imx208->exposure,
+					 imx208->exposure->minimum,
+					 max, imx208->exposure->step, max);
+	}
+
 	/*
 	 * Applying V4L2 control value only happens
 	 * when power is up for streaming
@@ -914,9 +924,11 @@
 		imx208->hblank->flags |= V4L2_CTRL_FLAG_READ_ONLY;
 
 	exposure_max = imx208->cur_mode->vts_def - 8;
-	v4l2_ctrl_new_std(ctrl_hdlr, &imx208_ctrl_ops, V4L2_CID_EXPOSURE,
-			  IMX208_EXPOSURE_MIN, exposure_max,
-			  IMX208_EXPOSURE_STEP, IMX208_EXPOSURE_DEFAULT);
+	imx208->exposure = v4l2_ctrl_new_std(ctrl_hdlr, &imx208_ctrl_ops,
+					     V4L2_CID_EXPOSURE,
+					     IMX208_EXPOSURE_MIN, exposure_max,
+					     IMX208_EXPOSURE_STEP,
+					     IMX208_EXPOSURE_DEFAULT);
 
 	imx208->hflip = v4l2_ctrl_new_std(ctrl_hdlr, &imx208_ctrl_ops,
 					  V4L2_CID_HFLIP, 0, 1, 1, 0);
diff -ruN a/drivers/media/i2c/ov8856.c b/drivers/media/i2c/ov8856.c
--- a/drivers/media/i2c/ov8856.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/media/i2c/ov8856.c	2021-12-23 08:35:32.000000000 +0100
@@ -107,6 +107,11 @@
 	"dvdd",		/* Digital core power */
 };
 
+enum {
+	OV8856_MEDIA_BUS_FMT_SBGGR10_1X10,
+	OV8856_MEDIA_BUS_FMT_SGRBG10_1X10,
+};
+
 struct ov8856_reg {
 	u16 address;
 	u8 val;
@@ -145,6 +150,9 @@
 
 	/* Number of data lanes */
 	u8 data_lanes;
+
+	/* Default MEDIA_BUS_FMT for this mode */
+	u32 default_mbus_index;
 };
 
 struct ov8856_mipi_data_rates {
@@ -1055,7 +1063,7 @@
 		{0x3810, 0x00},
 		{0x3811, 0x04},
 		{0x3812, 0x00},
-		{0x3813, 0x01},
+		{0x3813, 0x02},
 		{0x3814, 0x01},
 		{0x3815, 0x01},
 		{0x3816, 0x00},
@@ -1259,7 +1267,7 @@
 		{0x3810, 0x00},
 		{0x3811, 0x02},
 		{0x3812, 0x00},
-		{0x3813, 0x01},
+		{0x3813, 0x02},
 		{0x3814, 0x03},
 		{0x3815, 0x01},
 		{0x3816, 0x00},
@@ -1372,6 +1380,19 @@
 		{0x5e10, 0xfc}
 };
 
+static const struct ov8856_reg mipi_data_mbus_sbggr10_1x10[] = {
+	{0x3813, 0x02},
+};
+
+static const struct ov8856_reg mipi_data_mbus_sgrbg10_1x10[] = {
+	{0x3813, 0x01},
+};
+
+static const u32 ov8856_mbus_codes[] = {
+	MEDIA_BUS_FMT_SBGGR10_1X10,
+	MEDIA_BUS_FMT_SGRBG10_1X10
+};
+
 static const char * const ov8856_test_pattern_menu[] = {
 	"Disabled",
 	"Standard Color Bar",
@@ -1380,6 +1401,17 @@
 	"Bottom-Top Darker Color Bar"
 };
 
+static const struct ov8856_reg_list bayer_offset_configs[] = {
+	[OV8856_MEDIA_BUS_FMT_SBGGR10_1X10] = {
+		.num_of_regs = ARRAY_SIZE(mipi_data_mbus_sbggr10_1x10),
+		.regs = mipi_data_mbus_sbggr10_1x10,
+	},
+	[OV8856_MEDIA_BUS_FMT_SGRBG10_1X10] = {
+		.num_of_regs = ARRAY_SIZE(mipi_data_mbus_sgrbg10_1x10),
+		.regs = mipi_data_mbus_sgrbg10_1x10,
+	}
+};
+
 struct ov8856 {
 	struct v4l2_subdev sd;
 	struct media_pad pad;
@@ -1399,6 +1431,9 @@
 	/* Current mode */
 	const struct ov8856_mode *cur_mode;
 
+	/* Application specified mbus format */
+	u32 cur_mbus_index;
+
 	/* To serialize asynchronus callbacks */
 	struct mutex mutex;
 
@@ -1450,6 +1485,7 @@
 		},
 		.link_freq_index = 0,
 		.data_lanes = 2,
+		.default_mbus_index = OV8856_MEDIA_BUS_FMT_SGRBG10_1X10,
 	},
 	{
 		.width = 1640,
@@ -1464,6 +1500,7 @@
 		},
 		.link_freq_index = 1,
 		.data_lanes = 2,
+		.default_mbus_index = OV8856_MEDIA_BUS_FMT_SGRBG10_1X10,
 	}}
 };
 
@@ -1499,6 +1536,7 @@
 			},
 			.link_freq_index = 0,
 			.data_lanes = 4,
+			.default_mbus_index = OV8856_MEDIA_BUS_FMT_SGRBG10_1X10,
 		},
 		{
 			.width = 1640,
@@ -1513,6 +1551,7 @@
 			},
 			.link_freq_index = 1,
 			.data_lanes = 4,
+			.default_mbus_index = OV8856_MEDIA_BUS_FMT_SGRBG10_1X10,
 		},
 		{
 			.width = 3264,
@@ -1527,6 +1566,7 @@
 			},
 			.link_freq_index = 0,
 			.data_lanes = 4,
+			.default_mbus_index = OV8856_MEDIA_BUS_FMT_SBGGR10_1X10,
 		},
 		{
 			.width = 1632,
@@ -1541,6 +1581,7 @@
 			},
 			.link_freq_index = 1,
 			.data_lanes = 4,
+			.default_mbus_index = OV8856_MEDIA_BUS_FMT_SBGGR10_1X10,
 		}}
 };
 
@@ -1904,12 +1945,21 @@
 	return 0;
 }
 
-static void ov8856_update_pad_format(const struct ov8856_mode *mode,
+static void ov8856_update_pad_format(struct ov8856 *ov8856,
+				     const struct ov8856_mode *mode,
 				     struct v4l2_mbus_framefmt *fmt)
 {
+	int index;
+
 	fmt->width = mode->width;
 	fmt->height = mode->height;
-	fmt->code = MEDIA_BUS_FMT_SGRBG10_1X10;
+	for (index = 0; index < ARRAY_SIZE(ov8856_mbus_codes); ++index)
+		if (ov8856_mbus_codes[index] == fmt->code)
+			break;
+	if (index == ARRAY_SIZE(ov8856_mbus_codes))
+		index = mode->default_mbus_index;
+	fmt->code = ov8856_mbus_codes[index];
+	ov8856->cur_mbus_index = index;
 	fmt->field = V4L2_FIELD_NONE;
 }
 
@@ -1935,6 +1985,13 @@
 		return ret;
 	}
 
+	reg_list = &bayer_offset_configs[ov8856->cur_mbus_index];
+	ret = ov8856_write_reg_list(ov8856, reg_list);
+	if (ret) {
+		dev_err(&client->dev, "failed to set mbus format");
+		return ret;
+	}
+
 	ret = __v4l2_ctrl_handler_setup(ov8856->sd.ctrl_handler);
 	if (ret)
 		return ret;
@@ -2096,7 +2153,7 @@
 				      fmt->format.height);
 
 	mutex_lock(&ov8856->mutex);
-	ov8856_update_pad_format(mode, &fmt->format);
+	ov8856_update_pad_format(ov8856, mode, &fmt->format);
 	if (fmt->which == V4L2_SUBDEV_FORMAT_TRY) {
 		*v4l2_subdev_get_try_format(sd, sd_state, fmt->pad) = fmt->format;
 	} else {
@@ -2140,7 +2197,7 @@
 							  sd_state,
 							  fmt->pad);
 	else
-		ov8856_update_pad_format(ov8856->cur_mode, &fmt->format);
+		ov8856_update_pad_format(ov8856, ov8856->cur_mode, &fmt->format);
 
 	mutex_unlock(&ov8856->mutex);
 
@@ -2151,11 +2208,10 @@
 				 struct v4l2_subdev_state *sd_state,
 				 struct v4l2_subdev_mbus_code_enum *code)
 {
-	/* Only one bayer order GRBG is supported */
-	if (code->index > 0)
+	if (code->index >= ARRAY_SIZE(ov8856_mbus_codes))
 		return -EINVAL;
 
-	code->code = MEDIA_BUS_FMT_SGRBG10_1X10;
+	code->code = ov8856_mbus_codes[code->index];
 
 	return 0;
 }
@@ -2165,11 +2221,15 @@
 				  struct v4l2_subdev_frame_size_enum *fse)
 {
 	struct ov8856 *ov8856 = to_ov8856(sd);
+	int index;
 
 	if (fse->index >= ov8856->modes_size)
 		return -EINVAL;
 
-	if (fse->code != MEDIA_BUS_FMT_SGRBG10_1X10)
+	for (index = 0; index < ARRAY_SIZE(ov8856_mbus_codes); ++index)
+		if (fse->code == ov8856_mbus_codes[index])
+			break;
+	if (index == ARRAY_SIZE(ov8856_mbus_codes))
 		return -EINVAL;
 
 	fse->min_width = ov8856->priv_lane->supported_modes[fse->index].width;
@@ -2185,7 +2245,7 @@
 	struct ov8856 *ov8856 = to_ov8856(sd);
 
 	mutex_lock(&ov8856->mutex);
-	ov8856_update_pad_format(&ov8856->priv_lane->supported_modes[0],
+	ov8856_update_pad_format(ov8856, &ov8856->priv_lane->supported_modes[0],
 				 v4l2_subdev_get_try_format(sd, fh->state, 0));
 	mutex_unlock(&ov8856->mutex);
 
@@ -2426,6 +2486,7 @@
 
 	mutex_init(&ov8856->mutex);
 	ov8856->cur_mode = &ov8856->priv_lane->supported_modes[0];
+	ov8856->cur_mbus_index = ov8856->cur_mode->default_mbus_index;
 	ret = ov8856_init_controls(ov8856);
 	if (ret) {
 		dev_err(&client->dev, "failed to init controls: %d", ret);
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6.c b/drivers/media/pci/intel/ipu6/ipu6.c
--- a/drivers/media/pci/intel/ipu6/ipu6.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,363 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2018 - 2021 Intel Corporation
+
+#include <linux/device.h>
+#include <linux/delay.h>
+#include <linux/firmware.h>
+#include <linux/module.h>
+#include <linux/pm_runtime.h>
+
+#include "ipu.h"
+#include "ipu-cpd.h"
+#include "ipu-isys.h"
+#include "ipu-psys.h"
+#include "ipu-platform.h"
+#include "ipu-platform-regs.h"
+#include "ipu-platform-buttress-regs.h"
+#include "ipu-platform-isys-csi2-reg.h"
+
+struct ipu_cell_program_t {
+	unsigned int magic_number;
+
+	unsigned int blob_offset;
+	unsigned int blob_size;
+
+	unsigned int start[3];
+
+	unsigned int icache_source;
+	unsigned int icache_target;
+	unsigned int icache_size;
+
+	unsigned int pmem_source;
+	unsigned int pmem_target;
+	unsigned int pmem_size;
+
+	unsigned int data_source;
+	unsigned int data_target;
+	unsigned int data_size;
+
+	unsigned int bss_target;
+	unsigned int bss_size;
+
+	unsigned int cell_id;
+	unsigned int regs_addr;
+
+	unsigned int cell_pmem_data_bus_address;
+	unsigned int cell_dmem_data_bus_address;
+	unsigned int cell_pmem_control_bus_address;
+	unsigned int cell_dmem_control_bus_address;
+
+	unsigned int next;
+	unsigned int dummy[2];
+};
+
+static unsigned int ipu6se_csi_offsets[] = {
+	IPU_CSI_PORT_A_ADDR_OFFSET,
+	IPU_CSI_PORT_B_ADDR_OFFSET,
+	IPU_CSI_PORT_C_ADDR_OFFSET,
+	IPU_CSI_PORT_D_ADDR_OFFSET,
+};
+
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+static unsigned int ipu6se_tpg_offsets[] = {
+	IPU_CSI_PORT_A_PIXGEN_ADDR_OFFSET,
+	IPU_CSI_PORT_B_PIXGEN_ADDR_OFFSET,
+	IPU_CSI_PORT_C_PIXGEN_ADDR_OFFSET,
+	IPU_CSI_PORT_D_PIXGEN_ADDR_OFFSET,
+};
+
+static unsigned int ipu6_tpg_offsets[] = {
+	IPU_CSI_PORT_A_PIXGEN_ADDR_OFFSET,
+	IPU_CSI_PORT_B_PIXGEN_ADDR_OFFSET,
+	IPU_CSI_PORT_C_PIXGEN_ADDR_OFFSET,
+	IPU_CSI_PORT_D_PIXGEN_ADDR_OFFSET,
+	IPU_CSI_PORT_E_PIXGEN_ADDR_OFFSET,
+	IPU_CSI_PORT_F_PIXGEN_ADDR_OFFSET,
+	IPU_CSI_PORT_G_PIXGEN_ADDR_OFFSET,
+	IPU_CSI_PORT_H_PIXGEN_ADDR_OFFSET
+};
+#endif
+
+static unsigned int ipu6_csi_offsets[] = {
+	IPU_CSI_PORT_A_ADDR_OFFSET,
+	IPU_CSI_PORT_B_ADDR_OFFSET,
+	IPU_CSI_PORT_C_ADDR_OFFSET,
+	IPU_CSI_PORT_D_ADDR_OFFSET,
+	IPU_CSI_PORT_E_ADDR_OFFSET,
+	IPU_CSI_PORT_F_ADDR_OFFSET,
+	IPU_CSI_PORT_G_ADDR_OFFSET,
+	IPU_CSI_PORT_H_ADDR_OFFSET
+};
+
+struct ipu_isys_internal_pdata isys_ipdata = {
+	.hw_variant = {
+		       .offset = IPU_UNIFIED_OFFSET,
+		       .nr_mmus = 3,
+		       .mmu_hw = {
+				{
+				   .offset = IPU_ISYS_IOMMU0_OFFSET,
+				   .info_bits =
+				   IPU_INFO_REQUEST_DESTINATION_IOSF,
+				   .nr_l1streams = 16,
+				   .l1_block_sz = {
+						   3, 8, 2, 2, 2, 2, 2, 2, 1, 1,
+						   1, 1, 1, 1, 1, 1
+				   },
+				   .nr_l2streams = 16,
+				   .l2_block_sz = {
+						   2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+						   2, 2, 2, 2, 2, 2
+				   },
+				   .insert_read_before_invalidate = false,
+				   .l1_stream_id_reg_offset =
+				   IPU_MMU_L1_STREAM_ID_REG_OFFSET,
+				   .l2_stream_id_reg_offset =
+				   IPU_MMU_L2_STREAM_ID_REG_OFFSET,
+				},
+				{
+				   .offset = IPU_ISYS_IOMMU1_OFFSET,
+				   .info_bits = IPU_INFO_STREAM_ID_SET(0),
+				   .nr_l1streams = 16,
+				   .l1_block_sz = {
+						   2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+						   2, 2, 2, 1, 1, 4
+				   },
+				   .nr_l2streams = 16,
+				   .l2_block_sz = {
+						   2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+						   2, 2, 2, 2, 2, 2
+				   },
+				   .insert_read_before_invalidate = false,
+				   .l1_stream_id_reg_offset =
+				   IPU_MMU_L1_STREAM_ID_REG_OFFSET,
+				   .l2_stream_id_reg_offset =
+				   IPU_MMU_L2_STREAM_ID_REG_OFFSET,
+				},
+				{
+				   .offset = IPU_ISYS_IOMMUI_OFFSET,
+				   .info_bits = IPU_INFO_STREAM_ID_SET(0),
+				   .nr_l1streams = 0,
+				   .nr_l2streams = 0,
+				   .insert_read_before_invalidate = false,
+				},
+			},
+		       .cdc_fifos = 3,
+		       .cdc_fifo_threshold = {6, 8, 2},
+		       .dmem_offset = IPU_ISYS_DMEM_OFFSET,
+		       .spc_offset = IPU_ISYS_SPC_OFFSET,
+	},
+	.isys_dma_overshoot = IPU_ISYS_OVERALLOC_MIN,
+};
+
+struct ipu_psys_internal_pdata psys_ipdata = {
+	.hw_variant = {
+		       .offset = IPU_UNIFIED_OFFSET,
+		       .nr_mmus = 4,
+		       .mmu_hw = {
+				{
+				   .offset = IPU_PSYS_IOMMU0_OFFSET,
+				   .info_bits =
+				   IPU_INFO_REQUEST_DESTINATION_IOSF,
+				   .nr_l1streams = 16,
+				   .l1_block_sz = {
+						   2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+						   2, 2, 2, 2, 2, 2
+				   },
+				   .nr_l2streams = 16,
+				   .l2_block_sz = {
+						   2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+						   2, 2, 2, 2, 2, 2
+				   },
+				   .insert_read_before_invalidate = false,
+				   .l1_stream_id_reg_offset =
+				   IPU_MMU_L1_STREAM_ID_REG_OFFSET,
+				   .l2_stream_id_reg_offset =
+				   IPU_MMU_L2_STREAM_ID_REG_OFFSET,
+				},
+				{
+				   .offset = IPU_PSYS_IOMMU1_OFFSET,
+				   .info_bits = IPU_INFO_STREAM_ID_SET(0),
+				   .nr_l1streams = 32,
+				   .l1_block_sz = {
+						   1, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+						   2, 2, 2, 2, 2, 10,
+						   5, 4, 14, 6, 4, 14, 6, 4, 8,
+						   4, 2, 1, 1, 1, 1, 14
+				   },
+				   .nr_l2streams = 32,
+				   .l2_block_sz = {
+						   2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+						   2, 2, 2, 2, 2, 2,
+						   2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+						   2, 2, 2, 2, 2, 2
+				   },
+				   .insert_read_before_invalidate = false,
+				   .l1_stream_id_reg_offset =
+				   IPU_MMU_L1_STREAM_ID_REG_OFFSET,
+				   .l2_stream_id_reg_offset =
+				   IPU_PSYS_MMU1W_L2_STREAM_ID_REG_OFFSET,
+				},
+				{
+				   .offset = IPU_PSYS_IOMMU1R_OFFSET,
+				   .info_bits = IPU_INFO_STREAM_ID_SET(0),
+				   .nr_l1streams = 16,
+				   .l1_block_sz = {
+						   1, 4, 4, 4, 4, 16, 8, 4, 32,
+						   16, 16, 2, 2, 2, 1, 12
+				   },
+				   .nr_l2streams = 16,
+				   .l2_block_sz = {
+						   2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
+						   2, 2, 2, 2, 2, 2
+				   },
+				   .insert_read_before_invalidate = false,
+				   .l1_stream_id_reg_offset =
+				   IPU_MMU_L1_STREAM_ID_REG_OFFSET,
+				   .l2_stream_id_reg_offset =
+				   IPU_MMU_L2_STREAM_ID_REG_OFFSET,
+				},
+				{
+				   .offset = IPU_PSYS_IOMMUI_OFFSET,
+				   .info_bits = IPU_INFO_STREAM_ID_SET(0),
+				   .nr_l1streams = 0,
+				   .nr_l2streams = 0,
+				   .insert_read_before_invalidate = false,
+				},
+		},
+	       .dmem_offset = IPU_PSYS_DMEM_OFFSET,
+	},
+};
+
+const struct ipu_buttress_ctrl isys_buttress_ctrl = {
+	.ratio = IPU_IS_FREQ_CTL_DEFAULT_RATIO,
+	.qos_floor = IPU_IS_FREQ_CTL_DEFAULT_QOS_FLOOR_RATIO,
+	.freq_ctl = IPU_BUTTRESS_REG_IS_FREQ_CTL,
+	.pwr_sts_shift = IPU_BUTTRESS_PWR_STATE_IS_PWR_SHIFT,
+	.pwr_sts_mask = IPU_BUTTRESS_PWR_STATE_IS_PWR_MASK,
+	.pwr_sts_on = IPU_BUTTRESS_PWR_STATE_UP_DONE,
+	.pwr_sts_off = IPU_BUTTRESS_PWR_STATE_DN_DONE,
+};
+
+const struct ipu_buttress_ctrl psys_buttress_ctrl = {
+	.ratio = IPU_PS_FREQ_CTL_DEFAULT_RATIO,
+	.qos_floor = IPU_PS_FREQ_CTL_DEFAULT_QOS_FLOOR_RATIO,
+	.freq_ctl = IPU_BUTTRESS_REG_PS_FREQ_CTL,
+	.pwr_sts_shift = IPU_BUTTRESS_PWR_STATE_PS_PWR_SHIFT,
+	.pwr_sts_mask = IPU_BUTTRESS_PWR_STATE_PS_PWR_MASK,
+	.pwr_sts_on = IPU_BUTTRESS_PWR_STATE_UP_DONE,
+	.pwr_sts_off = IPU_BUTTRESS_PWR_STATE_DN_DONE,
+};
+
+static void ipu6_pkg_dir_configure_spc(struct ipu_device *isp,
+				       const struct ipu_hw_variants *hw_variant,
+				       int pkg_dir_idx, void __iomem *base,
+				       u64 *pkg_dir,
+				       dma_addr_t pkg_dir_vied_address)
+{
+	struct ipu_psys *psys = ipu_bus_get_drvdata(isp->psys);
+	struct ipu_isys *isys = ipu_bus_get_drvdata(isp->isys);
+	unsigned int server_fw_virtaddr;
+	struct ipu_cell_program_t *prog;
+	void __iomem *spc_base;
+	dma_addr_t dma_addr;
+
+	if (!pkg_dir || !isp->cpd_fw) {
+		dev_err(&isp->pdev->dev, "invalid addr\n");
+		return;
+	}
+
+	server_fw_virtaddr = *(pkg_dir + (pkg_dir_idx + 1) * 2);
+	if (pkg_dir_idx == IPU_CPD_PKG_DIR_ISYS_SERVER_IDX) {
+		dma_addr = sg_dma_address(isys->fw_sgt.sgl);
+		prog = (struct ipu_cell_program_t *)((u64)isp->cpd_fw->data +
+							(server_fw_virtaddr -
+							 dma_addr));
+	} else {
+		dma_addr = sg_dma_address(psys->fw_sgt.sgl);
+		prog = (struct ipu_cell_program_t *)((u64)isp->cpd_fw->data +
+							(server_fw_virtaddr -
+							 dma_addr));
+	}
+
+	spc_base = base + prog->regs_addr;
+	if (spc_base != (base + hw_variant->spc_offset))
+		dev_warn(&isp->pdev->dev,
+			 "SPC reg addr 0x%p not matching value from CPD 0x%p\n",
+			 base + hw_variant->spc_offset, spc_base);
+	writel(server_fw_virtaddr + prog->blob_offset +
+	       prog->icache_source, spc_base + IPU_PSYS_REG_SPC_ICACHE_BASE);
+	writel(IPU_INFO_REQUEST_DESTINATION_IOSF,
+	       spc_base + IPU_REG_PSYS_INFO_SEG_0_CONFIG_ICACHE_MASTER);
+	writel(prog->start[1], spc_base + IPU_PSYS_REG_SPC_START_PC);
+	writel(pkg_dir_vied_address, base + hw_variant->dmem_offset);
+}
+
+void ipu_configure_spc(struct ipu_device *isp,
+		       const struct ipu_hw_variants *hw_variant,
+		       int pkg_dir_idx, void __iomem *base, u64 *pkg_dir,
+		       dma_addr_t pkg_dir_dma_addr)
+{
+	u32 val;
+	void __iomem *dmem_base = base + hw_variant->dmem_offset;
+	void __iomem *spc_regs_base = base + hw_variant->spc_offset;
+
+	val = readl(spc_regs_base + IPU_PSYS_REG_SPC_STATUS_CTRL);
+	val |= IPU_PSYS_SPC_STATUS_CTRL_ICACHE_INVALIDATE;
+	writel(val, spc_regs_base + IPU_PSYS_REG_SPC_STATUS_CTRL);
+
+	if (isp->secure_mode)
+		writel(IPU_PKG_DIR_IMR_OFFSET, dmem_base);
+	else
+		ipu6_pkg_dir_configure_spc(isp, hw_variant, pkg_dir_idx, base,
+					   pkg_dir, pkg_dir_dma_addr);
+}
+EXPORT_SYMBOL(ipu_configure_spc);
+
+int ipu_buttress_psys_freq_get(void *data, u64 *val)
+{
+	struct ipu_device *isp = data;
+	u32 reg_val;
+	int rval;
+
+	rval = pm_runtime_get_sync(&isp->psys->dev);
+	if (rval < 0) {
+		pm_runtime_put(&isp->psys->dev);
+		dev_err(&isp->pdev->dev, "Runtime PM failed (%d)\n", rval);
+		return rval;
+	}
+
+	reg_val = readl(isp->base + BUTTRESS_REG_PS_FREQ_CTL);
+
+	pm_runtime_put(&isp->psys->dev);
+
+	*val = IPU_PS_FREQ_RATIO_BASE *
+	    (reg_val & IPU_BUTTRESS_PS_FREQ_CTL_DIVISOR_MASK);
+
+	return 0;
+}
+
+void ipu_internal_pdata_init(void)
+{
+	if (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) {
+		isys_ipdata.csi2.nports = ARRAY_SIZE(ipu6_csi_offsets);
+		isys_ipdata.csi2.offsets = ipu6_csi_offsets;
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+		isys_ipdata.tpg.ntpgs = ARRAY_SIZE(ipu6_tpg_offsets);
+		isys_ipdata.tpg.offsets = ipu6_tpg_offsets;
+		isys_ipdata.tpg.sels = NULL;
+#endif
+		isys_ipdata.num_parallel_streams = IPU6_ISYS_NUM_STREAMS;
+		psys_ipdata.hw_variant.spc_offset = IPU6_PSYS_SPC_OFFSET;
+
+	} else if (ipu_ver == IPU_VER_6SE) {
+		isys_ipdata.csi2.nports = ARRAY_SIZE(ipu6se_csi_offsets);
+		isys_ipdata.csi2.offsets = ipu6se_csi_offsets;
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+		isys_ipdata.tpg.ntpgs = ARRAY_SIZE(ipu6se_tpg_offsets);
+		isys_ipdata.tpg.offsets = ipu6se_tpg_offsets;
+		isys_ipdata.tpg.sels = NULL;
+#endif
+		isys_ipdata.num_parallel_streams = IPU6SE_ISYS_NUM_STREAMS;
+		psys_ipdata.hw_variant.spc_offset = IPU6SE_PSYS_SPC_OFFSET;
+	}
+}
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6ep-fw-resources.c b/drivers/media/pci/intel/ipu6/ipu6ep-fw-resources.c
--- a/drivers/media/pci/intel/ipu6/ipu6ep-fw-resources.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6ep-fw-resources.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,393 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2020 Intel Corporation
+
+#include <linux/err.h>
+#include <linux/string.h>
+
+#include "ipu-psys.h"
+#include "ipu-fw-psys.h"
+#include "ipu6-platform-resources.h"
+#include "ipu6ep-platform-resources.h"
+
+/* resources table */
+
+/*
+ * Cell types by cell IDs
+ */
+static const u8 ipu6ep_fw_psys_cell_types[IPU6EP_FW_PSYS_N_CELL_ID] = {
+	IPU6_FW_PSYS_SP_CTRL_TYPE_ID,
+	IPU6_FW_PSYS_VP_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_OSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_OSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_OSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID, /* AF */
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID, /* X2B_MD */
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID, /* X2B_SVE_RGBIR */
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID, /* PAF */
+	IPU6_FW_PSYS_GDC_TYPE_ID,
+	IPU6_FW_PSYS_TNR_TYPE_ID,
+};
+
+static const u16 ipu6ep_fw_num_dev_channels[IPU6_FW_PSYS_N_DEV_CHN_ID] = {
+	IPU6_FW_PSYS_DEV_CHN_DMA_EXT0_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_CHN_DMA_EXT1_READ_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_CHN_DMA_EXT1_WRITE_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_CHN_DMA_INTERNAL_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_CHN_DMA_ISA_MAX_SIZE,
+};
+
+static const u16 ipu6ep_fw_psys_mem_size[IPU6_FW_PSYS_N_MEM_ID] = {
+	IPU6_FW_PSYS_VMEM0_MAX_SIZE,
+	IPU6_FW_PSYS_TRANSFER_VMEM0_MAX_SIZE,
+	IPU6_FW_PSYS_TRANSFER_VMEM1_MAX_SIZE,
+	IPU6_FW_PSYS_LB_VMEM_MAX_SIZE,
+	IPU6_FW_PSYS_BAMEM0_MAX_SIZE,
+	IPU6_FW_PSYS_DMEM0_MAX_SIZE,
+	IPU6_FW_PSYS_DMEM1_MAX_SIZE,
+	IPU6_FW_PSYS_DMEM2_MAX_SIZE,
+	IPU6_FW_PSYS_DMEM3_MAX_SIZE,
+	IPU6_FW_PSYS_PMEM0_MAX_SIZE
+};
+
+static const u16 ipu6ep_fw_psys_dfms[IPU6_FW_PSYS_N_DEV_DFM_ID] = {
+	IPU6_FW_PSYS_DEV_DFM_BB_FULL_PORT_ID_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_DFM_BB_EMPTY_PORT_ID_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_DFM_ISL_FULL_PORT_ID_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_DFM_ISL_EMPTY_PORT_ID_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_DFM_LB_FULL_PORT_ID_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_DFM_LB_EMPTY_PORT_ID_MAX_SIZE,
+};
+
+static const u8
+ipu6ep_fw_psys_c_mem[IPU6EP_FW_PSYS_N_CELL_ID][IPU6_FW_PSYS_N_MEM_TYPE_ID] = {
+	{
+		/* IPU6_FW_PSYS_SP0_ID */
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_DMEM0_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_SP1_ID */
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_DMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_VP0_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_DMEM3_ID,
+		IPU6_FW_PSYS_VMEM0_ID,
+		IPU6_FW_PSYS_BAMEM0_ID,
+		IPU6_FW_PSYS_PMEM0_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC1_ID BNLM */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC2_ID DM */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC3_ID ACM */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC4_ID GTC YUV1 */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC5_ID OFS pin main */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC6_ID OFS pin display */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC7_ID OFS pin pp */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC8_ID GAMMASTAR */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC9_ID GLTM */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC10_ID XNR */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_ICA_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_LSC_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_DPC_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_SIS_A_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_SIS_B_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_B2B_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_B2R_ID and ISA_R2I_SIE */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_R2I_DS_A_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_AWB_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_AE_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_AF_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_X2B_MD_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_X2B_SVE_RGBIR_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_PAF_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_BB_ACC_GDC0_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_BB_ACC_TNR_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	}
+};
+
+static const struct ipu_fw_resource_definitions ipu6ep_defs = {
+	.cells = ipu6ep_fw_psys_cell_types,
+	.num_cells = IPU6EP_FW_PSYS_N_CELL_ID,
+	.num_cells_type = IPU6_FW_PSYS_N_CELL_TYPE_ID,
+
+	.dev_channels = ipu6ep_fw_num_dev_channels,
+	.num_dev_channels = IPU6_FW_PSYS_N_DEV_CHN_ID,
+
+	.num_ext_mem_types = IPU6_FW_PSYS_N_DATA_MEM_TYPE_ID,
+	.num_ext_mem_ids = IPU6_FW_PSYS_N_MEM_ID,
+	.ext_mem_ids = ipu6ep_fw_psys_mem_size,
+
+	.num_dfm_ids = IPU6_FW_PSYS_N_DEV_DFM_ID,
+
+	.dfms = ipu6ep_fw_psys_dfms,
+
+	.cell_mem_row = IPU6_FW_PSYS_N_MEM_TYPE_ID,
+	.cell_mem = &ipu6ep_fw_psys_c_mem[0][0],
+};
+
+const struct ipu_fw_resource_definitions *ipu6ep_res_defs = &ipu6ep_defs;
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6ep-platform-resources.h b/drivers/media/pci/intel/ipu6/ipu6ep-platform-resources.h
--- a/drivers/media/pci/intel/ipu6/ipu6ep-platform-resources.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6ep-platform-resources.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,42 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2020 Intel Corporation */
+
+#ifndef IPU6EP_PLATFORM_RESOURCES_H
+#define IPU6EP_PLATFORM_RESOURCES_H
+
+#include <linux/kernel.h>
+#include <linux/device.h>
+
+enum {
+	IPU6EP_FW_PSYS_SP0_ID = 0,
+	IPU6EP_FW_PSYS_VP0_ID,
+	IPU6EP_FW_PSYS_PSA_ACC_BNLM_ID,
+	IPU6EP_FW_PSYS_PSA_ACC_DM_ID,
+	IPU6EP_FW_PSYS_PSA_ACC_ACM_ID,
+	IPU6EP_FW_PSYS_PSA_ACC_GTC_YUV1_ID,
+	IPU6EP_FW_PSYS_BB_ACC_OFS_PIN_MAIN_ID,
+	IPU6EP_FW_PSYS_BB_ACC_OFS_PIN_DISPLAY_ID,
+	IPU6EP_FW_PSYS_BB_ACC_OFS_PIN_PP_ID,
+	IPU6EP_FW_PSYS_PSA_ACC_GAMMASTAR_ID,
+	IPU6EP_FW_PSYS_PSA_ACC_GLTM_ID,
+	IPU6EP_FW_PSYS_PSA_ACC_XNR_ID,
+	IPU6EP_FW_PSYS_PSA_VCSC_ID,	/* VCSC */
+	IPU6EP_FW_PSYS_ISA_ICA_ID,
+	IPU6EP_FW_PSYS_ISA_LSC_ID,
+	IPU6EP_FW_PSYS_ISA_DPC_ID,
+	IPU6EP_FW_PSYS_ISA_SIS_A_ID,
+	IPU6EP_FW_PSYS_ISA_SIS_B_ID,
+	IPU6EP_FW_PSYS_ISA_B2B_ID,
+	IPU6EP_FW_PSYS_ISA_B2R_R2I_SIE_ID,
+	IPU6EP_FW_PSYS_ISA_R2I_DS_A_ID,
+	IPU6EP_FW_PSYS_ISA_AWB_ID,
+	IPU6EP_FW_PSYS_ISA_AE_ID,
+	IPU6EP_FW_PSYS_ISA_AF_ID,
+	IPU6EP_FW_PSYS_ISA_X2B_MD_ID,
+	IPU6EP_FW_PSYS_ISA_X2B_SVE_RGBIR_ID,
+	IPU6EP_FW_PSYS_ISA_PAF_ID,
+	IPU6EP_FW_PSYS_BB_ACC_GDC0_ID,
+	IPU6EP_FW_PSYS_BB_ACC_TNR_ID,
+	IPU6EP_FW_PSYS_N_CELL_ID
+};
+#endif /* IPU6EP_PLATFORM_RESOURCES_H */
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-fw-resources.c b/drivers/media/pci/intel/ipu6/ipu6-fw-resources.c
--- a/drivers/media/pci/intel/ipu6/ipu6-fw-resources.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-fw-resources.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,608 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2015 - 2021 Intel Corporation
+
+#include <linux/err.h>
+#include <linux/string.h>
+
+#include "ipu-psys.h"
+#include "ipu-fw-psys.h"
+#include "ipu6-platform-resources.h"
+
+/* resources table */
+
+/*
+ * Cell types by cell IDs
+ */
+static const u8 ipu6_fw_psys_cell_types[IPU6_FW_PSYS_N_CELL_ID] = {
+	IPU6_FW_PSYS_SP_CTRL_TYPE_ID,
+	IPU6_FW_PSYS_VP_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_OSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_OSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_OSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID, /* X2B_MD */
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID, /* ICA_MEDIUM */
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID, /* X2B_SVE_RGBIR */
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID, /* PAF */
+	IPU6_FW_PSYS_GDC_TYPE_ID,
+	IPU6_FW_PSYS_TNR_TYPE_ID,
+};
+
+static const u16 ipu6_fw_num_dev_channels[IPU6_FW_PSYS_N_DEV_CHN_ID] = {
+	IPU6_FW_PSYS_DEV_CHN_DMA_EXT0_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_CHN_DMA_EXT1_READ_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_CHN_DMA_EXT1_WRITE_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_CHN_DMA_INTERNAL_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_CHN_DMA_ISA_MAX_SIZE,
+};
+
+static const u16 ipu6_fw_psys_mem_size[IPU6_FW_PSYS_N_MEM_ID] = {
+	IPU6_FW_PSYS_VMEM0_MAX_SIZE,
+	IPU6_FW_PSYS_TRANSFER_VMEM0_MAX_SIZE,
+	IPU6_FW_PSYS_TRANSFER_VMEM1_MAX_SIZE,
+	IPU6_FW_PSYS_LB_VMEM_MAX_SIZE,
+	IPU6_FW_PSYS_BAMEM0_MAX_SIZE,
+	IPU6_FW_PSYS_DMEM0_MAX_SIZE,
+	IPU6_FW_PSYS_DMEM1_MAX_SIZE,
+	IPU6_FW_PSYS_DMEM2_MAX_SIZE,
+	IPU6_FW_PSYS_DMEM3_MAX_SIZE,
+	IPU6_FW_PSYS_PMEM0_MAX_SIZE
+};
+
+static const u16 ipu6_fw_psys_dfms[IPU6_FW_PSYS_N_DEV_DFM_ID] = {
+	IPU6_FW_PSYS_DEV_DFM_BB_FULL_PORT_ID_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_DFM_BB_EMPTY_PORT_ID_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_DFM_ISL_FULL_PORT_ID_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_DFM_ISL_EMPTY_PORT_ID_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_DFM_LB_FULL_PORT_ID_MAX_SIZE,
+	IPU6_FW_PSYS_DEV_DFM_LB_EMPTY_PORT_ID_MAX_SIZE,
+};
+
+static const u8
+ipu6_fw_psys_c_mem[IPU6_FW_PSYS_N_CELL_ID][IPU6_FW_PSYS_N_MEM_TYPE_ID] = {
+	{
+		/* IPU6_FW_PSYS_SP0_ID */
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_DMEM0_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_SP1_ID */
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_DMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_VP0_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_DMEM3_ID,
+		IPU6_FW_PSYS_VMEM0_ID,
+		IPU6_FW_PSYS_BAMEM0_ID,
+		IPU6_FW_PSYS_PMEM0_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC1_ID BNLM */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC2_ID DM */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC3_ID ACM */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC4_ID GTC YUV1 */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC5_ID OFS pin main */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC6_ID OFS pin display */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC7_ID OFS pin pp */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC8_ID GAMMASTAR */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC9_ID GLTM */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ACC10_ID XNR */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_ICA_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_LSC_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_DPC_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_SIS_A_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_SIS_B_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_B2B_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_B2R_ID and ISA_R2I_SIE */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_R2I_DS_A_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_R2I_DS_B_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_AWB_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_AE_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_AF_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_DOL_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_X2B_MD_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_ICA_MEDIUM_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_X2B_SVE_RGBIR_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_ISA_PAF_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_LB_VMEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_BB_ACC_GDC0_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	},
+	{
+		/* IPU6_FW_PSYS_BB_ACC_TNR_ID */
+		IPU6_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6_FW_PSYS_TRANSFER_VMEM1_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+		IPU6_FW_PSYS_N_MEM_ID,
+	}
+};
+
+static const struct ipu_fw_resource_definitions ipu6_defs = {
+	.cells = ipu6_fw_psys_cell_types,
+	.num_cells = IPU6_FW_PSYS_N_CELL_ID,
+	.num_cells_type = IPU6_FW_PSYS_N_CELL_TYPE_ID,
+
+	.dev_channels = ipu6_fw_num_dev_channels,
+	.num_dev_channels = IPU6_FW_PSYS_N_DEV_CHN_ID,
+
+	.num_ext_mem_types = IPU6_FW_PSYS_N_DATA_MEM_TYPE_ID,
+	.num_ext_mem_ids = IPU6_FW_PSYS_N_MEM_ID,
+	.ext_mem_ids = ipu6_fw_psys_mem_size,
+
+	.num_dfm_ids = IPU6_FW_PSYS_N_DEV_DFM_ID,
+
+	.dfms = ipu6_fw_psys_dfms,
+
+	.cell_mem_row = IPU6_FW_PSYS_N_MEM_TYPE_ID,
+	.cell_mem = &ipu6_fw_psys_c_mem[0][0],
+};
+
+const struct ipu_fw_resource_definitions *ipu6_res_defs = &ipu6_defs;
+
+/********** Generic resource handling **********/
+
+int ipu6_fw_psys_set_proc_dev_chn(struct ipu_fw_psys_process *ptr, u16 offset,
+				  u16 value)
+{
+	struct ipu6_fw_psys_process_ext *pm_ext;
+	u8 ps_ext_offset;
+
+	ps_ext_offset = ptr->process_extension_offset;
+	if (!ps_ext_offset)
+		return -EINVAL;
+
+	pm_ext = (struct ipu6_fw_psys_process_ext *)((u8 *)ptr + ps_ext_offset);
+
+	pm_ext->dev_chn_offset[offset] = value;
+
+	return 0;
+}
+
+int ipu6_fw_psys_set_proc_dfm_bitmap(struct ipu_fw_psys_process *ptr,
+				     u16 id, u32 bitmap,
+				     u32 active_bitmap)
+{
+	struct ipu6_fw_psys_process_ext *pm_ext;
+	u8 ps_ext_offset;
+
+	ps_ext_offset = ptr->process_extension_offset;
+	if (!ps_ext_offset)
+		return -EINVAL;
+
+	pm_ext = (struct ipu6_fw_psys_process_ext *)((u8 *)ptr + ps_ext_offset);
+
+	pm_ext->dfm_port_bitmap[id] = bitmap;
+	pm_ext->dfm_active_port_bitmap[id] = active_bitmap;
+
+	return 0;
+}
+
+int ipu6_fw_psys_set_process_ext_mem(struct ipu_fw_psys_process *ptr,
+				     u16 type_id, u16 mem_id, u16 offset)
+{
+	struct ipu6_fw_psys_process_ext *pm_ext;
+	u8 ps_ext_offset;
+
+	ps_ext_offset = ptr->process_extension_offset;
+	if (!ps_ext_offset)
+		return -EINVAL;
+
+	pm_ext = (struct ipu6_fw_psys_process_ext *)((u8 *)ptr + ps_ext_offset);
+
+	pm_ext->ext_mem_offset[type_id] = offset;
+	pm_ext->ext_mem_id[type_id] = mem_id;
+
+	return 0;
+}
+
+static struct ipu_fw_psys_program_manifest *
+get_program_manifest(const struct ipu_fw_psys_program_group_manifest *manifest,
+		     const unsigned int program_index)
+{
+	struct ipu_fw_psys_program_manifest *prg_manifest_base;
+	u8 *program_manifest = NULL;
+	u8 program_count;
+	unsigned int i;
+
+	program_count = manifest->program_count;
+
+	prg_manifest_base = (struct ipu_fw_psys_program_manifest *)
+		((char *)manifest + manifest->program_manifest_offset);
+	if (program_index < program_count) {
+		program_manifest = (u8 *)prg_manifest_base;
+		for (i = 0; i < program_index; i++)
+			program_manifest +=
+				((struct ipu_fw_psys_program_manifest *)
+				 program_manifest)->size;
+	}
+
+	return (struct ipu_fw_psys_program_manifest *)program_manifest;
+}
+
+int ipu6_fw_psys_get_program_manifest_by_process(
+	struct ipu_fw_generic_program_manifest *gen_pm,
+	const struct ipu_fw_psys_program_group_manifest *pg_manifest,
+	struct ipu_fw_psys_process *process)
+{
+	u32 program_id = process->program_idx;
+	struct ipu_fw_psys_program_manifest *pm;
+	struct ipu6_fw_psys_program_manifest_ext *pm_ext;
+
+	pm = get_program_manifest(pg_manifest, program_id);
+
+	if (!pm)
+		return -ENOENT;
+
+	if (pm->program_extension_offset) {
+		pm_ext = (struct ipu6_fw_psys_program_manifest_ext *)
+			((u8 *)pm + pm->program_extension_offset);
+
+		gen_pm->dev_chn_size = pm_ext->dev_chn_size;
+		gen_pm->dev_chn_offset = pm_ext->dev_chn_offset;
+		gen_pm->ext_mem_size = pm_ext->ext_mem_size;
+		gen_pm->ext_mem_offset = (u16 *)pm_ext->ext_mem_offset;
+		gen_pm->is_dfm_relocatable = pm_ext->is_dfm_relocatable;
+		gen_pm->dfm_port_bitmap = pm_ext->dfm_port_bitmap;
+		gen_pm->dfm_active_port_bitmap =
+			pm_ext->dfm_active_port_bitmap;
+	}
+
+	memcpy(gen_pm->cells, pm->cells, sizeof(pm->cells));
+	gen_pm->cell_id = pm->cells[0];
+	gen_pm->cell_type_id = pm->cell_type_id;
+
+	return 0;
+}
+
+#if defined(DEBUG) || defined(CONFIG_DYNAMIC_DEBUG) || \
+	(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))
+void ipu6_fw_psys_pg_dump(struct ipu_psys *psys,
+			  struct ipu_psys_kcmd *kcmd, const char *note)
+{
+	struct ipu_fw_psys_process_group *pg = kcmd->kpg->pg;
+	u32 pgid = pg->ID;
+	u8 processes = pg->process_count;
+	u16 *process_offset_table = (u16 *)((char *)pg + pg->processes_offset);
+	unsigned int p, chn, mem, mem_id;
+	unsigned int mem_type, max_mem_id, dev_chn;
+
+	if (ipu_ver == IPU_VER_6SE) {
+		mem_type = IPU6SE_FW_PSYS_N_DATA_MEM_TYPE_ID;
+		max_mem_id = IPU6SE_FW_PSYS_N_MEM_ID;
+		dev_chn = IPU6SE_FW_PSYS_N_DEV_CHN_ID;
+	} else if (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) {
+		mem_type = IPU6_FW_PSYS_N_DATA_MEM_TYPE_ID;
+		max_mem_id = IPU6_FW_PSYS_N_MEM_ID;
+		dev_chn = IPU6_FW_PSYS_N_DEV_CHN_ID;
+	} else {
+		WARN(1, "%s ipu_ver:[%u] is unsupported!\n", __func__, ipu_ver);
+		return;
+	}
+
+	dev_dbg(&psys->adev->dev, "%s %s pgid %i has %i processes:\n",
+		__func__, note, pgid, processes);
+
+	for (p = 0; p < processes; p++) {
+		struct ipu_fw_psys_process *process =
+		    (struct ipu_fw_psys_process *)
+		    ((char *)pg + process_offset_table[p]);
+		struct ipu6_fw_psys_process_ext *pm_ext =
+		    (struct ipu6_fw_psys_process_ext *)((u8 *)process
+		    + process->process_extension_offset);
+		dev_dbg(&psys->adev->dev, "\t process %i size=%u",
+			p, process->size);
+		if (!process->process_extension_offset)
+			continue;
+
+		for (mem = 0; mem < mem_type; mem++) {
+			mem_id = pm_ext->ext_mem_id[mem];
+			if (mem_id != max_mem_id)
+				dev_dbg(&psys->adev->dev,
+					"\t mem type %u id %d offset=0x%x",
+					mem, mem_id,
+					pm_ext->ext_mem_offset[mem]);
+		}
+		for (chn = 0; chn < dev_chn; chn++) {
+			if (pm_ext->dev_chn_offset[chn] != (u16)(-1))
+				dev_dbg(&psys->adev->dev,
+					"\t dev_chn[%u]=0x%x\n",
+					chn, pm_ext->dev_chn_offset[chn]);
+		}
+	}
+}
+#else
+void ipu6_fw_psys_pg_dump(struct ipu_psys *psys,
+			  struct ipu_psys_kcmd *kcmd, const char *note)
+{
+	if (ipu_ver == IPU_VER_6SE || ipu_ver == IPU_VER_6 ||
+	    ipu_ver == IPU_VER_6EP)
+		return;
+
+	WARN(1, "%s ipu_ver:[%u] is unsupported!\n", __func__, ipu_ver);
+}
+#endif
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-isys.c b/drivers/media/pci/intel/ipu6/ipu6-isys.c
--- a/drivers/media/pci/intel/ipu6/ipu6-isys.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-isys.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,322 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2020 Intel Corporation
+
+#include <linux/module.h>
+#include <media/v4l2-event.h>
+
+#include "ipu.h"
+#include "ipu-platform-regs.h"
+#include "ipu-trace.h"
+#include "ipu-isys.h"
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+#include "ipu-isys-tpg.h"
+#endif
+#include "ipu-platform-isys-csi2-reg.h"
+
+const struct ipu_isys_pixelformat ipu_isys_pfmts[] = {
+	{V4L2_PIX_FMT_SBGGR12, 16, 12, 0, MEDIA_BUS_FMT_SBGGR12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SGBRG12, 16, 12, 0, MEDIA_BUS_FMT_SGBRG12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SGRBG12, 16, 12, 0, MEDIA_BUS_FMT_SGRBG12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SRGGB12, 16, 12, 0, MEDIA_BUS_FMT_SRGGB12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SBGGR10, 16, 10, 0, MEDIA_BUS_FMT_SBGGR10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SGBRG10, 16, 10, 0, MEDIA_BUS_FMT_SGBRG10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SGRBG10, 16, 10, 0, MEDIA_BUS_FMT_SGRBG10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SRGGB10, 16, 10, 0, MEDIA_BUS_FMT_SRGGB10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SBGGR8, 8, 8, 0, MEDIA_BUS_FMT_SBGGR8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{V4L2_PIX_FMT_SGBRG8, 8, 8, 0, MEDIA_BUS_FMT_SGBRG8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{V4L2_PIX_FMT_SGRBG8, 8, 8, 0, MEDIA_BUS_FMT_SGRBG8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{V4L2_PIX_FMT_SRGGB8, 8, 8, 0, MEDIA_BUS_FMT_SRGGB8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{}
+};
+
+struct ipu_trace_block isys_trace_blocks[] = {
+	{
+		.offset = IPU_TRACE_REG_IS_TRACE_UNIT_BASE,
+		.type = IPU_TRACE_BLOCK_TUN,
+	},
+	{
+		.offset = IPU_TRACE_REG_IS_SP_EVQ_BASE,
+		.type = IPU_TRACE_BLOCK_TM,
+	},
+	{
+		.offset = IPU_TRACE_REG_IS_SP_GPC_BASE,
+		.type = IPU_TRACE_BLOCK_GPC,
+	},
+	{
+		.offset = IPU_TRACE_REG_IS_ISL_GPC_BASE,
+		.type = IPU_TRACE_BLOCK_GPC,
+	},
+	{
+		.offset = IPU_TRACE_REG_IS_MMU_GPC_BASE,
+		.type = IPU_TRACE_BLOCK_GPC,
+	},
+	{
+		/* Note! this covers all 8 blocks */
+		.offset = IPU_TRACE_REG_CSI2_TM_BASE(0),
+		.type = IPU_TRACE_CSI2,
+	},
+	{
+		/* Note! this covers all 11 blocks */
+		.offset = IPU_TRACE_REG_CSI2_PORT_SIG2SIO_GR_BASE(0),
+		.type = IPU_TRACE_SIG2CIOS,
+	},
+	{
+		.offset = IPU_TRACE_REG_IS_GPREG_TRACE_TIMER_RST_N,
+		.type = IPU_TRACE_TIMER_RST,
+	},
+	{
+		.type = IPU_TRACE_BLOCK_END,
+	}
+};
+
+void isys_setup_hw(struct ipu_isys *isys)
+{
+	void __iomem *base = isys->pdata->base;
+	const u8 *thd = isys->pdata->ipdata->hw_variant.cdc_fifo_threshold;
+	u32 irqs = 0;
+	unsigned int i, nr;
+
+	nr = (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) ?
+		IPU6_ISYS_CSI_PORT_NUM : IPU6SE_ISYS_CSI_PORT_NUM;
+
+	/* Enable irqs for all MIPI ports */
+	for (i = 0; i < nr; i++)
+		irqs |= IPU_ISYS_UNISPART_IRQ_CSI2(i);
+
+	writel(irqs, base + IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_EDGE);
+	writel(irqs, base + IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_LEVEL_NOT_PULSE);
+	writel(0xffffffff, base + IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_CLEAR);
+	writel(irqs, base + IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_MASK);
+	writel(irqs, base + IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_ENABLE);
+
+	irqs = ISYS_UNISPART_IRQS;
+	writel(irqs, base + IPU_REG_ISYS_UNISPART_IRQ_EDGE);
+	writel(irqs, base + IPU_REG_ISYS_UNISPART_IRQ_LEVEL_NOT_PULSE);
+	writel(0xffffffff, base + IPU_REG_ISYS_UNISPART_IRQ_CLEAR);
+	writel(irqs, base + IPU_REG_ISYS_UNISPART_IRQ_MASK);
+	writel(irqs, base + IPU_REG_ISYS_UNISPART_IRQ_ENABLE);
+
+	writel(0, base + IPU_REG_ISYS_UNISPART_SW_IRQ_REG);
+	writel(0, base + IPU_REG_ISYS_UNISPART_SW_IRQ_MUX_REG);
+
+	/* Write CDC FIFO threshold values for isys */
+	for (i = 0; i < isys->pdata->ipdata->hw_variant.cdc_fifos; i++)
+		writel(thd[i], base + IPU_REG_ISYS_CDC_THRESHOLD(i));
+}
+
+irqreturn_t isys_isr(struct ipu_bus_device *adev)
+{
+	struct ipu_isys *isys = ipu_bus_get_drvdata(adev);
+	void __iomem *base = isys->pdata->base;
+	u32 status_sw, status_csi;
+
+	spin_lock(&isys->power_lock);
+	if (!isys->power) {
+		spin_unlock(&isys->power_lock);
+		return IRQ_NONE;
+	}
+
+	status_csi = readl(isys->pdata->base +
+			   IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_STATUS);
+	status_sw = readl(isys->pdata->base + IPU_REG_ISYS_UNISPART_IRQ_STATUS);
+
+	writel(ISYS_UNISPART_IRQS & ~IPU_ISYS_UNISPART_IRQ_SW,
+	       base + IPU_REG_ISYS_UNISPART_IRQ_MASK);
+
+	do {
+		writel(status_csi, isys->pdata->base +
+			   IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_CLEAR);
+		writel(status_sw, isys->pdata->base +
+			   IPU_REG_ISYS_UNISPART_IRQ_CLEAR);
+
+		if (isys->isr_csi2_bits & status_csi) {
+			unsigned int i;
+
+			for (i = 0; i < isys->pdata->ipdata->csi2.nports; i++) {
+				/* irq from not enabled port */
+				if (!isys->csi2[i].base)
+					continue;
+				if (status_csi & IPU_ISYS_UNISPART_IRQ_CSI2(i))
+					ipu_isys_csi2_isr(&isys->csi2[i]);
+			}
+		}
+
+		writel(0, base + IPU_REG_ISYS_UNISPART_SW_IRQ_REG);
+
+		if (!isys_isr_one(adev))
+			status_sw = IPU_ISYS_UNISPART_IRQ_SW;
+		else
+			status_sw = 0;
+
+		status_csi = readl(isys->pdata->base +
+				       IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_STATUS);
+		status_sw |= readl(isys->pdata->base +
+				       IPU_REG_ISYS_UNISPART_IRQ_STATUS);
+	} while (((status_csi & isys->isr_csi2_bits) ||
+		  (status_sw & IPU_ISYS_UNISPART_IRQ_SW)) &&
+		 !isys->adev->isp->flr_done);
+
+	writel(ISYS_UNISPART_IRQS, base + IPU_REG_ISYS_UNISPART_IRQ_MASK);
+
+	spin_unlock(&isys->power_lock);
+
+	return IRQ_HANDLED;
+}
+
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+void ipu_isys_tpg_sof_event(struct ipu_isys_tpg *tpg)
+{
+	struct ipu_isys_pipeline *ip = NULL;
+	struct v4l2_event ev = {
+		.type = V4L2_EVENT_FRAME_SYNC,
+	};
+	struct video_device *vdev = tpg->asd.sd.devnode;
+	unsigned long flags;
+	unsigned int i, nr;
+
+	nr = (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) ?
+		IPU6_ISYS_CSI_PORT_NUM : IPU6SE_ISYS_CSI_PORT_NUM;
+
+	spin_lock_irqsave(&tpg->isys->lock, flags);
+	for (i = 0; i < nr; i++) {
+		if (tpg->isys->pipes[i] && tpg->isys->pipes[i]->tpg == tpg) {
+			ip = tpg->isys->pipes[i];
+			break;
+		}
+	}
+
+	/* Pipe already vanished */
+	if (!ip) {
+		spin_unlock_irqrestore(&tpg->isys->lock, flags);
+		return;
+	}
+
+	ev.u.frame_sync.frame_sequence =
+		atomic_inc_return(&ip->sequence) - 1;
+	spin_unlock_irqrestore(&tpg->isys->lock, flags);
+
+	v4l2_event_queue(vdev, &ev);
+
+	dev_dbg(&tpg->isys->adev->dev,
+		"sof_event::tpg-%i sequence: %i\n",
+		tpg->index, ev.u.frame_sync.frame_sequence);
+}
+
+void ipu_isys_tpg_eof_event(struct ipu_isys_tpg *tpg)
+{
+	struct ipu_isys_pipeline *ip = NULL;
+	unsigned long flags;
+	unsigned int i, nr;
+	u32 frame_sequence;
+
+	nr = (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) ?
+		IPU6_ISYS_CSI_PORT_NUM : IPU6SE_ISYS_CSI_PORT_NUM;
+
+	spin_lock_irqsave(&tpg->isys->lock, flags);
+	for (i = 0; i < nr; i++) {
+		if (tpg->isys->pipes[i] && tpg->isys->pipes[i]->tpg == tpg) {
+			ip = tpg->isys->pipes[i];
+			break;
+		}
+	}
+
+	/* Pipe already vanished */
+	if (!ip) {
+		spin_unlock_irqrestore(&tpg->isys->lock, flags);
+		return;
+	}
+
+	frame_sequence = atomic_read(&ip->sequence);
+
+	spin_unlock_irqrestore(&tpg->isys->lock, flags);
+
+	dev_dbg(&tpg->isys->adev->dev,
+		"eof_event::tpg-%i sequence: %i\n",
+		tpg->index, frame_sequence);
+}
+
+int tpg_set_stream(struct v4l2_subdev *sd, int enable)
+{
+	struct ipu_isys_tpg *tpg = to_ipu_isys_tpg(sd);
+	__u32 code = tpg->asd.ffmt[TPG_PAD_SOURCE].code;
+	unsigned int bpp = ipu_isys_mbus_code_to_bpp(code);
+	struct ipu_isys_pipeline *ip =
+			to_ipu_isys_pipeline(sd->entity.pipe);
+
+	/*
+	 * MIPI_GEN block is CSI2 FB. Need to enable/disable TPG selection
+	 * register to control the TPG streaming.
+	 */
+	if (tpg->sel)
+		writel(enable ? 1 : 0, tpg->sel);
+
+	if (!enable) {
+		ip->tpg = NULL;
+		writel(0, tpg->base +
+		       CSI_REG_CSI_FE_ENABLE -
+		       CSI_REG_PIXGEN_COM_BASE_OFFSET);
+		writel(CSI_SENSOR_INPUT, tpg->base +
+		       CSI_REG_CSI_FE_MUX_CTRL -
+		       CSI_REG_PIXGEN_COM_BASE_OFFSET);
+		writel(CSI_CNTR_SENSOR_LINE_ID |
+		       CSI_CNTR_SENSOR_FRAME_ID,
+		       tpg->base + CSI_REG_CSI_FE_SYNC_CNTR_SEL -
+		       CSI_REG_PIXGEN_COM_BASE_OFFSET);
+		writel(0, tpg->base + MIPI_GEN_REG_COM_ENABLE);
+		return 0;
+	}
+
+	ip->has_sof = true;
+	ip->tpg = tpg;
+	/* Select MIPI GEN as input */
+	writel(0, tpg->base + CSI_REG_CSI_FE_MODE -
+	       CSI_REG_PIXGEN_COM_BASE_OFFSET);
+	writel(1, tpg->base + CSI_REG_CSI_FE_ENABLE -
+	       CSI_REG_PIXGEN_COM_BASE_OFFSET);
+	writel(CSI_MIPIGEN_INPUT, tpg->base +
+	       CSI_REG_CSI_FE_MUX_CTRL - CSI_REG_PIXGEN_COM_BASE_OFFSET);
+	writel(0, tpg->base + CSI_REG_CSI_FE_SYNC_CNTR_SEL -
+	       CSI_REG_PIXGEN_COM_BASE_OFFSET);
+
+	writel(MIPI_GEN_COM_DTYPE_RAW(bpp),
+	       tpg->base + MIPI_GEN_REG_COM_DTYPE);
+	writel(ipu_isys_mbus_code_to_mipi(code),
+	       tpg->base + MIPI_GEN_REG_COM_VTYPE);
+	writel(0, tpg->base + MIPI_GEN_REG_COM_VCHAN);
+
+	writel(0, tpg->base + MIPI_GEN_REG_SYNG_NOF_FRAMES);
+
+	writel(DIV_ROUND_UP(tpg->asd.ffmt[TPG_PAD_SOURCE].width *
+			    bpp, BITS_PER_BYTE),
+	       tpg->base + MIPI_GEN_REG_COM_WCOUNT);
+	writel(DIV_ROUND_UP(tpg->asd.ffmt[TPG_PAD_SOURCE].width,
+			    MIPI_GEN_PPC),
+	       tpg->base + MIPI_GEN_REG_SYNG_NOF_PIXELS);
+	writel(tpg->asd.ffmt[TPG_PAD_SOURCE].height,
+	       tpg->base + MIPI_GEN_REG_SYNG_NOF_LINES);
+
+	writel(0, tpg->base + MIPI_GEN_REG_TPG_MODE);
+	writel(-1, tpg->base + MIPI_GEN_REG_TPG_HCNT_MASK);
+	writel(-1, tpg->base + MIPI_GEN_REG_TPG_VCNT_MASK);
+	writel(-1, tpg->base + MIPI_GEN_REG_TPG_XYCNT_MASK);
+	writel(0, tpg->base + MIPI_GEN_REG_TPG_HCNT_DELTA);
+	writel(0, tpg->base + MIPI_GEN_REG_TPG_VCNT_DELTA);
+
+	v4l2_ctrl_handler_setup(&tpg->asd.ctrl_handler);
+
+	writel(2, tpg->base + MIPI_GEN_REG_COM_ENABLE);
+	return 0;
+}
+#endif
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-isys-csi2.c b/drivers/media/pci/intel/ipu6/ipu6-isys-csi2.c
--- a/drivers/media/pci/intel/ipu6/ipu6-isys-csi2.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-isys-csi2.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,513 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2020 Intel Corporation
+
+#include <linux/delay.h>
+#include <linux/spinlock.h>
+#include <media/ipu-isys.h>
+#include "ipu.h"
+#include "ipu-buttress.h"
+#include "ipu-isys.h"
+#include "ipu-platform-buttress-regs.h"
+#include "ipu-platform-regs.h"
+#include "ipu-platform-isys-csi2-reg.h"
+#include "ipu6-isys-csi2.h"
+#include "ipu6-isys-phy.h"
+#include "ipu-isys-csi2.h"
+
+struct ipu6_csi2_error {
+	const char *error_string;
+	bool is_info_only;
+};
+
+struct ipu6_csi_irq_info_map {
+	u32 irq_error_mask;
+	u32 irq_num;
+	unsigned int irq_base;
+	unsigned int irq_base_ctrl2;
+	struct ipu6_csi2_error *errors;
+};
+
+/*
+ * Strings corresponding to CSI-2 receiver errors are here.
+ * Corresponding macros are defined in the header file.
+ */
+static struct ipu6_csi2_error dphy_rx_errors[] = {
+	{"Single packet header error corrected", true},
+	{"Multiple packet header errors detected", true},
+	{"Payload checksum (CRC) error", true},
+	{"Transfer FIFO overflow", false},
+	{"Reserved short packet data type detected", true},
+	{"Reserved long packet data type detected", true},
+	{"Incomplete long packet detected", false},
+	{"Frame sync error", false},
+	{"Line sync error", false},
+	{"DPHY recoverable synchronization error", true},
+	{"DPHY fatal error", false},
+	{"DPHY elastic FIFO overflow", false},
+	{"Inter-frame short packet discarded", true},
+	{"Inter-frame long packet discarded", true},
+	{"MIPI pktgen overflow", false},
+	{"MIPI pktgen data loss", false},
+	{"FIFO overflow", false},
+	{"Lane deskew", false},
+	{"SOT sync error", false},
+	{"HSIDLE detected", false}
+};
+
+static refcount_t phy_power_ref_count[IPU_ISYS_CSI_PHY_NUM];
+
+static int ipu6_csi2_phy_power_set(struct ipu_isys *isys,
+				   struct ipu_isys_csi2_config *cfg, bool on)
+{
+	int ret = 0;
+	unsigned int port, phy_id;
+	refcount_t *ref;
+	void __iomem *isys_base = isys->pdata->base;
+	unsigned int nr;
+
+	port = cfg->port;
+	phy_id = port / 4;
+	ref = &phy_power_ref_count[phy_id];
+	dev_dbg(&isys->adev->dev, "for phy %d port %d, lanes: %d\n",
+		phy_id, port, cfg->nlanes);
+
+	nr = (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) ?
+		IPU6_ISYS_CSI_PORT_NUM : IPU6SE_ISYS_CSI_PORT_NUM;
+
+	if (!isys_base || port >= nr) {
+		dev_warn(&isys->adev->dev, "invalid port ID %d\n", port);
+		return -EINVAL;
+	}
+
+	if (on) {
+		if (refcount_read(ref)) {
+			/* already up */
+			dev_warn(&isys->adev->dev, "for phy %d is already UP",
+				 phy_id);
+			refcount_inc(ref);
+			return 0;
+		}
+
+		ret = ipu6_isys_phy_powerup_ack(isys, phy_id);
+		if (ret)
+			return ret;
+
+		ipu6_isys_phy_reset(isys, phy_id, 0);
+		ipu6_isys_phy_common_init(isys);
+
+		ret = ipu6_isys_phy_config(isys);
+		if (ret)
+			return ret;
+
+		ipu6_isys_phy_reset(isys, phy_id, 1);
+		ret = ipu6_isys_phy_ready(isys, phy_id);
+		if (ret)
+			return ret;
+
+		refcount_set(ref, 1);
+		return 0;
+	}
+
+	/* power off process */
+	if (refcount_dec_and_test(ref))
+		ret = ipu6_isys_phy_powerdown_ack(isys, phy_id);
+	if (ret)
+		dev_err(&isys->adev->dev, "phy poweroff failed!");
+
+	return ret;
+}
+
+static void ipu6_isys_register_errors(struct ipu_isys_csi2 *csi2)
+{
+	u32 mask = 0;
+	u32 irq = readl(csi2->base + CSI_PORT_REG_BASE_IRQ_CSI +
+			CSI_PORT_REG_BASE_IRQ_STATUS_OFFSET);
+
+	mask = (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) ?
+		IPU6_CSI_RX_ERROR_IRQ_MASK : IPU6SE_CSI_RX_ERROR_IRQ_MASK;
+
+	writel(irq & mask,
+	       csi2->base + CSI_PORT_REG_BASE_IRQ_CSI +
+	       CSI_PORT_REG_BASE_IRQ_CLEAR_OFFSET);
+	csi2->receiver_errors |= irq & mask;
+}
+
+void ipu_isys_csi2_error(struct ipu_isys_csi2 *csi2)
+{
+	struct ipu6_csi2_error *errors;
+	u32 status;
+	unsigned int i;
+
+	/* Register errors once more in case of error interrupts are disabled */
+	ipu6_isys_register_errors(csi2);
+	status = csi2->receiver_errors;
+	csi2->receiver_errors = 0;
+	errors = dphy_rx_errors;
+
+	for (i = 0; i < CSI_RX_NUM_ERRORS_IN_IRQ; i++) {
+		if (status & BIT(i))
+			dev_err_ratelimited(&csi2->isys->adev->dev,
+					    "csi2-%i error: %s\n",
+					    csi2->index,
+					    errors[i].error_string);
+	}
+}
+
+const unsigned int csi2_port_cfg[][3] = {
+	{0, 0, 0x1f}, /* no link */
+	{4, 0, 0x10}, /* x4 + x4 config */
+	{2, 0, 0x12}, /* x2 + x2 config */
+	{1, 0, 0x13}, /* x1 + x1 config */
+	{2, 1, 0x15}, /* x2x1 + x2x1 config */
+	{1, 1, 0x16}, /* x1x1 + x1x1 config */
+	{2, 2, 0x18}, /* x2x2 + x2x2 config */
+	{1, 2, 0x19}, /* x1x2 + x1x2 config */
+};
+
+const unsigned int phy_port_cfg[][4] = {
+	/* port, nlanes, bbindex, portcfg */
+	/* sip0 */
+	{0, 1, 0, 0x15},
+	{0, 2, 0, 0x15},
+	{0, 4, 0, 0x15},
+	{0, 4, 2, 0x22},
+	/* sip1 */
+	{2, 1, 4, 0x15},
+	{2, 2, 4, 0x15},
+	{2, 4, 4, 0x15},
+	{2, 4, 6, 0x22},
+};
+
+static int ipu_isys_csi2_phy_config_by_port(struct ipu_isys *isys,
+					    unsigned int port,
+					    unsigned int nlanes)
+{
+	void __iomem *base = isys->adev->isp->base;
+	u32 val, reg, i;
+	unsigned int bbnum;
+
+	dev_dbg(&isys->adev->dev, "%s port %u with %u lanes", __func__,
+		port, nlanes);
+
+	/* hard code for x2x2 + x2x2 with <1.5Gbps */
+	for (i = 0; i < IPU6SE_ISYS_PHY_BB_NUM; i++) {
+		/* cphy_dll_ovrd.crcdc_fsm_dlane0 = 13 */
+		reg = IPU6SE_ISYS_PHY_0_BASE + PHY_CPHY_DLL_OVRD(i);
+		val = readl(base + reg);
+		val |= 13 << 1;
+		/* val &= ~0x1; */
+		writel(val, base + reg);
+
+		/* cphy_rx_control1.en_crc1 = 1 */
+		reg = IPU6SE_ISYS_PHY_0_BASE + PHY_CPHY_RX_CONTROL1(i);
+		val = readl(base + reg);
+		val |= 0x1 << 31;
+		writel(val, base + reg);
+
+		/* dphy_cfg.reserved = 1
+		 * dphy_cfg.lden_from_dll_ovrd_0 = 1
+		 */
+		reg = IPU6SE_ISYS_PHY_0_BASE + PHY_DPHY_CFG(i);
+		val = readl(base + reg);
+		val |= 0x1 << 25;
+		val |= 0x1 << 26;
+		writel(val, base + reg);
+
+		/* cphy_dll_ovrd.lden_crcdc_fsm_dlane0 = 1 */
+		reg = IPU6SE_ISYS_PHY_0_BASE + PHY_CPHY_DLL_OVRD(i);
+		val = readl(base + reg);
+		val |= 1;
+		writel(val, base + reg);
+	}
+
+	/* bb afe config, use minimal channel loss */
+	for (i = 0; i < ARRAY_SIZE(phy_port_cfg); i++) {
+		if (phy_port_cfg[i][0] == port &&
+		    phy_port_cfg[i][1] == nlanes) {
+			bbnum = phy_port_cfg[i][2] / 2;
+			reg = IPU6SE_ISYS_PHY_0_BASE + PHY_BB_AFE_CONFIG(bbnum);
+			val = readl(base + reg);
+			val |= phy_port_cfg[i][3];
+			writel(val, base + reg);
+		}
+	}
+
+	return 0;
+}
+
+static void ipu_isys_csi2_rx_control(struct ipu_isys *isys)
+{
+	void __iomem *base = isys->adev->isp->base;
+	u32 val, reg;
+
+	/* lp11 release */
+	reg = CSI2_HUB_GPREG_SIP0_CSI_RX_A_CONTROL;
+	val = readl(base + reg);
+	val |= 0x1;
+	writel(0x1, base + CSI2_HUB_GPREG_SIP0_CSI_RX_A_CONTROL);
+
+	reg = CSI2_HUB_GPREG_SIP0_CSI_RX_B_CONTROL;
+	val = readl(base + reg);
+	val |= 0x1;
+	writel(0x1, base + CSI2_HUB_GPREG_SIP0_CSI_RX_B_CONTROL);
+
+	reg = CSI2_HUB_GPREG_SIP1_CSI_RX_A_CONTROL;
+	val = readl(base + reg);
+	val |= 0x1;
+	writel(0x1, base + CSI2_HUB_GPREG_SIP1_CSI_RX_A_CONTROL);
+
+	reg = CSI2_HUB_GPREG_SIP1_CSI_RX_B_CONTROL;
+	val = readl(base + reg);
+	val |= 0x1;
+	writel(0x1, base + CSI2_HUB_GPREG_SIP1_CSI_RX_B_CONTROL);
+}
+
+static int ipu_isys_csi2_set_port_cfg(struct v4l2_subdev *sd, unsigned int port,
+				      unsigned int nlanes)
+{
+	struct ipu_isys_csi2 *csi2 = to_ipu_isys_csi2(sd);
+	struct ipu_isys *isys = csi2->isys;
+	unsigned int sip = port / 2;
+	unsigned int index;
+
+	switch (nlanes) {
+	case 1:
+		index = 5;
+		break;
+	case 2:
+		index = 6;
+		break;
+	case 4:
+		index = 1;
+		break;
+	default:
+		dev_err(&isys->adev->dev, "lanes nr %u is unsupported\n",
+			nlanes);
+		return -EINVAL;
+	}
+
+	dev_dbg(&isys->adev->dev, "port config for port %u with %u lanes\n",
+		port, nlanes);
+	writel(csi2_port_cfg[index][2],
+	       isys->pdata->base + CSI2_HUB_GPREG_SIP_FB_PORT_CFG(sip));
+
+	return 0;
+}
+
+static void ipu_isys_csi2_set_timing(struct v4l2_subdev *sd,
+				     struct ipu_isys_csi2_timing timing,
+				     unsigned int port,
+				     unsigned int nlanes)
+{
+	u32 port_base;
+	void __iomem *reg;
+	struct ipu_isys_csi2 *csi2 = to_ipu_isys_csi2(sd);
+	struct ipu_isys *isys = csi2->isys;
+	unsigned int i;
+
+	port_base = (port % 2) ? CSI2_SIP_TOP_CSI_RX_PORT_BASE_1(port) :
+		CSI2_SIP_TOP_CSI_RX_PORT_BASE_0(port);
+
+	dev_dbg(&isys->adev->dev,
+		"set timing for port %u base 0x%x with %u lanes\n",
+		port, port_base, nlanes);
+
+	reg = isys->pdata->base + port_base;
+	reg += CSI2_SIP_TOP_CSI_RX_DLY_CNT_TERMEN_CLANE;
+
+	writel(timing.ctermen, reg);
+
+	reg = isys->pdata->base + port_base;
+	reg += CSI2_SIP_TOP_CSI_RX_DLY_CNT_SETTLE_CLANE;
+	writel(timing.csettle, reg);
+
+	for (i = 0; i < nlanes; i++) {
+		reg = isys->pdata->base + port_base;
+		reg += CSI2_SIP_TOP_CSI_RX_DLY_CNT_TERMEN_DLANE(i);
+		writel(timing.dtermen, reg);
+
+		reg = isys->pdata->base + port_base;
+		reg += CSI2_SIP_TOP_CSI_RX_DLY_CNT_SETTLE_DLANE(i);
+		writel(timing.dsettle, reg);
+	}
+}
+
+int ipu_isys_csi2_set_stream(struct v4l2_subdev *sd,
+			     struct ipu_isys_csi2_timing timing,
+			     unsigned int nlanes, int enable)
+{
+	struct ipu_isys_csi2 *csi2 = to_ipu_isys_csi2(sd);
+	struct ipu_isys *isys = csi2->isys;
+	struct ipu_isys_pipeline *ip = container_of(sd->entity.pipe,
+						    struct ipu_isys_pipeline,
+						    pipe);
+	struct ipu_isys_csi2_config *cfg =
+		v4l2_get_subdev_hostdata(media_entity_to_v4l2_subdev
+					 (ip->external->entity));
+	unsigned int port;
+	int ret;
+	u32 mask = 0;
+
+	port = cfg->port;
+	dev_dbg(&isys->adev->dev, "for port %u\n", port);
+
+	mask = (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) ?
+		IPU6_CSI_RX_ERROR_IRQ_MASK : IPU6SE_CSI_RX_ERROR_IRQ_MASK;
+
+	if (!enable) {
+
+		writel(0, csi2->base + CSI_REG_CSI_FE_ENABLE);
+		writel(0, csi2->base + CSI_REG_PPI2CSI_ENABLE);
+
+		/* Disable interrupts */
+		writel(0,
+		       csi2->base + CSI_PORT_REG_BASE_IRQ_CSI +
+		       CSI_PORT_REG_BASE_IRQ_ENABLE_OFFSET);
+		writel(mask,
+		       csi2->base + CSI_PORT_REG_BASE_IRQ_CSI +
+		       CSI_PORT_REG_BASE_IRQ_CLEAR_OFFSET);
+		writel(0,
+		       csi2->base + CSI_PORT_REG_BASE_IRQ_CSI_SYNC +
+		       CSI_PORT_REG_BASE_IRQ_ENABLE_OFFSET);
+		writel(0xffffffff,
+		       csi2->base + CSI_PORT_REG_BASE_IRQ_CSI_SYNC +
+		       CSI_PORT_REG_BASE_IRQ_CLEAR_OFFSET);
+
+		/* Disable clock */
+		writel(0, isys->pdata->base +
+		       CSI_REG_HUB_FW_ACCESS_PORT(port));
+		writel(0, isys->pdata->base +
+		       CSI_REG_HUB_DRV_ACCESS_PORT(port));
+
+		if (ipu_ver == IPU_VER_6SE)
+			return 0;
+
+		/* power down */
+		return ipu6_csi2_phy_power_set(isys, cfg, false);
+	}
+
+	if (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) {
+		/* Enable DPHY power */
+		ret = ipu6_csi2_phy_power_set(isys, cfg, true);
+		if (ret) {
+			dev_err(&isys->adev->dev,
+				"CSI-%d PHY power up failed %d\n",
+				cfg->port, ret);
+			return ret;
+		}
+	}
+
+	/* reset port reset */
+	writel(0x1, csi2->base + CSI_REG_PORT_GPREG_SRST);
+	usleep_range(100, 200);
+	writel(0x0, csi2->base + CSI_REG_PORT_GPREG_SRST);
+
+	/* Enable port clock */
+	writel(1, isys->pdata->base + CSI_REG_HUB_DRV_ACCESS_PORT(port));
+	writel(1, isys->pdata->base + CSI_REG_HUB_FW_ACCESS_PORT(port));
+
+	if (ipu_ver == IPU_VER_6SE) {
+		ipu_isys_csi2_phy_config_by_port(isys, port, nlanes);
+
+		/* 9'b00010.1000 for 400Mhz isys freqency */
+		writel(0x28,
+		       isys->pdata->base + CSI2_HUB_GPREG_DPHY_TIMER_INCR);
+		/* set port cfg and rx timing */
+		ipu_isys_csi2_set_timing(sd, timing, port, nlanes);
+
+		ret = ipu_isys_csi2_set_port_cfg(sd, port, nlanes);
+		if (ret)
+			return ret;
+
+		ipu_isys_csi2_rx_control(isys);
+	}
+
+	/* enable all error related irq */
+	writel(mask,
+	       csi2->base + CSI_PORT_REG_BASE_IRQ_CSI +
+	       CSI_PORT_REG_BASE_IRQ_STATUS_OFFSET);
+	writel(mask,
+	       csi2->base + CSI_PORT_REG_BASE_IRQ_CSI +
+	       CSI_PORT_REG_BASE_IRQ_MASK_OFFSET);
+	writel(mask,
+	       csi2->base + CSI_PORT_REG_BASE_IRQ_CSI +
+	       CSI_PORT_REG_BASE_IRQ_CLEAR_OFFSET);
+	writel(mask,
+	       csi2->base + CSI_PORT_REG_BASE_IRQ_CSI +
+	       CSI_PORT_REG_BASE_IRQ_LEVEL_NOT_PULSE_OFFSET);
+	writel(mask,
+	       csi2->base + CSI_PORT_REG_BASE_IRQ_CSI +
+	       CSI_PORT_REG_BASE_IRQ_ENABLE_OFFSET);
+
+	/* To save CPU wakeups, disable CSI SOF/EOF irq */
+	writel(0xffffffff, csi2->base + CSI_PORT_REG_BASE_IRQ_CSI_SYNC +
+	       CSI_PORT_REG_BASE_IRQ_STATUS_OFFSET);
+	writel(0, csi2->base + CSI_PORT_REG_BASE_IRQ_CSI_SYNC +
+	       CSI_PORT_REG_BASE_IRQ_MASK_OFFSET);
+	writel(0xffffffff, csi2->base + CSI_PORT_REG_BASE_IRQ_CSI_SYNC +
+	       CSI_PORT_REG_BASE_IRQ_CLEAR_OFFSET);
+	writel(0, csi2->base + CSI_PORT_REG_BASE_IRQ_CSI_SYNC +
+	       CSI_PORT_REG_BASE_IRQ_LEVEL_NOT_PULSE_OFFSET);
+	writel(0xffffffff, csi2->base + CSI_PORT_REG_BASE_IRQ_CSI_SYNC +
+	       CSI_PORT_REG_BASE_IRQ_ENABLE_OFFSET);
+
+	/* Configure FE/PPI2CSI and enable FE/ PPI2CSI */
+	writel(0, csi2->base + CSI_REG_CSI_FE_MODE);
+	writel(CSI_SENSOR_INPUT, csi2->base + CSI_REG_CSI_FE_MUX_CTRL);
+	writel(CSI_CNTR_SENSOR_LINE_ID | CSI_CNTR_SENSOR_FRAME_ID,
+	       csi2->base + CSI_REG_CSI_FE_SYNC_CNTR_SEL);
+	writel(((nlanes - 1) <<
+		PPI_INTF_CONFIG_NOF_ENABLED_DATALANES_SHIFT) |
+	       (0 << PPI_INTF_CONFIG_RX_AUTO_CLKGATING_SHIFT),
+	       csi2->base + CSI_REG_PPI2CSI_CONFIG_PPI_INTF);
+	writel(0x06, csi2->base + CSI_REG_PPI2CSI_CONFIG_CSI_FEATURE);
+	writel(1, csi2->base + CSI_REG_PPI2CSI_ENABLE);
+	writel(1, csi2->base + CSI_REG_CSI_FE_ENABLE);
+
+	return 0;
+}
+
+void ipu_isys_csi2_isr(struct ipu_isys_csi2 *csi2)
+{
+	u32 status;
+
+	ipu6_isys_register_errors(csi2);
+
+	status = readl(csi2->base + CSI_PORT_REG_BASE_IRQ_CSI_SYNC +
+		       CSI_PORT_REG_BASE_IRQ_STATUS_OFFSET);
+
+	writel(status, csi2->base + CSI_PORT_REG_BASE_IRQ_CSI_SYNC +
+	       CSI_PORT_REG_BASE_IRQ_CLEAR_OFFSET);
+
+	if (status & IPU_CSI_RX_IRQ_FS_VC)
+		ipu_isys_csi2_sof_event(csi2);
+	if (status & IPU_CSI_RX_IRQ_FE_VC)
+		ipu_isys_csi2_eof_event(csi2);
+}
+
+unsigned int ipu_isys_csi2_get_current_field(struct ipu_isys_pipeline *ip,
+					     unsigned int *timestamp)
+{
+	struct ipu_isys_video *av = container_of(ip, struct ipu_isys_video, ip);
+	struct ipu_isys *isys = av->isys;
+	unsigned int field = V4L2_FIELD_TOP;
+
+	struct ipu_isys_buffer *short_packet_ib =
+		list_last_entry(&ip->short_packet_active,
+				struct ipu_isys_buffer, head);
+	struct ipu_isys_private_buffer *pb =
+		ipu_isys_buffer_to_private_buffer(short_packet_ib);
+	struct ipu_isys_mipi_packet_header *ph =
+		(struct ipu_isys_mipi_packet_header *)
+		pb->buffer;
+
+	/* Check if the first SOF packet is received. */
+	if ((ph->dtype & IPU_ISYS_SHORT_PACKET_DTYPE_MASK) != 0)
+		dev_warn(&isys->adev->dev, "First short packet is not SOF.\n");
+	field = (ph->word_count % 2) ? V4L2_FIELD_TOP : V4L2_FIELD_BOTTOM;
+	dev_dbg(&isys->adev->dev,
+		"Interlaced field ready. frame_num = %d field = %d\n",
+		ph->word_count, field);
+
+	return field;
+}
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-isys-csi2.h b/drivers/media/pci/intel/ipu6/ipu6-isys-csi2.h
--- a/drivers/media/pci/intel/ipu6/ipu6-isys-csi2.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-isys-csi2.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,14 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2020 Intel Corporation */
+
+#ifndef IPU6_ISYS_CSI2_H
+#define IPU6_ISYS_CSI2_H
+
+struct ipu_isys_csi2_timing;
+struct ipu_isys_csi2;
+struct ipu_isys_pipeline;
+struct v4l2_subdev;
+
+#define IPU_ISYS_SHORT_PACKET_DTYPE_MASK	0x3f
+
+#endif /* IPU6_ISYS_CSI2_H */
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-isys-gpc.c b/drivers/media/pci/intel/ipu6/ipu6-isys-gpc.c
--- a/drivers/media/pci/intel/ipu6/ipu6-isys-gpc.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-isys-gpc.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,203 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2020 Intel Corporation
+
+#ifdef CONFIG_DEBUG_FS
+#include <linux/debugfs.h>
+#include <linux/pm_runtime.h>
+
+#include "ipu-isys.h"
+#include "ipu-platform-regs.h"
+
+#define IPU_ISYS_GPC_NUM		16
+
+#ifndef CONFIG_PM
+#define pm_runtime_get_sync(d)		0
+#define pm_runtime_put(d)		0
+#endif
+
+struct ipu_isys_gpc {
+	bool enable;
+	unsigned int route;
+	unsigned int source;
+	unsigned int sense;
+	unsigned int gpcindex;
+	void *prit;
+};
+
+struct ipu_isys_gpcs {
+	bool gpc_enable;
+	struct ipu_isys_gpc gpc[IPU_ISYS_GPC_NUM];
+	void *prit;
+};
+
+static int ipu6_isys_gpc_global_enable_get(void *data, u64 *val)
+{
+	struct ipu_isys_gpcs *isys_gpcs = data;
+	struct ipu_isys *isys = isys_gpcs->prit;
+
+	mutex_lock(&isys->mutex);
+
+	*val = isys_gpcs->gpc_enable;
+
+	mutex_unlock(&isys->mutex);
+	return 0;
+}
+
+static int ipu6_isys_gpc_global_enable_set(void *data, u64 val)
+{
+	struct ipu_isys_gpcs *isys_gpcs = data;
+	struct ipu_isys *isys = isys_gpcs->prit;
+	void __iomem *base;
+	int i, ret;
+
+	if (val != 0 && val != 1)
+		return -EINVAL;
+
+	if (!isys || !isys->pdata || !isys->pdata->base)
+		return -EINVAL;
+
+	mutex_lock(&isys->mutex);
+
+	base = isys->pdata->base + IPU_ISYS_GPC_BASE;
+
+	ret = pm_runtime_get_sync(&isys->adev->dev);
+	if (ret < 0) {
+		pm_runtime_put(&isys->adev->dev);
+		mutex_unlock(&isys->mutex);
+		return ret;
+	}
+
+	if (!val) {
+		writel(0x0, base + IPU_ISYS_GPREG_TRACE_TIMER_RST);
+		writel(0x0, base + IPU_ISF_CDC_MMU_GPC_OVERALL_ENABLE);
+		writel(0xffff, base + IPU_ISF_CDC_MMU_GPC_SOFT_RESET);
+		isys_gpcs->gpc_enable = false;
+		for (i = 0; i < IPU_ISYS_GPC_NUM; i++) {
+			isys_gpcs->gpc[i].enable = 0;
+			isys_gpcs->gpc[i].sense = 0;
+			isys_gpcs->gpc[i].route = 0;
+			isys_gpcs->gpc[i].source = 0;
+		}
+		pm_runtime_mark_last_busy(&isys->adev->dev);
+		pm_runtime_put_autosuspend(&isys->adev->dev);
+	} else {
+		/*
+		 * Set gpc reg and start all gpc here.
+		 * RST free running local timer.
+		 */
+		writel(0x0, base + IPU_ISYS_GPREG_TRACE_TIMER_RST);
+		writel(0x1, base + IPU_ISYS_GPREG_TRACE_TIMER_RST);
+
+		for (i = 0; i < IPU_ISYS_GPC_NUM; i++) {
+			/* Enable */
+			writel(isys_gpcs->gpc[i].enable,
+			       base + IPU_ISF_CDC_MMU_GPC_ENABLE0 + 4 * i);
+			/* Setting (route/source/sense) */
+			writel((isys_gpcs->gpc[i].sense
+					<< IPU_GPC_SENSE_OFFSET)
+				+ (isys_gpcs->gpc[i].route
+					<< IPU_GPC_ROUTE_OFFSET)
+				+ (isys_gpcs->gpc[i].source
+					<< IPU_GPC_SOURCE_OFFSET),
+				base + IPU_ISF_CDC_MMU_GPC_CNT_SEL0 + 4 * i);
+		}
+
+		/* Soft reset and Overall Enable. */
+		writel(0x0, base + IPU_ISF_CDC_MMU_GPC_OVERALL_ENABLE);
+		writel(0xffff, base + IPU_ISF_CDC_MMU_GPC_SOFT_RESET);
+		writel(0x1, base + IPU_ISF_CDC_MMU_GPC_OVERALL_ENABLE);
+
+		isys_gpcs->gpc_enable = true;
+	}
+
+	mutex_unlock(&isys->mutex);
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(isys_gpc_globe_enable_fops,
+			ipu6_isys_gpc_global_enable_get,
+			ipu6_isys_gpc_global_enable_set, "%llu\n");
+
+static int ipu6_isys_gpc_count_get(void *data, u64 *val)
+{
+	struct ipu_isys_gpc *isys_gpc = data;
+	struct ipu_isys *isys = isys_gpc->prit;
+	void __iomem *base;
+
+	if (!isys || !isys->pdata || !isys->pdata->base)
+		return -EINVAL;
+
+	spin_lock(&isys->power_lock);
+	if (isys->power) {
+		base = isys->pdata->base + IPU_ISYS_GPC_BASE;
+		*val = readl(base + IPU_ISF_CDC_MMU_GPC_VALUE0
+				 + 4 * isys_gpc->gpcindex);
+	} else {
+		*val = 0;
+	}
+	spin_unlock(&isys->power_lock);
+
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(isys_gpc_count_fops, ipu6_isys_gpc_count_get,
+			NULL, "%llu\n");
+
+int ipu_isys_gpc_init_debugfs(struct ipu_isys *isys)
+{
+	struct dentry *gpcdir;
+	struct dentry *dir;
+	struct dentry *file;
+	int i;
+	char gpcname[10];
+	struct ipu_isys_gpcs *isys_gpcs;
+
+	isys_gpcs = devm_kzalloc(&isys->adev->dev, sizeof(*isys_gpcs),
+				 GFP_KERNEL);
+	if (!isys_gpcs)
+		return -ENOMEM;
+
+	gpcdir = debugfs_create_dir("gpcs", isys->debugfsdir);
+	if (IS_ERR(gpcdir))
+		return -ENOMEM;
+
+	isys_gpcs->prit = isys;
+	file = debugfs_create_file("enable", 0600, gpcdir, isys_gpcs,
+				   &isys_gpc_globe_enable_fops);
+	if (IS_ERR(file))
+		goto err;
+
+	for (i = 0; i < IPU_ISYS_GPC_NUM; i++) {
+		sprintf(gpcname, "gpc%d", i);
+		dir = debugfs_create_dir(gpcname, gpcdir);
+		if (IS_ERR(dir))
+			goto err;
+
+		debugfs_create_bool("enable", 0600, dir,
+					&isys_gpcs->gpc[i].enable);
+
+		debugfs_create_u32("source", 0600, dir,
+				   &isys_gpcs->gpc[i].source);
+
+		debugfs_create_u32("route", 0600, dir,
+				   &isys_gpcs->gpc[i].route);
+
+		debugfs_create_u32("sense", 0600, dir,
+				   &isys_gpcs->gpc[i].sense);
+
+		isys_gpcs->gpc[i].gpcindex = i;
+		isys_gpcs->gpc[i].prit = isys;
+		file = debugfs_create_file("count", 0400, dir,
+					   &isys_gpcs->gpc[i],
+					   &isys_gpc_count_fops);
+		if (IS_ERR(file))
+			goto err;
+	}
+
+	return 0;
+
+err:
+	debugfs_remove_recursive(gpcdir);
+	return -ENOMEM;
+}
+#endif
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-isys-phy.c b/drivers/media/pci/intel/ipu6/ipu6-isys-phy.c
--- a/drivers/media/pci/intel/ipu6/ipu6-isys-phy.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-isys-phy.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,595 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2013 - 2020 Intel Corporation
+ */
+
+#include <linux/delay.h>
+#include <media/ipu-isys.h>
+#include <media/v4l2-device.h>
+#include "ipu.h"
+#include "ipu-buttress.h"
+#include "ipu-isys.h"
+#include "ipu-isys-csi2.h"
+#include "ipu-platform-regs.h"
+#include "ipu-platform-isys-csi2-reg.h"
+#include "ipu6-isys-csi2.h"
+#include "ipu6-isys-phy.h"
+
+#define LOOP (2000)
+
+#define PHY_REG_INIT_CTL	     0x00000694
+#define PHY_REG_INIT_CTL_PORT_OFFSET 0x00000600
+
+struct phy_reg {
+	u32 reg;
+	u32 val;
+};
+
+static const struct phy_reg common_init_regs[] = {
+	/* for TGL-U, use 0x80000000 */
+	{0x00000040, 0x80000000},
+	{0x00000044, 0x00a80880},
+	{0x00000044, 0x00b80880},
+	{0x00000010, 0x0000078c},
+	{0x00000344, 0x2f4401e2},
+	{0x00000544, 0x924401e2},
+	{0x00000744, 0x594401e2},
+	{0x00000944, 0x624401e2},
+	{0x00000b44, 0xfc4401e2},
+	{0x00000d44, 0xc54401e2},
+	{0x00000f44, 0x034401e2},
+	{0x00001144, 0x8f4401e2},
+	{0x00001344, 0x754401e2},
+	{0x00001544, 0xe94401e2},
+	{0x00001744, 0xcb4401e2},
+	{0x00001944, 0xfa4401e2}
+};
+
+static const struct phy_reg x1_port0_config_regs[] = {
+	{0x00000694, 0xc80060fa},
+	{0x00000680, 0x3d4f78ea},
+	{0x00000690, 0x10a0140b},
+	{0x000006a8, 0xdf04010a},
+	{0x00000700, 0x57050060},
+	{0x00000710, 0x0030001c},
+	{0x00000738, 0x5f004444},
+	{0x0000073c, 0x78464204},
+	{0x00000748, 0x7821f940},
+	{0x0000074c, 0xb2000433},
+	{0x00000494, 0xfe6030fa},
+	{0x00000480, 0x29ef5ed0},
+	{0x00000490, 0x10a0540b},
+	{0x000004a8, 0x7a01010a},
+	{0x00000500, 0xef053460},
+	{0x00000510, 0xe030101c},
+	{0x00000538, 0xdf808444},
+	{0x0000053c, 0xc8422204},
+	{0x00000540, 0x0180088c},
+	{0x00000574, 0x00000000},
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x1_port1_config_regs[] = {
+	{0x00000c94, 0xc80060fa},
+	{0x00000c80, 0xcf47abea},
+	{0x00000c90, 0x10a0840b},
+	{0x00000ca8, 0xdf04010a},
+	{0x00000d00, 0x57050060},
+	{0x00000d10, 0x0030001c},
+	{0x00000d38, 0x5f004444},
+	{0x00000d3c, 0x78464204},
+	{0x00000d48, 0x7821f940},
+	{0x00000d4c, 0xb2000433},
+	{0x00000a94, 0xc91030fa},
+	{0x00000a80, 0x5a166ed0},
+	{0x00000a90, 0x10a0540b},
+	{0x00000aa8, 0x5d060100},
+	{0x00000b00, 0xef053460},
+	{0x00000b10, 0xa030101c},
+	{0x00000b38, 0xdf808444},
+	{0x00000b3c, 0xc8422204},
+	{0x00000b40, 0x0180088c},
+	{0x00000b74, 0x00000000},
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x1_port2_config_regs[] = {
+	{0x00001294, 0x28f000fa},
+	{0x00001280, 0x08130cea},
+	{0x00001290, 0x10a0140b},
+	{0x000012a8, 0xd704010a},
+	{0x00001300, 0x8d050060},
+	{0x00001310, 0x0030001c},
+	{0x00001338, 0xdf008444},
+	{0x0000133c, 0x78422204},
+	{0x00001348, 0x7821f940},
+	{0x0000134c, 0x5a000433},
+	{0x00001094, 0x2d20b0fa},
+	{0x00001080, 0xade75dd0},
+	{0x00001090, 0x10a0540b},
+	{0x000010a8, 0xb101010a},
+	{0x00001100, 0x33053460},
+	{0x00001110, 0x0030101c},
+	{0x00001138, 0xdf808444},
+	{0x0000113c, 0xc8422204},
+	{0x00001140, 0x8180088c},
+	{0x00001174, 0x00000000},
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x1_port3_config_regs[] = {
+	{0x00001894, 0xc80060fa},
+	{0x00001880, 0x0f90fd6a},
+	{0x00001890, 0x10a0840b},
+	{0x000018a8, 0xdf04010a},
+	{0x00001900, 0x57050060},
+	{0x00001910, 0x0030001c},
+	{0x00001938, 0x5f004444},
+	{0x0000193c, 0x78464204},
+	{0x00001948, 0x7821f940},
+	{0x0000194c, 0xb2000433},
+	{0x00001694, 0x3050d0fa},
+	{0x00001680, 0x0ef6d050},
+	{0x00001690, 0x10a0540b},
+	{0x000016a8, 0xe301010a},
+	{0x00001700, 0x69053460},
+	{0x00001710, 0xa030101c},
+	{0x00001738, 0xdf808444},
+	{0x0000173c, 0xc8422204},
+	{0x00001740, 0x0180088c},
+	{0x00001774, 0x00000000},
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x2_port0_config_regs[] = {
+	{0x00000694, 0xc80060fa},
+	{0x00000680, 0x3d4f78ea},
+	{0x00000690, 0x10a0140b},
+	{0x000006a8, 0xdf04010a},
+	{0x00000700, 0x57050060},
+	{0x00000710, 0x0030001c},
+	{0x00000738, 0x5f004444},
+	{0x0000073c, 0x78464204},
+	{0x00000748, 0x7821f940},
+	{0x0000074c, 0xb2000433},
+	{0x00000494, 0xc80060fa},
+	{0x00000480, 0x29ef5ed8},
+	{0x00000490, 0x10a0540b},
+	{0x000004a8, 0x7a01010a},
+	{0x00000500, 0xef053460},
+	{0x00000510, 0xe030101c},
+	{0x00000538, 0xdf808444},
+	{0x0000053c, 0xc8422204},
+	{0x00000540, 0x0180088c},
+	{0x00000574, 0x00000000},
+	{0x00000294, 0xc80060fa},
+	{0x00000280, 0xcb45b950},
+	{0x00000290, 0x10a0540b},
+	{0x000002a8, 0x8c01010a},
+	{0x00000300, 0xef053460},
+	{0x00000310, 0x8030101c},
+	{0x00000338, 0x41808444},
+	{0x0000033c, 0x32422204},
+	{0x00000340, 0x0180088c},
+	{0x00000374, 0x00000000},
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x2_port1_config_regs[] = {
+	{0x00000c94, 0xc80060fa},
+	{0x00000c80, 0xcf47abea},
+	{0x00000c90, 0x10a0840b},
+	{0x00000ca8, 0xdf04010a},
+	{0x00000d00, 0x57050060},
+	{0x00000d10, 0x0030001c},
+	{0x00000d38, 0x5f004444},
+	{0x00000d3c, 0x78464204},
+	{0x00000d48, 0x7821f940},
+	{0x00000d4c, 0xb2000433},
+	{0x00000a94, 0xc80060fa},
+	{0x00000a80, 0x5a166ed8},
+	{0x00000a90, 0x10a0540b},
+	{0x00000aa8, 0x7a01010a},
+	{0x00000b00, 0xef053460},
+	{0x00000b10, 0xa030101c},
+	{0x00000b38, 0xdf808444},
+	{0x00000b3c, 0xc8422204},
+	{0x00000b40, 0x0180088c},
+	{0x00000b74, 0x00000000},
+	{0x00000894, 0xc80060fa},
+	{0x00000880, 0x4d4f21d0},
+	{0x00000890, 0x10a0540b},
+	{0x000008a8, 0x5601010a},
+	{0x00000900, 0xef053460},
+	{0x00000910, 0x8030101c},
+	{0x00000938, 0xdf808444},
+	{0x0000093c, 0xc8422204},
+	{0x00000940, 0x0180088c},
+	{0x00000974, 0x00000000},
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x2_port2_config_regs[] = {
+	{0x00001294, 0xc80060fa},
+	{0x00001280, 0x08130cea},
+	{0x00001290, 0x10a0140b},
+	{0x000012a8, 0xd704010a},
+	{0x00001300, 0x8d050060},
+	{0x00001310, 0x0030001c},
+	{0x00001338, 0xdf008444},
+	{0x0000133c, 0x78422204},
+	{0x00001348, 0x7821f940},
+	{0x0000134c, 0x5a000433},
+	{0x00001094, 0xc80060fa},
+	{0x00001080, 0xade75dd8},
+	{0x00001090, 0x10a0540b},
+	{0x000010a8, 0xb101010a},
+	{0x00001100, 0x33053460},
+	{0x00001110, 0x0030101c},
+	{0x00001138, 0xdf808444},
+	{0x0000113c, 0xc8422204},
+	{0x00001140, 0x8180088c},
+	{0x00001174, 0x00000000},
+	{0x00000e94, 0xc80060fa},
+	{0x00000e80, 0x0fbf16d0},
+	{0x00000e90, 0x10a0540b},
+	{0x00000ea8, 0x7a01010a},
+	{0x00000f00, 0xf5053460},
+	{0x00000f10, 0xc030101c},
+	{0x00000f38, 0xdf808444},
+	{0x00000f3c, 0xc8422204},
+	{0x00000f40, 0x8180088c},
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x2_port3_config_regs[] = {
+	{0x00001894, 0xc80060fa},
+	{0x00001880, 0x0f90fd6a},
+	{0x00001890, 0x10a0840b},
+	{0x000018a8, 0xdf04010a},
+	{0x00001900, 0x57050060},
+	{0x00001910, 0x0030001c},
+	{0x00001938, 0x5f004444},
+	{0x0000193c, 0x78464204},
+	{0x00001948, 0x7821f940},
+	{0x0000194c, 0xb2000433},
+	{0x00001694, 0xc80060fa},
+	{0x00001680, 0x0ef6d058},
+	{0x00001690, 0x10a0540b},
+	{0x000016a8, 0x7a01010a},
+	{0x00001700, 0x69053460},
+	{0x00001710, 0xa030101c},
+	{0x00001738, 0xdf808444},
+	{0x0000173c, 0xc8422204},
+	{0x00001740, 0x0180088c},
+	{0x00001774, 0x00000000},
+	{0x00001494, 0xc80060fa},
+	{0x00001480, 0xf9d34bd0},
+	{0x00001490, 0x10a0540b},
+	{0x000014a8, 0x7a01010a},
+	{0x00001500, 0x1b053460},
+	{0x00001510, 0x0030101c},
+	{0x00001538, 0xdf808444},
+	{0x0000153c, 0xc8422204},
+	{0x00001540, 0x8180088c},
+	{0x00001574, 0x00000000},
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x4_port0_config_regs[] = {
+	{0x00000694, 0xc80060fa},
+	{0x00000680, 0x3d4f78fa},
+	{0x00000690, 0x10a0140b},
+	{0x000006a8, 0xdf04010a},
+	{0x00000700, 0x57050060},
+	{0x00000710, 0x0030001c},
+	{0x00000738, 0x5f004444},
+	{0x0000073c, 0x78464204},
+	{0x00000748, 0x7821f940},
+	{0x0000074c, 0xb2000433},
+	{0x00000494, 0xfe6030fa},
+	{0x00000480, 0x29ef5ed8},
+	{0x00000490, 0x10a0540b},
+	{0x000004a8, 0x7a01010a},
+	{0x00000500, 0xef053460},
+	{0x00000510, 0xe030101c},
+	{0x00000538, 0xdf808444},
+	{0x0000053c, 0xc8422204},
+	{0x00000540, 0x0180088c},
+	{0x00000574, 0x00000004},
+	{0x00000294, 0x23e030fa},
+	{0x00000280, 0xcb45b950},
+	{0x00000290, 0x10a0540b},
+	{0x000002a8, 0x8c01010a},
+	{0x00000300, 0xef053460},
+	{0x00000310, 0x8030101c},
+	{0x00000338, 0x41808444},
+	{0x0000033c, 0x32422204},
+	{0x00000340, 0x0180088c},
+	{0x00000374, 0x00000004},
+	{0x00000894, 0x5620b0fa},
+	{0x00000880, 0x4d4f21dc},
+	{0x00000890, 0x10a0540b},
+	{0x000008a8, 0x5601010a},
+	{0x00000900, 0xef053460},
+	{0x00000910, 0x8030101c},
+	{0x00000938, 0xdf808444},
+	{0x0000093c, 0xc8422204},
+	{0x00000940, 0x0180088c},
+	{0x00000974, 0x00000004},
+	{0x00000a94, 0xc91030fa},
+	{0x00000a80, 0x5a166ecc},
+	{0x00000a90, 0x10a0540b},
+	{0x00000aa8, 0x5d01010a},
+	{0x00000b00, 0xef053460},
+	{0x00000b10, 0xa030101c},
+	{0x00000b38, 0xdf808444},
+	{0x00000b3c, 0xc8422204},
+	{0x00000b40, 0x0180088c},
+	{0x00000b74, 0x00000004},
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x4_port1_config_regs[] = {
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x4_port2_config_regs[] = {
+	{0x00001294, 0x28f000fa},
+	{0x00001280, 0x08130cfa},
+	{0x00001290, 0x10c0140b},
+	{0x000012a8, 0xd704010a},
+	{0x00001300, 0x8d050060},
+	{0x00001310, 0x0030001c},
+	{0x00001338, 0xdf008444},
+	{0x0000133c, 0x78422204},
+	{0x00001348, 0x7821f940},
+	{0x0000134c, 0x5a000433},
+	{0x00001094, 0x2d20b0fa},
+	{0x00001080, 0xade75dd8},
+	{0x00001090, 0x10a0540b},
+	{0x000010a8, 0xb101010a},
+	{0x00001100, 0x33053460},
+	{0x00001110, 0x0030101c},
+	{0x00001138, 0xdf808444},
+	{0x0000113c, 0xc8422204},
+	{0x00001140, 0x8180088c},
+	{0x00001174, 0x00000004},
+	{0x00000e94, 0xd308d0fa},
+	{0x00000e80, 0x0fbf16d0},
+	{0x00000e90, 0x10a0540b},
+	{0x00000ea8, 0x2c01010a},
+	{0x00000f00, 0xf5053460},
+	{0x00000f10, 0xc030101c},
+	{0x00000f38, 0xdf808444},
+	{0x00000f3c, 0xc8422204},
+	{0x00000f40, 0x8180088c},
+	{0x00000f74, 0x00000004},
+	{0x00001494, 0x136850fa},
+	{0x00001480, 0xf9d34bdc},
+	{0x00001490, 0x10a0540b},
+	{0x000014a8, 0x5a01010a},
+	{0x00001500, 0x1b053460},
+	{0x00001510, 0x0030101c},
+	{0x00001538, 0xdf808444},
+	{0x0000153c, 0xc8422204},
+	{0x00001540, 0x8180088c},
+	{0x00001574, 0x00000004},
+	{0x00001694, 0x3050d0fa},
+	{0x00001680, 0x0ef6d04c},
+	{0x00001690, 0x10a0540b},
+	{0x000016a8, 0xe301010a},
+	{0x00001700, 0x69053460},
+	{0x00001710, 0xa030101c},
+	{0x00001738, 0xdf808444},
+	{0x0000173c, 0xc8422204},
+	{0x00001740, 0x0180088c},
+	{0x00001774, 0x00000004},
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg x4_port3_config_regs[] = {
+	{0x00000000, 0x00000000}
+};
+
+static const struct phy_reg *x1_config_regs[4] = {
+	x1_port0_config_regs,
+	x1_port1_config_regs,
+	x1_port2_config_regs,
+	x1_port3_config_regs
+};
+
+static const struct phy_reg *x2_config_regs[4] = {
+	x2_port0_config_regs,
+	x2_port1_config_regs,
+	x2_port2_config_regs,
+	x2_port3_config_regs
+};
+
+static const struct phy_reg *x4_config_regs[4] = {
+	x4_port0_config_regs,
+	x4_port1_config_regs,
+	x4_port2_config_regs,
+	x4_port3_config_regs
+};
+
+static const struct phy_reg **config_regs[3] = {
+	x1_config_regs,
+	x2_config_regs,
+	x4_config_regs,
+};
+
+int ipu6_isys_phy_powerup_ack(struct ipu_isys *isys, unsigned int phy_id)
+{
+	unsigned int i;
+	u32 val;
+	void __iomem *isys_base = isys->pdata->base;
+
+	val = readl(isys_base + CSI_REG_HUB_GPREG_PHY_CONTROL(phy_id));
+	val |= CSI_REG_HUB_GPREG_PHY_CONTROL_PWR_EN;
+	writel(val, isys_base + CSI_REG_HUB_GPREG_PHY_CONTROL(phy_id));
+
+	for (i = 0; i < LOOP; i++) {
+		if (readl(isys_base + CSI_REG_HUB_GPREG_PHY_STATUS(phy_id)) &
+		    CSI_REG_HUB_GPREG_PHY_STATUS_POWER_ACK)
+			return 0;
+		usleep_range(100, 200);
+	}
+
+	dev_warn(&isys->adev->dev, "PHY%d powerup ack timeout", phy_id);
+
+	return -ETIMEDOUT;
+}
+
+int ipu6_isys_phy_powerdown_ack(struct ipu_isys *isys, unsigned int phy_id)
+{
+	unsigned int i;
+	u32 val;
+	void __iomem *isys_base = isys->pdata->base;
+
+	writel(0, isys_base + CSI_REG_HUB_GPREG_PHY_CONTROL(phy_id));
+	for (i = 0; i < LOOP; i++) {
+		usleep_range(10, 20);
+		val = readl(isys_base + CSI_REG_HUB_GPREG_PHY_STATUS(phy_id));
+		if (!(val & CSI_REG_HUB_GPREG_PHY_STATUS_POWER_ACK))
+			return 0;
+	}
+
+	dev_warn(&isys->adev->dev, "PHY %d poweroff ack timeout.\n", phy_id);
+
+	return -ETIMEDOUT;
+}
+
+int ipu6_isys_phy_reset(struct ipu_isys *isys, unsigned int phy_id,
+			bool assert)
+{
+	void __iomem *isys_base = isys->pdata->base;
+	u32 val;
+
+	val = readl(isys_base + CSI_REG_HUB_GPREG_PHY_CONTROL(phy_id));
+	if (assert)
+		val |= CSI_REG_HUB_GPREG_PHY_CONTROL_RESET;
+	else
+		val &= ~(CSI_REG_HUB_GPREG_PHY_CONTROL_RESET);
+
+	writel(val, isys_base + CSI_REG_HUB_GPREG_PHY_CONTROL(phy_id));
+
+	return 0;
+}
+
+int ipu6_isys_phy_ready(struct ipu_isys *isys, unsigned int phy_id)
+{
+	unsigned int i;
+	u32 val;
+	void __iomem *isys_base = isys->pdata->base;
+
+	for (i = 0; i < LOOP; i++) {
+		val = readl(isys_base + CSI_REG_HUB_GPREG_PHY_STATUS(phy_id));
+		dev_dbg(&isys->adev->dev, "PHY%d ready status 0x%x\n",
+			phy_id, val);
+		if (val & CSI_REG_HUB_GPREG_PHY_STATUS_PHY_READY)
+			return 0;
+		usleep_range(10, 20);
+	}
+
+	dev_warn(&isys->adev->dev, "PHY%d ready timeout\n", phy_id);
+
+	return -ETIMEDOUT;
+}
+
+int ipu6_isys_phy_common_init(struct ipu_isys *isys)
+{
+	unsigned int phy_id;
+	void __iomem *phy_base;
+	struct ipu_bus_device *adev = to_ipu_bus_device(&isys->adev->dev);
+	struct ipu_device *isp = adev->isp;
+	void __iomem *isp_base = isp->base;
+	struct v4l2_async_subdev *asd;
+	struct sensor_async_subdev *s_asd;
+	unsigned int i;
+
+	list_for_each_entry(asd, &isys->notifier.asd_list, asd_list) {
+		s_asd = container_of(asd, struct sensor_async_subdev, asd);
+		phy_id = s_asd->csi2.port / 4;
+		phy_base = isp_base + IPU6_ISYS_PHY_BASE(phy_id);
+
+		for (i = 0 ; i < ARRAY_SIZE(common_init_regs); i++) {
+			writel(common_init_regs[i].val,
+				phy_base + common_init_regs[i].reg);
+		}
+	}
+
+	return 0;
+}
+
+static int ipu6_isys_driver_port_to_phy_port(struct ipu_isys_csi2_config *cfg)
+{
+	int phy_port;
+	int ret;
+
+	if (!(cfg->nlanes == 4 || cfg->nlanes == 2 || cfg->nlanes == 1))
+		return -EINVAL;
+
+	/* B,F -> C0 A,E -> C1 C,G -> C2 D,H -> C4 */
+	/* normalize driver port number */
+	phy_port = cfg->port % 4;
+
+	/* swap port number only for A and B */
+	if (phy_port == 0)
+		phy_port = 1;
+	else if (phy_port == 1)
+		phy_port = 0;
+
+	ret = phy_port;
+
+	/* check validity per lane configuration */
+	if ((cfg->nlanes == 4) &&
+		 !(phy_port == 0 || phy_port == 2))
+		ret = -EINVAL;
+	else if ((cfg->nlanes == 2 || cfg->nlanes == 1) &&
+		 !(phy_port >= 0 && phy_port <= 3))
+		ret = -EINVAL;
+
+	return ret;
+}
+
+int ipu6_isys_phy_config(struct ipu_isys *isys)
+{
+	int phy_port;
+	unsigned int phy_id;
+	void __iomem *phy_base;
+	struct ipu_bus_device *adev = to_ipu_bus_device(&isys->adev->dev);
+	struct ipu_device *isp = adev->isp;
+	void __iomem *isp_base = isp->base;
+	const struct phy_reg **phy_config_regs;
+	struct v4l2_async_subdev *asd;
+	struct sensor_async_subdev *s_asd;
+	struct ipu_isys_csi2_config cfg;
+	int i;
+
+	list_for_each_entry(asd, &isys->notifier.asd_list, asd_list) {
+		s_asd = container_of(asd, struct sensor_async_subdev, asd);
+		cfg.port = s_asd->csi2.port;
+		cfg.nlanes = s_asd->csi2.nlanes;
+		phy_port = ipu6_isys_driver_port_to_phy_port(&cfg);
+		if (phy_port < 0) {
+			dev_err(&isys->adev->dev, "invalid port %d for lane %d",
+				cfg.port, cfg.nlanes);
+			return -ENXIO;
+		}
+
+		phy_id = cfg.port / 4;
+		phy_base = isp_base + IPU6_ISYS_PHY_BASE(phy_id);
+		dev_dbg(&isys->adev->dev, "port%d PHY%u lanes %u\n",
+			cfg.port, phy_id, cfg.nlanes);
+
+		phy_config_regs = config_regs[cfg.nlanes/2];
+		cfg.port = phy_port;
+		for (i = 0; phy_config_regs[cfg.port][i].reg; i++) {
+			writel(phy_config_regs[cfg.port][i].val,
+				phy_base + phy_config_regs[cfg.port][i].reg);
+		}
+	}
+
+	return 0;
+}
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-isys-phy.h b/drivers/media/pci/intel/ipu6/ipu6-isys-phy.h
--- a/drivers/media/pci/intel/ipu6/ipu6-isys-phy.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-isys-phy.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,159 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2013 - 2020 Intel Corporation
+ */
+
+#ifndef IPU6_ISYS_PHY_H
+#define IPU6_ISYS_PHY_H
+
+/* bridge to phy in buttress reg map, each phy has 16 kbytes
+ * for tgl u/y, only 2 phys
+ */
+#define IPU6_ISYS_PHY_0_BASE			0x10000
+#define IPU6_ISYS_PHY_1_BASE			0x14000
+#define IPU6_ISYS_PHY_2_BASE			0x18000
+#define IPU6_ISYS_PHY_BASE(i)			(0x10000 + (i) * 0x4000)
+
+/* ppi mapping per phy :
+ *
+ * x4x4:
+ * port0 - PPI range {0, 1, 2, 3, 4}
+ * port2 - PPI range {6, 7, 8, 9, 10}
+ *
+ * x4x2:
+ * port0 - PPI range {0, 1, 2, 3, 4}
+ * port2 - PPI range {6, 7, 8}
+ *
+ * x2x4:
+ * port0 - PPI range {0, 1, 2}
+ * port2 - PPI range {6, 7, 8, 9, 10}
+ *
+ * x2x2:
+ * port0 - PPI range {0, 1, 2}
+ * port1 - PPI range {3, 4, 5}
+ * port2 - PPI range {6, 7, 8}
+ * port3 - PPI range {9, 10, 11}
+ */
+
+/* cbbs config regs */
+#define PHY_CBBS1_BASE				0x0
+/* register offset */
+#define PHY_CBBS1_DFX_VMISCCTL			0x0
+#define PHY_CBBS1_DFX_VBYTESEL0			0x4
+#define PHY_CBBS1_DFX_VBYTESEL1			0x8
+#define PHY_CBBS1_VISA2OBS_CTRL_REG		0xc
+#define PHY_CBBS1_PGL_CTRL_REG			0x10
+#define PHY_CBBS1_RCOMP_CTRL_REG_1		0x14
+#define PHY_CBBS1_RCOMP_CTRL_REG_2		0x18
+#define PHY_CBBS1_RCOMP_CTRL_REG_3		0x1c
+#define PHY_CBBS1_RCOMP_CTRL_REG_4		0x20
+#define PHY_CBBS1_RCOMP_CTRL_REG_5		0x24
+#define PHY_CBBS1_RCOMP_STATUS_REG_1		0x28
+#define PHY_CBBS1_RCOMP_STATUS_REG_2		0x2c
+#define PHY_CBBS1_CLOCK_CTRL_REG		0x30
+#define PHY_CBBS1_CBB_ISOLATION_REG		0x34
+#define PHY_CBBS1_CBB_PLL_CONTROL		0x38
+#define PHY_CBBS1_CBB_STATUS_REG		0x3c
+#define PHY_CBBS1_AFE_CONTROL_REG_1		0x40
+#define PHY_CBBS1_AFE_CONTROL_REG_2		0x44
+#define PHY_CBBS1_CBB_SPARE			0x48
+#define PHY_CBBS1_CRI_CLK_CONTROL		0x4c
+
+/* dbbs shared, i = [0..11] */
+#define PHY_DBBS_SHARED(ppi)			((ppi) * 0x200 + 0x200)
+/* register offset */
+#define PHY_DBBDFE_DFX_V1MISCCTL		0x0
+#define PHY_DBBDFE_DFX_V1BYTESEL0		0x4
+#define PHY_DBBDFE_DFX_V1BYTESEL1		0x8
+#define PHY_DBBDFE_DFX_V2MISCCTL		0xc
+#define PHY_DBBDFE_DFX_V2BYTESEL0		0x10
+#define PHY_DBBDFE_DFX_V2BYTESEL1		0x14
+#define PHY_DBBDFE_GBLCTL			0x18
+#define PHY_DBBDFE_GBL_STATUS			0x1c
+
+/* dbbs udln, i = [0..11] */
+#define IPU6_ISYS_PHY_DBBS_UDLN(ppi)		((ppi) * 0x200 + 0x280)
+/* register offset */
+#define PHY_DBBUDLN_CTL				0x0
+#define PHY_DBBUDLN_CLK_CTL			0x4
+#define PHY_DBBUDLN_SOFT_RST_CTL		0x8
+#define PHY_DBBUDLN_STRAP_VALUES		0xc
+#define PHY_DBBUDLN_TXRX_CTL			0x10
+#define PHY_DBBUDLN_MST_SLV_INIT_CTL		0x14
+#define PHY_DBBUDLN_TX_TIMING_CTL0		0x18
+#define PHY_DBBUDLN_TX_TIMING_CTL1		0x1c
+#define PHY_DBBUDLN_TX_TIMING_CTL2		0x20
+#define PHY_DBBUDLN_TX_TIMING_CTL3		0x24
+#define PHY_DBBUDLN_RX_TIMING_CTL		0x28
+#define PHY_DBBUDLN_PPI_STATUS_CTL		0x2c
+#define PHY_DBBUDLN_PPI_STATUS			0x30
+#define PHY_DBBUDLN_ERR_CTL			0x34
+#define PHY_DBBUDLN_ERR_STATUS			0x38
+#define PHY_DBBUDLN_DFX_LPBK_CTL		0x3c
+#define PHY_DBBUDLN_DFX_PPI_CTL			0x40
+#define PHY_DBBUDLN_DFX_TX_DPHY_CTL		0x44
+#define PHY_DBBUDLN_DFX_TXRX_PRBSPAT_CTL	0x48
+#define PHY_DBBUDLN_DFX_TXRX_PRBSPAT_SEED	0x4c
+#define PHY_DBBUDLN_DFX_PRBSPAT_MAX_WRD_CNT	0x50
+#define PHY_DBBUDLN_DFX_PRBSPAT_STATUS		0x54
+#define PHY_DBBUDLN_DFX_PRBSPAT_WRD_CNT0_STATUS	0x58
+#define PHY_DBBUDLN_DFX_PRBSPAT_WRD_CNT1_STATUS	0x5c
+#define PHY_DBBUDLN_DFX_PRBSPAT_FF_ERR_STATUS	0x60
+#define PHY_DBBUDLN_DFX_PRBSPAT_FF_REF_STATUS		0x64
+#define PHY_DBBUDLN_DFX_PRBSPAT_FF_WRD_CNT0_STATUS	0x68
+#define PHY_DBBUDLN_DFX_PRBSPAT_FF_WRD_CNT1_STATUS	0x6c
+#define PHY_DBBUDLN_RSVD_CTL				0x70
+#define PHY_DBBUDLN_TINIT_DONE				BIT(27)
+
+/* dbbs supar, i = [0..11] */
+#define IPU6_ISYS_PHY_DBBS_SUPAR(ppi)		((ppi) * 0x200 + 0x300)
+/* register offset */
+#define PHY_DBBSUPAR_TXRX_FUPAR_CTL		0x0
+#define PHY_DBBSUPAR_TXHS_AFE_CTL		0x4
+#define PHY_DBBSUPAR_TXHS_AFE_LEGDIS_CTL	0x8
+#define PHY_DBBSUPAR_TXHS_AFE_EQ_CTL		0xc
+#define PHY_DBBSUPAR_RXHS_AFE_CTL1		0x10
+#define PHY_DBBSUPAR_RXHS_AFE_PICTL1		0x14
+#define PHY_DBBSUPAR_TXRXLP_AFE_CTL		0x18
+#define PHY_DBBSUPAR_DFX_TXRX_STATUS		0x1c
+#define PHY_DBBSUPAR_DFX_TXRX_CTL		0x20
+#define PHY_DBBSUPAR_DFX_DIGMON_CTL		0x24
+#define PHY_DBBSUPAR_DFX_LOCMON_CTL		0x28
+#define PHY_DBBSUPAR_DFX_RCOMP_CTL1		0x2c
+#define PHY_DBBSUPAR_DFX_RCOMP_CTL2		0x30
+#define PHY_DBBSUPAR_CAL_TOP1			0x34
+#define PHY_DBBSUPAR_CAL_SHARED1		0x38
+#define PHY_DBBSUPAR_CAL_SHARED2		0x3c
+#define PHY_DBBSUPAR_CAL_CDR1			0x40
+#define PHY_DBBSUPAR_CAL_OCAL1			0x44
+#define PHY_DBBSUPAR_CAL_DCC_DLL1		0x48
+#define PHY_DBBSUPAR_CAL_DLL2			0x4c
+#define PHY_DBBSUPAR_CAL_DFX1			0x50
+#define PHY_DBBSUPAR_CAL_DFX2			0x54
+#define PHY_DBBSUPAR_CAL_DFX3			0x58
+#define PHY_BBSUPAR_CAL_DFX4			0x5c
+#define PHY_DBBSUPAR_CAL_DFX5			0x60
+#define PHY_DBBSUPAR_CAL_DFX6			0x64
+#define PHY_DBBSUPAR_CAL_DFX7			0x68
+#define PHY_DBBSUPAR_DFX_AFE_SPARE_CTL1		0x6c
+#define PHY_DBBSUPAR_DFX_AFE_SPARE_CTL2		0x70
+#define	PHY_DBBSUPAR_SPARE			0x74
+
+/* sai, i = [0..11] */
+#define	IPU6_ISYS_PHY_SAI			0xf800
+/* register offset */
+#define PHY_SAI_CTRL_REG0                       0x40
+#define PHY_SAI_CTRL_REG0_1                     0x44
+#define PHY_SAI_WR_REG0                         0x48
+#define PHY_SAI_WR_REG0_1                       0x4c
+#define PHY_SAI_RD_REG0                         0x50
+#define PHY_SAI_RD_REG0_1                       0x54
+
+int ipu6_isys_phy_powerup_ack(struct ipu_isys *isys, unsigned int phy_id);
+int ipu6_isys_phy_powerdown_ack(struct ipu_isys *isys, unsigned int phy_id);
+int ipu6_isys_phy_reset(struct ipu_isys *isys, unsigned int phy_id,
+			bool assert);
+int ipu6_isys_phy_ready(struct ipu_isys *isys, unsigned int phy_id);
+int ipu6_isys_phy_common_init(struct ipu_isys *isys);
+int ipu6_isys_phy_config(struct ipu_isys *isys);
+#endif
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-l-scheduler.c b/drivers/media/pci/intel/ipu6/ipu6-l-scheduler.c
--- a/drivers/media/pci/intel/ipu6/ipu6-l-scheduler.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-l-scheduler.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,615 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2020 Intel Corporation
+
+#include "ipu-psys.h"
+#include "ipu6-ppg.h"
+
+extern bool enable_power_gating;
+
+struct sched_list {
+	struct list_head list;
+	/* to protect the list */
+	struct mutex lock;
+};
+
+static struct sched_list start_list = {
+	.list	= LIST_HEAD_INIT(start_list.list),
+	.lock	= __MUTEX_INITIALIZER(start_list.lock),
+};
+
+static struct sched_list stop_list = {
+	.list	= LIST_HEAD_INIT(stop_list.list),
+	.lock	= __MUTEX_INITIALIZER(stop_list.lock),
+};
+
+static struct sched_list *get_sc_list(enum SCHED_LIST type)
+{
+	/* for debug purposes */
+	WARN_ON(type != SCHED_START_LIST && type != SCHED_STOP_LIST);
+
+	if (type == SCHED_START_LIST)
+		return &start_list;
+	return &stop_list;
+}
+
+static bool is_kppg_in_list(struct ipu_psys_ppg *kppg, struct list_head *head)
+{
+	struct ipu_psys_ppg *tmp;
+
+	list_for_each_entry(tmp, head, sched_list) {
+		if (kppg == tmp)
+			return true;
+	}
+
+	return false;
+}
+
+void ipu_psys_scheduler_remove_kppg(struct ipu_psys_ppg *kppg,
+				    enum SCHED_LIST type)
+{
+	struct sched_list *sc_list = get_sc_list(type);
+	struct ipu_psys_ppg *tmp0, *tmp1;
+	struct ipu_psys *psys = kppg->fh->psys;
+
+	mutex_lock(&sc_list->lock);
+	list_for_each_entry_safe(tmp0, tmp1, &sc_list->list, sched_list) {
+		if (tmp0 == kppg) {
+			dev_dbg(&psys->adev->dev,
+				 "remove from %s list, kppg(%d 0x%p) state %d\n",
+				 type == SCHED_START_LIST ? "start" : "stop",
+				 kppg->kpg->pg->ID, kppg, kppg->state);
+			list_del_init(&kppg->sched_list);
+		}
+	}
+	mutex_unlock(&sc_list->lock);
+}
+
+void ipu_psys_scheduler_add_kppg(struct ipu_psys_ppg *kppg,
+				 enum SCHED_LIST type)
+{
+	int cur_pri = kppg->pri_base + kppg->pri_dynamic;
+	struct sched_list *sc_list = get_sc_list(type);
+	struct ipu_psys *psys = kppg->fh->psys;
+	struct ipu_psys_ppg *tmp0, *tmp1;
+
+	dev_dbg(&psys->adev->dev,
+		"add to %s list, kppg(%d 0x%p) state %d prio(%d %d) fh 0x%p\n",
+		type == SCHED_START_LIST ? "start" : "stop",
+		kppg->kpg->pg->ID, kppg, kppg->state,
+		kppg->pri_base, kppg->pri_dynamic, kppg->fh);
+
+	mutex_lock(&sc_list->lock);
+	if (list_empty(&sc_list->list)) {
+		list_add(&kppg->sched_list, &sc_list->list);
+		goto out;
+	}
+
+	if (is_kppg_in_list(kppg, &sc_list->list)) {
+		dev_dbg(&psys->adev->dev, "kppg already in list\n");
+		goto out;
+	}
+
+	list_for_each_entry_safe(tmp0, tmp1, &sc_list->list, sched_list) {
+		int tmp_pri = tmp0->pri_base + tmp0->pri_dynamic;
+
+		dev_dbg(&psys->adev->dev,
+			"found kppg(%d 0x%p), state %d pri(%d %d) fh 0x%p\n",
+			tmp0->kpg->pg->ID, tmp0, tmp0->state,
+			tmp0->pri_base, tmp0->pri_dynamic, tmp0->fh);
+
+		if (type == SCHED_START_LIST && tmp_pri > cur_pri) {
+			list_add(&kppg->sched_list, tmp0->sched_list.prev);
+			goto out;
+		} else if (type == SCHED_STOP_LIST && tmp_pri < cur_pri) {
+			list_add(&kppg->sched_list, tmp0->sched_list.prev);
+			goto out;
+		}
+	}
+
+	list_add_tail(&kppg->sched_list, &sc_list->list);
+out:
+	mutex_unlock(&sc_list->lock);
+}
+
+static int ipu_psys_detect_resource_contention(struct ipu_psys_ppg *kppg)
+{
+	struct ipu_psys_resource_pool *try_res_pool;
+	struct ipu_psys *psys = kppg->fh->psys;
+	int ret = 0;
+	int state;
+
+	try_res_pool = kzalloc(sizeof(*try_res_pool), GFP_KERNEL);
+	if (IS_ERR_OR_NULL(try_res_pool))
+		return -ENOMEM;
+
+	mutex_lock(&kppg->mutex);
+	state = kppg->state;
+	mutex_unlock(&kppg->mutex);
+	if (state == PPG_STATE_STARTED || state == PPG_STATE_RUNNING ||
+	    state == PPG_STATE_RESUMED)
+		goto exit;
+
+	ret = ipu_psys_resource_pool_init(try_res_pool);
+	if (ret < 0) {
+		dev_err(&psys->adev->dev, "unable to alloc pg resources\n");
+		WARN_ON(1);
+		goto exit;
+	}
+
+	ipu_psys_resource_copy(&psys->resource_pool_running, try_res_pool);
+	ret = ipu_psys_try_allocate_resources(&psys->adev->dev,
+					      kppg->kpg->pg,
+					      kppg->manifest,
+					      try_res_pool);
+
+	ipu_psys_resource_pool_cleanup(try_res_pool);
+exit:
+	kfree(try_res_pool);
+
+	return ret;
+}
+
+static void ipu_psys_scheduler_ppg_sort(struct ipu_psys *psys, bool *stopping)
+{
+	struct ipu_psys_ppg *kppg, *tmp;
+	struct ipu_psys_scheduler *sched;
+	struct ipu_psys_fh *fh;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		mutex_lock(&fh->mutex);
+		sched = &fh->sched;
+
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+
+		list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+			mutex_lock(&kppg->mutex);
+			if (kppg->state == PPG_STATE_START ||
+			    kppg->state == PPG_STATE_RESUME) {
+				ipu_psys_scheduler_add_kppg(kppg,
+							    SCHED_START_LIST);
+			} else if (kppg->state == PPG_STATE_RUNNING) {
+				ipu_psys_scheduler_add_kppg(kppg,
+							    SCHED_STOP_LIST);
+			} else if (kppg->state == PPG_STATE_SUSPENDING ||
+				   kppg->state == PPG_STATE_STOPPING) {
+				/* there are some suspending/stopping ppgs */
+				*stopping = true;
+			} else if (kppg->state == PPG_STATE_RESUMING ||
+				   kppg->state == PPG_STATE_STARTING) {
+				   /* how about kppg are resuming/starting? */
+			}
+			mutex_unlock(&kppg->mutex);
+		}
+		mutex_unlock(&fh->mutex);
+	}
+}
+
+static void ipu_psys_scheduler_update_start_ppg_priority(void)
+{
+	struct sched_list *sc_list = get_sc_list(SCHED_START_LIST);
+	struct ipu_psys_ppg *kppg, *tmp;
+
+	mutex_lock(&sc_list->lock);
+	if (!list_empty(&sc_list->list))
+		list_for_each_entry_safe(kppg, tmp, &sc_list->list, sched_list)
+			kppg->pri_dynamic--;
+	mutex_unlock(&sc_list->lock);
+}
+
+static bool ipu_psys_scheduler_switch_ppg(struct ipu_psys *psys)
+{
+	struct sched_list *sc_list = get_sc_list(SCHED_STOP_LIST);
+	struct ipu_psys_ppg *kppg;
+	bool resched = false;
+
+	mutex_lock(&sc_list->lock);
+	if (list_empty(&sc_list->list)) {
+		/* some ppgs are RESUMING/STARTING */
+		dev_dbg(&psys->adev->dev, "no candidated stop ppg\n");
+		mutex_unlock(&sc_list->lock);
+		return false;
+	}
+	kppg = list_first_entry(&sc_list->list, struct ipu_psys_ppg,
+				sched_list);
+	mutex_unlock(&sc_list->lock);
+
+	mutex_lock(&kppg->mutex);
+	if (!(kppg->state & PPG_STATE_STOP)) {
+		dev_dbg(&psys->adev->dev, "s_change:%s: %p %d -> %d\n",
+			__func__, kppg, kppg->state, PPG_STATE_SUSPEND);
+		kppg->state = PPG_STATE_SUSPEND;
+		resched = true;
+	}
+	mutex_unlock(&kppg->mutex);
+
+	return resched;
+}
+
+/*
+ * search all kppgs and sort them into start_list and stop_list, alway start
+ * first kppg(high priority) in start_list;
+ * if there is resource contention, it would switch kppgs in stop_list
+ * to suspend state one by one
+ */
+static bool ipu_psys_scheduler_ppg_start(struct ipu_psys *psys)
+{
+	struct sched_list *sc_list = get_sc_list(SCHED_START_LIST);
+	struct ipu_psys_ppg *kppg, *kppg0;
+	bool stopping_existed = false;
+	int ret;
+
+	ipu_psys_scheduler_ppg_sort(psys, &stopping_existed);
+
+	mutex_lock(&sc_list->lock);
+	if (list_empty(&sc_list->list)) {
+		dev_dbg(&psys->adev->dev, "no ppg to start\n");
+		mutex_unlock(&sc_list->lock);
+		return false;
+	}
+
+	list_for_each_entry_safe(kppg, kppg0,
+				 &sc_list->list, sched_list) {
+		mutex_unlock(&sc_list->lock);
+
+		ret = ipu_psys_detect_resource_contention(kppg);
+		if (ret < 0) {
+			dev_dbg(&psys->adev->dev,
+				"ppg %d resource detect failed(%d)\n",
+				kppg->kpg->pg->ID, ret);
+			/*
+			 * switch out other ppg in 2 cases:
+			 * 1. resource contention
+			 * 2. no suspending/stopping ppg
+			 */
+			if (ret == -ENOSPC) {
+				if (!stopping_existed &&
+				    ipu_psys_scheduler_switch_ppg(psys)) {
+					return true;
+				}
+				dev_dbg(&psys->adev->dev,
+					"ppg is suspending/stopping\n");
+			} else {
+				dev_err(&psys->adev->dev,
+					"detect resource error %d\n", ret);
+			}
+		} else {
+			kppg->pri_dynamic = 0;
+
+			mutex_lock(&kppg->mutex);
+			if (kppg->state == PPG_STATE_START)
+				ipu_psys_ppg_start(kppg);
+			else
+				ipu_psys_ppg_resume(kppg);
+			mutex_unlock(&kppg->mutex);
+
+			ipu_psys_scheduler_remove_kppg(kppg,
+						       SCHED_START_LIST);
+			ipu_psys_scheduler_update_start_ppg_priority();
+		}
+		mutex_lock(&sc_list->lock);
+	}
+	mutex_unlock(&sc_list->lock);
+
+	return false;
+}
+
+static bool ipu_psys_scheduler_ppg_enqueue_bufset(struct ipu_psys *psys)
+{
+	struct ipu_psys_scheduler *sched;
+	struct ipu_psys_ppg *kppg;
+	struct ipu_psys_fh *fh;
+	bool resched = false;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		mutex_lock(&fh->mutex);
+		sched = &fh->sched;
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+
+		list_for_each_entry(kppg, &sched->ppgs, list) {
+			if (ipu_psys_ppg_enqueue_bufsets(kppg))
+				resched = true;
+		}
+		mutex_unlock(&fh->mutex);
+	}
+
+	return resched;
+}
+
+/*
+ * This function will check all kppgs within fhs, and if kppg state
+ * is STOP or SUSPEND, l-scheduler will call ppg function to stop
+ * or suspend it and update stop list
+ */
+
+static bool ipu_psys_scheduler_ppg_halt(struct ipu_psys *psys)
+{
+	struct ipu_psys_scheduler *sched;
+	struct ipu_psys_ppg *kppg, *tmp;
+	struct ipu_psys_fh *fh;
+	bool stopping_exit = false;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		mutex_lock(&fh->mutex);
+		sched = &fh->sched;
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+
+		list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+			mutex_lock(&kppg->mutex);
+			if (kppg->state & PPG_STATE_STOP) {
+				ipu_psys_ppg_stop(kppg);
+				ipu_psys_scheduler_remove_kppg(kppg,
+							       SCHED_STOP_LIST);
+			} else if (kppg->state == PPG_STATE_SUSPEND) {
+				ipu_psys_ppg_suspend(kppg);
+				ipu_psys_scheduler_remove_kppg(kppg,
+							       SCHED_STOP_LIST);
+			} else if (kppg->state == PPG_STATE_SUSPENDING ||
+				   kppg->state == PPG_STATE_STOPPING) {
+				stopping_exit = true;
+			}
+			mutex_unlock(&kppg->mutex);
+		}
+		mutex_unlock(&fh->mutex);
+	}
+	return stopping_exit;
+}
+
+static void ipu_psys_update_ppg_state_by_kcmd(struct ipu_psys *psys,
+					      struct ipu_psys_ppg *kppg,
+					      struct ipu_psys_kcmd *kcmd)
+{
+	int old_ppg_state = kppg->state;
+
+	/*
+	 * Respond kcmd when ppg is in stable state:
+	 * STARTED/RESUMED/RUNNING/SUSPENDED/STOPPED
+	 */
+	if (kppg->state == PPG_STATE_STARTED ||
+	    kppg->state == PPG_STATE_RESUMED ||
+	    kppg->state == PPG_STATE_RUNNING) {
+		if (kcmd->state == KCMD_STATE_PPG_START)
+			ipu_psys_kcmd_complete(kppg, kcmd, 0);
+		else if (kcmd->state == KCMD_STATE_PPG_STOP)
+			kppg->state = PPG_STATE_STOP;
+	} else if (kppg->state == PPG_STATE_SUSPENDED) {
+		if (kcmd->state == KCMD_STATE_PPG_START)
+			ipu_psys_kcmd_complete(kppg, kcmd, 0);
+		else if (kcmd->state == KCMD_STATE_PPG_STOP)
+			/*
+			 * Record the previous state
+			 * because here need resume at first
+			 */
+			kppg->state |= PPG_STATE_STOP;
+		else if (kcmd->state == KCMD_STATE_PPG_ENQUEUE)
+			kppg->state = PPG_STATE_RESUME;
+	} else if (kppg->state == PPG_STATE_STOPPED) {
+		if (kcmd->state == KCMD_STATE_PPG_START)
+			kppg->state = PPG_STATE_START;
+		else if (kcmd->state == KCMD_STATE_PPG_STOP)
+			ipu_psys_kcmd_complete(kppg, kcmd, 0);
+		else if (kcmd->state == KCMD_STATE_PPG_ENQUEUE) {
+			dev_err(&psys->adev->dev, "ppg %p stopped!\n", kppg);
+			ipu_psys_kcmd_complete(kppg, kcmd, -EIO);
+		}
+	}
+
+	if (old_ppg_state != kppg->state)
+		dev_dbg(&psys->adev->dev, "s_change:%s: %p %d -> %d\n",
+			__func__, kppg, old_ppg_state, kppg->state);
+}
+
+static void ipu_psys_scheduler_kcmd_set(struct ipu_psys *psys)
+{
+	struct ipu_psys_kcmd *kcmd;
+	struct ipu_psys_scheduler *sched;
+	struct ipu_psys_ppg *kppg, *tmp;
+	struct ipu_psys_fh *fh;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		mutex_lock(&fh->mutex);
+		sched = &fh->sched;
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+
+		list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+			mutex_lock(&kppg->mutex);
+			if (list_empty(&kppg->kcmds_new_list)) {
+				mutex_unlock(&kppg->mutex);
+				continue;
+			};
+
+			kcmd = list_first_entry(&kppg->kcmds_new_list,
+						struct ipu_psys_kcmd, list);
+			ipu_psys_update_ppg_state_by_kcmd(psys, kppg, kcmd);
+			mutex_unlock(&kppg->mutex);
+		}
+		mutex_unlock(&fh->mutex);
+	}
+}
+
+static bool is_ready_to_enter_power_gating(struct ipu_psys *psys)
+{
+	struct ipu_psys_scheduler *sched;
+	struct ipu_psys_ppg *kppg, *tmp;
+	struct ipu_psys_fh *fh;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		mutex_lock(&fh->mutex);
+		sched = &fh->sched;
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+
+		list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+			mutex_lock(&kppg->mutex);
+			if (!list_empty(&kppg->kcmds_new_list) ||
+			    !list_empty(&kppg->kcmds_processing_list)) {
+				mutex_unlock(&kppg->mutex);
+				mutex_unlock(&fh->mutex);
+				return false;
+			}
+			if (!(kppg->state == PPG_STATE_RUNNING ||
+			      kppg->state == PPG_STATE_STOPPED ||
+			      kppg->state == PPG_STATE_SUSPENDED)) {
+				mutex_unlock(&kppg->mutex);
+				mutex_unlock(&fh->mutex);
+				return false;
+			}
+			mutex_unlock(&kppg->mutex);
+		}
+		mutex_unlock(&fh->mutex);
+	}
+
+	return true;
+}
+
+static bool has_pending_kcmd(struct ipu_psys *psys)
+{
+	struct ipu_psys_scheduler *sched;
+	struct ipu_psys_ppg *kppg, *tmp;
+	struct ipu_psys_fh *fh;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		mutex_lock(&fh->mutex);
+		sched = &fh->sched;
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+
+		list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+			mutex_lock(&kppg->mutex);
+			if (!list_empty(&kppg->kcmds_new_list) ||
+			    !list_empty(&kppg->kcmds_processing_list)) {
+				mutex_unlock(&kppg->mutex);
+				mutex_unlock(&fh->mutex);
+				return true;
+			}
+			mutex_unlock(&kppg->mutex);
+		}
+		mutex_unlock(&fh->mutex);
+	}
+
+	return false;
+}
+
+static bool ipu_psys_scheduler_exit_power_gating(struct ipu_psys *psys)
+{
+	/* Assume power gating process can be aborted directly during START */
+	if (psys->power_gating == PSYS_POWER_GATED) {
+		dev_dbg(&psys->adev->dev, "powergating: exit ---\n");
+		ipu_psys_exit_power_gating(psys);
+	}
+	psys->power_gating = PSYS_POWER_NORMAL;
+	return false;
+}
+
+static bool ipu_psys_scheduler_enter_power_gating(struct ipu_psys *psys)
+{
+	struct ipu_psys_scheduler *sched;
+	struct ipu_psys_ppg *kppg, *tmp;
+	struct ipu_psys_fh *fh;
+
+	if (!enable_power_gating)
+		return false;
+
+	if (psys->power_gating == PSYS_POWER_NORMAL &&
+	    is_ready_to_enter_power_gating(psys)) {
+		/* Enter power gating */
+		dev_dbg(&psys->adev->dev, "powergating: enter +++\n");
+		psys->power_gating = PSYS_POWER_GATING;
+	}
+
+	if (psys->power_gating != PSYS_POWER_GATING)
+		return false;
+
+	/* Suspend ppgs one by one */
+	list_for_each_entry(fh, &psys->fhs, list) {
+		mutex_lock(&fh->mutex);
+		sched = &fh->sched;
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+
+		list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+			mutex_lock(&kppg->mutex);
+			if (kppg->state == PPG_STATE_RUNNING) {
+				kppg->state = PPG_STATE_SUSPEND;
+				mutex_unlock(&kppg->mutex);
+				mutex_unlock(&fh->mutex);
+				return true;
+			}
+
+			if (kppg->state != PPG_STATE_SUSPENDED &&
+			    kppg->state != PPG_STATE_STOPPED) {
+				/* Can't enter power gating */
+				mutex_unlock(&kppg->mutex);
+				mutex_unlock(&fh->mutex);
+				/* Need re-run l-scheduler to suspend ppg? */
+				return (kppg->state & PPG_STATE_STOP ||
+					kppg->state == PPG_STATE_SUSPEND);
+			}
+			mutex_unlock(&kppg->mutex);
+		}
+		mutex_unlock(&fh->mutex);
+	}
+
+	psys->power_gating = PSYS_POWER_GATED;
+	ipu_psys_enter_power_gating(psys);
+
+	return false;
+}
+
+void ipu_psys_run_next(struct ipu_psys *psys)
+{
+	/* Wake up scheduler due to unfinished work */
+	bool need_trigger = false;
+	/* Wait FW callback if there are stopping/suspending/running ppg */
+	bool wait_fw_finish = false;
+	/*
+	 * Code below will crash if fhs is empty. Normally this
+	 * shouldn't happen.
+	 */
+	if (list_empty(&psys->fhs)) {
+		WARN_ON(1);
+		return;
+	}
+
+	/* Abort power gating process */
+	if (psys->power_gating != PSYS_POWER_NORMAL &&
+	    has_pending_kcmd(psys))
+		need_trigger = ipu_psys_scheduler_exit_power_gating(psys);
+
+	/* Handle kcmd and related ppg switch */
+	if (psys->power_gating == PSYS_POWER_NORMAL) {
+		ipu_psys_scheduler_kcmd_set(psys);
+		wait_fw_finish = ipu_psys_scheduler_ppg_halt(psys);
+		need_trigger |= ipu_psys_scheduler_ppg_start(psys);
+		need_trigger |= ipu_psys_scheduler_ppg_enqueue_bufset(psys);
+	}
+	if (!(need_trigger || wait_fw_finish)) {
+		/* Nothing to do, enter power gating */
+		need_trigger = ipu_psys_scheduler_enter_power_gating(psys);
+		if (psys->power_gating == PSYS_POWER_GATING)
+			wait_fw_finish = ipu_psys_scheduler_ppg_halt(psys);
+	}
+
+	if (need_trigger && !wait_fw_finish) {
+		dev_dbg(&psys->adev->dev, "scheduler: wake up\n");
+		atomic_set(&psys->wakeup_count, 1);
+		wake_up_interruptible(&psys->sched_cmd_wq);
+	}
+}
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-platform-resources.h b/drivers/media/pci/intel/ipu6/ipu6-platform-resources.h
--- a/drivers/media/pci/intel/ipu6/ipu6-platform-resources.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-platform-resources.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,196 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2018 - 2020 Intel Corporation */
+
+#ifndef IPU6_PLATFORM_RESOURCES_H
+#define IPU6_PLATFORM_RESOURCES_H
+
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include "ipu-platform-resources.h"
+
+#define	IPU6_FW_PSYS_N_PADDING_UINT8_IN_PROCESS_EXT_STRUCT		0
+
+enum {
+	IPU6_FW_PSYS_CMD_QUEUE_COMMAND_ID = 0,
+	IPU6_FW_PSYS_CMD_QUEUE_DEVICE_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG0_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG1_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG2_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG3_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG4_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG5_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG6_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG7_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG8_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG9_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG10_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG11_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG12_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG13_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG14_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG15_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG16_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG17_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG18_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG19_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG20_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG21_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG22_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG23_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG24_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG25_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG26_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG27_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG28_COMMAND_ID,
+	IPU6_FW_PSYS_CMD_QUEUE_PPG29_COMMAND_ID,
+	IPU6_FW_PSYS_N_PSYS_CMD_QUEUE_ID
+};
+
+enum {
+	IPU6_FW_PSYS_TRANSFER_VMEM0_TYPE_ID = 0,
+	IPU6_FW_PSYS_TRANSFER_VMEM1_TYPE_ID,
+	IPU6_FW_PSYS_LB_VMEM_TYPE_ID,
+	IPU6_FW_PSYS_DMEM_TYPE_ID,
+	IPU6_FW_PSYS_VMEM_TYPE_ID,
+	IPU6_FW_PSYS_BAMEM_TYPE_ID,
+	IPU6_FW_PSYS_PMEM_TYPE_ID,
+	IPU6_FW_PSYS_N_MEM_TYPE_ID
+};
+
+enum ipu6_mem_id {
+	IPU6_FW_PSYS_VMEM0_ID = 0,	/* ISP0 VMEM */
+	IPU6_FW_PSYS_TRANSFER_VMEM0_ID,	/* TRANSFER VMEM 0 */
+	IPU6_FW_PSYS_TRANSFER_VMEM1_ID,	/* TRANSFER VMEM 1 */
+	IPU6_FW_PSYS_LB_VMEM_ID,	/* LB VMEM */
+	IPU6_FW_PSYS_BAMEM0_ID,	/* ISP0 BAMEM */
+	IPU6_FW_PSYS_DMEM0_ID,	/* SPC0 Dmem */
+	IPU6_FW_PSYS_DMEM1_ID,	/* SPP0 Dmem */
+	IPU6_FW_PSYS_DMEM2_ID,	/* SPP1 Dmem */
+	IPU6_FW_PSYS_DMEM3_ID,	/* ISP0 Dmem */
+	IPU6_FW_PSYS_PMEM0_ID,	/* ISP0 PMEM */
+	IPU6_FW_PSYS_N_MEM_ID
+};
+
+enum {
+	IPU6_FW_PSYS_DEV_CHN_DMA_EXT0_ID = 0,
+	IPU6_FW_PSYS_DEV_CHN_DMA_EXT1_READ_ID,
+	IPU6_FW_PSYS_DEV_CHN_DMA_EXT1_WRITE_ID,
+	IPU6_FW_PSYS_DEV_CHN_DMA_INTERNAL_ID,
+	IPU6_FW_PSYS_DEV_CHN_DMA_ISA_ID,
+	IPU6_FW_PSYS_N_DEV_CHN_ID
+};
+
+enum {
+	IPU6_FW_PSYS_SP_CTRL_TYPE_ID = 0,
+	IPU6_FW_PSYS_SP_SERVER_TYPE_ID,
+	IPU6_FW_PSYS_VP_TYPE_ID,
+	IPU6_FW_PSYS_ACC_PSA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6_FW_PSYS_ACC_OSA_TYPE_ID,
+	IPU6_FW_PSYS_GDC_TYPE_ID,
+	IPU6_FW_PSYS_TNR_TYPE_ID,
+	IPU6_FW_PSYS_N_CELL_TYPE_ID
+};
+
+enum {
+	IPU6_FW_PSYS_SP0_ID = 0,
+	IPU6_FW_PSYS_VP0_ID,
+	IPU6_FW_PSYS_PSA_ACC_BNLM_ID,
+	IPU6_FW_PSYS_PSA_ACC_DM_ID,
+	IPU6_FW_PSYS_PSA_ACC_ACM_ID,
+	IPU6_FW_PSYS_PSA_ACC_GTC_YUV1_ID,
+	IPU6_FW_PSYS_BB_ACC_OFS_PIN_MAIN_ID,
+	IPU6_FW_PSYS_BB_ACC_OFS_PIN_DISPLAY_ID,
+	IPU6_FW_PSYS_BB_ACC_OFS_PIN_PP_ID,
+	IPU6_FW_PSYS_PSA_ACC_GAMMASTAR_ID,
+	IPU6_FW_PSYS_PSA_ACC_GLTM_ID,
+	IPU6_FW_PSYS_PSA_ACC_XNR_ID,
+	IPU6_FW_PSYS_PSA_VCSC_ID,	/* VCSC */
+	IPU6_FW_PSYS_ISA_ICA_ID,
+	IPU6_FW_PSYS_ISA_LSC_ID,
+	IPU6_FW_PSYS_ISA_DPC_ID,
+	IPU6_FW_PSYS_ISA_SIS_A_ID,
+	IPU6_FW_PSYS_ISA_SIS_B_ID,
+	IPU6_FW_PSYS_ISA_B2B_ID,
+	IPU6_FW_PSYS_ISA_B2R_R2I_SIE_ID,
+	IPU6_FW_PSYS_ISA_R2I_DS_A_ID,
+	IPU6_FW_PSYS_ISA_R2I_DS_B_ID,
+	IPU6_FW_PSYS_ISA_AWB_ID,
+	IPU6_FW_PSYS_ISA_AE_ID,
+	IPU6_FW_PSYS_ISA_AF_ID,
+	IPU6_FW_PSYS_ISA_DOL_ID,
+	IPU6_FW_PSYS_ISA_ICA_MEDIUM_ID,
+	IPU6_FW_PSYS_ISA_X2B_MD_ID,
+	IPU6_FW_PSYS_ISA_X2B_SVE_RGBIR_ID,
+	IPU6_FW_PSYS_ISA_PAF_ID,
+	IPU6_FW_PSYS_BB_ACC_GDC0_ID,
+	IPU6_FW_PSYS_BB_ACC_TNR_ID,
+	IPU6_FW_PSYS_N_CELL_ID
+};
+
+enum {
+	IPU6_FW_PSYS_DEV_DFM_BB_FULL_PORT_ID = 0,
+	IPU6_FW_PSYS_DEV_DFM_BB_EMPTY_PORT_ID,
+	IPU6_FW_PSYS_DEV_DFM_ISL_FULL_PORT_ID,
+	IPU6_FW_PSYS_DEV_DFM_ISL_EMPTY_PORT_ID,
+	IPU6_FW_PSYS_DEV_DFM_LB_FULL_PORT_ID,
+	IPU6_FW_PSYS_DEV_DFM_LB_EMPTY_PORT_ID,
+};
+
+/* Excluding PMEM */
+#define IPU6_FW_PSYS_N_DATA_MEM_TYPE_ID	(IPU6_FW_PSYS_N_MEM_TYPE_ID - 1)
+#define IPU6_FW_PSYS_N_DEV_DFM_ID	\
+	(IPU6_FW_PSYS_DEV_DFM_LB_EMPTY_PORT_ID + 1)
+
+#define IPU6_FW_PSYS_VMEM0_MAX_SIZE		0x0800
+/* Transfer VMEM0 words, ref HAS Transfer*/
+#define IPU6_FW_PSYS_TRANSFER_VMEM0_MAX_SIZE	0x0800
+/* Transfer VMEM1 words, ref HAS Transfer*/
+#define IPU6_FW_PSYS_TRANSFER_VMEM1_MAX_SIZE	0x0800
+#define IPU6_FW_PSYS_LB_VMEM_MAX_SIZE		0x0400	/* LB VMEM words */
+#define IPU6_FW_PSYS_BAMEM0_MAX_SIZE		0x0800
+#define IPU6_FW_PSYS_DMEM0_MAX_SIZE		0x4000
+#define IPU6_FW_PSYS_DMEM1_MAX_SIZE		0x1000
+#define IPU6_FW_PSYS_DMEM2_MAX_SIZE		0x1000
+#define IPU6_FW_PSYS_DMEM3_MAX_SIZE		0x1000
+#define IPU6_FW_PSYS_PMEM0_MAX_SIZE		0x0500
+
+#define IPU6_FW_PSYS_DEV_CHN_DMA_EXT0_MAX_SIZE		30
+#define IPU6_FW_PSYS_DEV_CHN_GDC_MAX_SIZE		0
+#define IPU6_FW_PSYS_DEV_CHN_DMA_EXT1_READ_MAX_SIZE	30
+#define IPU6_FW_PSYS_DEV_CHN_DMA_EXT1_WRITE_MAX_SIZE	43
+#define IPU6_FW_PSYS_DEV_CHN_DMA_INTERNAL_MAX_SIZE	8
+#define IPU6_FW_PSYS_DEV_CHN_DMA_IPFD_MAX_SIZE		0
+#define IPU6_FW_PSYS_DEV_CHN_DMA_ISA_MAX_SIZE		2
+
+#define IPU6_FW_PSYS_DEV_DFM_BB_FULL_PORT_ID_MAX_SIZE		32
+#define IPU6_FW_PSYS_DEV_DFM_ISL_FULL_PORT_ID_MAX_SIZE		32
+#define IPU6_FW_PSYS_DEV_DFM_LB_FULL_PORT_ID_MAX_SIZE		32
+#define IPU6_FW_PSYS_DEV_DFM_BB_EMPTY_PORT_ID_MAX_SIZE		32
+#define IPU6_FW_PSYS_DEV_DFM_ISL_EMPTY_PORT_ID_MAX_SIZE		32
+#define IPU6_FW_PSYS_DEV_DFM_LB_EMPTY_PORT_ID_MAX_SIZE		32
+
+struct ipu6_fw_psys_program_manifest_ext {
+	u32 dfm_port_bitmap[IPU6_FW_PSYS_N_DEV_DFM_ID];
+	u32 dfm_active_port_bitmap[IPU6_FW_PSYS_N_DEV_DFM_ID];
+	u16 ext_mem_size[IPU6_FW_PSYS_N_DATA_MEM_TYPE_ID];
+	u16 ext_mem_offset[IPU6_FW_PSYS_N_DATA_MEM_TYPE_ID];
+	u16 dev_chn_size[IPU6_FW_PSYS_N_DEV_CHN_ID];
+	u16 dev_chn_offset[IPU6_FW_PSYS_N_DEV_CHN_ID];
+	u8 is_dfm_relocatable[IPU6_FW_PSYS_N_DEV_DFM_ID];
+	u8 dec_resources_input[IPU_FW_PSYS_MAX_INPUT_DEC_RESOURCES];
+	u8 dec_resources_input_terminal[IPU_FW_PSYS_MAX_INPUT_DEC_RESOURCES];
+	u8 dec_resources_output[IPU_FW_PSYS_MAX_OUTPUT_DEC_RESOURCES];
+	u8 dec_resources_output_terminal[IPU_FW_PSYS_MAX_OUTPUT_DEC_RESOURCES];
+	u8 padding[IPU_FW_PSYS_N_PADDING_UINT8_IN_PROGRAM_MANIFEST_EXT];
+};
+
+struct ipu6_fw_psys_process_ext {
+	u32 dfm_port_bitmap[IPU6_FW_PSYS_N_DEV_DFM_ID];
+	u32 dfm_active_port_bitmap[IPU6_FW_PSYS_N_DEV_DFM_ID];
+	u16 ext_mem_offset[IPU6_FW_PSYS_N_DATA_MEM_TYPE_ID];
+	u16 dev_chn_offset[IPU6_FW_PSYS_N_DEV_CHN_ID];
+	u8 ext_mem_id[IPU6_FW_PSYS_N_DATA_MEM_TYPE_ID];
+};
+
+#endif /* IPU6_PLATFORM_RESOURCES_H */
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-ppg.c b/drivers/media/pci/intel/ipu6/ipu6-ppg.c
--- a/drivers/media/pci/intel/ipu6/ipu6-ppg.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-ppg.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,560 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2020 Intel Corporation
+
+#include <linux/module.h>
+#include <linux/pm_runtime.h>
+
+#include <asm/cacheflush.h>
+
+#include "ipu6-ppg.h"
+
+static bool enable_suspend_resume;
+module_param(enable_suspend_resume, bool, 0664);
+MODULE_PARM_DESC(enable_suspend_resume, "enable fw ppg suspend/resume api");
+
+static struct ipu_psys_kcmd *
+ipu_psys_ppg_get_kcmd(struct ipu_psys_ppg *kppg, enum ipu_psys_cmd_state state)
+{
+	struct ipu_psys_kcmd *kcmd;
+
+	if (list_empty(&kppg->kcmds_new_list))
+		return NULL;
+
+	list_for_each_entry(kcmd, &kppg->kcmds_new_list, list) {
+		if (kcmd->state == state)
+			return kcmd;
+	}
+
+	return NULL;
+}
+
+struct ipu_psys_kcmd *ipu_psys_ppg_get_stop_kcmd(struct ipu_psys_ppg *kppg)
+{
+	struct ipu_psys_kcmd *kcmd;
+
+	WARN(!mutex_is_locked(&kppg->mutex), "ppg locking error");
+
+	if (list_empty(&kppg->kcmds_processing_list))
+		return NULL;
+
+	list_for_each_entry(kcmd, &kppg->kcmds_processing_list, list) {
+		if (kcmd->state == KCMD_STATE_PPG_STOP)
+			return kcmd;
+	}
+
+	return NULL;
+}
+
+static struct ipu_psys_buffer_set *
+__get_buf_set(struct ipu_psys_fh *fh, size_t buf_set_size)
+{
+	struct ipu_psys_buffer_set *kbuf_set;
+	struct ipu_psys_scheduler *sched = &fh->sched;
+
+	mutex_lock(&sched->bs_mutex);
+	list_for_each_entry(kbuf_set, &sched->buf_sets, list) {
+		if (!kbuf_set->buf_set_size &&
+		    kbuf_set->size >= buf_set_size) {
+			kbuf_set->buf_set_size = buf_set_size;
+			mutex_unlock(&sched->bs_mutex);
+			return kbuf_set;
+		}
+	}
+
+	mutex_unlock(&sched->bs_mutex);
+	/* no suitable buffer available, allocate new one */
+	kbuf_set = kzalloc(sizeof(*kbuf_set), GFP_KERNEL);
+	if (!kbuf_set)
+		return NULL;
+
+	kbuf_set->kaddr = dma_alloc_attrs(&fh->psys->adev->dev,
+					  buf_set_size, &kbuf_set->dma_addr,
+					  GFP_KERNEL, 0);
+	if (!kbuf_set->kaddr) {
+		kfree(kbuf_set);
+		return NULL;
+	}
+
+	kbuf_set->buf_set_size = buf_set_size;
+	kbuf_set->size = buf_set_size;
+	mutex_lock(&sched->bs_mutex);
+	list_add(&kbuf_set->list, &sched->buf_sets);
+	mutex_unlock(&sched->bs_mutex);
+
+	return kbuf_set;
+}
+
+static struct ipu_psys_buffer_set *
+ipu_psys_create_buffer_set(struct ipu_psys_kcmd *kcmd,
+			   struct ipu_psys_ppg *kppg)
+{
+	struct ipu_psys_fh *fh = kcmd->fh;
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_psys_buffer_set *kbuf_set;
+	size_t buf_set_size;
+	u32 *keb;
+
+	buf_set_size = ipu_fw_psys_ppg_get_buffer_set_size(kcmd);
+
+	kbuf_set = __get_buf_set(fh, buf_set_size);
+	if (!kbuf_set) {
+		dev_err(&psys->adev->dev, "failed to create buffer set\n");
+		return NULL;
+	}
+
+	kbuf_set->buf_set = ipu_fw_psys_ppg_create_buffer_set(kcmd,
+							      kbuf_set->kaddr,
+							      0);
+
+	ipu_fw_psys_ppg_buffer_set_vaddress(kbuf_set->buf_set,
+					    kbuf_set->dma_addr);
+	keb = kcmd->kernel_enable_bitmap;
+	ipu_fw_psys_ppg_buffer_set_set_kernel_enable_bitmap(kbuf_set->buf_set,
+							    keb);
+
+	return kbuf_set;
+}
+
+int ipu_psys_ppg_get_bufset(struct ipu_psys_kcmd *kcmd,
+			    struct ipu_psys_ppg *kppg)
+{
+	struct ipu_psys *psys = kppg->fh->psys;
+	struct ipu_psys_buffer_set *kbuf_set;
+	unsigned int i;
+	int ret;
+
+	kbuf_set = ipu_psys_create_buffer_set(kcmd, kppg);
+	if (!kbuf_set) {
+		ret = -EINVAL;
+		goto error;
+	}
+	kcmd->kbuf_set = kbuf_set;
+	kbuf_set->kcmd = kcmd;
+
+	for (i = 0; i < kcmd->nbuffers; i++) {
+		struct ipu_fw_psys_terminal *terminal;
+		u32 buffer;
+
+		terminal = ipu_fw_psys_pg_get_terminal(kcmd, i);
+		if (!terminal)
+			continue;
+
+		buffer = (u32)kcmd->kbufs[i]->dma_addr +
+				    kcmd->buffers[i].data_offset;
+
+		ret = ipu_fw_psys_ppg_set_buffer_set(kcmd, terminal, i, buffer);
+		if (ret) {
+			dev_err(&psys->adev->dev, "Unable to set bufset\n");
+			goto error;
+		}
+	}
+
+	return 0;
+
+error:
+	dev_err(&psys->adev->dev, "failed to get buffer set\n");
+	return ret;
+}
+
+void ipu_psys_ppg_complete(struct ipu_psys *psys, struct ipu_psys_ppg *kppg)
+{
+	u8 queue_id;
+	int old_ppg_state;
+
+	if (!psys || !kppg)
+		return;
+
+	mutex_lock(&kppg->mutex);
+	old_ppg_state = kppg->state;
+	if (kppg->state == PPG_STATE_STOPPING) {
+		struct ipu_psys_kcmd tmp_kcmd = {
+			.kpg = kppg->kpg,
+		};
+
+		kppg->state = PPG_STATE_STOPPED;
+		ipu_psys_free_resources(&kppg->kpg->resource_alloc,
+					&psys->resource_pool_running);
+		queue_id = ipu_fw_psys_ppg_get_base_queue_id(&tmp_kcmd);
+		ipu_psys_free_cmd_queue_resource(&psys->resource_pool_running,
+						 queue_id);
+		pm_runtime_put(&psys->adev->dev);
+	} else {
+		if (kppg->state == PPG_STATE_SUSPENDING) {
+			kppg->state = PPG_STATE_SUSPENDED;
+			ipu_psys_free_resources(&kppg->kpg->resource_alloc,
+						&psys->resource_pool_running);
+		} else if (kppg->state == PPG_STATE_STARTED ||
+			   kppg->state == PPG_STATE_RESUMED) {
+			kppg->state = PPG_STATE_RUNNING;
+		}
+
+		/* Kick l-scheduler thread for FW callback,
+		 * also for checking if need to enter power gating
+		 */
+		atomic_set(&psys->wakeup_count, 1);
+		wake_up_interruptible(&psys->sched_cmd_wq);
+	}
+	if (old_ppg_state != kppg->state)
+		dev_dbg(&psys->adev->dev, "s_change:%s: %p %d -> %d\n",
+			__func__, kppg, old_ppg_state, kppg->state);
+
+	mutex_unlock(&kppg->mutex);
+}
+
+int ipu_psys_ppg_start(struct ipu_psys_ppg *kppg)
+{
+	struct ipu_psys *psys = kppg->fh->psys;
+	struct ipu_psys_kcmd *kcmd = ipu_psys_ppg_get_kcmd(kppg,
+						KCMD_STATE_PPG_START);
+	unsigned int i;
+	int ret;
+
+	if (!kcmd) {
+		dev_err(&psys->adev->dev, "failed to find start kcmd!\n");
+		return -EINVAL;
+	}
+
+	dev_dbg(&psys->adev->dev, "start ppg id %d, addr 0x%p\n",
+		ipu_fw_psys_pg_get_id(kcmd), kppg);
+
+	kppg->state = PPG_STATE_STARTING;
+	for (i = 0; i < kcmd->nbuffers; i++) {
+		struct ipu_fw_psys_terminal *terminal;
+
+		terminal = ipu_fw_psys_pg_get_terminal(kcmd, i);
+		if (!terminal)
+			continue;
+
+		ret = ipu_fw_psys_terminal_set(terminal, i, kcmd, 0,
+					       kcmd->buffers[i].len);
+		if (ret) {
+			dev_err(&psys->adev->dev, "Unable to set terminal\n");
+			return ret;
+		}
+	}
+
+	ret = ipu_fw_psys_pg_submit(kcmd);
+	if (ret) {
+		dev_err(&psys->adev->dev, "failed to submit kcmd!\n");
+		return ret;
+	}
+
+	ret = ipu_psys_allocate_resources(&psys->adev->dev,
+					  kcmd->kpg->pg,
+					  kcmd->pg_manifest,
+					  &kcmd->kpg->resource_alloc,
+					  &psys->resource_pool_running);
+	if (ret) {
+		dev_err(&psys->adev->dev, "alloc resources failed!\n");
+		return ret;
+	}
+
+	ret = pm_runtime_get_sync(&psys->adev->dev);
+	if (ret < 0) {
+		dev_err(&psys->adev->dev, "failed to power on psys\n");
+		goto error;
+	}
+
+	ret = ipu_psys_kcmd_start(psys, kcmd);
+	if (ret) {
+		ipu_psys_kcmd_complete(kppg, kcmd, -EIO);
+		goto error;
+	}
+
+	dev_dbg(&psys->adev->dev, "s_change:%s: %p %d -> %d\n",
+		__func__, kppg, kppg->state, PPG_STATE_STARTED);
+	kppg->state = PPG_STATE_STARTED;
+	ipu_psys_kcmd_complete(kppg, kcmd, 0);
+
+	return 0;
+
+error:
+	pm_runtime_put_noidle(&psys->adev->dev);
+	ipu_psys_reset_process_cell(&psys->adev->dev,
+				    kcmd->kpg->pg,
+				    kcmd->pg_manifest,
+				    kcmd->kpg->pg->process_count);
+	ipu_psys_free_resources(&kppg->kpg->resource_alloc,
+				&psys->resource_pool_running);
+
+	dev_err(&psys->adev->dev, "failed to start ppg\n");
+	return ret;
+}
+
+int ipu_psys_ppg_resume(struct ipu_psys_ppg *kppg)
+{
+	struct ipu_psys *psys = kppg->fh->psys;
+	struct ipu_psys_kcmd tmp_kcmd = {
+		.kpg = kppg->kpg,
+		.fh = kppg->fh,
+	};
+	int ret;
+
+	dev_dbg(&psys->adev->dev, "resume ppg id %d, addr 0x%p\n",
+		ipu_fw_psys_pg_get_id(&tmp_kcmd), kppg);
+
+	kppg->state = PPG_STATE_RESUMING;
+	if (enable_suspend_resume) {
+		ret = ipu_psys_allocate_resources(&psys->adev->dev,
+						  kppg->kpg->pg,
+						  kppg->manifest,
+						  &kppg->kpg->resource_alloc,
+						  &psys->resource_pool_running);
+		if (ret) {
+			dev_err(&psys->adev->dev, "failed to allocate res\n");
+			return -EIO;
+		}
+
+		ret = ipu_fw_psys_ppg_resume(&tmp_kcmd);
+		if (ret) {
+			dev_err(&psys->adev->dev, "failed to resume ppg\n");
+			goto error;
+		}
+	} else {
+		kppg->kpg->pg->state = IPU_FW_PSYS_PROCESS_GROUP_READY;
+		ret = ipu_fw_psys_pg_submit(&tmp_kcmd);
+		if (ret) {
+			dev_err(&psys->adev->dev, "failed to submit kcmd!\n");
+			return ret;
+		}
+
+		ret = ipu_psys_allocate_resources(&psys->adev->dev,
+						  kppg->kpg->pg,
+						  kppg->manifest,
+						  &kppg->kpg->resource_alloc,
+						  &psys->resource_pool_running);
+		if (ret) {
+			dev_err(&psys->adev->dev, "failed to allocate res\n");
+			return ret;
+		}
+
+		ret = ipu_psys_kcmd_start(psys, &tmp_kcmd);
+		if (ret) {
+			dev_err(&psys->adev->dev, "failed to start kcmd!\n");
+			goto error;
+		}
+	}
+	dev_dbg(&psys->adev->dev, "s_change:%s: %p %d -> %d\n",
+		__func__, kppg, kppg->state, PPG_STATE_RESUMED);
+	kppg->state = PPG_STATE_RESUMED;
+
+	return 0;
+
+error:
+	ipu_psys_reset_process_cell(&psys->adev->dev,
+				    kppg->kpg->pg,
+				    kppg->manifest,
+				    kppg->kpg->pg->process_count);
+	ipu_psys_free_resources(&kppg->kpg->resource_alloc,
+				&psys->resource_pool_running);
+
+	return ret;
+}
+
+int ipu_psys_ppg_stop(struct ipu_psys_ppg *kppg)
+{
+	struct ipu_psys_kcmd *kcmd = ipu_psys_ppg_get_kcmd(kppg,
+							   KCMD_STATE_PPG_STOP);
+	struct ipu_psys *psys = kppg->fh->psys;
+	struct ipu_psys_kcmd kcmd_temp;
+	int ppg_id, ret = 0;
+
+	if (kcmd) {
+		list_move_tail(&kcmd->list, &kppg->kcmds_processing_list);
+	} else {
+		dev_dbg(&psys->adev->dev, "Exceptional stop happened!\n");
+		kcmd_temp.kpg = kppg->kpg;
+		kcmd_temp.fh = kppg->fh;
+		kcmd = &kcmd_temp;
+		/* delete kppg in stop list to avoid this ppg resuming */
+		ipu_psys_scheduler_remove_kppg(kppg, SCHED_STOP_LIST);
+	}
+
+	ppg_id = ipu_fw_psys_pg_get_id(kcmd);
+	dev_dbg(&psys->adev->dev, "stop ppg(%d, addr 0x%p)\n", ppg_id, kppg);
+
+	if (kppg->state & PPG_STATE_SUSPENDED) {
+		if (enable_suspend_resume) {
+			dev_dbg(&psys->adev->dev, "need resume before stop!\n");
+			kcmd_temp.kpg = kppg->kpg;
+			kcmd_temp.fh = kppg->fh;
+			ret = ipu_fw_psys_ppg_resume(&kcmd_temp);
+			if (ret)
+				dev_err(&psys->adev->dev,
+					"ppg(%d) failed to resume\n", ppg_id);
+		} else if (kcmd != &kcmd_temp) {
+			ipu_psys_free_cmd_queue_resource(
+				&psys->resource_pool_running,
+				ipu_fw_psys_ppg_get_base_queue_id(kcmd));
+			ipu_psys_kcmd_complete(kppg, kcmd, 0);
+			dev_dbg(&psys->adev->dev,
+				"s_change:%s %p %d -> %d\n", __func__,
+				kppg, kppg->state, PPG_STATE_STOPPED);
+			pm_runtime_put(&psys->adev->dev);
+			kppg->state = PPG_STATE_STOPPED;
+			return 0;
+		} else {
+			return 0;
+		}
+	}
+	dev_dbg(&psys->adev->dev, "s_change:%s %p %d -> %d\n",
+		__func__, kppg, kppg->state, PPG_STATE_STOPPING);
+	kppg->state = PPG_STATE_STOPPING;
+	ret = ipu_fw_psys_pg_abort(kcmd);
+	if (ret)
+		dev_err(&psys->adev->dev, "ppg(%d) failed to abort\n", ppg_id);
+
+	return ret;
+}
+
+int ipu_psys_ppg_suspend(struct ipu_psys_ppg *kppg)
+{
+	struct ipu_psys *psys = kppg->fh->psys;
+	struct ipu_psys_kcmd tmp_kcmd = {
+		.kpg = kppg->kpg,
+		.fh = kppg->fh,
+	};
+	int ppg_id = ipu_fw_psys_pg_get_id(&tmp_kcmd);
+	int ret = 0;
+
+	dev_dbg(&psys->adev->dev, "suspend ppg(%d, addr 0x%p)\n", ppg_id, kppg);
+
+	dev_dbg(&psys->adev->dev, "s_change:%s %p %d -> %d\n",
+		__func__, kppg, kppg->state, PPG_STATE_SUSPENDING);
+	kppg->state = PPG_STATE_SUSPENDING;
+	if (enable_suspend_resume)
+		ret = ipu_fw_psys_ppg_suspend(&tmp_kcmd);
+	else
+		ret = ipu_fw_psys_pg_abort(&tmp_kcmd);
+	if (ret)
+		dev_err(&psys->adev->dev, "failed to %s ppg(%d)\n",
+			enable_suspend_resume ? "suspend" : "stop", ret);
+
+	return ret;
+}
+
+static bool ipu_psys_ppg_is_bufset_existing(struct ipu_psys_ppg *kppg)
+{
+	return !list_empty(&kppg->kcmds_new_list);
+}
+
+/*
+ * ipu_psys_ppg_enqueue_bufsets - enqueue buffer sets to firmware
+ * Sometimes, if the ppg is at suspended state, this function will return true
+ * to reschedule and let the resume command scheduled before the buffer sets
+ * enqueuing.
+ */
+bool ipu_psys_ppg_enqueue_bufsets(struct ipu_psys_ppg *kppg)
+{
+	struct ipu_psys_kcmd *kcmd, *kcmd0;
+	struct ipu_psys *psys = kppg->fh->psys;
+	bool need_resume = false;
+
+	mutex_lock(&kppg->mutex);
+
+	if (kppg->state & (PPG_STATE_STARTED | PPG_STATE_RESUMED |
+			   PPG_STATE_RUNNING)) {
+		if (ipu_psys_ppg_is_bufset_existing(kppg)) {
+			list_for_each_entry_safe(kcmd, kcmd0,
+						 &kppg->kcmds_new_list, list) {
+				int ret;
+
+				if (kcmd->state != KCMD_STATE_PPG_ENQUEUE) {
+					need_resume = true;
+					break;
+				}
+
+				ret = ipu_fw_psys_ppg_enqueue_bufs(kcmd);
+				if (ret) {
+					dev_err(&psys->adev->dev,
+						"kppg 0x%p fail to qbufset %d",
+						kppg, ret);
+					break;
+				}
+				list_move_tail(&kcmd->list,
+					       &kppg->kcmds_processing_list);
+				dev_dbg(&psys->adev->dev,
+					"kppg %d %p queue kcmd 0x%p fh 0x%p\n",
+					ipu_fw_psys_pg_get_id(kcmd),
+					kppg, kcmd, kcmd->fh);
+			}
+		}
+	}
+
+	mutex_unlock(&kppg->mutex);
+	return need_resume;
+}
+
+void ipu_psys_enter_power_gating(struct ipu_psys *psys)
+{
+	struct ipu_psys_scheduler *sched;
+	struct ipu_psys_ppg *kppg, *tmp;
+	struct ipu_psys_fh *fh;
+	int ret = 0;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		mutex_lock(&fh->mutex);
+		sched = &fh->sched;
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+
+		list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+			mutex_lock(&kppg->mutex);
+			/*
+			 * Only for SUSPENDED kppgs, STOPPED kppgs has already
+			 * power down and new kppgs might come now.
+			 */
+			if (kppg->state != PPG_STATE_SUSPENDED) {
+				mutex_unlock(&kppg->mutex);
+				continue;
+			}
+
+			ret = pm_runtime_put_autosuspend(&psys->adev->dev);
+			if (ret < 0) {
+				dev_err(&psys->adev->dev,
+					"failed to power gating off\n");
+				pm_runtime_get_sync(&psys->adev->dev);
+
+			}
+			mutex_unlock(&kppg->mutex);
+		}
+		mutex_unlock(&fh->mutex);
+	}
+}
+
+void ipu_psys_exit_power_gating(struct ipu_psys *psys)
+{
+	struct ipu_psys_scheduler *sched;
+	struct ipu_psys_ppg *kppg, *tmp;
+	struct ipu_psys_fh *fh;
+	int ret = 0;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		mutex_lock(&fh->mutex);
+		sched = &fh->sched;
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+
+		list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+			mutex_lock(&kppg->mutex);
+			/* Only for SUSPENDED kppgs */
+			if (kppg->state != PPG_STATE_SUSPENDED) {
+				mutex_unlock(&kppg->mutex);
+				continue;
+			}
+
+			ret = pm_runtime_get_sync(&psys->adev->dev);
+			if (ret < 0) {
+				dev_err(&psys->adev->dev,
+					"failed to power gating\n");
+				pm_runtime_put_noidle(&psys->adev->dev);
+			}
+			mutex_unlock(&kppg->mutex);
+		}
+		mutex_unlock(&fh->mutex);
+	}
+}
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-ppg.h b/drivers/media/pci/intel/ipu6/ipu6-ppg.h
--- a/drivers/media/pci/intel/ipu6/ipu6-ppg.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-ppg.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,38 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020 Intel Corporation
+ */
+
+#ifndef IPU6_PPG_H
+#define IPU6_PPG_H
+
+#include "ipu-psys.h"
+/* starting from '2' in case of someone passes true or false */
+enum SCHED_LIST {
+	SCHED_START_LIST = 2,
+	SCHED_STOP_LIST
+};
+
+enum ipu_psys_power_gating_state {
+	PSYS_POWER_NORMAL = 0,
+	PSYS_POWER_GATING,
+	PSYS_POWER_GATED
+};
+
+int ipu_psys_ppg_get_bufset(struct ipu_psys_kcmd *kcmd,
+			    struct ipu_psys_ppg *kppg);
+struct ipu_psys_kcmd *ipu_psys_ppg_get_stop_kcmd(struct ipu_psys_ppg *kppg);
+void ipu_psys_scheduler_remove_kppg(struct ipu_psys_ppg *kppg,
+				    enum SCHED_LIST type);
+void ipu_psys_scheduler_add_kppg(struct ipu_psys_ppg *kppg,
+				 enum SCHED_LIST type);
+int ipu_psys_ppg_start(struct ipu_psys_ppg *kppg);
+int ipu_psys_ppg_resume(struct ipu_psys_ppg *kppg);
+int ipu_psys_ppg_stop(struct ipu_psys_ppg *kppg);
+int ipu_psys_ppg_suspend(struct ipu_psys_ppg *kppg);
+void ipu_psys_ppg_complete(struct ipu_psys *psys, struct ipu_psys_ppg *kppg);
+bool ipu_psys_ppg_enqueue_bufsets(struct ipu_psys_ppg *kppg);
+void ipu_psys_enter_power_gating(struct ipu_psys *psys);
+void ipu_psys_exit_power_gating(struct ipu_psys *psys);
+
+#endif /* IPU6_PPG_H */
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-psys.c b/drivers/media/pci/intel/ipu6/ipu6-psys.c
--- a/drivers/media/pci/intel/ipu6/ipu6-psys.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-psys.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,1032 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2020 Intel Corporation
+
+#include <linux/uaccess.h>
+#include <linux/device.h>
+#include <linux/delay.h>
+#include <linux/highmem.h>
+#include <linux/mm.h>
+#include <linux/pm_runtime.h>
+#include <linux/kthread.h>
+#include <linux/init_task.h>
+#include <linux/version.h>
+#include <uapi/linux/sched/types.h>
+#include <linux/module.h>
+#include <linux/fs.h>
+
+#include "ipu.h"
+#include "ipu-psys.h"
+#include "ipu6-ppg.h"
+#include "ipu-platform-regs.h"
+#include "ipu-trace.h"
+
+static bool early_pg_transfer;
+module_param(early_pg_transfer, bool, 0664);
+MODULE_PARM_DESC(early_pg_transfer,
+		 "Copy PGs back to user after resource allocation");
+
+bool enable_power_gating = true;
+module_param(enable_power_gating, bool, 0664);
+MODULE_PARM_DESC(enable_power_gating, "enable power gating");
+
+struct ipu_trace_block psys_trace_blocks[] = {
+	{
+		.offset = IPU_TRACE_REG_PS_TRACE_UNIT_BASE,
+		.type = IPU_TRACE_BLOCK_TUN,
+	},
+	{
+		.offset = IPU_TRACE_REG_PS_SPC_EVQ_BASE,
+		.type = IPU_TRACE_BLOCK_TM,
+	},
+	{
+		.offset = IPU_TRACE_REG_PS_SPP0_EVQ_BASE,
+		.type = IPU_TRACE_BLOCK_TM,
+	},
+	{
+		.offset = IPU_TRACE_REG_PS_SPC_GPC_BASE,
+		.type = IPU_TRACE_BLOCK_GPC,
+	},
+	{
+		.offset = IPU_TRACE_REG_PS_SPP0_GPC_BASE,
+		.type = IPU_TRACE_BLOCK_GPC,
+	},
+	{
+		.offset = IPU_TRACE_REG_PS_MMU_GPC_BASE,
+		.type = IPU_TRACE_BLOCK_GPC,
+	},
+	{
+		.offset = IPU_TRACE_REG_PS_GPREG_TRACE_TIMER_RST_N,
+		.type = IPU_TRACE_TIMER_RST,
+	},
+	{
+		.type = IPU_TRACE_BLOCK_END,
+	}
+};
+
+static void ipu6_set_sp_info_bits(void *base)
+{
+	int i;
+
+	writel(IPU_INFO_REQUEST_DESTINATION_IOSF,
+	       base + IPU_REG_PSYS_INFO_SEG_0_CONFIG_ICACHE_MASTER);
+
+	for (i = 0; i < 4; i++)
+		writel(IPU_INFO_REQUEST_DESTINATION_IOSF,
+		       base + IPU_REG_PSYS_INFO_SEG_CMEM_MASTER(i));
+	for (i = 0; i < 4; i++)
+		writel(IPU_INFO_REQUEST_DESTINATION_IOSF,
+		       base + IPU_REG_PSYS_INFO_SEG_XMEM_MASTER(i));
+}
+
+#define PSYS_SUBDOMAINS_STATUS_WAIT_COUNT        1000
+void ipu_psys_subdomains_power(struct ipu_psys *psys, bool on)
+{
+	unsigned int i;
+	u32 val;
+
+	/* power domain req */
+	dev_dbg(&psys->adev->dev, "power %s psys sub-domains",
+		on ? "UP" : "DOWN");
+	if (on)
+		writel(IPU_PSYS_SUBDOMAINS_POWER_MASK,
+		       psys->adev->isp->base + IPU_PSYS_SUBDOMAINS_POWER_REQ);
+	else
+		writel(0x0,
+		       psys->adev->isp->base + IPU_PSYS_SUBDOMAINS_POWER_REQ);
+
+	i = 0;
+	do {
+		usleep_range(10, 20);
+		val = readl(psys->adev->isp->base +
+			    IPU_PSYS_SUBDOMAINS_POWER_STATUS);
+		if (!(val & BIT(31))) {
+			dev_dbg(&psys->adev->dev,
+				"PS sub-domains req done with status 0x%x",
+				val);
+			break;
+		}
+		i++;
+	} while (i < PSYS_SUBDOMAINS_STATUS_WAIT_COUNT);
+
+	if (i == PSYS_SUBDOMAINS_STATUS_WAIT_COUNT)
+		dev_warn(&psys->adev->dev, "Psys sub-domains %s req timeout!",
+			 on ? "UP" : "DOWN");
+}
+
+void ipu_psys_setup_hw(struct ipu_psys *psys)
+{
+	void __iomem *base = psys->pdata->base;
+	void __iomem *spc_regs_base =
+	    base + psys->pdata->ipdata->hw_variant.spc_offset;
+	void *psys_iommu0_ctrl;
+	u32 irqs;
+	const u8 r3 = IPU_DEVICE_AB_GROUP1_TARGET_ID_R3_SPC_STATUS_REG;
+	const u8 r4 = IPU_DEVICE_AB_GROUP1_TARGET_ID_R4_SPC_MASTER_BASE_ADDR;
+	const u8 r5 = IPU_DEVICE_AB_GROUP1_TARGET_ID_R5_SPC_PC_STALL;
+
+	if (!psys->adev->isp->secure_mode) {
+		/* configure access blocker for non-secure mode */
+		writel(NCI_AB_ACCESS_MODE_RW,
+		       base + IPU_REG_DMA_TOP_AB_GROUP1_BASE_ADDR +
+		       IPU_REG_DMA_TOP_AB_RING_ACCESS_OFFSET(r3));
+		writel(NCI_AB_ACCESS_MODE_RW,
+		       base + IPU_REG_DMA_TOP_AB_GROUP1_BASE_ADDR +
+		       IPU_REG_DMA_TOP_AB_RING_ACCESS_OFFSET(r4));
+		writel(NCI_AB_ACCESS_MODE_RW,
+		       base + IPU_REG_DMA_TOP_AB_GROUP1_BASE_ADDR +
+		       IPU_REG_DMA_TOP_AB_RING_ACCESS_OFFSET(r5));
+	}
+	psys_iommu0_ctrl = base +
+		psys->pdata->ipdata->hw_variant.mmu_hw[0].offset +
+		IPU_MMU_INFO_OFFSET;
+	writel(IPU_INFO_REQUEST_DESTINATION_IOSF, psys_iommu0_ctrl);
+
+	ipu6_set_sp_info_bits(spc_regs_base + IPU_PSYS_REG_SPC_STATUS_CTRL);
+	ipu6_set_sp_info_bits(spc_regs_base + IPU_PSYS_REG_SPP0_STATUS_CTRL);
+
+	/* Enable FW interrupt #0 */
+	writel(0, base + IPU_REG_PSYS_GPDEV_FWIRQ(0));
+	irqs = IPU_PSYS_GPDEV_IRQ_FWIRQ(IPU_PSYS_GPDEV_FWIRQ0);
+	writel(irqs, base + IPU_REG_PSYS_GPDEV_IRQ_EDGE);
+	writel(irqs, base + IPU_REG_PSYS_GPDEV_IRQ_LEVEL_NOT_PULSE);
+	writel(0xffffffff, base + IPU_REG_PSYS_GPDEV_IRQ_CLEAR);
+	writel(irqs, base + IPU_REG_PSYS_GPDEV_IRQ_MASK);
+	writel(irqs, base + IPU_REG_PSYS_GPDEV_IRQ_ENABLE);
+}
+
+static struct ipu_psys_ppg *ipu_psys_identify_kppg(struct ipu_psys_kcmd *kcmd)
+{
+	struct ipu_psys_scheduler *sched = &kcmd->fh->sched;
+	struct ipu_psys_ppg *kppg, *tmp;
+
+	mutex_lock(&kcmd->fh->mutex);
+	if (list_empty(&sched->ppgs))
+		goto not_found;
+
+	list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+		if (ipu_fw_psys_pg_get_token(kcmd)
+		    != kppg->token)
+			continue;
+		mutex_unlock(&kcmd->fh->mutex);
+		return kppg;
+	}
+
+not_found:
+	mutex_unlock(&kcmd->fh->mutex);
+	return NULL;
+}
+
+/*
+ * Called to free up all resources associated with a kcmd.
+ * After this the kcmd doesn't anymore exist in the driver.
+ */
+static void ipu_psys_kcmd_free(struct ipu_psys_kcmd *kcmd)
+{
+	struct ipu_psys_ppg *kppg;
+	struct ipu_psys_scheduler *sched;
+
+	if (!kcmd)
+		return;
+
+	kppg = ipu_psys_identify_kppg(kcmd);
+	sched = &kcmd->fh->sched;
+
+	if (kcmd->kbuf_set) {
+		mutex_lock(&sched->bs_mutex);
+		kcmd->kbuf_set->buf_set_size = 0;
+		mutex_unlock(&sched->bs_mutex);
+		kcmd->kbuf_set = NULL;
+	}
+
+	if (kppg) {
+		mutex_lock(&kppg->mutex);
+		if (!list_empty(&kcmd->list))
+			list_del(&kcmd->list);
+		mutex_unlock(&kppg->mutex);
+	}
+
+	kfree(kcmd->pg_manifest);
+	kfree(kcmd->kbufs);
+	kfree(kcmd->buffers);
+	kfree(kcmd);
+}
+
+static struct ipu_psys_kcmd *ipu_psys_copy_cmd(struct ipu_psys_command *cmd,
+					       struct ipu_psys_fh *fh)
+{
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_psys_kcmd *kcmd;
+	struct ipu_psys_kbuffer *kpgbuf;
+	unsigned int i;
+	int ret, prevfd, fd;
+
+	fd = prevfd = -1;
+
+	if (cmd->bufcount > IPU_MAX_PSYS_CMD_BUFFERS)
+		return NULL;
+
+	if (!cmd->pg_manifest_size)
+		return NULL;
+
+	kcmd = kzalloc(sizeof(*kcmd), GFP_KERNEL);
+	if (!kcmd)
+		return NULL;
+
+	kcmd->state = KCMD_STATE_PPG_NEW;
+	kcmd->fh = fh;
+	INIT_LIST_HEAD(&kcmd->list);
+
+	mutex_lock(&fh->mutex);
+	fd = cmd->pg;
+	kpgbuf = ipu_psys_lookup_kbuffer(fh, fd);
+	if (!kpgbuf || !kpgbuf->sgt) {
+		dev_err(&psys->adev->dev, "%s kbuf %p with fd %d not found.\n",
+			__func__, kpgbuf, fd);
+		mutex_unlock(&fh->mutex);
+		goto error;
+	}
+
+	/* check and remap if possibe */
+	ret = ipu_psys_mapbuf_locked(fd, fh, kpgbuf);
+	if (ret) {
+		dev_err(&psys->adev->dev, "%s remap failed\n", __func__);
+		mutex_unlock(&fh->mutex);
+		goto error;
+	}
+
+	kpgbuf = ipu_psys_lookup_kbuffer(fh, fd);
+	if (!kpgbuf || !kpgbuf->sgt) {
+		WARN(1, "kbuf not found or unmapped.\n");
+		mutex_unlock(&fh->mutex);
+		goto error;
+	}
+	mutex_unlock(&fh->mutex);
+
+	kcmd->pg_user = kpgbuf->kaddr;
+	kcmd->kpg = __get_pg_buf(psys, kpgbuf->len);
+	if (!kcmd->kpg)
+		goto error;
+
+	memcpy(kcmd->kpg->pg, kcmd->pg_user, kcmd->kpg->pg_size);
+
+	kcmd->pg_manifest = kzalloc(cmd->pg_manifest_size, GFP_KERNEL);
+	if (!kcmd->pg_manifest)
+		goto error;
+
+	ret = copy_from_user(kcmd->pg_manifest, cmd->pg_manifest,
+			     cmd->pg_manifest_size);
+	if (ret)
+		goto error;
+
+	kcmd->pg_manifest_size = cmd->pg_manifest_size;
+
+	kcmd->user_token = cmd->user_token;
+	kcmd->issue_id = cmd->issue_id;
+	kcmd->priority = cmd->priority;
+	if (kcmd->priority >= IPU_PSYS_CMD_PRIORITY_NUM)
+		goto error;
+
+	/*
+	 * Kenel enable bitmap be used only.
+	 */
+	memcpy(kcmd->kernel_enable_bitmap, cmd->kernel_enable_bitmap,
+	       sizeof(cmd->kernel_enable_bitmap));
+
+	kcmd->nbuffers = ipu_fw_psys_pg_get_terminal_count(kcmd);
+	kcmd->buffers = kcalloc(kcmd->nbuffers, sizeof(*kcmd->buffers),
+				GFP_KERNEL);
+	if (!kcmd->buffers)
+		goto error;
+
+	kcmd->kbufs = kcalloc(kcmd->nbuffers, sizeof(kcmd->kbufs[0]),
+			      GFP_KERNEL);
+	if (!kcmd->kbufs)
+		goto error;
+
+	/* should be stop cmd for ppg */
+	if (!cmd->buffers) {
+		kcmd->state = KCMD_STATE_PPG_STOP;
+		return kcmd;
+	}
+
+	if (!cmd->bufcount || kcmd->nbuffers > cmd->bufcount)
+		goto error;
+
+	ret = copy_from_user(kcmd->buffers, cmd->buffers,
+			     kcmd->nbuffers * sizeof(*kcmd->buffers));
+	if (ret)
+		goto error;
+
+	for (i = 0; i < kcmd->nbuffers; i++) {
+		struct ipu_fw_psys_terminal *terminal;
+
+		terminal = ipu_fw_psys_pg_get_terminal(kcmd, i);
+		if (!terminal)
+			continue;
+
+		if (!(kcmd->buffers[i].flags & IPU_BUFFER_FLAG_DMA_HANDLE)) {
+			kcmd->state = KCMD_STATE_PPG_START;
+			continue;
+		}
+		if (kcmd->state == KCMD_STATE_PPG_START) {
+			dev_err(&psys->adev->dev,
+				"err: all buffer.flags&DMA_HANDLE must 0\n");
+			goto error;
+		}
+
+		mutex_lock(&fh->mutex);
+		fd = kcmd->buffers[i].base.fd;
+		kpgbuf = ipu_psys_lookup_kbuffer(fh, fd);
+		if (!kpgbuf || !kpgbuf->sgt) {
+			dev_err(&psys->adev->dev,
+				"%s kcmd->buffers[%d] %p fd %d not found.\n",
+				__func__, i, kpgbuf, fd);
+			mutex_unlock(&fh->mutex);
+			goto error;
+		}
+
+		ret = ipu_psys_mapbuf_locked(fd, fh, kpgbuf);
+		if (ret) {
+			dev_err(&psys->adev->dev, "%s remap failed\n",
+				__func__);
+			mutex_unlock(&fh->mutex);
+			goto error;
+		}
+
+		kpgbuf = ipu_psys_lookup_kbuffer(fh, fd);
+		if (!kpgbuf || !kpgbuf->sgt) {
+			WARN(1, "kbuf not found or unmapped.\n");
+			mutex_unlock(&fh->mutex);
+			goto error;
+		}
+		mutex_unlock(&fh->mutex);
+		kcmd->kbufs[i] = kpgbuf;
+		if (!kcmd->kbufs[i] || !kcmd->kbufs[i]->sgt ||
+		    kcmd->kbufs[i]->len < kcmd->buffers[i].bytes_used)
+			goto error;
+		if ((kcmd->kbufs[i]->flags &
+		     IPU_BUFFER_FLAG_NO_FLUSH) ||
+		    (kcmd->buffers[i].flags &
+		     IPU_BUFFER_FLAG_NO_FLUSH) ||
+		    prevfd == kcmd->buffers[i].base.fd)
+			continue;
+
+		prevfd = kcmd->buffers[i].base.fd;
+		dma_sync_sg_for_device(&psys->adev->dev,
+				       kcmd->kbufs[i]->sgt->sgl,
+				       kcmd->kbufs[i]->sgt->orig_nents,
+				       DMA_BIDIRECTIONAL);
+	}
+
+	if (kcmd->state != KCMD_STATE_PPG_START)
+		kcmd->state = KCMD_STATE_PPG_ENQUEUE;
+
+	return kcmd;
+error:
+	ipu_psys_kcmd_free(kcmd);
+
+	dev_dbg(&psys->adev->dev, "failed to copy cmd\n");
+
+	return NULL;
+}
+
+static struct ipu_psys_buffer_set *
+ipu_psys_lookup_kbuffer_set(struct ipu_psys *psys, u32 addr)
+{
+	struct ipu_psys_fh *fh;
+	struct ipu_psys_buffer_set *kbuf_set;
+	struct ipu_psys_scheduler *sched;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		sched = &fh->sched;
+		mutex_lock(&sched->bs_mutex);
+		list_for_each_entry(kbuf_set, &sched->buf_sets, list) {
+			if (kbuf_set->buf_set &&
+			    kbuf_set->buf_set->ipu_virtual_address == addr) {
+				mutex_unlock(&sched->bs_mutex);
+				return kbuf_set;
+			}
+		}
+		mutex_unlock(&sched->bs_mutex);
+	}
+
+	return NULL;
+}
+
+static struct ipu_psys_ppg *ipu_psys_lookup_ppg(struct ipu_psys *psys,
+						dma_addr_t pg_addr)
+{
+	struct ipu_psys_scheduler *sched;
+	struct ipu_psys_ppg *kppg, *tmp;
+	struct ipu_psys_fh *fh;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		sched = &fh->sched;
+		mutex_lock(&fh->mutex);
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+
+		list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+			if (pg_addr != kppg->kpg->pg_dma_addr)
+				continue;
+			mutex_unlock(&fh->mutex);
+			return kppg;
+		}
+		mutex_unlock(&fh->mutex);
+	}
+
+	return NULL;
+}
+
+/*
+ * Move kcmd into completed state (due to running finished or failure).
+ * Fill up the event struct and notify waiters.
+ */
+void ipu_psys_kcmd_complete(struct ipu_psys_ppg *kppg,
+			    struct ipu_psys_kcmd *kcmd, int error)
+{
+	struct ipu_psys_fh *fh = kcmd->fh;
+	struct ipu_psys *psys = fh->psys;
+
+	kcmd->ev.type = IPU_PSYS_EVENT_TYPE_CMD_COMPLETE;
+	kcmd->ev.user_token = kcmd->user_token;
+	kcmd->ev.issue_id = kcmd->issue_id;
+	kcmd->ev.error = error;
+	list_move_tail(&kcmd->list, &kppg->kcmds_finished_list);
+
+	if (kcmd->constraint.min_freq)
+		ipu_buttress_remove_psys_constraint(psys->adev->isp,
+						    &kcmd->constraint);
+
+	if (!early_pg_transfer && kcmd->pg_user && kcmd->kpg->pg) {
+		struct ipu_psys_kbuffer *kbuf;
+
+		kbuf = ipu_psys_lookup_kbuffer_by_kaddr(kcmd->fh,
+							kcmd->pg_user);
+		if (kbuf && kbuf->valid)
+			memcpy(kcmd->pg_user,
+			       kcmd->kpg->pg, kcmd->kpg->pg_size);
+		else
+			dev_dbg(&psys->adev->dev, "Skipping unmapped buffer\n");
+	}
+
+	kcmd->state = KCMD_STATE_PPG_COMPLETE;
+	wake_up_interruptible(&fh->wait);
+}
+
+/*
+ * Submit kcmd into psys queue. If running fails, complete the kcmd
+ * with an error.
+ *
+ * Found a runnable PG. Move queue to the list tail for round-robin
+ * scheduling and run the PG. Start the watchdog timer if the PG was
+ * started successfully. Enable PSYS power if requested.
+ */
+int ipu_psys_kcmd_start(struct ipu_psys *psys, struct ipu_psys_kcmd *kcmd)
+{
+	int ret;
+
+	if (psys->adev->isp->flr_done)
+		return -EIO;
+
+	if (early_pg_transfer && kcmd->pg_user && kcmd->kpg->pg)
+		memcpy(kcmd->pg_user, kcmd->kpg->pg, kcmd->kpg->pg_size);
+
+	ret = ipu_fw_psys_pg_start(kcmd);
+	if (ret) {
+		dev_err(&psys->adev->dev, "failed to start kcmd!\n");
+		return ret;
+	}
+
+	ipu_fw_psys_pg_dump(psys, kcmd, "run");
+
+	ret = ipu_fw_psys_pg_disown(kcmd);
+	if (ret) {
+		dev_err(&psys->adev->dev, "failed to start kcmd!\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static int ipu_psys_kcmd_send_to_ppg_start(struct ipu_psys_kcmd *kcmd)
+{
+	struct ipu_psys_fh *fh = kcmd->fh;
+	struct ipu_psys_scheduler *sched = &fh->sched;
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_psys_ppg *kppg;
+	struct ipu_psys_resource_pool *rpr;
+	int queue_id;
+	int ret;
+
+	rpr = &psys->resource_pool_running;
+
+	kppg = kzalloc(sizeof(*kppg), GFP_KERNEL);
+	if (!kppg)
+		return -ENOMEM;
+
+	kppg->fh = fh;
+	kppg->kpg = kcmd->kpg;
+	kppg->state = PPG_STATE_START;
+	kppg->pri_base = kcmd->priority;
+	kppg->pri_dynamic = 0;
+	INIT_LIST_HEAD(&kppg->list);
+
+	mutex_init(&kppg->mutex);
+	INIT_LIST_HEAD(&kppg->kcmds_new_list);
+	INIT_LIST_HEAD(&kppg->kcmds_processing_list);
+	INIT_LIST_HEAD(&kppg->kcmds_finished_list);
+	INIT_LIST_HEAD(&kppg->sched_list);
+
+	kppg->manifest = kzalloc(kcmd->pg_manifest_size, GFP_KERNEL);
+	if (!kppg->manifest) {
+		kfree(kppg);
+		return -ENOMEM;
+	}
+	memcpy(kppg->manifest, kcmd->pg_manifest,
+	       kcmd->pg_manifest_size);
+
+	queue_id = ipu_psys_allocate_cmd_queue_resource(rpr);
+	if (queue_id == -ENOSPC) {
+		dev_err(&psys->adev->dev, "no available queue\n");
+		kfree(kppg->manifest);
+		kfree(kppg);
+		mutex_unlock(&psys->mutex);
+		return -ENOMEM;
+	}
+
+	/*
+	 * set token as start cmd will immediately be followed by a
+	 * enqueue cmd so that kppg could be retrieved.
+	 */
+	kppg->token = (u64)kcmd->kpg;
+	ipu_fw_psys_pg_set_token(kcmd, kppg->token);
+	ipu_fw_psys_ppg_set_base_queue_id(kcmd, queue_id);
+	ret = ipu_fw_psys_pg_set_ipu_vaddress(kcmd,
+					      kcmd->kpg->pg_dma_addr);
+	if (ret) {
+		ipu_psys_free_cmd_queue_resource(rpr, queue_id);
+		kfree(kppg->manifest);
+		kfree(kppg);
+		return -EIO;
+	}
+	memcpy(kcmd->pg_user, kcmd->kpg->pg, kcmd->kpg->pg_size);
+
+	mutex_lock(&fh->mutex);
+	list_add_tail(&kppg->list, &sched->ppgs);
+	mutex_unlock(&fh->mutex);
+
+	mutex_lock(&kppg->mutex);
+	list_add(&kcmd->list, &kppg->kcmds_new_list);
+	mutex_unlock(&kppg->mutex);
+
+	dev_dbg(&psys->adev->dev,
+		"START ppg(%d, 0x%p) kcmd 0x%p, queue %d\n",
+		ipu_fw_psys_pg_get_id(kcmd), kppg, kcmd, queue_id);
+
+	/* Kick l-scheduler thread */
+	atomic_set(&psys->wakeup_count, 1);
+	wake_up_interruptible(&psys->sched_cmd_wq);
+
+	return 0;
+}
+
+static int ipu_psys_kcmd_send_to_ppg(struct ipu_psys_kcmd *kcmd)
+{
+	struct ipu_psys_fh *fh = kcmd->fh;
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_psys_ppg *kppg;
+	struct ipu_psys_resource_pool *rpr;
+	unsigned long flags;
+	u8 id;
+	bool resche = true;
+
+	rpr = &psys->resource_pool_running;
+	if (kcmd->state == KCMD_STATE_PPG_START)
+		return ipu_psys_kcmd_send_to_ppg_start(kcmd);
+
+	kppg = ipu_psys_identify_kppg(kcmd);
+	spin_lock_irqsave(&psys->pgs_lock, flags);
+	kcmd->kpg->pg_size = 0;
+	spin_unlock_irqrestore(&psys->pgs_lock, flags);
+	if (!kppg) {
+		dev_err(&psys->adev->dev, "token not match\n");
+		return -EINVAL;
+	}
+
+	kcmd->kpg = kppg->kpg;
+
+	dev_dbg(&psys->adev->dev, "%s ppg(%d, 0x%p) kcmd %p\n",
+		(kcmd->state == KCMD_STATE_PPG_STOP) ?
+		"STOP" : "ENQUEUE",
+		ipu_fw_psys_pg_get_id(kcmd), kppg, kcmd);
+
+	if (kcmd->state == KCMD_STATE_PPG_STOP) {
+		mutex_lock(&kppg->mutex);
+		if (kppg->state == PPG_STATE_STOPPED) {
+			dev_dbg(&psys->adev->dev,
+				"kppg 0x%p  stopped!\n", kppg);
+			id = ipu_fw_psys_ppg_get_base_queue_id(kcmd);
+			ipu_psys_free_cmd_queue_resource(rpr, id);
+			ipu_psys_kcmd_complete(kppg, kcmd, 0);
+			pm_runtime_put(&psys->adev->dev);
+			resche = false;
+		} else {
+			list_add(&kcmd->list, &kppg->kcmds_new_list);
+		}
+		mutex_unlock(&kppg->mutex);
+	} else {
+		int ret;
+
+		ret = ipu_psys_ppg_get_bufset(kcmd, kppg);
+		if (ret)
+			return ret;
+
+		mutex_lock(&kppg->mutex);
+		list_add_tail(&kcmd->list, &kppg->kcmds_new_list);
+		mutex_unlock(&kppg->mutex);
+	}
+
+	if (resche) {
+		/* Kick l-scheduler thread */
+		atomic_set(&psys->wakeup_count, 1);
+		wake_up_interruptible(&psys->sched_cmd_wq);
+	}
+	return 0;
+}
+
+int ipu_psys_kcmd_new(struct ipu_psys_command *cmd, struct ipu_psys_fh *fh)
+{
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_psys_kcmd *kcmd;
+	size_t pg_size;
+	int ret;
+
+	if (psys->adev->isp->flr_done)
+		return -EIO;
+
+	kcmd = ipu_psys_copy_cmd(cmd, fh);
+	if (!kcmd)
+		return -EINVAL;
+
+	pg_size = ipu_fw_psys_pg_get_size(kcmd);
+	if (pg_size > kcmd->kpg->pg_size) {
+		dev_dbg(&psys->adev->dev, "pg size mismatch %lu %lu\n",
+			pg_size, kcmd->kpg->pg_size);
+		ret = -EINVAL;
+		goto error;
+	}
+
+	if (ipu_fw_psys_pg_get_protocol(kcmd) !=
+			IPU_FW_PSYS_PROCESS_GROUP_PROTOCOL_PPG) {
+		dev_err(&psys->adev->dev, "No support legacy pg now\n");
+		ret = -EINVAL;
+		goto error;
+	}
+
+	if (cmd->min_psys_freq) {
+		kcmd->constraint.min_freq = cmd->min_psys_freq;
+		ipu_buttress_add_psys_constraint(psys->adev->isp,
+						 &kcmd->constraint);
+	}
+
+	ret = ipu_psys_kcmd_send_to_ppg(kcmd);
+	if (ret)
+		goto error;
+
+	dev_dbg(&psys->adev->dev,
+		"IOC_QCMD: user_token:%llx issue_id:0x%llx pri:%d\n",
+		cmd->user_token, cmd->issue_id, cmd->priority);
+
+	return 0;
+
+error:
+	ipu_psys_kcmd_free(kcmd);
+
+	return ret;
+}
+
+static bool ipu_psys_kcmd_is_valid(struct ipu_psys *psys,
+				   struct ipu_psys_kcmd *kcmd)
+{
+	struct ipu_psys_fh *fh;
+	struct ipu_psys_kcmd *kcmd0;
+	struct ipu_psys_ppg *kppg, *tmp;
+	struct ipu_psys_scheduler *sched;
+
+	list_for_each_entry(fh, &psys->fhs, list) {
+		sched = &fh->sched;
+		mutex_lock(&fh->mutex);
+		if (list_empty(&sched->ppgs)) {
+			mutex_unlock(&fh->mutex);
+			continue;
+		}
+		list_for_each_entry_safe(kppg, tmp, &sched->ppgs, list) {
+			mutex_lock(&kppg->mutex);
+			list_for_each_entry(kcmd0,
+					    &kppg->kcmds_processing_list,
+					    list) {
+				if (kcmd0 == kcmd) {
+					mutex_unlock(&kppg->mutex);
+					mutex_unlock(&fh->mutex);
+					return true;
+				}
+			}
+			mutex_unlock(&kppg->mutex);
+		}
+		mutex_unlock(&fh->mutex);
+	}
+
+	return false;
+}
+
+void ipu_psys_handle_events(struct ipu_psys *psys)
+{
+	struct ipu_psys_kcmd *kcmd;
+	struct ipu_fw_psys_event event;
+	struct ipu_psys_ppg *kppg;
+	bool error;
+	u32 hdl;
+	u16 cmd, status;
+	int res;
+
+	do {
+		memset(&event, 0, sizeof(event));
+		if (!ipu_fw_psys_rcv_event(psys, &event))
+			break;
+
+		if (!event.context_handle)
+			break;
+
+		dev_dbg(&psys->adev->dev, "ppg event: 0x%x, %d, status %d\n",
+			event.context_handle, event.command, event.status);
+
+		error = false;
+		/*
+		 * event.command == CMD_RUN shows this is fw processing frame
+		 * done as pPG mode, and event.context_handle should be pointer
+		 * of buffer set; so we make use of this pointer to lookup
+		 * kbuffer_set and kcmd
+		 */
+		hdl = event.context_handle;
+		cmd = event.command;
+		status = event.status;
+
+		kppg = NULL;
+		kcmd = NULL;
+		if (cmd == IPU_FW_PSYS_PROCESS_GROUP_CMD_RUN) {
+			struct ipu_psys_buffer_set *kbuf_set;
+			/*
+			 * Need change ppg state when the 1st running is done
+			 * (after PPG started/resumed)
+			 */
+			kbuf_set = ipu_psys_lookup_kbuffer_set(psys, hdl);
+			if (kbuf_set)
+				kcmd = kbuf_set->kcmd;
+			if (!kbuf_set || !kcmd)
+				error = true;
+			else
+				kppg = ipu_psys_identify_kppg(kcmd);
+		} else if (cmd == IPU_FW_PSYS_PROCESS_GROUP_CMD_STOP ||
+			   cmd == IPU_FW_PSYS_PROCESS_GROUP_CMD_SUSPEND ||
+			   cmd == IPU_FW_PSYS_PROCESS_GROUP_CMD_RESUME) {
+			/*
+			 * STOP/SUSPEND/RESUME cmd event would run this branch;
+			 * only stop cmd queued by user has stop_kcmd and need
+			 * to notify user to dequeue.
+			 */
+			kppg = ipu_psys_lookup_ppg(psys, hdl);
+			if (kppg) {
+				mutex_lock(&kppg->mutex);
+				if (kppg->state == PPG_STATE_STOPPING) {
+					kcmd = ipu_psys_ppg_get_stop_kcmd(kppg);
+					if (!kcmd)
+						error = true;
+				}
+				mutex_unlock(&kppg->mutex);
+			}
+		} else {
+			dev_err(&psys->adev->dev, "invalid event\n");
+			continue;
+		}
+
+		if (error || !kppg) {
+			dev_err(&psys->adev->dev, "event error, command %d\n",
+				cmd);
+			break;
+		}
+
+		dev_dbg(&psys->adev->dev, "event to kppg 0x%p, kcmd 0x%p\n",
+			kppg, kcmd);
+
+		ipu_psys_ppg_complete(psys, kppg);
+
+		if (kcmd && ipu_psys_kcmd_is_valid(psys, kcmd)) {
+			res = (status == IPU_PSYS_EVENT_CMD_COMPLETE ||
+			       status == IPU_PSYS_EVENT_FRAGMENT_COMPLETE) ?
+				0 : -EIO;
+			mutex_lock(&kppg->mutex);
+			ipu_psys_kcmd_complete(kppg, kcmd, res);
+			mutex_unlock(&kppg->mutex);
+		}
+	} while (1);
+}
+
+int ipu_psys_fh_init(struct ipu_psys_fh *fh)
+{
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_psys_buffer_set *kbuf_set, *kbuf_set_tmp;
+	struct ipu_psys_scheduler *sched = &fh->sched;
+	int i;
+
+	mutex_init(&sched->bs_mutex);
+	INIT_LIST_HEAD(&sched->buf_sets);
+	INIT_LIST_HEAD(&sched->ppgs);
+	pm_runtime_dont_use_autosuspend(&psys->adev->dev);
+	/* allocate and map memory for buf_sets */
+	for (i = 0; i < IPU_PSYS_BUF_SET_POOL_SIZE; i++) {
+		kbuf_set = kzalloc(sizeof(*kbuf_set), GFP_KERNEL);
+		if (!kbuf_set)
+			goto out_free_buf_sets;
+		kbuf_set->kaddr = dma_alloc_attrs(&psys->adev->dev,
+						  IPU_PSYS_BUF_SET_MAX_SIZE,
+						  &kbuf_set->dma_addr,
+						  GFP_KERNEL,
+						  0);
+		if (!kbuf_set->kaddr) {
+			kfree(kbuf_set);
+			goto out_free_buf_sets;
+		}
+		kbuf_set->size = IPU_PSYS_BUF_SET_MAX_SIZE;
+		list_add(&kbuf_set->list, &sched->buf_sets);
+	}
+
+	return 0;
+
+out_free_buf_sets:
+	list_for_each_entry_safe(kbuf_set, kbuf_set_tmp,
+				 &sched->buf_sets, list) {
+		dma_free_attrs(&psys->adev->dev,
+			       kbuf_set->size, kbuf_set->kaddr,
+			       kbuf_set->dma_addr, 0);
+		list_del(&kbuf_set->list);
+		kfree(kbuf_set);
+	}
+	mutex_destroy(&sched->bs_mutex);
+
+	return -ENOMEM;
+}
+
+int ipu_psys_fh_deinit(struct ipu_psys_fh *fh)
+{
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_psys_ppg *kppg, *kppg0;
+	struct ipu_psys_kcmd *kcmd, *kcmd0;
+	struct ipu_psys_buffer_set *kbuf_set, *kbuf_set0;
+	struct ipu_psys_scheduler *sched = &fh->sched;
+	struct ipu_psys_resource_pool *rpr;
+	struct ipu_psys_resource_alloc *alloc;
+	u8 id;
+
+	mutex_lock(&fh->mutex);
+	if (!list_empty(&sched->ppgs)) {
+		list_for_each_entry_safe(kppg, kppg0, &sched->ppgs, list) {
+			unsigned long flags;
+
+			mutex_lock(&kppg->mutex);
+			if (!(kppg->state &
+			      (PPG_STATE_STOPPED |
+			       PPG_STATE_STOPPING))) {
+				struct ipu_psys_kcmd tmp = {
+					.kpg = kppg->kpg,
+				};
+
+				rpr = &psys->resource_pool_running;
+				alloc = &kppg->kpg->resource_alloc;
+				id = ipu_fw_psys_ppg_get_base_queue_id(&tmp);
+				ipu_psys_ppg_stop(kppg);
+				ipu_psys_free_resources(alloc, rpr);
+				ipu_psys_free_cmd_queue_resource(rpr, id);
+				dev_dbg(&psys->adev->dev,
+				    "s_change:%s %p %d -> %d\n", __func__,
+				    kppg, kppg->state, PPG_STATE_STOPPED);
+				kppg->state = PPG_STATE_STOPPED;
+				if (psys->power_gating != PSYS_POWER_GATED)
+					pm_runtime_put(&psys->adev->dev);
+			}
+			list_del(&kppg->list);
+			mutex_unlock(&kppg->mutex);
+
+			list_for_each_entry_safe(kcmd, kcmd0,
+						 &kppg->kcmds_new_list, list) {
+				kcmd->pg_user = NULL;
+				mutex_unlock(&fh->mutex);
+				ipu_psys_kcmd_free(kcmd);
+				mutex_lock(&fh->mutex);
+			}
+
+			list_for_each_entry_safe(kcmd, kcmd0,
+						 &kppg->kcmds_processing_list,
+						 list) {
+				kcmd->pg_user = NULL;
+				mutex_unlock(&fh->mutex);
+				ipu_psys_kcmd_free(kcmd);
+				mutex_lock(&fh->mutex);
+			}
+
+			list_for_each_entry_safe(kcmd, kcmd0,
+						 &kppg->kcmds_finished_list,
+						 list) {
+				kcmd->pg_user = NULL;
+				mutex_unlock(&fh->mutex);
+				ipu_psys_kcmd_free(kcmd);
+				mutex_lock(&fh->mutex);
+			}
+
+			spin_lock_irqsave(&psys->pgs_lock, flags);
+			kppg->kpg->pg_size = 0;
+			spin_unlock_irqrestore(&psys->pgs_lock, flags);
+
+			mutex_destroy(&kppg->mutex);
+			kfree(kppg->manifest);
+			kfree(kppg);
+		}
+	}
+	mutex_unlock(&fh->mutex);
+
+	mutex_lock(&sched->bs_mutex);
+	list_for_each_entry_safe(kbuf_set, kbuf_set0, &sched->buf_sets, list) {
+		dma_free_attrs(&psys->adev->dev,
+			       kbuf_set->size, kbuf_set->kaddr,
+			       kbuf_set->dma_addr, 0);
+		list_del(&kbuf_set->list);
+		kfree(kbuf_set);
+	}
+	mutex_unlock(&sched->bs_mutex);
+	mutex_destroy(&sched->bs_mutex);
+
+	return 0;
+}
+
+struct ipu_psys_kcmd *ipu_get_completed_kcmd(struct ipu_psys_fh *fh)
+{
+	struct ipu_psys_scheduler *sched = &fh->sched;
+	struct ipu_psys_kcmd *kcmd;
+	struct ipu_psys_ppg *kppg;
+
+	mutex_lock(&fh->mutex);
+	if (list_empty(&sched->ppgs)) {
+		mutex_unlock(&fh->mutex);
+		return NULL;
+	}
+
+	list_for_each_entry(kppg, &sched->ppgs, list) {
+		mutex_lock(&kppg->mutex);
+		if (list_empty(&kppg->kcmds_finished_list)) {
+			mutex_unlock(&kppg->mutex);
+			continue;
+		}
+
+		kcmd = list_first_entry(&kppg->kcmds_finished_list,
+					struct ipu_psys_kcmd, list);
+		mutex_unlock(&fh->mutex);
+		mutex_unlock(&kppg->mutex);
+		dev_dbg(&fh->psys->adev->dev,
+			"get completed kcmd 0x%p\n", kcmd);
+		return kcmd;
+	}
+	mutex_unlock(&fh->mutex);
+
+	return NULL;
+}
+
+long ipu_ioctl_dqevent(struct ipu_psys_event *event,
+		       struct ipu_psys_fh *fh, unsigned int f_flags)
+{
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_psys_kcmd *kcmd = NULL;
+	int rval;
+
+	dev_dbg(&psys->adev->dev, "IOC_DQEVENT\n");
+
+	if (!(f_flags & O_NONBLOCK)) {
+		rval = wait_event_interruptible(fh->wait,
+						(kcmd =
+						 ipu_get_completed_kcmd(fh)));
+		if (rval == -ERESTARTSYS)
+			return rval;
+	}
+
+	if (!kcmd) {
+		kcmd = ipu_get_completed_kcmd(fh);
+		if (!kcmd)
+			return -ENODATA;
+	}
+
+	*event = kcmd->ev;
+	ipu_psys_kcmd_free(kcmd);
+
+	return 0;
+}
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6-psys-gpc.c b/drivers/media/pci/intel/ipu6/ipu6-psys-gpc.c
--- a/drivers/media/pci/intel/ipu6/ipu6-psys-gpc.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6-psys-gpc.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,210 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2020 Intel Corporation
+
+#ifdef CONFIG_DEBUG_FS
+#include <linux/debugfs.h>
+#include <linux/pm_runtime.h>
+
+#include "ipu-psys.h"
+#include "ipu-platform-regs.h"
+
+/*
+ * GPC (Gerneral Performance Counters)
+ */
+#define IPU_PSYS_GPC_NUM 16
+
+#ifndef CONFIG_PM
+#define pm_runtime_get_sync(d)			0
+#define pm_runtime_put(d)			0
+#endif
+
+struct ipu_psys_gpc {
+	bool enable;
+	unsigned int route;
+	unsigned int source;
+	unsigned int sense;
+	unsigned int gpcindex;
+	void *prit;
+};
+
+struct ipu_psys_gpcs {
+	bool gpc_enable;
+	struct ipu_psys_gpc gpc[IPU_PSYS_GPC_NUM];
+	void *prit;
+};
+
+static int ipu6_psys_gpc_global_enable_get(void *data, u64 *val)
+{
+	struct ipu_psys_gpcs *psys_gpcs = data;
+	struct ipu_psys *psys = psys_gpcs->prit;
+
+	mutex_lock(&psys->mutex);
+
+	*val = psys_gpcs->gpc_enable;
+
+	mutex_unlock(&psys->mutex);
+	return 0;
+}
+
+static int ipu6_psys_gpc_global_enable_set(void *data, u64 val)
+{
+	struct ipu_psys_gpcs *psys_gpcs = data;
+	struct ipu_psys *psys = psys_gpcs->prit;
+	void __iomem *base;
+	int idx, res;
+
+	if (val != 0 && val != 1)
+		return -EINVAL;
+
+	if (!psys || !psys->pdata || !psys->pdata->base)
+		return -EINVAL;
+
+	mutex_lock(&psys->mutex);
+
+	base = psys->pdata->base + IPU_GPC_BASE;
+
+	res = pm_runtime_get_sync(&psys->adev->dev);
+	if (res < 0) {
+		pm_runtime_put(&psys->adev->dev);
+		mutex_unlock(&psys->mutex);
+		return res;
+	}
+
+	if (val == 0) {
+		writel(0x0, base + IPU_GPREG_TRACE_TIMER_RST);
+		writel(0x0, base + IPU_CDC_MMU_GPC_OVERALL_ENABLE);
+		writel(0xffff, base + IPU_CDC_MMU_GPC_SOFT_RESET);
+		psys_gpcs->gpc_enable = false;
+		for (idx = 0; idx < IPU_PSYS_GPC_NUM; idx++) {
+			psys_gpcs->gpc[idx].enable = 0;
+			psys_gpcs->gpc[idx].sense = 0;
+			psys_gpcs->gpc[idx].route = 0;
+			psys_gpcs->gpc[idx].source = 0;
+		}
+		pm_runtime_mark_last_busy(&psys->adev->dev);
+		pm_runtime_put_autosuspend(&psys->adev->dev);
+	} else {
+		/* Set gpc reg and start all gpc here.
+		 * RST free running local timer.
+		 */
+		writel(0x0, base + IPU_GPREG_TRACE_TIMER_RST);
+		writel(0x1, base + IPU_GPREG_TRACE_TIMER_RST);
+
+		for (idx = 0; idx < IPU_PSYS_GPC_NUM; idx++) {
+			/* Enable */
+			writel(psys_gpcs->gpc[idx].enable,
+			       base + IPU_CDC_MMU_GPC_ENABLE0 + 4 * idx);
+			/* Setting (route/source/sense) */
+			writel((psys_gpcs->gpc[idx].sense
+					<< IPU_GPC_SENSE_OFFSET)
+				+ (psys_gpcs->gpc[idx].route
+					<< IPU_GPC_ROUTE_OFFSET)
+				+ (psys_gpcs->gpc[idx].source
+					<< IPU_GPC_SOURCE_OFFSET),
+				base + IPU_CDC_MMU_GPC_CNT_SEL0 + 4 * idx);
+		}
+
+		/* Soft reset and Overall Enable. */
+		writel(0x0, base + IPU_CDC_MMU_GPC_OVERALL_ENABLE);
+		writel(0xffff, base + IPU_CDC_MMU_GPC_SOFT_RESET);
+		writel(0x1, base + IPU_CDC_MMU_GPC_OVERALL_ENABLE);
+
+		psys_gpcs->gpc_enable = true;
+	}
+
+	mutex_unlock(&psys->mutex);
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(psys_gpc_globe_enable_fops,
+			ipu6_psys_gpc_global_enable_get,
+			ipu6_psys_gpc_global_enable_set, "%llu\n");
+
+static int ipu6_psys_gpc_count_get(void *data, u64 *val)
+{
+	struct ipu_psys_gpc *psys_gpc = data;
+	struct ipu_psys *psys = psys_gpc->prit;
+	void __iomem *base;
+	int res;
+
+	if (!psys || !psys->pdata || !psys->pdata->base)
+		return -EINVAL;
+
+	mutex_lock(&psys->mutex);
+
+	base = psys->pdata->base + IPU_GPC_BASE;
+
+	res = pm_runtime_get_sync(&psys->adev->dev);
+	if (res < 0) {
+		pm_runtime_put(&psys->adev->dev);
+		mutex_unlock(&psys->mutex);
+		return res;
+	}
+
+	*val = readl(base + IPU_CDC_MMU_GPC_VALUE0 + 4 * psys_gpc->gpcindex);
+
+	mutex_unlock(&psys->mutex);
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(psys_gpc_count_fops,
+			ipu6_psys_gpc_count_get,
+			NULL, "%llu\n");
+
+int ipu_psys_gpc_init_debugfs(struct ipu_psys *psys)
+{
+	struct dentry *gpcdir;
+	struct dentry *dir;
+	struct dentry *file;
+	int idx;
+	char gpcname[10];
+	struct ipu_psys_gpcs *psys_gpcs;
+
+	psys_gpcs = devm_kzalloc(&psys->dev, sizeof(*psys_gpcs), GFP_KERNEL);
+	if (!psys_gpcs)
+		return -ENOMEM;
+
+	gpcdir = debugfs_create_dir("gpc", psys->debugfsdir);
+	if (IS_ERR(gpcdir))
+		return -ENOMEM;
+
+	psys_gpcs->prit = psys;
+	file = debugfs_create_file("enable", 0600, gpcdir, psys_gpcs,
+				   &psys_gpc_globe_enable_fops);
+	if (IS_ERR(file))
+		goto err;
+
+	for (idx = 0; idx < IPU_PSYS_GPC_NUM; idx++) {
+		sprintf(gpcname, "gpc%d", idx);
+		dir = debugfs_create_dir(gpcname, gpcdir);
+		if (IS_ERR(dir))
+			goto err;
+
+		debugfs_create_bool("enable", 0600, dir,
+					&psys_gpcs->gpc[idx].enable);
+
+		debugfs_create_u32("source", 0600, dir,
+				   &psys_gpcs->gpc[idx].source);
+
+		debugfs_create_u32("route", 0600, dir,
+				   &psys_gpcs->gpc[idx].route);
+
+		debugfs_create_u32("sense", 0600, dir,
+				   &psys_gpcs->gpc[idx].sense);
+
+		psys_gpcs->gpc[idx].gpcindex = idx;
+		psys_gpcs->gpc[idx].prit = psys;
+		file = debugfs_create_file("count", 0400, dir,
+					   &psys_gpcs->gpc[idx],
+					   &psys_gpc_count_fops);
+		if (IS_ERR(file))
+			goto err;
+	}
+
+	return 0;
+
+err:
+	debugfs_remove_recursive(gpcdir);
+	return -ENOMEM;
+}
+#endif
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6se-fw-resources.c b/drivers/media/pci/intel/ipu6/ipu6se-fw-resources.c
--- a/drivers/media/pci/intel/ipu6/ipu6se-fw-resources.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6se-fw-resources.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,194 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2015 - 2019 Intel Corporation
+
+#include <linux/err.h>
+#include <linux/string.h>
+
+#include "ipu-psys.h"
+#include "ipu-fw-psys.h"
+#include "ipu6se-platform-resources.h"
+
+/* resources table */
+
+/*
+ * Cell types by cell IDs
+ */
+/* resources table */
+
+/*
+ * Cell types by cell IDs
+ */
+const u8 ipu6se_fw_psys_cell_types[IPU6SE_FW_PSYS_N_CELL_ID] = {
+	IPU6SE_FW_PSYS_SP_CTRL_TYPE_ID,
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_ICA_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_LSC_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_DPC_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_B2B_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_BNLM_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_DM_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_R2I_SIE_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_R2I_DS_A_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_R2I_DS_B_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_AWB_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_AE_ID */
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID, /* IPU6SE_FW_PSYS_ISA_AF_ID*/
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID  /* PAF */
+};
+
+const u16 ipu6se_fw_num_dev_channels[IPU6SE_FW_PSYS_N_DEV_CHN_ID] = {
+	IPU6SE_FW_PSYS_DEV_CHN_DMA_EXT0_MAX_SIZE,
+	IPU6SE_FW_PSYS_DEV_CHN_DMA_EXT1_READ_MAX_SIZE,
+	IPU6SE_FW_PSYS_DEV_CHN_DMA_EXT1_WRITE_MAX_SIZE,
+	IPU6SE_FW_PSYS_DEV_CHN_DMA_ISA_MAX_SIZE,
+};
+
+const u16 ipu6se_fw_psys_mem_size[IPU6SE_FW_PSYS_N_MEM_ID] = {
+	IPU6SE_FW_PSYS_TRANSFER_VMEM0_MAX_SIZE,
+	IPU6SE_FW_PSYS_LB_VMEM_MAX_SIZE,
+	IPU6SE_FW_PSYS_DMEM0_MAX_SIZE,
+	IPU6SE_FW_PSYS_DMEM1_MAX_SIZE
+};
+
+const u16 ipu6se_fw_psys_dfms[IPU6SE_FW_PSYS_N_DEV_DFM_ID] = {
+	IPU6SE_FW_PSYS_DEV_DFM_ISL_FULL_PORT_ID_MAX_SIZE,
+	IPU6SE_FW_PSYS_DEV_DFM_ISL_EMPTY_PORT_ID_MAX_SIZE
+};
+
+const u8
+ipu6se_fw_psys_c_mem[IPU6SE_FW_PSYS_N_CELL_ID][IPU6SE_FW_PSYS_N_MEM_TYPE_ID] = {
+	{ /* IPU6SE_FW_PSYS_SP0_ID */
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_DMEM0_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_ICA_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_LSC_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_DPC_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_B2B_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+
+	{ /* IPU6SE_FW_PSYS_ISA_BNLM_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_DM_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_R2I_SIE_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_R2I_DS_A_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_R2I_DS_B_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_AWB_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_AE_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_AF_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	},
+	{ /* IPU6SE_FW_PSYS_ISA_PAF_ID */
+		IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID,
+		IPU6SE_FW_PSYS_LB_VMEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+		IPU6SE_FW_PSYS_N_MEM_ID,
+	}
+};
+
+static const struct ipu_fw_resource_definitions ipu6se_defs = {
+	.cells = ipu6se_fw_psys_cell_types,
+	.num_cells = IPU6SE_FW_PSYS_N_CELL_ID,
+	.num_cells_type = IPU6SE_FW_PSYS_N_CELL_TYPE_ID,
+
+	.dev_channels = ipu6se_fw_num_dev_channels,
+	.num_dev_channels = IPU6SE_FW_PSYS_N_DEV_CHN_ID,
+
+	.num_ext_mem_types = IPU6SE_FW_PSYS_N_DATA_MEM_TYPE_ID,
+	.num_ext_mem_ids = IPU6SE_FW_PSYS_N_MEM_ID,
+	.ext_mem_ids = ipu6se_fw_psys_mem_size,
+
+	.num_dfm_ids = IPU6SE_FW_PSYS_N_DEV_DFM_ID,
+
+	.dfms = ipu6se_fw_psys_dfms,
+
+	.cell_mem_row = IPU6SE_FW_PSYS_N_MEM_TYPE_ID,
+	.cell_mem = &ipu6se_fw_psys_c_mem[0][0],
+};
+
+const struct ipu_fw_resource_definitions *ipu6se_res_defs = &ipu6se_defs;
diff -ruN a/drivers/media/pci/intel/ipu6/ipu6se-platform-resources.h b/drivers/media/pci/intel/ipu6/ipu6se-platform-resources.h
--- a/drivers/media/pci/intel/ipu6/ipu6se-platform-resources.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu6se-platform-resources.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,103 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2018 - 2020 Intel Corporation */
+
+#ifndef IPU6SE_PLATFORM_RESOURCES_H
+#define IPU6SE_PLATFORM_RESOURCES_H
+
+#include <linux/kernel.h>
+#include <linux/device.h>
+#include "ipu-platform-resources.h"
+
+#define	IPU6SE_FW_PSYS_N_PADDING_UINT8_IN_PROCESS_EXT_STRUCT		1
+
+enum {
+	IPU6SE_FW_PSYS_CMD_QUEUE_COMMAND_ID = 0,
+	IPU6SE_FW_PSYS_CMD_QUEUE_DEVICE_ID,
+	IPU6SE_FW_PSYS_CMD_QUEUE_PPG0_COMMAND_ID,
+	IPU6SE_FW_PSYS_CMD_QUEUE_PPG1_COMMAND_ID,
+	IPU6SE_FW_PSYS_CMD_QUEUE_PPG2_COMMAND_ID,
+	IPU6SE_FW_PSYS_CMD_QUEUE_PPG3_COMMAND_ID,
+	IPU6SE_FW_PSYS_CMD_QUEUE_PPG4_COMMAND_ID,
+	IPU6SE_FW_PSYS_CMD_QUEUE_PPG5_COMMAND_ID,
+	IPU6SE_FW_PSYS_N_PSYS_CMD_QUEUE_ID
+};
+
+enum {
+	IPU6SE_FW_PSYS_TRANSFER_VMEM0_TYPE_ID = 0,
+	IPU6SE_FW_PSYS_LB_VMEM_TYPE_ID,
+	IPU6SE_FW_PSYS_DMEM_TYPE_ID,
+	IPU6SE_FW_PSYS_VMEM_TYPE_ID,
+	IPU6SE_FW_PSYS_BAMEM_TYPE_ID,
+	IPU6SE_FW_PSYS_PMEM_TYPE_ID,
+	IPU6SE_FW_PSYS_N_MEM_TYPE_ID
+};
+
+enum ipu6se_mem_id {
+	IPU6SE_FW_PSYS_TRANSFER_VMEM0_ID = 0,	/* TRANSFER VMEM 0 */
+	IPU6SE_FW_PSYS_LB_VMEM_ID,	/* LB VMEM */
+	IPU6SE_FW_PSYS_DMEM0_ID,	/* SPC0 Dmem */
+	IPU6SE_FW_PSYS_DMEM1_ID,	/* SPP0 Dmem */
+	IPU6SE_FW_PSYS_N_MEM_ID
+};
+
+enum {
+	IPU6SE_FW_PSYS_DEV_CHN_DMA_EXT0_ID = 0,
+	IPU6SE_FW_PSYS_DEV_CHN_DMA_EXT1_READ_ID,
+	IPU6SE_FW_PSYS_DEV_CHN_DMA_EXT1_WRITE_ID,
+	IPU6SE_FW_PSYS_DEV_CHN_DMA_ISA_ID,
+	IPU6SE_FW_PSYS_N_DEV_CHN_ID
+};
+
+enum {
+	IPU6SE_FW_PSYS_SP_CTRL_TYPE_ID = 0,
+	IPU6SE_FW_PSYS_SP_SERVER_TYPE_ID,
+	IPU6SE_FW_PSYS_ACC_ISA_TYPE_ID,
+	IPU6SE_FW_PSYS_N_CELL_TYPE_ID
+};
+
+enum {
+	IPU6SE_FW_PSYS_SP0_ID = 0,
+	IPU6SE_FW_PSYS_ISA_ICA_ID,
+	IPU6SE_FW_PSYS_ISA_LSC_ID,
+	IPU6SE_FW_PSYS_ISA_DPC_ID,
+	IPU6SE_FW_PSYS_ISA_B2B_ID,
+	IPU6SE_FW_PSYS_ISA_BNLM_ID,
+	IPU6SE_FW_PSYS_ISA_DM_ID,
+	IPU6SE_FW_PSYS_ISA_R2I_SIE_ID,
+	IPU6SE_FW_PSYS_ISA_R2I_DS_A_ID,
+	IPU6SE_FW_PSYS_ISA_R2I_DS_B_ID,
+	IPU6SE_FW_PSYS_ISA_AWB_ID,
+	IPU6SE_FW_PSYS_ISA_AE_ID,
+	IPU6SE_FW_PSYS_ISA_AF_ID,
+	IPU6SE_FW_PSYS_ISA_PAF_ID,
+	IPU6SE_FW_PSYS_N_CELL_ID
+};
+
+enum {
+	IPU6SE_FW_PSYS_DEV_DFM_ISL_FULL_PORT_ID = 0,
+	IPU6SE_FW_PSYS_DEV_DFM_ISL_EMPTY_PORT_ID,
+};
+
+/* Excluding PMEM */
+#define IPU6SE_FW_PSYS_N_DATA_MEM_TYPE_ID (IPU6SE_FW_PSYS_N_MEM_TYPE_ID - 1)
+#define IPU6SE_FW_PSYS_N_DEV_DFM_ID	\
+	(IPU6SE_FW_PSYS_DEV_DFM_ISL_EMPTY_PORT_ID + 1)
+#define IPU6SE_FW_PSYS_VMEM0_MAX_SIZE		0x0800
+/* Transfer VMEM0 words, ref HAS Transfer*/
+#define IPU6SE_FW_PSYS_TRANSFER_VMEM0_MAX_SIZE	0x0800
+#define IPU6SE_FW_PSYS_LB_VMEM_MAX_SIZE		0x0400	/* LB VMEM words */
+#define IPU6SE_FW_PSYS_DMEM0_MAX_SIZE		0x4000
+#define IPU6SE_FW_PSYS_DMEM1_MAX_SIZE		0x1000
+
+#define IPU6SE_FW_PSYS_DEV_CHN_DMA_EXT0_MAX_SIZE		22
+#define IPU6SE_FW_PSYS_DEV_CHN_DMA_EXT1_READ_MAX_SIZE	22
+#define IPU6SE_FW_PSYS_DEV_CHN_DMA_EXT1_WRITE_MAX_SIZE	22
+#define IPU6SE_FW_PSYS_DEV_CHN_DMA_IPFD_MAX_SIZE		0
+#define IPU6SE_FW_PSYS_DEV_CHN_DMA_ISA_MAX_SIZE		2
+
+#define IPU6SE_FW_PSYS_DEV_DFM_ISL_FULL_PORT_ID_MAX_SIZE		32
+#define IPU6SE_FW_PSYS_DEV_DFM_LB_FULL_PORT_ID_MAX_SIZE		32
+#define IPU6SE_FW_PSYS_DEV_DFM_ISL_EMPTY_PORT_ID_MAX_SIZE		32
+#define IPU6SE_FW_PSYS_DEV_DFM_LB_EMPTY_PORT_ID_MAX_SIZE		32
+
+#endif /* IPU6SE_PLATFORM_RESOURCES_H */
diff -ruN a/drivers/media/pci/intel/ipu6/ipu-fw-resources.c b/drivers/media/pci/intel/ipu6/ipu-fw-resources.c
--- a/drivers/media/pci/intel/ipu6/ipu-fw-resources.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu-fw-resources.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,103 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2015 - 2019 Intel Corporation
+
+#include <linux/err.h>
+#include <linux/string.h>
+
+#include "ipu-psys.h"
+#include "ipu-fw-psys.h"
+#include "ipu6-platform-resources.h"
+#include "ipu6se-platform-resources.h"
+
+/********** Generic resource handling **********/
+
+/*
+ * Extension library gives byte offsets to its internal structures.
+ * use those offsets to update fields. Without extension lib access
+ * structures directly.
+ */
+const struct ipu6_psys_hw_res_variant *var = &hw_var;
+
+int ipu_fw_psys_set_process_cell_id(struct ipu_fw_psys_process *ptr, u8 index,
+				    u8 value)
+{
+	struct ipu_fw_psys_process_group *parent =
+		(struct ipu_fw_psys_process_group *)((char *)ptr +
+						      ptr->parent_offset);
+
+	ptr->cells[index] = value;
+	parent->resource_bitmap |= 1 << value;
+
+	return 0;
+}
+
+u8 ipu_fw_psys_get_process_cell_id(struct ipu_fw_psys_process *ptr, u8 index)
+{
+	return ptr->cells[index];
+}
+
+int ipu_fw_psys_clear_process_cell(struct ipu_fw_psys_process *ptr)
+{
+	struct ipu_fw_psys_process_group *parent;
+	u8 cell_id = ipu_fw_psys_get_process_cell_id(ptr, 0);
+	int retval = -1;
+	u8 value;
+
+	parent = (struct ipu_fw_psys_process_group *)((char *)ptr +
+						       ptr->parent_offset);
+
+	value = var->cell_num;
+	if ((1 << cell_id) != 0 &&
+	    ((1 << cell_id) & parent->resource_bitmap)) {
+		ipu_fw_psys_set_process_cell_id(ptr, 0, value);
+		parent->resource_bitmap &= ~(1 << cell_id);
+		retval = 0;
+	}
+
+	return retval;
+}
+
+int ipu_fw_psys_set_proc_dev_chn(struct ipu_fw_psys_process *ptr, u16 offset,
+				 u16 value)
+{
+	if (var->set_proc_dev_chn)
+		return var->set_proc_dev_chn(ptr, offset, value);
+
+	WARN(1, "ipu6 psys res var is not initialised correctly.");
+	return 0;
+}
+
+int ipu_fw_psys_set_proc_dfm_bitmap(struct ipu_fw_psys_process *ptr,
+				    u16 id, u32 bitmap,
+				    u32 active_bitmap)
+{
+	if (var->set_proc_dfm_bitmap)
+		return var->set_proc_dfm_bitmap(ptr, id, bitmap,
+						active_bitmap);
+
+	WARN(1, "ipu6 psys res var is not initialised correctly.");
+	return 0;
+}
+
+int ipu_fw_psys_set_process_ext_mem(struct ipu_fw_psys_process *ptr,
+				    u16 type_id, u16 mem_id, u16 offset)
+{
+	if (var->set_proc_ext_mem)
+		return var->set_proc_ext_mem(ptr, type_id, mem_id, offset);
+
+	WARN(1, "ipu6 psys res var is not initialised correctly.");
+	return 0;
+}
+
+int ipu_fw_psys_get_program_manifest_by_process(
+	struct ipu_fw_generic_program_manifest *gen_pm,
+	const struct ipu_fw_psys_program_group_manifest *pg_manifest,
+	struct ipu_fw_psys_process *process)
+{
+	if (var->get_pgm_by_proc)
+		return var->get_pgm_by_proc(gen_pm, pg_manifest, process);
+
+	WARN(1, "ipu6 psys res var is not initialised correctly.");
+	return 0;
+}
+
diff -ruN a/drivers/media/pci/intel/ipu6/ipu-platform-buttress-regs.h b/drivers/media/pci/intel/ipu6/ipu-platform-buttress-regs.h
--- a/drivers/media/pci/intel/ipu6/ipu-platform-buttress-regs.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu-platform-buttress-regs.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,317 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2020 Intel Corporation */
+
+#ifndef IPU_PLATFORM_BUTTRESS_REGS_H
+#define IPU_PLATFORM_BUTTRESS_REGS_H
+
+/* IS_WORKPOINT_REQ */
+#define IPU_BUTTRESS_REG_IS_FREQ_CTL		0x34
+/* PS_WORKPOINT_REQ */
+#define IPU_BUTTRESS_REG_PS_FREQ_CTL		0x38
+
+#define IPU_BUTTRESS_IS_FREQ_RATIO_MASK	0xff
+#define IPU_BUTTRESS_PS_FREQ_RATIO_MASK	0xff
+
+#define IPU_IS_FREQ_MAX		533
+#define IPU_IS_FREQ_MIN		200
+#define IPU_PS_FREQ_MAX		450
+#define IPU_IS_FREQ_RATIO_BASE		25
+#define IPU_PS_FREQ_RATIO_BASE		25
+#define IPU_BUTTRESS_IS_FREQ_CTL_DIVISOR_MASK	0xff
+#define IPU_BUTTRESS_PS_FREQ_CTL_DIVISOR_MASK	0xff
+
+/* should be tuned for real silicon */
+#define IPU_IS_FREQ_CTL_DEFAULT_RATIO		0x08
+#define IPU6SE_IS_FREQ_CTL_DEFAULT_RATIO	0x0a
+#define IPU_PS_FREQ_CTL_DEFAULT_RATIO		0x10
+
+#define IPU_IS_FREQ_CTL_DEFAULT_QOS_FLOOR_RATIO	0x10
+#define IPU_PS_FREQ_CTL_DEFAULT_QOS_FLOOR_RATIO	0x0708
+
+#define IPU_BUTTRESS_PWR_STATE_IS_PWR_SHIFT	3
+#define IPU_BUTTRESS_PWR_STATE_IS_PWR_MASK	\
+	(0x3 << IPU_BUTTRESS_PWR_STATE_IS_PWR_SHIFT)
+
+#define IPU_BUTTRESS_PWR_STATE_PS_PWR_SHIFT	6
+#define IPU_BUTTRESS_PWR_STATE_PS_PWR_MASK	\
+	(0x3 << IPU_BUTTRESS_PWR_STATE_PS_PWR_SHIFT)
+
+#define IPU_BUTTRESS_PWR_STATE_DN_DONE		0x0
+#define IPU_BUTTRESS_PWR_STATE_UP_PROCESS	0x1
+#define IPU_BUTTRESS_PWR_STATE_DN_PROCESS	0x2
+#define IPU_BUTTRESS_PWR_STATE_UP_DONE		0x3
+
+#define IPU_BUTTRESS_REG_FPGA_SUPPORT_0	0x270
+#define IPU_BUTTRESS_REG_FPGA_SUPPORT_1	0x274
+#define IPU_BUTTRESS_REG_FPGA_SUPPORT_2	0x278
+#define IPU_BUTTRESS_REG_FPGA_SUPPORT_3	0x27c
+#define IPU_BUTTRESS_REG_FPGA_SUPPORT_4	0x280
+#define IPU_BUTTRESS_REG_FPGA_SUPPORT_5	0x284
+#define IPU_BUTTRESS_REG_FPGA_SUPPORT_6	0x288
+#define IPU_BUTTRESS_REG_FPGA_SUPPORT_7	0x28c
+
+#define BUTTRESS_REG_WDT			0x8
+#define BUTTRESS_REG_BTRS_CTRL			0xc
+#define BUTTRESS_REG_BTRS_CTRL_STALL_MODE_VC0	BIT(0)
+#define BUTTRESS_REG_BTRS_CTRL_STALL_MODE_VC1	BIT(1)
+
+#define BUTTRESS_REG_FW_RESET_CTL	0x30
+#define BUTTRESS_FW_RESET_CTL_START	BIT(0)
+#define BUTTRESS_FW_RESET_CTL_DONE	BIT(1)
+
+#define BUTTRESS_REG_IS_FREQ_CTL	0x34
+
+#define BUTTRESS_IS_FREQ_CTL_DIVISOR_MASK	0xf
+
+#define BUTTRESS_REG_PS_FREQ_CTL	0x38
+
+#define BUTTRESS_PS_FREQ_CTL_RATIO_MASK		0xff
+
+#define BUTTRESS_FREQ_CTL_START		BIT(31)
+#define BUTTRESS_FREQ_CTL_START_SHIFT		31
+#define BUTTRESS_FREQ_CTL_QOS_FLOOR_SHIFT	8
+#define BUTTRESS_FREQ_CTL_ICCMAX_LEVEL		(GENMASK(19, 16))
+#define BUTTRESS_FREQ_CTL_QOS_FLOOR_MASK	(0xff << 8)
+
+#define BUTTRESS_REG_PWR_STATE	0x5c
+
+#define BUTTRESS_PWR_STATE_IS_PWR_SHIFT	3
+#define BUTTRESS_PWR_STATE_IS_PWR_MASK	(0x3 << 3)
+
+#define BUTTRESS_PWR_STATE_PS_PWR_SHIFT	6
+#define BUTTRESS_PWR_STATE_PS_PWR_MASK	(0x3 << 6)
+
+#define BUTTRESS_PWR_STATE_RESET		0x0
+#define BUTTRESS_PWR_STATE_PWR_ON_DONE		0x1
+#define BUTTRESS_PWR_STATE_PWR_RDY		0x3
+#define BUTTRESS_PWR_STATE_PWR_IDLE		0x4
+
+#define BUTTRESS_PWR_STATE_HH_STATUS_SHIFT	11
+#define BUTTRESS_PWR_STATE_HH_STATUS_MASK	(0x3 << 11)
+
+enum {
+	BUTTRESS_PWR_STATE_HH_STATE_IDLE,
+	BUTTRESS_PWR_STATE_HH_STATE_IN_PRGS,
+	BUTTRESS_PWR_STATE_HH_STATE_DONE,
+	BUTTRESS_PWR_STATE_HH_STATE_ERR,
+};
+
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_SHIFT	19
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_MASK	(0xf << 19)
+
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_IDLE			0x0
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_WAIT_4_PLL_CMP		0x1
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_WAIT_4_CLKACK		0x2
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_WAIT_4_PG_ACK		0x3
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_RST_ASSRT_CYCLES		0x4
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_STOP_CLK_CYCLES1		0x5
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_STOP_CLK_CYCLES2		0x6
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_RST_DEASSRT_CYCLES	0x7
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_WAIT_4_FUSE_WR_CMP	0x8
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_BRK_POINT			0x9
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_IS_RDY			0xa
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_HALT_HALTED		0xb
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_RST_DURATION_CNT3		0xc
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_WAIT_4_CLKACK_PD		0xd
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_PD_BRK_POINT		0xe
+#define BUTTRESS_PWR_STATE_IS_PWR_FSM_WAIT_4_PD_PG_ACK0		0xf
+
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_SHIFT	24
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_MASK	(0x1f << 24)
+
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_IDLE			0x0
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_PU_PLL_IP_RDY	0x1
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_RO_PRE_CNT_EXH	0x2
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_PU_VGI_PWRGOOD	0x3
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_RO_POST_CNT_EXH	0x4
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WR_PLL_RATIO		0x5
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_PU_PLL_CMP		0x6
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_PU_CLKACK		0x7
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_RST_ASSRT_CYCLES		0x8
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_STOP_CLK_CYCLES1		0x9
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_STOP_CLK_CYCLES2		0xa
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_RST_DEASSRT_CYCLES	0xb
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_PU_BRK_PNT		0xc
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_FUSE_ACCPT		0xd
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_PS_PWR_UP			0xf
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_4_HALTED		0x10
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_RESET_CNT3		0x11
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_PD_CLKACK		0x12
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_PD_OFF_IND		0x13
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_DVFS_PH4		0x14
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_DVFS_PLL_CMP		0x15
+#define BUTTRESS_PWR_STATE_PS_PWR_FSM_WAIT_DVFS_CLKACK		0x16
+
+#define BUTTRESS_REG_SECURITY_CTL	0x300
+
+#define BUTTRESS_SECURITY_CTL_FW_SECURE_MODE	BIT(16)
+#define BUTTRESS_SECURITY_CTL_FW_SETUP_SHIFT		0
+#define BUTTRESS_SECURITY_CTL_FW_SETUP_MASK		0x1f
+
+#define BUTTRESS_SECURITY_CTL_FW_SETUP_DONE		0x1
+#define BUTTRESS_SECURITY_CTL_AUTH_DONE			0x2
+#define BUTTRESS_SECURITY_CTL_AUTH_FAILED			0x8
+
+#define BUTTRESS_REG_SENSOR_FREQ_CTL	0x16c
+
+#define BUTTRESS_SENSOR_FREQ_CTL_OSC_OUT_FREQ_DEFAULT(i) \
+					(0x1b << ((i) * 10))
+#define BUTTRESS_SENSOR_FREQ_CTL_OSC_OUT_FREQ_SHIFT(i)	((i) * 10)
+#define BUTTRESS_SENSOR_FREQ_CTL_OSC_OUT_FREQ_MASK(i) \
+					(0x1ff << ((i) * 10))
+
+#define BUTTRESS_SENSOR_CLK_FREQ_6P75MHZ	0x176
+#define BUTTRESS_SENSOR_CLK_FREQ_8MHZ		0x164
+#define BUTTRESS_SENSOR_CLK_FREQ_9P6MHZ		0x2
+#define BUTTRESS_SENSOR_CLK_FREQ_12MHZ		0x1b2
+#define BUTTRESS_SENSOR_CLK_FREQ_13P6MHZ	0x1ac
+#define BUTTRESS_SENSOR_CLK_FREQ_14P4MHZ	0x1cc
+#define BUTTRESS_SENSOR_CLK_FREQ_15P8MHZ	0x1a6
+#define BUTTRESS_SENSOR_CLK_FREQ_16P2MHZ	0xca
+#define BUTTRESS_SENSOR_CLK_FREQ_17P3MHZ	0x12e
+#define BUTTRESS_SENSOR_CLK_FREQ_18P6MHZ	0x1c0
+#define BUTTRESS_SENSOR_CLK_FREQ_19P2MHZ	0x0
+#define BUTTRESS_SENSOR_CLK_FREQ_24MHZ		0xb2
+#define BUTTRESS_SENSOR_CLK_FREQ_26MHZ		0xae
+#define BUTTRESS_SENSOR_CLK_FREQ_27MHZ		0x196
+
+#define BUTTRESS_SENSOR_FREQ_CTL_LJPLL_FB_RATIO_MASK		0xff
+#define BUTTRESS_SENSOR_FREQ_CTL_SEL_MIPICLK_A_SHIFT		8
+#define BUTTRESS_SENSOR_FREQ_CTL_SEL_MIPICLK_A_MASK		(0x2 << 8)
+#define BUTTRESS_SENSOR_FREQ_CTL_SEL_MIPICLK_C_SHIFT		10
+#define BUTTRESS_SENSOR_FREQ_CTL_SEL_MIPICLK_C_MASK		(0x2 << 10)
+#define BUTTRESS_SENSOR_FREQ_CTL_LJPLL_FORCE_OFF_SHIFT		12
+#define BUTTRESS_SENSOR_FREQ_CTL_LJPLL_REF_RATIO_SHIFT		14
+#define BUTTRESS_SENSOR_FREQ_CTL_LJPLL_REF_RATIO_MASK		(0x2 << 14)
+#define BUTTRESS_SENSOR_FREQ_CTL_LJPLL_PVD_RATIO_SHIFT		16
+#define BUTTRESS_SENSOR_FREQ_CTL_LJPLL_PVD_RATIO_MASK		(0x2 << 16)
+#define BUTTRESS_SENSOR_FREQ_CTL_LJPLL_OUTPUT_RATIO_SHIFT	18
+#define BUTTRESS_SENSOR_FREQ_CTL_LJPLL_OUTPUT_RATIO_MASK	(0x2 << 18)
+#define BUTTRESS_SENSOR_FREQ_CTL_START_SHIFT			31
+
+#define BUTTRESS_REG_SENSOR_CLK_CTL	0x170
+
+/* 0 <= i <= 2 */
+#define BUTTRESS_SENSOR_CLK_CTL_OSC_CLK_OUT_EN_SHIFT(i)		((i) * 2)
+#define BUTTRESS_SENSOR_CLK_CTL_OSC_CLK_OUT_SEL_SHIFT(i)	((i) * 2 + 1)
+
+#define BUTTRESS_REG_FW_SOURCE_BASE_LO	0x78
+#define BUTTRESS_REG_FW_SOURCE_BASE_HI	0x7C
+#define BUTTRESS_REG_FW_SOURCE_SIZE	0x80
+
+#define BUTTRESS_REG_ISR_STATUS		0x90
+#define BUTTRESS_REG_ISR_ENABLED_STATUS	0x94
+#define BUTTRESS_REG_ISR_ENABLE		0x98
+#define BUTTRESS_REG_ISR_CLEAR		0x9C
+
+#define BUTTRESS_ISR_IS_IRQ			BIT(0)
+#define BUTTRESS_ISR_PS_IRQ			BIT(1)
+#define BUTTRESS_ISR_IPC_EXEC_DONE_BY_CSE	BIT(2)
+#define BUTTRESS_ISR_IPC_EXEC_DONE_BY_ISH	BIT(3)
+#define BUTTRESS_ISR_IPC_FROM_CSE_IS_WAITING	BIT(4)
+#define BUTTRESS_ISR_IPC_FROM_ISH_IS_WAITING	BIT(5)
+#define BUTTRESS_ISR_CSE_CSR_SET		BIT(6)
+#define BUTTRESS_ISR_ISH_CSR_SET		BIT(7)
+#define BUTTRESS_ISR_SPURIOUS_CMP		BIT(8)
+#define BUTTRESS_ISR_WATCHDOG_EXPIRED		BIT(9)
+#define BUTTRESS_ISR_PUNIT_2_IUNIT_IRQ		BIT(10)
+#define BUTTRESS_ISR_SAI_VIOLATION		BIT(11)
+#define BUTTRESS_ISR_HW_ASSERTION		BIT(12)
+
+#define BUTTRESS_REG_IU2CSEDB0	0x100
+
+#define BUTTRESS_IU2CSEDB0_BUSY		BIT(31)
+#define BUTTRESS_IU2CSEDB0_SHORT_FORMAT_SHIFT	27
+#define BUTTRESS_IU2CSEDB0_CLIENT_ID_SHIFT	10
+#define BUTTRESS_IU2CSEDB0_IPC_CLIENT_ID_VAL	2
+
+#define BUTTRESS_REG_IU2CSEDATA0	0x104
+
+#define BUTTRESS_IU2CSEDATA0_IPC_BOOT_LOAD		1
+#define BUTTRESS_IU2CSEDATA0_IPC_AUTH_RUN		2
+#define BUTTRESS_IU2CSEDATA0_IPC_AUTH_REPLACE		3
+#define BUTTRESS_IU2CSEDATA0_IPC_UPDATE_SECURE_TOUCH	16
+
+#define BUTTRESS_CSE2IUDATA0_IPC_BOOT_LOAD_DONE			1
+#define BUTTRESS_CSE2IUDATA0_IPC_AUTH_RUN_DONE			2
+#define BUTTRESS_CSE2IUDATA0_IPC_AUTH_REPLACE_DONE		4
+#define BUTTRESS_CSE2IUDATA0_IPC_UPDATE_SECURE_TOUCH_DONE	16
+
+#define BUTTRESS_REG_IU2CSECSR		0x108
+
+#define BUTTRESS_IU2CSECSR_IPC_PEER_COMP_ACTIONS_RST_PHASE1		BIT(0)
+#define BUTTRESS_IU2CSECSR_IPC_PEER_COMP_ACTIONS_RST_PHASE2		BIT(1)
+#define BUTTRESS_IU2CSECSR_IPC_PEER_QUERIED_IP_COMP_ACTIONS_RST_PHASE	BIT(2)
+#define BUTTRESS_IU2CSECSR_IPC_PEER_ASSERTED_REG_VALID_REQ		BIT(3)
+#define BUTTRESS_IU2CSECSR_IPC_PEER_ACKED_REG_VALID			BIT(4)
+#define BUTTRESS_IU2CSECSR_IPC_PEER_DEASSERTED_REG_VALID_REQ		BIT(5)
+
+#define BUTTRESS_REG_CSE2IUDB0		0x304
+#define BUTTRESS_REG_CSE2IUCSR		0x30C
+#define BUTTRESS_REG_CSE2IUDATA0	0x308
+
+/* 0x20 == NACK, 0xf == unknown command */
+#define BUTTRESS_CSE2IUDATA0_IPC_NACK      0xf20
+#define BUTTRESS_CSE2IUDATA0_IPC_NACK_MASK 0xffff
+
+#define BUTTRESS_REG_ISH2IUCSR		0x50
+#define BUTTRESS_REG_ISH2IUDB0		0x54
+#define BUTTRESS_REG_ISH2IUDATA0	0x58
+
+#define BUTTRESS_REG_IU2ISHDB0		0x10C
+#define BUTTRESS_REG_IU2ISHDATA0	0x110
+#define BUTTRESS_REG_IU2ISHDATA1	0x114
+#define BUTTRESS_REG_IU2ISHCSR		0x118
+
+#define BUTTRESS_REG_ISH_START_DETECT		0x198
+#define BUTTRESS_REG_ISH_START_DETECT_MASK	0x19C
+
+#define BUTTRESS_REG_FABRIC_CMD	0x88
+
+#define BUTTRESS_FABRIC_CMD_START_TSC_SYNC	BIT(0)
+#define BUTTRESS_FABRIC_CMD_IS_DRAIN		BIT(4)
+
+#define BUTTRESS_REG_TSW_CTL		0x120
+#define BUTTRESS_TSW_CTL_SOFT_RESET	BIT(8)
+
+#define BUTTRESS_REG_TSC_LO	0x164
+#define BUTTRESS_REG_TSC_HI	0x168
+
+#define BUTTRESS_REG_CSI2_PORT_CONFIG_AB		0x200
+#define BUTTRESS_CSI2_PORT_CONFIG_AB_MUX_MASK		0x1f
+#define BUTTRESS_CSI2_PORT_CONFIG_AB_COMBO_SHIFT_B0	16
+
+#define BUTTRESS_REG_PS_FREQ_CAPABILITIES			0xf7498
+
+#define BUTTRESS_PS_FREQ_CAPABILITIES_LAST_RESOLVED_RATIO_SHIFT	24
+#define BUTTRESS_PS_FREQ_CAPABILITIES_LAST_RESOLVED_RATIO_MASK	(0xff << 24)
+#define BUTTRESS_PS_FREQ_CAPABILITIES_MAX_RATIO_SHIFT		16
+#define BUTTRESS_PS_FREQ_CAPABILITIES_MAX_RATIO_MASK		(0xff << 16)
+#define BUTTRESS_PS_FREQ_CAPABILITIES_EFFICIENT_RATIO_SHIFT	8
+#define BUTTRESS_PS_FREQ_CAPABILITIES_EFFICIENT_RATIO_MASK	(0xff << 8)
+#define BUTTRESS_PS_FREQ_CAPABILITIES_MIN_RATIO_SHIFT		0
+#define BUTTRESS_PS_FREQ_CAPABILITIES_MIN_RATIO_MASK		(0xff)
+
+#define BUTTRESS_IRQS		(BUTTRESS_ISR_IPC_FROM_CSE_IS_WAITING |	\
+				 BUTTRESS_ISR_IPC_EXEC_DONE_BY_CSE |	\
+				 BUTTRESS_ISR_IS_IRQ |			\
+				 BUTTRESS_ISR_PS_IRQ)
+
+#define IPU6SE_ISYS_PHY_0_BASE		0x10000
+
+/* only use BB0, BB2, BB4, and BB6 on PHY0 */
+#define IPU6SE_ISYS_PHY_BB_NUM		4
+
+/* offset from PHY base */
+#define PHY_CSI_CFG			0xc0
+#define PHY_CSI_RCOMP_CONTROL		0xc8
+#define PHY_CSI_BSCAN_EXCLUDE		0xd8
+
+#define PHY_CPHY_DLL_OVRD(x)		(0x100 + 0x100 * (x))
+#define PHY_DPHY_DLL_OVRD(x)		(0x14c + 0x100 * (x))
+#define PHY_CPHY_RX_CONTROL1(x)		(0x110 + 0x100 * (x))
+#define PHY_CPHY_RX_CONTROL2(x)		(0x114 + 0x100 * (x))
+#define PHY_DPHY_CFG(x)			(0x148 + 0x100 * (x))
+#define PHY_BB_AFE_CONFIG(x)		(0x174 + 0x100 * (x))
+
+#endif /* IPU_PLATFORM_BUTTRESS_REGS_H */
diff -ruN a/drivers/media/pci/intel/ipu6/ipu-platform.h b/drivers/media/pci/intel/ipu6/ipu-platform.h
--- a/drivers/media/pci/intel/ipu6/ipu-platform.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu-platform.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,34 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_PLATFORM_H
+#define IPU_PLATFORM_H
+
+#define IPU_NAME			"intel-ipu6"
+
+#define IPU6SE_FIRMWARE_NAME		"intel/ipu6se_fw.bin"
+#define IPU6EP_FIRMWARE_NAME		"intel/ipu6ep_fw.bin"
+#define IPU6_FIRMWARE_NAME		"intel/ipu6_fw.bin"
+
+/*
+ * The following definitions are encoded to the media_device's model field so
+ * that the software components which uses IPU driver can get the hw stepping
+ * information.
+ */
+#define IPU_MEDIA_DEV_MODEL_NAME		"ipu6"
+
+#define IPU6SE_ISYS_NUM_STREAMS          IPU6SE_NONSECURE_STREAM_ID_MAX
+#define IPU6_ISYS_NUM_STREAMS            IPU6_NONSECURE_STREAM_ID_MAX
+
+/* declearations, definitions in ipu6.c */
+extern struct ipu_isys_internal_pdata isys_ipdata;
+extern struct ipu_psys_internal_pdata psys_ipdata;
+extern const struct ipu_buttress_ctrl isys_buttress_ctrl;
+extern const struct ipu_buttress_ctrl psys_buttress_ctrl;
+
+/* definitions in ipu6-isys.c */
+extern struct ipu_trace_block isys_trace_blocks[];
+/* definitions in ipu6-psys.c */
+extern struct ipu_trace_block psys_trace_blocks[];
+
+#endif
diff -ruN a/drivers/media/pci/intel/ipu6/ipu-platform-isys-csi2-reg.h b/drivers/media/pci/intel/ipu6/ipu-platform-isys-csi2-reg.h
--- a/drivers/media/pci/intel/ipu6/ipu-platform-isys-csi2-reg.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu-platform-isys-csi2-reg.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,277 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2020 Intel Corporation */
+
+#ifndef IPU_PLATFORM_ISYS_CSI2_REG_H
+#define IPU_PLATFORM_ISYS_CSI2_REG_H
+
+#define CSI_REG_BASE			0x220000
+#define CSI_REG_BASE_PORT(id)		((id) * 0x1000)
+
+#define IPU_CSI_PORT_A_ADDR_OFFSET	\
+		(CSI_REG_BASE + CSI_REG_BASE_PORT(0))
+#define IPU_CSI_PORT_B_ADDR_OFFSET	\
+		(CSI_REG_BASE + CSI_REG_BASE_PORT(1))
+#define IPU_CSI_PORT_C_ADDR_OFFSET	\
+		(CSI_REG_BASE + CSI_REG_BASE_PORT(2))
+#define IPU_CSI_PORT_D_ADDR_OFFSET	\
+		(CSI_REG_BASE + CSI_REG_BASE_PORT(3))
+#define IPU_CSI_PORT_E_ADDR_OFFSET	\
+		(CSI_REG_BASE + CSI_REG_BASE_PORT(4))
+#define IPU_CSI_PORT_F_ADDR_OFFSET	\
+		(CSI_REG_BASE + CSI_REG_BASE_PORT(5))
+#define IPU_CSI_PORT_G_ADDR_OFFSET	\
+		(CSI_REG_BASE + CSI_REG_BASE_PORT(6))
+#define IPU_CSI_PORT_H_ADDR_OFFSET	\
+		(CSI_REG_BASE + CSI_REG_BASE_PORT(7))
+
+/* CSI Port Genral Purpose Registers */
+#define CSI_REG_PORT_GPREG_SRST                 0x0
+#define CSI_REG_PORT_GPREG_CSI2_SLV_REG_SRST    0x4
+#define CSI_REG_PORT_GPREG_CSI2_PORT_CONTROL    0x8
+
+/*
+ * Port IRQs mapping events:
+ * IRQ0 - CSI_FE event
+ * IRQ1 - CSI_SYNC
+ * IRQ2 - S2M_SIDS0TO7
+ * IRQ3 - S2M_SIDS8TO15
+ */
+#define CSI_PORT_REG_BASE_IRQ_CSI               0x80
+#define CSI_PORT_REG_BASE_IRQ_CSI_SYNC          0xA0
+#define CSI_PORT_REG_BASE_IRQ_S2M_SIDS0TOS7     0xC0
+#define CSI_PORT_REG_BASE_IRQ_S2M_SIDS8TOS15    0xE0
+
+#define CSI_PORT_REG_BASE_IRQ_EDGE_OFFSET	0x0
+#define CSI_PORT_REG_BASE_IRQ_MASK_OFFSET	0x4
+#define CSI_PORT_REG_BASE_IRQ_STATUS_OFFSET	0x8
+#define CSI_PORT_REG_BASE_IRQ_CLEAR_OFFSET	0xc
+#define CSI_PORT_REG_BASE_IRQ_ENABLE_OFFSET	0x10
+#define CSI_PORT_REG_BASE_IRQ_LEVEL_NOT_PULSE_OFFSET	0x14
+
+#define IPU6SE_CSI_RX_ERROR_IRQ_MASK		0x7ffff
+#define IPU6_CSI_RX_ERROR_IRQ_MASK		0xfffff
+
+#define CSI_RX_NUM_ERRORS_IN_IRQ		20
+#define CSI_RX_NUM_IRQ				32
+
+#define IPU_CSI_RX_IRQ_FS_VC		1
+#define IPU_CSI_RX_IRQ_FE_VC		2
+
+/* PPI2CSI */
+#define CSI_REG_PPI2CSI_ENABLE                  0x200
+#define CSI_REG_PPI2CSI_CONFIG_PPI_INTF         0x204
+#define PPI_INTF_CONFIG_NOF_ENABLED_DATALANES_SHIFT	3
+#define PPI_INTF_CONFIG_RX_AUTO_CLKGATING_SHIFT		5
+#define CSI_REG_PPI2CSI_CONFIG_CSI_FEATURE      0x208
+
+enum CSI_PPI2CSI_CTRL {
+	CSI_PPI2CSI_DISABLE = 0,
+	CSI_PPI2CSI_ENABLE = 1,
+};
+
+/* CSI_FE */
+#define CSI_REG_CSI_FE_ENABLE                   0x280
+#define CSI_REG_CSI_FE_MODE                     0x284
+#define CSI_REG_CSI_FE_MUX_CTRL                 0x288
+#define CSI_REG_CSI_FE_SYNC_CNTR_SEL            0x290
+
+enum CSI_FE_ENABLE_TYPE {
+	CSI_FE_DISABLE = 0,
+	CSI_FE_ENABLE = 1,
+};
+
+enum CSI_FE_MODE_TYPE {
+	CSI_FE_DPHY_MODE = 0,
+	CSI_FE_CPHY_MODE = 1,
+};
+
+enum CSI_FE_INPUT_SELECTOR {
+	CSI_SENSOR_INPUT = 0,
+	CSI_MIPIGEN_INPUT = 1,
+};
+
+enum CSI_FE_SYNC_CNTR_SEL_TYPE {
+	CSI_CNTR_SENSOR_LINE_ID = (1 << 0),
+	CSI_CNTR_INT_LINE_PKT_ID = ~CSI_CNTR_SENSOR_LINE_ID,
+	CSI_CNTR_SENSOR_FRAME_ID = (1 << 1),
+	CSI_CNTR_INT_FRAME_PKT_ID = ~CSI_CNTR_SENSOR_FRAME_ID,
+};
+
+/* MIPI_PKT_GEN */
+#define CSI_REG_PIXGEN_COM_BASE_OFFSET		0x300
+
+#define IPU_CSI_PORT_A_PIXGEN_ADDR_OFFSET	\
+	(CSI_REG_BASE + CSI_REG_BASE_PORT(0) + CSI_REG_PIXGEN_COM_BASE_OFFSET)
+#define IPU_CSI_PORT_B_PIXGEN_ADDR_OFFSET	\
+	(CSI_REG_BASE + CSI_REG_BASE_PORT(1) + CSI_REG_PIXGEN_COM_BASE_OFFSET)
+#define IPU_CSI_PORT_C_PIXGEN_ADDR_OFFSET	\
+	(CSI_REG_BASE + CSI_REG_BASE_PORT(2) + CSI_REG_PIXGEN_COM_BASE_OFFSET)
+#define IPU_CSI_PORT_D_PIXGEN_ADDR_OFFSET	\
+	(CSI_REG_BASE + CSI_REG_BASE_PORT(3) + CSI_REG_PIXGEN_COM_BASE_OFFSET)
+#define IPU_CSI_PORT_E_PIXGEN_ADDR_OFFSET	\
+	(CSI_REG_BASE + CSI_REG_BASE_PORT(4) + CSI_REG_PIXGEN_COM_BASE_OFFSET)
+#define IPU_CSI_PORT_F_PIXGEN_ADDR_OFFSET	\
+	(CSI_REG_BASE + CSI_REG_BASE_PORT(5) + CSI_REG_PIXGEN_COM_BASE_OFFSET)
+#define IPU_CSI_PORT_G_PIXGEN_ADDR_OFFSET	\
+	(CSI_REG_BASE + CSI_REG_BASE_PORT(6) + CSI_REG_PIXGEN_COM_BASE_OFFSET)
+#define IPU_CSI_PORT_H_PIXGEN_ADDR_OFFSET	\
+	(CSI_REG_BASE + CSI_REG_BASE_PORT(7) + CSI_REG_PIXGEN_COM_BASE_OFFSET)
+
+#define CSI_REG_PIXGEN_COM_ENABLE_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x300)
+#define CSI_REG_PIXGEN_COM_DTYPE_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x304)
+#define CSI_REG_PIXGEN_COM_VTYPE_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x308)
+#define CSI_REG_PIXGEN_COM_VCHAN_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x30C)
+#define CSI_REG_PIXGEN_COM_WCOUNT_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x310)
+/* PRBS */
+#define CSI_REG_PIXGEN_PRBS_RSTVAL_REG0_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x314)
+#define CSI_REG_PIXGEN_PRBS_RSTVAL_REG1_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x318)
+/* SYNC_GENERATOR_CONFIG */
+#define CSI_REG_PIXGEN_SYNG_FREE_RUN_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x31C)
+#define CSI_REG_PIXGEN_SYNG_PAUSE_REG_IDX(id)		\
+	(CSI_REG_BASE_PORT(id) + 0x320)
+#define CSI_REG_PIXGEN_SYNG_NOF_FRAME_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x324)
+#define CSI_REG_PIXGEN_SYNG_NOF_PIXEL_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x328)
+#define CSI_REG_PIXGEN_SYNG_NOF_LINE_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x32C)
+#define CSI_REG_PIXGEN_SYNG_HBLANK_CYC_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x330)
+#define CSI_REG_PIXGEN_SYNG_VBLANK_CYC_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x334)
+#define CSI_REG_PIXGEN_SYNG_STAT_HCNT_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x338)
+#define CSI_REG_PIXGEN_SYNG_STAT_VCNT_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x33C)
+#define CSI_REG_PIXGEN_SYNG_STAT_FCNT_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x340)
+#define CSI_REG_PIXGEN_SYNG_STAT_DONE_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x344)
+/* TPG */
+#define CSI_REG_PIXGEN_TPG_MODE_REG_IDX(id)		\
+	(CSI_REG_BASE_PORT(id) + 0x348)
+/* TPG: mask_cfg */
+#define CSI_REG_PIXGEN_TPG_HCNT_MASK_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x34C)
+#define CSI_REG_PIXGEN_TPG_VCNT_MASK_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x350)
+#define CSI_REG_PIXGEN_TPG_XYCNT_MASK_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x354)
+/* TPG: delta_cfg */
+#define CSI_REG_PIXGEN_TPG_HCNT_DELTA_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x358)
+#define CSI_REG_PIXGEN_TPG_VCNT_DELTA_REG_IDX(id)	\
+	(CSI_REG_BASE_PORT(id) + 0x35C)
+/* TPG: color_cfg */
+#define CSI_REG_PIXGEN_TPG_R1_REG_IDX(id)	(CSI_REG_BASE_PORT(id) + 0x360)
+#define CSI_REG_PIXGEN_TPG_G1_REG_IDX(id)	(CSI_REG_BASE_PORT(id) + 0x364)
+#define CSI_REG_PIXGEN_TPG_B1_REG_IDX(id)	(CSI_REG_BASE_PORT(id) + 0x368)
+#define CSI_REG_PIXGEN_TPG_R2_REG_IDX(id)	(CSI_REG_BASE_PORT(id) + 0x36C)
+#define CSI_REG_PIXGEN_TPG_G2_REG_IDX(id)	(CSI_REG_BASE_PORT(id) + 0x370)
+#define CSI_REG_PIXGEN_TPG_B2_REG_IDX(id)	(CSI_REG_BASE_PORT(id) + 0x374)
+
+#define CSI_REG_PIXGEN_PRBS_RSTVAL_REG0	CSI_REG_PIXGEN_PRBS_RSTVAL_REG0_IDX(0)
+#define CSI_REG_PIXGEN_PRBS_RSTVAL_REG1	CSI_REG_PIXGEN_PRBS_RSTVAL_REG1_IDX(0)
+#define CSI_REG_PIXGEN_COM_ENABLE_REG	CSI_REG_PIXGEN_COM_ENABLE_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_MODE_REG	CSI_REG_PIXGEN_TPG_MODE_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_R1_REG	CSI_REG_PIXGEN_TPG_R1_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_G1_REG	CSI_REG_PIXGEN_TPG_G1_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_B1_REG	CSI_REG_PIXGEN_TPG_B1_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_R2_REG	CSI_REG_PIXGEN_TPG_R2_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_G2_REG	CSI_REG_PIXGEN_TPG_G2_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_B2_REG	CSI_REG_PIXGEN_TPG_B2_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_HCNT_MASK_REG CSI_REG_PIXGEN_TPG_HCNT_MASK_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_VCNT_MASK_REG CSI_REG_PIXGEN_TPG_VCNT_MASK_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_XYCNT_MASK_REG	\
+	CSI_REG_PIXGEN_TPG_XYCNT_MASK_REG_IDX(0)
+
+#define CSI_REG_PIXGEN_SYNG_NOF_FRAME_REG	\
+	CSI_REG_PIXGEN_SYNG_NOF_FRAME_REG_IDX(0)
+#define CSI_REG_PIXGEN_SYNG_NOF_LINE_REG	\
+	CSI_REG_PIXGEN_SYNG_NOF_LINE_REG_IDX(0)
+#define CSI_REG_PIXGEN_SYNG_HBLANK_CYC_REG	\
+	CSI_REG_PIXGEN_SYNG_HBLANK_CYC_REG_IDX(0)
+#define CSI_REG_PIXGEN_SYNG_VBLANK_CYC_REG	\
+	CSI_REG_PIXGEN_SYNG_VBLANK_CYC_REG_IDX(0)
+#define CSI_REG_PIXGEN_SYNG_NOF_PIXEL_REG	\
+	CSI_REG_PIXGEN_SYNG_NOF_PIXEL_REG_IDX(0)
+#define CSI_REG_PIXGEN_COM_WCOUNT_REG	CSI_REG_PIXGEN_COM_WCOUNT_REG_IDX(0)
+#define CSI_REG_PIXGEN_COM_DTYPE_REG	CSI_REG_PIXGEN_COM_DTYPE_REG_IDX(0)
+#define CSI_REG_PIXGEN_COM_VTYPE_REG	CSI_REG_PIXGEN_COM_VTYPE_REG_IDX(0)
+#define CSI_REG_PIXGEN_COM_VCHAN_REG	CSI_REG_PIXGEN_COM_VCHAN_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_HCNT_DELTA_REG	\
+	CSI_REG_PIXGEN_TPG_HCNT_DELTA_REG_IDX(0)
+#define CSI_REG_PIXGEN_TPG_VCNT_DELTA_REG	\
+	CSI_REG_PIXGEN_TPG_VCNT_DELTA_REG_IDX(0)
+
+/* CSI HUB General Purpose Registers */
+#define CSI_REG_HUB_GPREG_SRST			(CSI_REG_BASE + 0x18000)
+#define CSI_REG_HUB_GPREG_SLV_REG_SRST		(CSI_REG_BASE + 0x18004)
+#define CSI_REG_HUB_GPREG_PHY_CONTROL(id)	\
+	(CSI_REG_BASE + 0x18008 + (id) * 0x8)
+#define CSI_REG_HUB_GPREG_PHY_CONTROL_RESET		BIT(4)
+#define CSI_REG_HUB_GPREG_PHY_CONTROL_PWR_EN		BIT(0)
+#define CSI_REG_HUB_GPREG_PHY_STATUS(id)	\
+	(CSI_REG_BASE + 0x1800c + (id) * 0x8)
+#define CSI_REG_HUB_GPREG_PHY_STATUS_POWER_ACK		BIT(0)
+#define CSI_REG_HUB_GPREG_PHY_STATUS_PHY_READY		BIT(4)
+
+#define CSI_REG_HUB_DRV_ACCESS_PORT(id)	(CSI_REG_BASE + 0x18018 + (id) * 4)
+#define CSI_REG_HUB_FW_ACCESS_PORT(id)	(CSI_REG_BASE + 0x17000 + (id) * 4)
+
+enum CSI_PORT_CLK_GATING_SWITCH {
+	CSI_PORT_CLK_GATING_OFF = 0,
+	CSI_PORT_CLK_GATING_ON = 1,
+};
+
+#define CSI_REG_BASE_HUB_IRQ                        0x18200
+
+#define IPU_NUM_OF_DLANE_REG_PORT0      2
+#define IPU_NUM_OF_DLANE_REG_PORT1      4
+#define IPU_NUM_OF_DLANE_REG_PORT2      4
+#define IPU_NUM_OF_DLANE_REG_PORT3      2
+#define IPU_NUM_OF_DLANE_REG_PORT4      2
+#define IPU_NUM_OF_DLANE_REG_PORT5      4
+#define IPU_NUM_OF_DLANE_REG_PORT6      4
+#define IPU_NUM_OF_DLANE_REG_PORT7      2
+
+/* ipu6se support 2 SIPs, 2 port per SIP, 4 ports 0..3
+ * sip0 - 0, 1
+ * sip1 - 2, 3
+ * 0 and 2 support 4 data lanes, 1 and 3 support 2 data lanes
+ * all offset are base from isys base address
+ */
+
+#define CSI2_HUB_GPREG_SIP_SRST(sip)			(0x238038 + (sip) * 4)
+#define CSI2_HUB_GPREG_SIP_FB_PORT_CFG(sip)		(0x238050 + (sip) * 4)
+
+#define CSI2_HUB_GPREG_DPHY_TIMER_INCR			(0x238040)
+#define CSI2_HUB_GPREG_HPLL_FREQ			(0x238044)
+#define CSI2_HUB_GPREG_IS_CLK_RATIO			(0x238048)
+#define CSI2_HUB_GPREG_HPLL_FREQ_ISCLK_RATE_OVERRIDE	(0x23804c)
+#define CSI2_HUB_GPREG_PORT_CLKGATING_DISABLE		(0x238058)
+#define CSI2_HUB_GPREG_SIP0_CSI_RX_A_CONTROL		(0x23805c)
+#define CSI2_HUB_GPREG_SIP0_CSI_RX_B_CONTROL		(0x238088)
+#define CSI2_HUB_GPREG_SIP1_CSI_RX_A_CONTROL		(0x2380a4)
+#define CSI2_HUB_GPREG_SIP1_CSI_RX_B_CONTROL		(0x2380d0)
+
+#define CSI2_SIP_TOP_CSI_RX_BASE(sip)		(0x23805c + (sip) * 0x48)
+#define CSI2_SIP_TOP_CSI_RX_PORT_BASE_0(port)	(0x23805c + ((port) / 2) * 0x48)
+#define CSI2_SIP_TOP_CSI_RX_PORT_BASE_1(port)	(0x238088 + ((port) / 2) * 0x48)
+
+/* offset from port base */
+#define CSI2_SIP_TOP_CSI_RX_PORT_CONTROL		(0x0)
+#define CSI2_SIP_TOP_CSI_RX_DLY_CNT_TERMEN_CLANE	(0x4)
+#define CSI2_SIP_TOP_CSI_RX_DLY_CNT_SETTLE_CLANE	(0x8)
+#define CSI2_SIP_TOP_CSI_RX_DLY_CNT_TERMEN_DLANE(lane)	(0xc + (lane) * 8)
+#define CSI2_SIP_TOP_CSI_RX_DLY_CNT_SETTLE_DLANE(lane)	(0x10 + (lane) * 8)
+
+#endif /* IPU6_ISYS_CSI2_REG_H */
diff -ruN a/drivers/media/pci/intel/ipu6/ipu-platform-isys.h b/drivers/media/pci/intel/ipu6/ipu-platform-isys.h
--- a/drivers/media/pci/intel/ipu6/ipu-platform-isys.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu-platform-isys.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,26 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2020 Intel Corporation */
+
+#ifndef IPU_PLATFORM_ISYS_H
+#define IPU_PLATFORM_ISYS_H
+
+#define IPU_ISYS_ENTITY_PREFIX		"Intel IPU6"
+
+/*
+ * FW support max 16 streams
+ */
+#define IPU_ISYS_MAX_STREAMS		16
+
+#define ISYS_UNISPART_IRQS	(IPU_ISYS_UNISPART_IRQ_SW |	\
+				 IPU_ISYS_UNISPART_IRQ_CSI0 |	\
+				 IPU_ISYS_UNISPART_IRQ_CSI1)
+
+/* IPU6 ISYS compression alignment */
+#define IPU_ISYS_COMPRESSION_LINE_ALIGN		512
+#define IPU_ISYS_COMPRESSION_HEIGHT_ALIGN	1
+#define IPU_ISYS_COMPRESSION_TILE_SIZE_BYTES	512
+#define IPU_ISYS_COMPRESSION_PAGE_ALIGN		0x1000
+#define IPU_ISYS_COMPRESSION_TILE_STATUS_BITS	4
+#define IPU_ISYS_COMPRESSION_MAX		3
+
+#endif
diff -ruN a/drivers/media/pci/intel/ipu6/ipu-platform-psys.h b/drivers/media/pci/intel/ipu6/ipu-platform-psys.h
--- a/drivers/media/pci/intel/ipu6/ipu-platform-psys.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu-platform-psys.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,78 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2020 Intel Corporation */
+
+#ifndef IPU_PLATFORM_PSYS_H
+#define IPU_PLATFORM_PSYS_H
+
+#include "ipu-psys.h"
+#include <uapi/linux/ipu-psys.h>
+
+#define IPU_PSYS_BUF_SET_POOL_SIZE 8
+#define IPU_PSYS_BUF_SET_MAX_SIZE 1024
+
+struct ipu_fw_psys_buffer_set;
+
+enum ipu_psys_cmd_state {
+	KCMD_STATE_PPG_NEW,
+	KCMD_STATE_PPG_START,
+	KCMD_STATE_PPG_ENQUEUE,
+	KCMD_STATE_PPG_STOP,
+	KCMD_STATE_PPG_COMPLETE
+};
+
+struct ipu_psys_scheduler {
+	struct list_head ppgs;
+	struct mutex bs_mutex;  /* Protects buf_set field */
+	struct list_head buf_sets;
+};
+
+enum ipu_psys_ppg_state {
+	PPG_STATE_START = (1 << 0),
+	PPG_STATE_STARTING = (1 << 1),
+	PPG_STATE_STARTED = (1 << 2),
+	PPG_STATE_RUNNING = (1 << 3),
+	PPG_STATE_SUSPEND = (1 << 4),
+	PPG_STATE_SUSPENDING = (1 << 5),
+	PPG_STATE_SUSPENDED = (1 << 6),
+	PPG_STATE_RESUME = (1 << 7),
+	PPG_STATE_RESUMING = (1 << 8),
+	PPG_STATE_RESUMED = (1 << 9),
+	PPG_STATE_STOP = (1 << 10),
+	PPG_STATE_STOPPING = (1 << 11),
+	PPG_STATE_STOPPED = (1 << 12),
+};
+
+struct ipu_psys_ppg {
+	struct ipu_psys_pg *kpg;
+	struct ipu_psys_fh *fh;
+	struct list_head list;
+	struct list_head sched_list;
+	u64 token;
+	void *manifest;
+	struct mutex mutex;     /* Protects kcmd and ppg state field */
+	struct list_head kcmds_new_list;
+	struct list_head kcmds_processing_list;
+	struct list_head kcmds_finished_list;
+	enum ipu_psys_ppg_state state;
+	u32 pri_base;
+	int pri_dynamic;
+};
+
+struct ipu_psys_buffer_set {
+	struct list_head list;
+	struct ipu_fw_psys_buffer_set *buf_set;
+	size_t size;
+	size_t buf_set_size;
+	dma_addr_t dma_addr;
+	void *kaddr;
+	struct ipu_psys_kcmd *kcmd;
+};
+
+int ipu_psys_kcmd_start(struct ipu_psys *psys, struct ipu_psys_kcmd *kcmd);
+void ipu_psys_kcmd_complete(struct ipu_psys_ppg *kppg,
+			    struct ipu_psys_kcmd *kcmd,
+			    int error);
+int ipu_psys_fh_init(struct ipu_psys_fh *fh);
+int ipu_psys_fh_deinit(struct ipu_psys_fh *fh);
+
+#endif /* IPU_PLATFORM_PSYS_H */
diff -ruN a/drivers/media/pci/intel/ipu6/ipu-platform-regs.h b/drivers/media/pci/intel/ipu6/ipu-platform-regs.h
--- a/drivers/media/pci/intel/ipu6/ipu-platform-regs.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu-platform-regs.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,333 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2018 - 2020 Intel Corporation */
+
+#ifndef IPU_PLATFORM_REGS_H
+#define IPU_PLATFORM_REGS_H
+
+/*
+ * IPU6 uses uniform address within IPU, therefore all subsystem registers
+ * locates in one signle space starts from 0 but in different sctions with
+ * different addresses, the subsystem offsets are defined to 0 as the
+ * register definition will have the address offset to 0.
+ */
+#define IPU_UNIFIED_OFFSET			0
+
+#define IPU_ISYS_IOMMU0_OFFSET		0x2E0000
+#define IPU_ISYS_IOMMU1_OFFSET		0x2E0500
+#define IPU_ISYS_IOMMUI_OFFSET		0x2E0A00
+
+#define IPU_PSYS_IOMMU0_OFFSET		0x1B0000
+#define IPU_PSYS_IOMMU1_OFFSET		0x1B0700
+#define IPU_PSYS_IOMMU1R_OFFSET		0x1B0E00
+#define IPU_PSYS_IOMMUI_OFFSET		0x1B1500
+
+/* the offset from IOMMU base register */
+#define IPU_MMU_L1_STREAM_ID_REG_OFFSET	0x0c
+#define IPU_MMU_L2_STREAM_ID_REG_OFFSET	0x4c
+#define IPU_PSYS_MMU1W_L2_STREAM_ID_REG_OFFSET	0x8c
+
+#define IPU_MMU_INFO_OFFSET		0x8
+
+#define IPU_ISYS_SPC_OFFSET		0x210000
+
+#define IPU6SE_PSYS_SPC_OFFSET		0x110000
+#define IPU6_PSYS_SPC_OFFSET		0x118000
+
+#define IPU_ISYS_DMEM_OFFSET		0x200000
+#define IPU_PSYS_DMEM_OFFSET		0x100000
+
+#define IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_EDGE		0x238200
+#define IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_MASK		0x238204
+#define IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_STATUS		0x238208
+#define IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_CLEAR		0x23820c
+#define IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_ENABLE		0x238210
+#define IPU_REG_ISYS_CSI_TOP_CTRL0_IRQ_LEVEL_NOT_PULSE	0x238214
+
+#define IPU_REG_ISYS_CSI_TOP_CTRL1_IRQ_EDGE		0x238220
+#define IPU_REG_ISYS_CSI_TOP_CTRL1_IRQ_MASK		0x238224
+#define IPU_REG_ISYS_CSI_TOP_CTRL1_IRQ_STATUS		0x238228
+#define IPU_REG_ISYS_CSI_TOP_CTRL1_IRQ_CLEAR		0x23822c
+#define IPU_REG_ISYS_CSI_TOP_CTRL1_IRQ_ENABLE		0x238230
+#define IPU_REG_ISYS_CSI_TOP_CTRL1_IRQ_LEVEL_NOT_PULSE	0x238234
+
+#define IPU_REG_ISYS_UNISPART_IRQ_EDGE			0x27c000
+#define IPU_REG_ISYS_UNISPART_IRQ_MASK			0x27c004
+#define IPU_REG_ISYS_UNISPART_IRQ_STATUS		0x27c008
+#define IPU_REG_ISYS_UNISPART_IRQ_CLEAR			0x27c00c
+#define IPU_REG_ISYS_UNISPART_IRQ_ENABLE			0x27c010
+#define IPU_REG_ISYS_UNISPART_IRQ_LEVEL_NOT_PULSE	0x27c014
+#define IPU_REG_ISYS_UNISPART_SW_IRQ_REG			0x27c414
+#define IPU_REG_ISYS_UNISPART_SW_IRQ_MUX_REG		0x27c418
+#define IPU_ISYS_UNISPART_IRQ_CSI0			BIT(2)
+#define IPU_ISYS_UNISPART_IRQ_CSI1			BIT(3)
+#define IPU_ISYS_UNISPART_IRQ_SW			BIT(22)
+
+#define IPU_REG_ISYS_ISL_TOP_IRQ_EDGE			0x2b0200
+#define IPU_REG_ISYS_ISL_TOP_IRQ_MASK			0x2b0204
+#define IPU_REG_ISYS_ISL_TOP_IRQ_STATUS			0x2b0208
+#define IPU_REG_ISYS_ISL_TOP_IRQ_CLEAR			0x2b020c
+#define IPU_REG_ISYS_ISL_TOP_IRQ_ENABLE			0x2b0210
+#define IPU_REG_ISYS_ISL_TOP_IRQ_LEVEL_NOT_PULSE	0x2b0214
+
+#define IPU_REG_ISYS_CMPR_TOP_IRQ_EDGE			0x2d2100
+#define IPU_REG_ISYS_CMPR_TOP_IRQ_MASK			0x2d2104
+#define IPU_REG_ISYS_CMPR_TOP_IRQ_STATUS		0x2d2108
+#define IPU_REG_ISYS_CMPR_TOP_IRQ_CLEAR			0x2d210c
+#define IPU_REG_ISYS_CMPR_TOP_IRQ_ENABLE		0x2d2110
+#define IPU_REG_ISYS_CMPR_TOP_IRQ_LEVEL_NOT_PULSE	0x2d2114
+
+/* CDC Burst collector thresholds for isys - 3 FIFOs i = 0..2 */
+#define IPU_REG_ISYS_CDC_THRESHOLD(i)		(0x27c400 + ((i) * 4))
+
+#define IPU_ISYS_CSI_PHY_NUM				2
+#define IPU_CSI_IRQ_NUM_PER_PIPE			4
+#define IPU6SE_ISYS_CSI_PORT_NUM			4
+#define IPU6_ISYS_CSI_PORT_NUM				8
+
+#define IPU_ISYS_CSI_PORT_IRQ(irq_num)		(1 << (irq_num))
+
+/* PSYS Info bits*/
+#define IPU_REG_PSYS_INFO_SEG_CMEM_MASTER(a)	(0x2C + ((a) * 12))
+#define IPU_REG_PSYS_INFO_SEG_XMEM_MASTER(a)	(0x5C + ((a) * 12))
+
+/* PKG DIR OFFSET in IMR in secure mode */
+#define IPU_PKG_DIR_IMR_OFFSET			0x40
+
+#define IPU_ISYS_REG_SPC_STATUS_CTRL		0x0
+
+#define IPU_ISYS_SPC_STATUS_START			BIT(1)
+#define IPU_ISYS_SPC_STATUS_RUN				BIT(3)
+#define IPU_ISYS_SPC_STATUS_READY			BIT(5)
+#define IPU_ISYS_SPC_STATUS_CTRL_ICACHE_INVALIDATE	BIT(12)
+#define IPU_ISYS_SPC_STATUS_ICACHE_PREFETCH		BIT(13)
+
+#define IPU_PSYS_REG_SPC_STATUS_CTRL		0x0
+#define IPU_PSYS_REG_SPC_START_PC		0x4
+#define IPU_PSYS_REG_SPC_ICACHE_BASE		0x10
+#define IPU_REG_PSYS_INFO_SEG_0_CONFIG_ICACHE_MASTER	0x14
+
+#define IPU_PSYS_SPC_STATUS_START			BIT(1)
+#define IPU_PSYS_SPC_STATUS_RUN				BIT(3)
+#define IPU_PSYS_SPC_STATUS_READY			BIT(5)
+#define IPU_PSYS_SPC_STATUS_CTRL_ICACHE_INVALIDATE	BIT(12)
+#define IPU_PSYS_SPC_STATUS_ICACHE_PREFETCH		BIT(13)
+
+#define IPU_PSYS_REG_SPP0_STATUS_CTRL			0x20000
+
+#define IPU_INFO_ENABLE_SNOOP			BIT(0)
+#define IPU_INFO_DEC_FORCE_FLUSH		BIT(1)
+#define IPU_INFO_DEC_PASS_THRU			BIT(2)
+#define IPU_INFO_ZLW                            BIT(3)
+#define IPU_INFO_STREAM_ID_SET(id)		(((id) & 0x1F) << 4)
+#define IPU_INFO_REQUEST_DESTINATION_IOSF	BIT(9)
+#define IPU_INFO_IMR_BASE			BIT(10)
+#define IPU_INFO_IMR_DESTINED			BIT(11)
+
+#define IPU_INFO_REQUEST_DESTINATION_PRIMARY IPU_INFO_REQUEST_DESTINATION_IOSF
+
+/* Trace unit related register definitions */
+#define TRACE_REG_MAX_ISYS_OFFSET	0xfffff
+#define TRACE_REG_MAX_PSYS_OFFSET	0xfffff
+#define IPU_ISYS_OFFSET			IPU_ISYS_DMEM_OFFSET
+#define IPU_PSYS_OFFSET			IPU_PSYS_DMEM_OFFSET
+/* ISYS trace unit registers */
+/* Trace unit base offset */
+#define IPU_TRACE_REG_IS_TRACE_UNIT_BASE		0x27d000
+/* Trace monitors */
+#define IPU_TRACE_REG_IS_SP_EVQ_BASE		0x211000
+/* GPC blocks */
+#define IPU_TRACE_REG_IS_SP_GPC_BASE		0x210800
+#define IPU_TRACE_REG_IS_ISL_GPC_BASE		0x2b0a00
+#define IPU_TRACE_REG_IS_MMU_GPC_BASE		0x2e0f00
+/* each CSI2 port has a dedicated trace monitor, index 0..7 */
+#define IPU_TRACE_REG_CSI2_TM_BASE(port)	(0x220400 + 0x1000 * (port))
+
+/* Trace timers */
+#define IPU_TRACE_REG_IS_GPREG_TRACE_TIMER_RST_N		0x27c410
+#define TRACE_REG_GPREG_TRACE_TIMER_RST_OFF		BIT(0)
+
+/* SIG2CIO */
+#define IPU_TRACE_REG_CSI2_PORT_SIG2SIO_GR_BASE(port)		\
+			(0x220e00 + (port) * 0x1000)
+
+/* PSYS trace unit registers */
+/* Trace unit base offset */
+#define IPU_TRACE_REG_PS_TRACE_UNIT_BASE		0x1b4000
+/* Trace monitors */
+#define IPU_TRACE_REG_PS_SPC_EVQ_BASE			0x119000
+#define IPU_TRACE_REG_PS_SPP0_EVQ_BASE			0x139000
+
+/* GPC blocks */
+#define IPU_TRACE_REG_PS_SPC_GPC_BASE			0x118800
+#define IPU_TRACE_REG_PS_SPP0_GPC_BASE			0x138800
+#define IPU_TRACE_REG_PS_MMU_GPC_BASE			0x1b1b00
+
+/* Trace timers */
+#define IPU_TRACE_REG_PS_GPREG_TRACE_TIMER_RST_N	0x1aa714
+
+/*
+ * s2m_pixel_soc_pixel_remapping is dedicated for the enableing of the
+ * pixel s2m remp ability.Remap here  means that s2m rearange the order
+ * of the pixels in each 4 pixels group.
+ * For examle, mirroring remping means that if input's 4 first pixels
+ * are 1 2 3 4 then in output we should see 4 3 2 1 in this 4 first pixels.
+ * 0xE4 is from s2m MAS document. It means no remaping.
+ */
+#define S2M_PIXEL_SOC_PIXEL_REMAPPING_FLAG_NO_REMAPPING 0xE4
+/*
+ * csi_be_soc_pixel_remapping is for the enabling of the csi\mipi be pixel
+ * remapping feature. This remapping is exactly like the stream2mmio remapping.
+ */
+#define CSI_BE_SOC_PIXEL_REMAPPING_FLAG_NO_REMAPPING    0xE4
+
+#define IPU_REG_DMA_TOP_AB_GROUP1_BASE_ADDR         0x1AE000
+#define IPU_REG_DMA_TOP_AB_GROUP2_BASE_ADDR         0x1AF000
+#define IPU_REG_DMA_TOP_AB_RING_MIN_OFFSET(n)       (0x4 + (n) * 0xC)
+#define IPU_REG_DMA_TOP_AB_RING_MAX_OFFSET(n)       (0x8 + (n) * 0xC)
+#define IPU_REG_DMA_TOP_AB_RING_ACCESS_OFFSET(n)    (0xC + (n) * 0xC)
+
+enum ipu_device_ab_group1_target_id {
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R0_SPC_DMEM,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R1_SPC_DMEM,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R2_SPC_DMEM,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R3_SPC_STATUS_REG,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R4_SPC_MASTER_BASE_ADDR,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R5_SPC_PC_STALL,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R6_SPC_EQ,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R7_SPC_RESERVED,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R8_SPC_RESERVED,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R9_SPP0,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R10_SPP1,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R11_CENTRAL_R1,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R12_IRQ,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R13_CENTRAL_R2,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R14_DMA,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R15_DMA,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R16_GP,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R17_ZLW_INSERTER,
+	IPU_DEVICE_AB_GROUP1_TARGET_ID_R18_AB,
+};
+
+enum nci_ab_access_mode {
+	NCI_AB_ACCESS_MODE_RW,	/* read & write */
+	NCI_AB_ACCESS_MODE_RO,	/* read only */
+	NCI_AB_ACCESS_MODE_WO,	/* write only */
+	NCI_AB_ACCESS_MODE_NA	/* No access at all */
+};
+
+/*
+ * 3:0 CSI_PORT.irq_out[3:0] CSI_PORT_CTRL0 IRQ outputs (4bits)
+ * [0] CSI_PORT.IRQ_CTRL0_csi
+ * [1] CSI_PORT.IRQ_CTRL1_csi_sync
+ * [2] CSI_PORT.IRQ_CTRL2_s2m_sids0to7
+ * [3] CSI_PORT.IRQ_CTRL3_s2m_sids8to15
+ */
+#define IPU_ISYS_UNISPART_IRQ_CSI2(port)		\
+				   (0x3 << ((port) * IPU_CSI_IRQ_NUM_PER_PIPE))
+
+/* IRQ-related registers in PSYS */
+#define IPU_REG_PSYS_GPDEV_IRQ_EDGE		0x1aa200
+#define IPU_REG_PSYS_GPDEV_IRQ_MASK		0x1aa204
+#define IPU_REG_PSYS_GPDEV_IRQ_STATUS		0x1aa208
+#define IPU_REG_PSYS_GPDEV_IRQ_CLEAR		0x1aa20c
+#define IPU_REG_PSYS_GPDEV_IRQ_ENABLE		0x1aa210
+#define IPU_REG_PSYS_GPDEV_IRQ_LEVEL_NOT_PULSE	0x1aa214
+/* There are 8 FW interrupts, n = 0..7 */
+#define IPU_PSYS_GPDEV_FWIRQ0			5
+#define IPU_PSYS_GPDEV_FWIRQ1			6
+#define IPU_PSYS_GPDEV_FWIRQ2			7
+#define IPU_PSYS_GPDEV_FWIRQ3			8
+#define IPU_PSYS_GPDEV_FWIRQ4			9
+#define IPU_PSYS_GPDEV_FWIRQ5			10
+#define IPU_PSYS_GPDEV_FWIRQ6			11
+#define IPU_PSYS_GPDEV_FWIRQ7			12
+#define IPU_PSYS_GPDEV_IRQ_FWIRQ(n)		(1 << (n))
+#define IPU_REG_PSYS_GPDEV_FWIRQ(n)		(4 * (n) + 0x1aa100)
+
+/*
+ * ISYS GPC (Gerneral Performance Counters) Registers
+ */
+#define IPU_ISYS_GPC_BASE			0x2E0000
+#define IPU_ISYS_GPREG_TRACE_TIMER_RST		0x27C410
+enum ipu_isf_cdc_mmu_gpc_registers {
+	IPU_ISF_CDC_MMU_GPC_SOFT_RESET  = 0x0F00,
+	IPU_ISF_CDC_MMU_GPC_OVERALL_ENABLE  = 0x0F04,
+	IPU_ISF_CDC_MMU_GPC_ENABLE0  = 0x0F20,
+	IPU_ISF_CDC_MMU_GPC_VALUE0  = 0x0F60,
+	IPU_ISF_CDC_MMU_GPC_CNT_SEL0 = 0x0FA0,
+};
+
+/*
+ * GPC (Gerneral Performance Counters) Registers
+ */
+#define IPU_GPC_BASE 0x1B0000
+#define IPU_GPREG_TRACE_TIMER_RST	0x1AA714
+enum ipu_cdc_mmu_gpc_registers {
+	IPU_CDC_MMU_GPC_SOFT_RESET  = 0x1B00,
+	IPU_CDC_MMU_GPC_OVERALL_ENABLE  = 0x1B04,
+	IPU_CDC_MMU_GPC_TRACE_HEADER  = 0x1B08,
+	IPU_CDC_MMU_GPC_TRACE_ADDR  = 0x1B0C,
+	IPU_CDC_MMU_GPC_TRACE_ENABLE_NPK  = 0x1B10,
+	IPU_CDC_MMU_GPC_TRACE_ENABLE_DDR  = 0x1B14,
+	IPU_CDC_MMU_GPC_LP_CLEAR  = 0x1B18,
+	IPU_CDC_MMU_GPC_LOST_PACKET  = 0x1B1C,
+	IPU_CDC_MMU_GPC_ENABLE0  = 0x1B20,
+	IPU_CDC_MMU_GPC_VALUE0  = 0x1B60,
+	IPU_CDC_MMU_GPC_CNT_SEL0 = 0x1BA0,
+	IPU_CDC_MMU_GPC_START_SEL0 = 0x1BE0,
+	IPU_CDC_MMU_GPC_STOP_SEL0 = 0x1C20,
+	IPU_CDC_MMU_GPC_MSG_SEL0 = 0x1C60,
+	IPU_CDC_MMU_GPC_PLOAD_SEL0 = 0x1CA0,
+	IPU_CDC_MMU_GPC_IRQ_TRIGGER_VALUE0 = 0x1CE0,
+	IPU_CDC_MMU_GPC_IRQ_TIMER_SEL0 = 0x1D20,
+	IPU_CDC_MMU_GPC_IRQ_ENABLE0 = 0x1D60,
+	IPU_CDC_MMU_GPC_END = 0x1D9C
+};
+
+#define IPU_GPC_SENSE_OFFSET		7
+#define IPU_GPC_ROUTE_OFFSET		5
+#define IPU_GPC_SOURCE_OFFSET		0
+
+/*
+ * Signals monitored by GPC
+ */
+#define IPU_GPC_TRACE_TLB_MISS_MMU_LB_IDX		0
+#define IPU_GPC_TRACE_FULL_WRITE_LB_IDX			1
+#define IPU_GPC_TRACE_NOFULL_WRITE_LB_IDX		2
+#define IPU_GPC_TRACE_FULL_READ_LB_IDX			3
+#define IPU_GPC_TRACE_NOFULL_READ_LB_IDX		4
+#define IPU_GPC_TRACE_STALL_LB_IDX			5
+#define IPU_GPC_TRACE_ZLW_LB_IDX			6
+#define IPU_GPC_TRACE_TLB_MISS_MMU_HBTX_IDX		7
+#define IPU_GPC_TRACE_FULL_WRITE_HBTX_IDX		8
+#define IPU_GPC_TRACE_NOFULL_WRITE_HBTX_IDX		9
+#define IPU_GPC_TRACE_FULL_READ_HBTX_IDX		10
+#define IPU_GPC_TRACE_STALL_HBTX_IDX			11
+#define IPU_GPC_TRACE_ZLW_HBTX_IDX			12
+#define IPU_GPC_TRACE_TLB_MISS_MMU_HBFRX_IDX		13
+#define IPU_GPC_TRACE_FULL_READ_HBFRX_IDX		14
+#define IPU_GPC_TRACE_NOFULL_READ_HBFRX_IDX		15
+#define IPU_GPC_TRACE_STALL_HBFRX_IDX			16
+#define IPU_GPC_TRACE_ZLW_HBFRX_IDX			17
+#define IPU_GPC_TRACE_TLB_MISS_ICACHE_IDX		18
+#define IPU_GPC_TRACE_FULL_READ_ICACHE_IDX		19
+#define IPU_GPC_TRACE_STALL_ICACHE_IDX			20
+/*
+ * psys subdomains power request regs
+ */
+enum ipu_device_buttress_psys_domain_pos {
+	IPU_PSYS_SUBDOMAIN_ISA		= 0,
+	IPU_PSYS_SUBDOMAIN_PSA		= 1,
+	IPU_PSYS_SUBDOMAIN_BB		= 2,
+	IPU_PSYS_SUBDOMAIN_IDSP1	= 3, /* only in IPU6M */
+	IPU_PSYS_SUBDOMAIN_IDSP2	= 4, /* only in IPU6M */
+};
+
+#define IPU_PSYS_SUBDOMAINS_POWER_MASK  (BIT(IPU_PSYS_SUBDOMAIN_ISA) | \
+					 BIT(IPU_PSYS_SUBDOMAIN_PSA) | \
+					 BIT(IPU_PSYS_SUBDOMAIN_BB))
+
+#define IPU_PSYS_SUBDOMAINS_POWER_REQ                   0xa0
+#define IPU_PSYS_SUBDOMAINS_POWER_STATUS                0xa4
+
+#endif /* IPU_PLATFORM_REGS_H */
diff -ruN a/drivers/media/pci/intel/ipu6/ipu-platform-resources.h b/drivers/media/pci/intel/ipu6/ipu-platform-resources.h
--- a/drivers/media/pci/intel/ipu6/ipu-platform-resources.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu-platform-resources.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,103 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2018 - 2020 Intel Corporation */
+
+#ifndef IPU_PLATFORM_RESOURCES_COMMON_H
+#define IPU_PLATFORM_RESOURCES_COMMON_H
+
+#define IPU_FW_PSYS_N_PADDING_UINT8_IN_PROGRAM_MANIFEST                 0
+
+#define	IPU_FW_PSYS_N_PADDING_UINT8_IN_PROCESS_STRUCT			0
+#define	IPU_FW_PSYS_N_PADDING_UINT8_IN_PROCESS_GROUP_STRUCT		2
+#define	IPU_FW_PSYS_N_PADDING_UINT8_IN_PROGRAM_MANIFEST_EXT		2
+
+#define IPU_FW_PSYS_N_PADDING_UINT8_IN_TERMINAL_STRUCT			5
+
+#define IPU_FW_PSYS_N_PADDING_UINT8_IN_PARAM_TERMINAL_STRUCT		6
+
+#define	IPU_FW_PSYS_N_PADDING_UINT8_IN_DATA_TERMINAL_STRUCT		3
+
+#define	IPU_FW_PSYS_N_PADDING_UINT8_IN_FRAME_DESC_STRUCT		3
+#define IPU_FW_PSYS_N_FRAME_PLANES					6
+#define IPU_FW_PSYS_N_PADDING_UINT8_IN_FRAME_STRUCT			4
+
+#define IPU_FW_PSYS_N_PADDING_UINT8_IN_BUFFER_SET_STRUCT		1
+
+#define IPU_FW_PSYS_MAX_INPUT_DEC_RESOURCES		4
+#define IPU_FW_PSYS_MAX_OUTPUT_DEC_RESOURCES		4
+
+#define IPU_FW_PSYS_PROCESS_MAX_CELLS			1
+#define IPU_FW_PSYS_KERNEL_BITMAP_NOF_ELEMS		4
+#define IPU_FW_PSYS_RBM_NOF_ELEMS			5
+#define IPU_FW_PSYS_KBM_NOF_ELEMS			4
+
+struct ipu_fw_psys_process {
+	s16 parent_offset;
+	u8 size;
+	u8 cell_dependencies_offset;
+	u8 terminal_dependencies_offset;
+	u8 process_extension_offset;
+	u8 ID;
+	u8 program_idx;
+	u8 state;
+	u8 cells[IPU_FW_PSYS_PROCESS_MAX_CELLS];
+	u8 cell_dependency_count;
+	u8 terminal_dependency_count;
+};
+
+struct ipu_fw_psys_program_manifest {
+	u32 kernel_bitmap[IPU_FW_PSYS_KERNEL_BITMAP_NOF_ELEMS];
+	s16 parent_offset;
+	u8  program_dependency_offset;
+	u8  terminal_dependency_offset;
+	u8  size;
+	u8  program_extension_offset;
+	u8 program_type;
+	u8 ID;
+	u8 cells[IPU_FW_PSYS_PROCESS_MAX_CELLS];
+	u8 cell_type_id;
+	u8 program_dependency_count;
+	u8 terminal_dependency_count;
+};
+
+/* platform specific resource interface */
+struct ipu_psys_resource_pool;
+struct ipu_psys_resource_alloc;
+struct ipu_fw_psys_process_group;
+int ipu_psys_allocate_resources(const struct device *dev,
+				struct ipu_fw_psys_process_group *pg,
+				void *pg_manifest,
+				struct ipu_psys_resource_alloc *alloc,
+				struct ipu_psys_resource_pool *pool);
+int ipu_psys_move_resources(const struct device *dev,
+			    struct ipu_psys_resource_alloc *alloc,
+			    struct ipu_psys_resource_pool *source_pool,
+			    struct ipu_psys_resource_pool *target_pool);
+
+void ipu_psys_resource_copy(struct ipu_psys_resource_pool *src,
+			    struct ipu_psys_resource_pool *dest);
+
+int ipu_psys_try_allocate_resources(struct device *dev,
+				    struct ipu_fw_psys_process_group *pg,
+				    void *pg_manifest,
+				    struct ipu_psys_resource_pool *pool);
+
+void ipu_psys_reset_process_cell(const struct device *dev,
+				 struct ipu_fw_psys_process_group *pg,
+				 void *pg_manifest,
+				 int process_count);
+void ipu_psys_free_resources(struct ipu_psys_resource_alloc *alloc,
+			     struct ipu_psys_resource_pool *pool);
+
+int ipu_fw_psys_set_proc_dfm_bitmap(struct ipu_fw_psys_process *ptr,
+				    u16 id, u32 bitmap,
+				    u32 active_bitmap);
+
+int ipu_psys_allocate_cmd_queue_resource(struct ipu_psys_resource_pool *pool);
+void ipu_psys_free_cmd_queue_resource(struct ipu_psys_resource_pool *pool,
+				      u8 queue_id);
+
+extern const struct ipu_fw_resource_definitions *ipu6_res_defs;
+extern const struct ipu_fw_resource_definitions *ipu6se_res_defs;
+extern const struct ipu_fw_resource_definitions *ipu6ep_res_defs;
+extern struct ipu6_psys_hw_res_variant hw_var;
+#endif /* IPU_PLATFORM_RESOURCES_COMMON_H */
diff -ruN a/drivers/media/pci/intel/ipu6/ipu-resources.c b/drivers/media/pci/intel/ipu6/ipu-resources.c
--- a/drivers/media/pci/intel/ipu6/ipu-resources.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/ipu-resources.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,851 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2015 - 2020 Intel Corporation
+
+#include <linux/bitmap.h>
+#include <linux/errno.h>
+#include <linux/gfp.h>
+#include <linux/slab.h>
+#include <linux/device.h>
+
+#include <uapi/linux/ipu-psys.h>
+
+#include "ipu-fw-psys.h"
+#include "ipu-psys.h"
+
+struct ipu6_psys_hw_res_variant hw_var;
+void ipu6_psys_hw_res_variant_init(void)
+{
+	if (ipu_ver == IPU_VER_6SE) {
+		hw_var.queue_num = IPU6SE_FW_PSYS_N_PSYS_CMD_QUEUE_ID;
+		hw_var.cell_num = IPU6SE_FW_PSYS_N_CELL_ID;
+	} else if (ipu_ver == IPU_VER_6) {
+		hw_var.queue_num = IPU6_FW_PSYS_N_PSYS_CMD_QUEUE_ID;
+		hw_var.cell_num = IPU6_FW_PSYS_N_CELL_ID;
+	} else if (ipu_ver == IPU_VER_6EP) {
+		hw_var.queue_num = IPU6_FW_PSYS_N_PSYS_CMD_QUEUE_ID;
+		hw_var.cell_num = IPU6EP_FW_PSYS_N_CELL_ID;
+	} else {
+		WARN(1, "ipu6 psys res var is not initialised correctly.");
+	}
+
+	hw_var.set_proc_dev_chn = ipu6_fw_psys_set_proc_dev_chn;
+	hw_var.set_proc_dfm_bitmap = ipu6_fw_psys_set_proc_dfm_bitmap;
+	hw_var.set_proc_ext_mem = ipu6_fw_psys_set_process_ext_mem;
+	hw_var.get_pgm_by_proc =
+		ipu6_fw_psys_get_program_manifest_by_process;
+	return;
+}
+
+static const struct ipu_fw_resource_definitions *get_res(void)
+{
+	if (ipu_ver == IPU_VER_6SE)
+		return ipu6se_res_defs;
+
+	if (ipu_ver == IPU_VER_6EP)
+		return ipu6ep_res_defs;
+
+	return ipu6_res_defs;
+}
+
+static int ipu_resource_init(struct ipu_resource *res, u32 id, int elements)
+{
+	if (elements <= 0) {
+		res->bitmap = NULL;
+		return 0;
+	}
+
+	res->bitmap = bitmap_zalloc(elements, GFP_KERNEL);
+	if (!res->bitmap)
+		return -ENOMEM;
+	res->elements = elements;
+	res->id = id;
+	return 0;
+}
+
+static unsigned long
+ipu_resource_alloc_with_pos(struct ipu_resource *res, int n,
+			    int pos,
+			    struct ipu_resource_alloc *alloc,
+			    enum ipu_resource_type type)
+{
+	unsigned long p;
+
+	if (n <= 0) {
+		alloc->elements = 0;
+		return 0;
+	}
+
+	if (!res->bitmap || pos >= res->elements)
+		return (unsigned long)(-ENOSPC);
+
+	p = bitmap_find_next_zero_area(res->bitmap, res->elements, pos, n, 0);
+	alloc->resource = NULL;
+
+	if (p != pos)
+		return (unsigned long)(-ENOSPC);
+	bitmap_set(res->bitmap, p, n);
+	alloc->resource = res;
+	alloc->elements = n;
+	alloc->pos = p;
+	alloc->type = type;
+
+	return pos;
+}
+
+static unsigned long
+ipu_resource_alloc(struct ipu_resource *res, int n,
+		   struct ipu_resource_alloc *alloc,
+		   enum ipu_resource_type type)
+{
+	unsigned long p;
+
+	if (n <= 0) {
+		alloc->elements = 0;
+		return 0;
+	}
+
+	if (!res->bitmap)
+		return (unsigned long)(-ENOSPC);
+
+	p = bitmap_find_next_zero_area(res->bitmap, res->elements, 0, n, 0);
+	alloc->resource = NULL;
+
+	if (p >= res->elements)
+		return (unsigned long)(-ENOSPC);
+	bitmap_set(res->bitmap, p, n);
+	alloc->resource = res;
+	alloc->elements = n;
+	alloc->pos = p;
+	alloc->type = type;
+
+	return p;
+}
+
+static void ipu_resource_free(struct ipu_resource_alloc *alloc)
+{
+	if (alloc->elements <= 0)
+		return;
+
+	if (alloc->type == IPU_RESOURCE_DFM)
+		*alloc->resource->bitmap &= ~(unsigned long)(alloc->elements);
+	else
+		bitmap_clear(alloc->resource->bitmap, alloc->pos,
+			     alloc->elements);
+	alloc->resource = NULL;
+}
+
+static void ipu_resource_cleanup(struct ipu_resource *res)
+{
+	bitmap_free(res->bitmap);
+	res->bitmap = NULL;
+}
+
+/********** IPU PSYS-specific resource handling **********/
+int ipu_psys_resource_pool_init(struct ipu_psys_resource_pool *pool)
+{
+	int i, j, k, ret;
+	const struct ipu_fw_resource_definitions *res_defs;
+
+	res_defs = get_res();
+
+	spin_lock_init(&pool->queues_lock);
+	pool->cells = 0;
+
+	for (i = 0; i < res_defs->num_dev_channels; i++) {
+		ret = ipu_resource_init(&pool->dev_channels[i], i,
+					res_defs->dev_channels[i]);
+		if (ret)
+			goto error;
+	}
+
+	for (j = 0; j < res_defs->num_ext_mem_ids; j++) {
+		ret = ipu_resource_init(&pool->ext_memory[j], j,
+					res_defs->ext_mem_ids[j]);
+		if (ret)
+			goto memory_error;
+	}
+
+	for (k = 0; k < res_defs->num_dfm_ids; k++) {
+		ret = ipu_resource_init(&pool->dfms[k], k, res_defs->dfms[k]);
+		if (ret)
+			goto dfm_error;
+	}
+
+	spin_lock(&pool->queues_lock);
+	if (ipu_ver == IPU_VER_6SE)
+		bitmap_zero(pool->cmd_queues,
+			    IPU6SE_FW_PSYS_N_PSYS_CMD_QUEUE_ID);
+	else
+		bitmap_zero(pool->cmd_queues,
+			    IPU6_FW_PSYS_N_PSYS_CMD_QUEUE_ID);
+	spin_unlock(&pool->queues_lock);
+
+	return 0;
+
+dfm_error:
+	for (k--; k >= 0; k--)
+		ipu_resource_cleanup(&pool->dfms[k]);
+
+memory_error:
+	for (j--; j >= 0; j--)
+		ipu_resource_cleanup(&pool->ext_memory[j]);
+
+error:
+	for (i--; i >= 0; i--)
+		ipu_resource_cleanup(&pool->dev_channels[i]);
+	return ret;
+}
+
+void ipu_psys_resource_copy(struct ipu_psys_resource_pool *src,
+			    struct ipu_psys_resource_pool *dest)
+{
+	int i;
+	const struct ipu_fw_resource_definitions *res_defs;
+
+	res_defs = get_res();
+
+	dest->cells = src->cells;
+	for (i = 0; i < res_defs->num_dev_channels; i++)
+		*dest->dev_channels[i].bitmap = *src->dev_channels[i].bitmap;
+
+	for (i = 0; i < res_defs->num_ext_mem_ids; i++)
+		*dest->ext_memory[i].bitmap = *src->ext_memory[i].bitmap;
+
+	for (i = 0; i < res_defs->num_dfm_ids; i++)
+		*dest->dfms[i].bitmap = *src->dfms[i].bitmap;
+}
+
+void ipu_psys_resource_pool_cleanup(struct ipu_psys_resource_pool
+				    *pool)
+{
+	u32 i;
+	const struct ipu_fw_resource_definitions *res_defs;
+
+	res_defs = get_res();
+	for (i = 0; i < res_defs->num_dev_channels; i++)
+		ipu_resource_cleanup(&pool->dev_channels[i]);
+
+	for (i = 0; i < res_defs->num_ext_mem_ids; i++)
+		ipu_resource_cleanup(&pool->ext_memory[i]);
+
+	for (i = 0; i < res_defs->num_dfm_ids; i++)
+		ipu_resource_cleanup(&pool->dfms[i]);
+}
+
+static int __alloc_one_resrc(const struct device *dev,
+			     struct ipu_fw_psys_process *process,
+			     struct ipu_resource *resource,
+			     struct ipu_fw_generic_program_manifest *pm,
+			     u32 resource_id,
+			     struct ipu_psys_resource_alloc *alloc)
+{
+	const u16 resource_req = pm->dev_chn_size[resource_id];
+	const u16 resource_offset_req = pm->dev_chn_offset[resource_id];
+	unsigned long retl;
+
+	if (resource_req <= 0)
+		return 0;
+
+	if (alloc->resources >= IPU_MAX_RESOURCES) {
+		dev_err(dev, "out of resource handles\n");
+		return -ENOSPC;
+	}
+	if (resource_offset_req != (u16)(-1))
+		retl = ipu_resource_alloc_with_pos
+		    (resource,
+		     resource_req,
+		     resource_offset_req,
+		     &alloc->resource_alloc[alloc->resources],
+		     IPU_RESOURCE_DEV_CHN);
+	else
+		retl = ipu_resource_alloc
+		    (resource, resource_req,
+		     &alloc->resource_alloc[alloc->resources],
+		     IPU_RESOURCE_DEV_CHN);
+	if (IS_ERR_VALUE(retl)) {
+		dev_dbg(dev, "out of device channel resources\n");
+		return (int)retl;
+	}
+	alloc->resources++;
+
+	return 0;
+}
+
+static int ipu_psys_allocate_one_dfm(const struct device *dev,
+				     struct ipu_fw_psys_process *process,
+				     struct ipu_resource *resource,
+				     struct ipu_fw_generic_program_manifest *pm,
+				     u32 resource_id,
+				     struct ipu_psys_resource_alloc *alloc)
+{
+	u32 dfm_bitmap_req = pm->dfm_port_bitmap[resource_id];
+	u32 active_dfm_bitmap_req = pm->dfm_active_port_bitmap[resource_id];
+	const u8 is_relocatable = pm->is_dfm_relocatable[resource_id];
+	struct ipu_resource_alloc *alloc_resource;
+	unsigned long p = 0;
+
+	if (dfm_bitmap_req == 0)
+		return 0;
+
+	if (alloc->resources >= IPU_MAX_RESOURCES) {
+		dev_err(dev, "out of resource handles\n");
+		return -ENOSPC;
+	}
+
+	if (!resource->bitmap)
+		return -ENOSPC;
+
+	if (!is_relocatable) {
+		if (*resource->bitmap & dfm_bitmap_req) {
+			dev_warn(dev,
+				 "out of dfm resources, req 0x%x, get 0x%lx\n",
+				 dfm_bitmap_req, *resource->bitmap);
+			return -ENOSPC;
+		}
+		*resource->bitmap |= dfm_bitmap_req;
+	} else {
+		unsigned int n = hweight32(dfm_bitmap_req);
+
+		p = bitmap_find_next_zero_area(resource->bitmap,
+					       resource->elements, 0, n, 0);
+
+		if (p >= resource->elements)
+			return -ENOSPC;
+
+		bitmap_set(resource->bitmap, p, n);
+		dfm_bitmap_req = dfm_bitmap_req << p;
+		active_dfm_bitmap_req = active_dfm_bitmap_req << p;
+	}
+
+	alloc_resource = &alloc->resource_alloc[alloc->resources];
+	alloc_resource->resource = resource;
+	/* Using elements to indicate the bitmap */
+	alloc_resource->elements = dfm_bitmap_req;
+	alloc_resource->pos = p;
+	alloc_resource->type = IPU_RESOURCE_DFM;
+
+	alloc->resources++;
+
+	return 0;
+}
+
+/*
+ * ext_mem_type_id is a generic type id for memory (like DMEM, VMEM)
+ * ext_mem_bank_id is detailed type id for  memory (like DMEM0, DMEM1 etc.)
+ */
+static int __alloc_mem_resrc(const struct device *dev,
+			     struct ipu_fw_psys_process *process,
+			     struct ipu_resource *resource,
+			     struct ipu_fw_generic_program_manifest *pm,
+			     u32 ext_mem_type_id, u32 ext_mem_bank_id,
+			     struct ipu_psys_resource_alloc *alloc)
+{
+	const u16 memory_resource_req = pm->ext_mem_size[ext_mem_type_id];
+	const u16 memory_offset_req = pm->ext_mem_offset[ext_mem_type_id];
+
+	unsigned long retl;
+
+	if (memory_resource_req <= 0)
+		return 0;
+
+	if (alloc->resources >= IPU_MAX_RESOURCES) {
+		dev_err(dev, "out of resource handles\n");
+		return -ENOSPC;
+	}
+	if (memory_offset_req != (u16)(-1))
+		retl = ipu_resource_alloc_with_pos
+		    (resource,
+		     memory_resource_req, memory_offset_req,
+		     &alloc->resource_alloc[alloc->resources],
+		     IPU_RESOURCE_EXT_MEM);
+	else
+		retl = ipu_resource_alloc
+		    (resource, memory_resource_req,
+		     &alloc->resource_alloc[alloc->resources],
+		     IPU_RESOURCE_EXT_MEM);
+	if (IS_ERR_VALUE(retl)) {
+		dev_dbg(dev, "out of memory resources\n");
+		return (int)retl;
+	}
+
+	alloc->resources++;
+
+	return 0;
+}
+
+int ipu_psys_allocate_cmd_queue_resource(struct ipu_psys_resource_pool *pool)
+{
+	unsigned long p;
+	int size, start;
+
+	size = IPU6_FW_PSYS_N_PSYS_CMD_QUEUE_ID;
+	start = IPU6_FW_PSYS_CMD_QUEUE_PPG0_COMMAND_ID;
+
+	if (ipu_ver == IPU_VER_6SE) {
+		size = IPU6SE_FW_PSYS_N_PSYS_CMD_QUEUE_ID;
+		start = IPU6SE_FW_PSYS_CMD_QUEUE_PPG0_COMMAND_ID;
+	}
+
+	spin_lock(&pool->queues_lock);
+	/* find available cmd queue from ppg0_cmd_id */
+	p = bitmap_find_next_zero_area(pool->cmd_queues, size, start, 1, 0);
+
+	if (p >= size) {
+		spin_unlock(&pool->queues_lock);
+		return -ENOSPC;
+	}
+
+	bitmap_set(pool->cmd_queues, p, 1);
+	spin_unlock(&pool->queues_lock);
+
+	return p;
+}
+
+void ipu_psys_free_cmd_queue_resource(struct ipu_psys_resource_pool *pool,
+				      u8 queue_id)
+{
+	spin_lock(&pool->queues_lock);
+	bitmap_clear(pool->cmd_queues, queue_id, 1);
+	spin_unlock(&pool->queues_lock);
+}
+
+int ipu_psys_try_allocate_resources(struct device *dev,
+				    struct ipu_fw_psys_process_group *pg,
+				    void *pg_manifest,
+				    struct ipu_psys_resource_pool *pool)
+{
+	u32 id, idx;
+	u32 mem_type_id;
+	int ret, i;
+	u16 *process_offset_table;
+	u8 processes;
+	u32 cells = 0;
+	struct ipu_psys_resource_alloc *alloc;
+	const struct ipu_fw_resource_definitions *res_defs;
+
+	if (!pg)
+		return -EINVAL;
+	process_offset_table = (u16 *)((u8 *)pg + pg->processes_offset);
+	processes = pg->process_count;
+
+	alloc = kzalloc(sizeof(*alloc), GFP_KERNEL);
+	if (!alloc)
+		return -ENOMEM;
+
+	res_defs = get_res();
+	for (i = 0; i < processes; i++) {
+		u32 cell;
+		struct ipu_fw_psys_process *process =
+			(struct ipu_fw_psys_process *)
+			((char *)pg + process_offset_table[i]);
+		struct ipu_fw_generic_program_manifest pm;
+
+		memset(&pm, 0, sizeof(pm));
+
+		if (!process) {
+			dev_err(dev, "can not get process\n");
+			ret = -ENOENT;
+			goto free_out;
+		}
+
+		ret = ipu_fw_psys_get_program_manifest_by_process
+			(&pm, pg_manifest, process);
+		if (ret < 0) {
+			dev_err(dev, "can not get manifest\n");
+			goto free_out;
+		}
+
+		if (pm.cell_id == res_defs->num_cells &&
+		    pm.cell_type_id == res_defs->num_cells_type) {
+			cell = res_defs->num_cells;
+		} else if ((pm.cell_id != res_defs->num_cells &&
+			    pm.cell_type_id == res_defs->num_cells_type)) {
+			cell = pm.cell_id;
+		} else {
+			/* Find a free cell of desired type */
+			u32 type = pm.cell_type_id;
+
+			for (cell = 0; cell < res_defs->num_cells; cell++)
+				if (res_defs->cells[cell] == type &&
+				    ((pool->cells | cells) & (1 << cell)) == 0)
+					break;
+			if (cell >= res_defs->num_cells) {
+				dev_dbg(dev, "no free cells of right type\n");
+				ret = -ENOSPC;
+				goto free_out;
+			}
+		}
+		if (cell < res_defs->num_cells)
+			cells |= 1 << cell;
+		if (pool->cells & cells) {
+			dev_dbg(dev, "out of cell resources\n");
+			ret = -ENOSPC;
+			goto free_out;
+		}
+
+		if (pm.dev_chn_size) {
+			for (id = 0; id < res_defs->num_dev_channels; id++) {
+				ret = __alloc_one_resrc(dev, process,
+							&pool->dev_channels[id],
+							&pm, id, alloc);
+				if (ret)
+					goto free_out;
+			}
+		}
+
+		if (pm.dfm_port_bitmap) {
+			for (id = 0; id < res_defs->num_dfm_ids; id++) {
+				ret = ipu_psys_allocate_one_dfm
+					(dev, process,
+					 &pool->dfms[id], &pm, id, alloc);
+				if (ret)
+					goto free_out;
+			}
+		}
+
+		if (pm.ext_mem_size) {
+			for (mem_type_id = 0;
+			     mem_type_id < res_defs->num_ext_mem_types;
+			     mem_type_id++) {
+				u32 bank = res_defs->num_ext_mem_ids;
+
+				if (cell != res_defs->num_cells) {
+					idx = res_defs->cell_mem_row * cell +
+						mem_type_id;
+					bank = res_defs->cell_mem[idx];
+				}
+
+				if (bank == res_defs->num_ext_mem_ids)
+					continue;
+
+				ret = __alloc_mem_resrc(dev, process,
+							&pool->ext_memory[bank],
+							&pm, mem_type_id, bank,
+							alloc);
+				if (ret)
+					goto free_out;
+			}
+		}
+	}
+	alloc->cells |= cells;
+	pool->cells |= cells;
+
+	kfree(alloc);
+	return 0;
+
+free_out:
+	dev_dbg(dev, "failed to try_allocate resource\n");
+	kfree(alloc);
+	return ret;
+}
+
+/*
+ * Allocate resources for pg from `pool'. Mark the allocated
+ * resources into `alloc'. Returns 0 on success, -ENOSPC
+ * if there are no enough resources, in which cases resources
+ * are not allocated at all, or some other error on other conditions.
+ */
+int ipu_psys_allocate_resources(const struct device *dev,
+				struct ipu_fw_psys_process_group *pg,
+				void *pg_manifest,
+				struct ipu_psys_resource_alloc
+				*alloc, struct ipu_psys_resource_pool
+				*pool)
+{
+	u32 id;
+	u32 mem_type_id;
+	int ret, i;
+	u16 *process_offset_table;
+	u8 processes;
+	u32 cells = 0;
+	int p, idx;
+	u32 bmp, a_bmp;
+	const struct ipu_fw_resource_definitions *res_defs;
+
+	if (!pg)
+		return -EINVAL;
+
+	res_defs = get_res();
+	process_offset_table = (u16 *)((u8 *)pg + pg->processes_offset);
+	processes = pg->process_count;
+
+	for (i = 0; i < processes; i++) {
+		u32 cell;
+		struct ipu_fw_psys_process *process =
+		    (struct ipu_fw_psys_process *)
+		    ((char *)pg + process_offset_table[i]);
+		struct ipu_fw_generic_program_manifest pm;
+
+		memset(&pm, 0, sizeof(pm));
+		if (!process) {
+			dev_err(dev, "can not get process\n");
+			ret = -ENOENT;
+			goto free_out;
+		}
+
+		ret = ipu_fw_psys_get_program_manifest_by_process
+		    (&pm, pg_manifest, process);
+		if (ret < 0) {
+			dev_err(dev, "can not get manifest\n");
+			goto free_out;
+		}
+
+		if (pm.cell_id == res_defs->num_cells &&
+		    pm.cell_type_id == res_defs->num_cells_type) {
+			cell = res_defs->num_cells;
+		} else if ((pm.cell_id != res_defs->num_cells &&
+			    pm.cell_type_id == res_defs->num_cells_type)) {
+			cell = pm.cell_id;
+		} else {
+			/* Find a free cell of desired type */
+			u32 type = pm.cell_type_id;
+
+			for (cell = 0; cell < res_defs->num_cells; cell++)
+				if (res_defs->cells[cell] == type &&
+				    ((pool->cells | cells) & (1 << cell)) == 0)
+					break;
+			if (cell >= res_defs->num_cells) {
+				dev_dbg(dev, "no free cells of right type\n");
+				ret = -ENOSPC;
+				goto free_out;
+			}
+			ret = ipu_fw_psys_set_process_cell_id(process, 0, cell);
+			if (ret)
+				goto free_out;
+		}
+		if (cell < res_defs->num_cells)
+			cells |= 1 << cell;
+		if (pool->cells & cells) {
+			dev_dbg(dev, "out of cell resources\n");
+			ret = -ENOSPC;
+			goto free_out;
+		}
+
+		if (pm.dev_chn_size) {
+			for (id = 0; id < res_defs->num_dev_channels; id++) {
+				ret = __alloc_one_resrc(dev, process,
+							&pool->dev_channels[id],
+							&pm, id, alloc);
+				if (ret)
+					goto free_out;
+
+				idx = alloc->resources - 1;
+				p = alloc->resource_alloc[idx].pos;
+				ret = ipu_fw_psys_set_proc_dev_chn(process, id,
+								   p);
+				if (ret)
+					goto free_out;
+			}
+		}
+
+		if (pm.dfm_port_bitmap) {
+			for (id = 0; id < res_defs->num_dfm_ids; id++) {
+				ret = ipu_psys_allocate_one_dfm(dev, process,
+								&pool->dfms[id],
+								&pm, id, alloc);
+				if (ret)
+					goto free_out;
+
+				idx = alloc->resources - 1;
+				p = alloc->resource_alloc[idx].pos;
+				bmp = pm.dfm_port_bitmap[id];
+				bmp = bmp << p;
+				a_bmp = pm.dfm_active_port_bitmap[id];
+				a_bmp = a_bmp << p;
+				ret = ipu_fw_psys_set_proc_dfm_bitmap(process,
+								      id, bmp,
+								      a_bmp);
+				if (ret)
+					goto free_out;
+			}
+		}
+
+		if (pm.ext_mem_size) {
+			for (mem_type_id = 0;
+			     mem_type_id < res_defs->num_ext_mem_types;
+			     mem_type_id++) {
+				u32 bank = res_defs->num_ext_mem_ids;
+
+				if (cell != res_defs->num_cells) {
+					idx = res_defs->cell_mem_row * cell +
+						mem_type_id;
+					bank = res_defs->cell_mem[idx];
+				}
+				if (bank == res_defs->num_ext_mem_ids)
+					continue;
+
+				ret = __alloc_mem_resrc(dev, process,
+							&pool->ext_memory[bank],
+							&pm, mem_type_id,
+							bank, alloc);
+				if (ret)
+					goto free_out;
+
+				/* no return value check here because fw api
+				 * will do some checks, and would return
+				 * non-zero except mem_type_id == 0.
+				 * This maybe caused by that above flow of
+				 * allocating mem_bank_id is improper.
+				 */
+				idx = alloc->resources - 1;
+				p = alloc->resource_alloc[idx].pos;
+				ipu_fw_psys_set_process_ext_mem(process,
+								mem_type_id,
+								bank, p);
+			}
+		}
+	}
+	alloc->cells |= cells;
+	pool->cells |= cells;
+	return 0;
+
+free_out:
+	dev_err(dev, "failed to allocate resources, ret %d\n", ret);
+	ipu_psys_reset_process_cell(dev, pg, pg_manifest, i + 1);
+	ipu_psys_free_resources(alloc, pool);
+	return ret;
+}
+
+int ipu_psys_move_resources(const struct device *dev,
+			    struct ipu_psys_resource_alloc *alloc,
+			    struct ipu_psys_resource_pool
+			    *source_pool, struct ipu_psys_resource_pool
+			    *target_pool)
+{
+	int i;
+
+	if (target_pool->cells & alloc->cells) {
+		dev_dbg(dev, "out of cell resources\n");
+		return -ENOSPC;
+	}
+
+	for (i = 0; i < alloc->resources; i++) {
+		unsigned long bitmap = 0;
+		unsigned int id = alloc->resource_alloc[i].resource->id;
+		unsigned long fbit, end;
+
+		switch (alloc->resource_alloc[i].type) {
+		case IPU_RESOURCE_DEV_CHN:
+			bitmap_set(&bitmap, alloc->resource_alloc[i].pos,
+				   alloc->resource_alloc[i].elements);
+			if (*target_pool->dev_channels[id].bitmap & bitmap)
+				return -ENOSPC;
+			break;
+		case IPU_RESOURCE_EXT_MEM:
+			end = alloc->resource_alloc[i].elements +
+			    alloc->resource_alloc[i].pos;
+
+			fbit = find_next_bit(target_pool->ext_memory[id].bitmap,
+					     end, alloc->resource_alloc[i].pos);
+			/* if find_next_bit returns "end" it didn't find 1bit */
+			if (end != fbit)
+				return -ENOSPC;
+			break;
+		case IPU_RESOURCE_DFM:
+			bitmap = alloc->resource_alloc[i].elements;
+			if (*target_pool->dfms[id].bitmap & bitmap)
+				return -ENOSPC;
+			break;
+		default:
+			dev_err(dev, "Illegal resource type\n");
+			return -EINVAL;
+		}
+	}
+
+	for (i = 0; i < alloc->resources; i++) {
+		u32 id = alloc->resource_alloc[i].resource->id;
+
+		switch (alloc->resource_alloc[i].type) {
+		case IPU_RESOURCE_DEV_CHN:
+			bitmap_set(target_pool->dev_channels[id].bitmap,
+				   alloc->resource_alloc[i].pos,
+				   alloc->resource_alloc[i].elements);
+			ipu_resource_free(&alloc->resource_alloc[i]);
+			alloc->resource_alloc[i].resource =
+			    &target_pool->dev_channels[id];
+			break;
+		case IPU_RESOURCE_EXT_MEM:
+			bitmap_set(target_pool->ext_memory[id].bitmap,
+				   alloc->resource_alloc[i].pos,
+				   alloc->resource_alloc[i].elements);
+			ipu_resource_free(&alloc->resource_alloc[i]);
+			alloc->resource_alloc[i].resource =
+			    &target_pool->ext_memory[id];
+			break;
+		case IPU_RESOURCE_DFM:
+			*target_pool->dfms[id].bitmap |=
+			    alloc->resource_alloc[i].elements;
+			*alloc->resource_alloc[i].resource->bitmap &=
+			    ~(alloc->resource_alloc[i].elements);
+			alloc->resource_alloc[i].resource =
+			    &target_pool->dfms[id];
+			break;
+		default:
+			/*
+			 * Just keep compiler happy. This case failed already
+			 * in above loop.
+			 */
+			break;
+		}
+	}
+
+	target_pool->cells |= alloc->cells;
+	source_pool->cells &= ~alloc->cells;
+
+	return 0;
+}
+
+void ipu_psys_reset_process_cell(const struct device *dev,
+				 struct ipu_fw_psys_process_group *pg,
+				 void *pg_manifest,
+				 int process_count)
+{
+	int i;
+	u16 *process_offset_table;
+	const struct ipu_fw_resource_definitions *res_defs;
+
+	if (!pg)
+		return;
+
+	res_defs = get_res();
+	process_offset_table = (u16 *)((u8 *)pg + pg->processes_offset);
+	for (i = 0; i < process_count; i++) {
+		struct ipu_fw_psys_process *process =
+		    (struct ipu_fw_psys_process *)
+		    ((char *)pg + process_offset_table[i]);
+		struct ipu_fw_generic_program_manifest pm;
+		int ret;
+
+		if (!process)
+			break;
+
+		ret = ipu_fw_psys_get_program_manifest_by_process(&pm,
+								  pg_manifest,
+								  process);
+		if (ret < 0) {
+			dev_err(dev, "can not get manifest\n");
+			break;
+		}
+		if ((pm.cell_id != res_defs->num_cells &&
+		     pm.cell_type_id == res_defs->num_cells_type))
+			continue;
+		/* no return value check here because if finding free cell
+		 * failed, process cell would not set then calling clear_cell
+		 * will return non-zero.
+		 */
+		ipu_fw_psys_clear_process_cell(process);
+	}
+}
+
+/* Free resources marked in `alloc' from `resources' */
+void ipu_psys_free_resources(struct ipu_psys_resource_alloc
+			     *alloc, struct ipu_psys_resource_pool *pool)
+{
+	unsigned int i;
+
+	pool->cells &= ~alloc->cells;
+	alloc->cells = 0;
+	for (i = 0; i < alloc->resources; i++)
+		ipu_resource_free(&alloc->resource_alloc[i]);
+	alloc->resources = 0;
+}
diff -ruN a/drivers/media/pci/intel/ipu6/Makefile b/drivers/media/pci/intel/ipu6/Makefile
--- a/drivers/media/pci/intel/ipu6/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu6/Makefile	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,63 @@
+# SPDX-License-Identifier: GPL-2.0
+# Copyright (c) 2017 - 2020 Intel Corporation.
+
+ifneq ($(EXTERNAL_BUILD), 1)
+srcpath := $(srctree)
+endif
+
+ccflags-y += -DIPU_TPG_FRAME_SYNC -DIPU_PSYS_GPC \
+		-DIPU_ISYS_GPC
+
+intel-ipu6-objs				+= ../ipu.o \
+					   ../ipu-bus.o \
+					   ../ipu-dma.o \
+					   ../ipu-mmu.o \
+					   ../ipu-buttress.o \
+					   ../ipu-trace.o \
+					   ../ipu-cpd.o \
+					   ipu6.o \
+					   ../ipu-fw-com.o
+
+obj-$(CONFIG_VIDEO_INTEL_IPU6)		+= intel-ipu6.o
+
+intel-ipu6-isys-objs			+= ../ipu-isys.o \
+					   ../ipu-isys-csi2.o \
+					   ipu6-isys.o \
+					   ipu6-isys-phy.o \
+					   ipu6-isys-csi2.o \
+					   ipu6-isys-gpc.o \
+					   ../ipu-isys-csi2-be-soc.o \
+					   ../ipu-fw-isys.o \
+					   ../ipu-isys-video.o \
+					   ../ipu-isys-queue.o \
+					   ../ipu-isys-subdev.o
+intel-ipu6-isys-objs			+= ../ipu-isys-csi2-be.o
+
+ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+intel-ipu6-isys-objs                    += ../ipu-isys-tpg.o
+endif
+
+obj-$(CONFIG_VIDEO_INTEL_IPU6)		+= intel-ipu6-isys.o
+
+intel-ipu6-psys-objs			+= ../ipu-psys.o \
+					   ipu6-psys.o \
+					   ipu-resources.o \
+					   ipu6-psys-gpc.o \
+					   ipu6-l-scheduler.o \
+					   ipu6-ppg.o
+
+intel-ipu6-psys-objs			+= ipu-fw-resources.o \
+					   ipu6-fw-resources.o \
+					   ipu6se-fw-resources.o \
+					   ipu6ep-fw-resources.o \
+					   ../ipu-fw-psys.o
+
+ifeq ($(CONFIG_COMPAT),y)
+intel-ipu6-psys-objs			+= ../ipu-psys-compat32.o
+endif
+
+obj-$(CONFIG_VIDEO_INTEL_IPU6)		+= intel-ipu6-psys.o
+
+ccflags-y += -I$(srcpath)/$(src)/../../../../../include/
+ccflags-y += -I$(srcpath)/$(src)/../
+ccflags-y += -I$(srcpath)/$(src)/
diff -ruN a/drivers/media/pci/intel/ipu-bus.c b/drivers/media/pci/intel/ipu-bus.c
--- a/drivers/media/pci/intel/ipu-bus.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-bus.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,254 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2020 Intel Corporation
+
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/pci.h>
+#include <linux/pm_runtime.h>
+#include <linux/sizes.h>
+
+#include "ipu.h"
+#include "ipu-platform.h"
+#include "ipu-dma.h"
+
+#ifdef CONFIG_PM
+static struct bus_type ipu_bus;
+
+static int bus_pm_runtime_suspend(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	int rval;
+
+	rval = pm_generic_runtime_suspend(dev);
+	if (rval)
+		return rval;
+
+	rval = ipu_buttress_power(dev, adev->ctrl, false);
+	dev_dbg(dev, "%s: buttress power down %d\n", __func__, rval);
+	if (!rval)
+		return 0;
+
+	dev_err(dev, "power down failed!\n");
+
+	/* Powering down failed, attempt to resume device now */
+	rval = pm_generic_runtime_resume(dev);
+	if (!rval)
+		return -EBUSY;
+
+	return -EIO;
+}
+
+static int bus_pm_runtime_resume(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	int rval;
+
+	rval = ipu_buttress_power(dev, adev->ctrl, true);
+	dev_dbg(dev, "%s: buttress power up %d\n", __func__, rval);
+	if (rval)
+		return rval;
+
+	rval = pm_generic_runtime_resume(dev);
+	dev_dbg(dev, "%s: resume %d\n", __func__, rval);
+	if (rval)
+		goto out_err;
+
+	return 0;
+
+out_err:
+	ipu_buttress_power(dev, adev->ctrl, false);
+
+	return -EBUSY;
+}
+
+static const struct dev_pm_ops ipu_bus_pm_ops = {
+	.runtime_suspend = bus_pm_runtime_suspend,
+	.runtime_resume = bus_pm_runtime_resume,
+};
+
+#define IPU_BUS_PM_OPS	(&ipu_bus_pm_ops)
+#else
+#define IPU_BUS_PM_OPS	NULL
+#endif
+
+static int ipu_bus_match(struct device *dev, struct device_driver *drv)
+{
+	struct ipu_bus_driver *adrv = to_ipu_bus_driver(drv);
+
+	dev_dbg(dev, "bus match: \"%s\" --- \"%s\"\n", dev_name(dev),
+		adrv->wanted);
+
+	return !strncmp(dev_name(dev), adrv->wanted, strlen(adrv->wanted));
+}
+
+static int ipu_bus_probe(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	struct ipu_bus_driver *adrv = to_ipu_bus_driver(dev->driver);
+	int rval;
+
+	dev_dbg(dev, "bus probe dev %s\n", dev_name(dev));
+
+	adev->adrv = adrv;
+	if (!adrv->probe) {
+		rval = -ENODEV;
+		goto out_err;
+	}
+	rval = pm_runtime_get_sync(&adev->dev);
+	if (rval < 0) {
+		dev_err(&adev->dev, "Failed to get runtime PM\n");
+		goto out_err;
+	}
+
+	rval = adrv->probe(adev);
+	pm_runtime_put(&adev->dev);
+
+	if (rval)
+		goto out_err;
+
+	return 0;
+
+out_err:
+	ipu_bus_set_drvdata(adev, NULL);
+	adev->adrv = NULL;
+
+	return rval;
+}
+
+static void ipu_bus_remove(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	struct ipu_bus_driver *adrv = to_ipu_bus_driver(dev->driver);
+
+	if (adrv->remove)
+		adrv->remove(adev);
+}
+
+static struct bus_type ipu_bus = {
+	.name = IPU_BUS_NAME,
+	.match = ipu_bus_match,
+	.probe = ipu_bus_probe,
+	.remove = ipu_bus_remove,
+	.pm = IPU_BUS_PM_OPS,
+};
+
+static struct mutex ipu_bus_mutex;
+
+static void ipu_bus_release(struct device *dev)
+{
+}
+
+struct ipu_bus_device *ipu_bus_add_device(struct pci_dev *pdev,
+					  struct device *parent, void *pdata,
+					  struct ipu_buttress_ctrl *ctrl,
+					  char *name, unsigned int nr)
+{
+	struct ipu_bus_device *adev;
+	struct ipu_device *isp = pci_get_drvdata(pdev);
+	int rval;
+
+	adev = devm_kzalloc(&pdev->dev, sizeof(*adev), GFP_KERNEL);
+	if (!adev)
+		return ERR_PTR(-ENOMEM);
+
+	adev->dev.parent = parent;
+	adev->dev.bus = &ipu_bus;
+	adev->dev.release = ipu_bus_release;
+	adev->dev.dma_ops = &ipu_dma_ops;
+	adev->dma_mask = DMA_BIT_MASK(isp->secure_mode ?
+				      IPU_MMU_ADDRESS_BITS :
+				      IPU_MMU_ADDRESS_BITS_NON_SECURE);
+	adev->dev.dma_mask = &adev->dma_mask;
+	adev->dev.dma_parms = pdev->dev.dma_parms;
+	adev->dev.coherent_dma_mask = adev->dma_mask;
+	adev->ctrl = ctrl;
+	adev->pdata = pdata;
+	adev->isp = isp;
+	mutex_init(&adev->resume_lock);
+	dev_set_name(&adev->dev, "%s%d", name, nr);
+
+	rval = device_register(&adev->dev);
+	if (rval) {
+		put_device(&adev->dev);
+		return ERR_PTR(rval);
+	}
+
+	mutex_lock(&ipu_bus_mutex);
+	list_add(&adev->list, &isp->devices);
+	mutex_unlock(&ipu_bus_mutex);
+
+	pm_runtime_allow(&adev->dev);
+	pm_runtime_enable(&adev->dev);
+
+	return adev;
+}
+
+void ipu_bus_del_devices(struct pci_dev *pdev)
+{
+	struct ipu_device *isp = pci_get_drvdata(pdev);
+	struct ipu_bus_device *adev, *save;
+
+	mutex_lock(&ipu_bus_mutex);
+
+	list_for_each_entry_safe(adev, save, &isp->devices, list) {
+		pm_runtime_disable(&adev->dev);
+		list_del(&adev->list);
+		device_unregister(&adev->dev);
+	}
+
+	mutex_unlock(&ipu_bus_mutex);
+}
+
+int ipu_bus_register_driver(struct ipu_bus_driver *adrv)
+{
+	adrv->drv.bus = &ipu_bus;
+	return driver_register(&adrv->drv);
+}
+EXPORT_SYMBOL(ipu_bus_register_driver);
+
+int ipu_bus_unregister_driver(struct ipu_bus_driver *adrv)
+{
+	driver_unregister(&adrv->drv);
+	return 0;
+}
+EXPORT_SYMBOL(ipu_bus_unregister_driver);
+
+int ipu_bus_register(void)
+{
+	mutex_init(&ipu_bus_mutex);
+	return bus_register(&ipu_bus);
+}
+
+void ipu_bus_unregister(void)
+{
+	mutex_destroy(&ipu_bus_mutex);
+	return bus_unregister(&ipu_bus);
+}
+
+static int flr_rpm_recovery(struct device *dev, void *p)
+{
+	dev_dbg(dev, "FLR recovery call\n");
+	/*
+	 * We are not necessarily going through device from child to
+	 * parent. runtime PM refuses to change state for parent if the child
+	 * is still active. At FLR (full reset for whole IPU) that doesn't
+	 * matter. Everything has been power gated by HW during the FLR cycle
+	 * and we are just cleaning up SW state. Thus, ignore child during
+	 * set_suspended.
+	 */
+	pm_suspend_ignore_children(dev, true);
+	pm_runtime_set_suspended(dev);
+	pm_suspend_ignore_children(dev, false);
+
+	return 0;
+}
+
+int ipu_bus_flr_recovery(void)
+{
+	bus_for_each_dev(&ipu_bus, NULL, NULL, flr_rpm_recovery);
+	return 0;
+}
diff -ruN a/drivers/media/pci/intel/ipu-bus.h b/drivers/media/pci/intel/ipu-bus.h
--- a/drivers/media/pci/intel/ipu-bus.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-bus.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,67 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_BUS_H
+#define IPU_BUS_H
+
+#include <linux/device.h>
+#include <linux/irqreturn.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/pci.h>
+
+#define IPU_BUS_NAME	IPU_NAME "-bus"
+
+struct ipu_buttress_ctrl;
+struct ipu_subsystem_trace_config;
+
+struct ipu_bus_device {
+	struct device dev;
+	struct list_head list;
+	void *pdata;
+	struct ipu_bus_driver *adrv;
+	struct ipu_mmu *mmu;
+	struct ipu_device *isp;
+	struct ipu_subsystem_trace_config *trace_cfg;
+	struct ipu_buttress_ctrl *ctrl;
+	u64 dma_mask;
+	/* Protect runtime_resume calls on the dev */
+	struct mutex resume_lock;
+};
+
+#define to_ipu_bus_device(_dev) container_of(_dev, struct ipu_bus_device, dev)
+
+struct ipu_bus_driver {
+	struct device_driver drv;
+	const char *wanted;
+	int (*probe)(struct ipu_bus_device *adev);
+	void (*remove)(struct ipu_bus_device *adev);
+	irqreturn_t (*isr)(struct ipu_bus_device *adev);
+	irqreturn_t (*isr_threaded)(struct ipu_bus_device *adev);
+	bool wake_isr_thread;
+};
+
+#define to_ipu_bus_driver(_drv) container_of(_drv, struct ipu_bus_driver, drv)
+
+struct ipu_bus_device *ipu_bus_add_device(struct pci_dev *pdev,
+					  struct device *parent, void *pdata,
+					  struct ipu_buttress_ctrl *ctrl,
+					  char *name, unsigned int nr);
+void ipu_bus_del_devices(struct pci_dev *pdev);
+
+int ipu_bus_register_driver(struct ipu_bus_driver *adrv);
+int ipu_bus_unregister_driver(struct ipu_bus_driver *adrv);
+
+int ipu_bus_register(void);
+void ipu_bus_unregister(void);
+
+#define module_ipu_bus_driver(drv)			\
+	module_driver(drv, ipu_bus_register_driver, \
+		ipu_bus_unregister_driver)
+
+#define ipu_bus_set_drvdata(adev, data) dev_set_drvdata(&(adev)->dev, data)
+#define ipu_bus_get_drvdata(adev) dev_get_drvdata(&(adev)->dev)
+
+int ipu_bus_flr_recovery(void);
+
+#endif
diff -ruN a/drivers/media/pci/intel/ipu-buttress.c b/drivers/media/pci/intel/ipu-buttress.c
--- a/drivers/media/pci/intel/ipu-buttress.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-buttress.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,1372 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2020 Intel Corporation
+
+#include <linux/clk.h>
+#include <linux/clkdev.h>
+#include <linux/clk-provider.h>
+#include <linux/completion.h>
+#include <linux/debugfs.h>
+#include <linux/device.h>
+#include <linux/delay.h>
+#include <linux/elf.h>
+#include <linux/errno.h>
+#include <linux/firmware.h>
+#include <linux/iopoll.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/pm_runtime.h>
+
+#include <media/ipu-isys.h>
+
+#include "ipu.h"
+#include "ipu-bus.h"
+#include "ipu-buttress.h"
+#include "ipu-platform-buttress-regs.h"
+#include "ipu-cpd.h"
+
+#define BOOTLOADER_STATUS_OFFSET       0x15c
+
+#define BOOTLOADER_MAGIC_KEY		0xb00710ad
+
+#define ENTRY	BUTTRESS_IU2CSECSR_IPC_PEER_COMP_ACTIONS_RST_PHASE1
+#define EXIT	BUTTRESS_IU2CSECSR_IPC_PEER_COMP_ACTIONS_RST_PHASE2
+#define QUERY	BUTTRESS_IU2CSECSR_IPC_PEER_QUERIED_IP_COMP_ACTIONS_RST_PHASE
+
+#define BUTTRESS_TSC_SYNC_RESET_TRIAL_MAX	10
+
+#define BUTTRESS_CSE_BOOTLOAD_TIMEOUT		5000000
+#define BUTTRESS_CSE_AUTHENTICATE_TIMEOUT	10000000
+#define BUTTRESS_CSE_FWRESET_TIMEOUT		100000
+
+#define BUTTRESS_IPC_TX_TIMEOUT			1000
+#define BUTTRESS_IPC_RESET_TIMEOUT		2000
+#define BUTTRESS_IPC_RX_TIMEOUT			1000
+#define BUTTRESS_IPC_VALIDITY_TIMEOUT		1000000
+#define BUTTRESS_TSC_SYNC_TIMEOUT		5000
+
+#define IPU_BUTTRESS_TSC_LIMIT	500	/* 26 us @ 19.2 MHz */
+#define IPU_BUTTRESS_TSC_RETRY	10
+
+#define BUTTRESS_CSE_IPC_RESET_RETRY	4
+
+#define BUTTRESS_IPC_CMD_SEND_RETRY	1
+
+static const u32 ipu_adev_irq_mask[] = {
+	BUTTRESS_ISR_IS_IRQ, BUTTRESS_ISR_PS_IRQ
+};
+
+int ipu_buttress_ipc_reset(struct ipu_device *isp, struct ipu_buttress_ipc *ipc)
+{
+	struct ipu_buttress *b = &isp->buttress;
+	unsigned int retries = BUTTRESS_IPC_RESET_TIMEOUT;
+	u32 val = 0, csr_in_clr;
+
+	if (!isp->secure_mode) {
+		dev_info(&isp->pdev->dev, "Skip ipc reset for non-secure mode");
+		return 0;
+	}
+
+	mutex_lock(&b->ipc_mutex);
+
+	/* Clear-by-1 CSR (all bits), corresponding internal states. */
+	val = readl(isp->base + ipc->csr_in);
+	writel(val, isp->base + ipc->csr_in);
+
+	/* Set peer CSR bit IPC_PEER_COMP_ACTIONS_RST_PHASE1 */
+	writel(ENTRY, isp->base + ipc->csr_out);
+	/*
+	 * Clear-by-1 all CSR bits EXCEPT following
+	 * bits:
+	 * A. IPC_PEER_COMP_ACTIONS_RST_PHASE1.
+	 * B. IPC_PEER_COMP_ACTIONS_RST_PHASE2.
+	 * C. Possibly custom bits, depending on
+	 * their role.
+	 */
+	csr_in_clr = BUTTRESS_IU2CSECSR_IPC_PEER_DEASSERTED_REG_VALID_REQ |
+		BUTTRESS_IU2CSECSR_IPC_PEER_ACKED_REG_VALID |
+		BUTTRESS_IU2CSECSR_IPC_PEER_ASSERTED_REG_VALID_REQ | QUERY;
+
+	while (retries--) {
+		usleep_range(400, 500);
+		val = readl(isp->base + ipc->csr_in);
+		switch (val) {
+		case (ENTRY | EXIT):
+		case (ENTRY | EXIT | QUERY):
+			dev_dbg(&isp->pdev->dev,
+				"%s:%s & %s\n", __func__,
+				"IPC_PEER_COMP_ACTIONS_RST_PHASE1",
+				"IPC_PEER_COMP_ACTIONS_RST_PHASE2");
+			/*
+			 * 1) Clear-by-1 CSR bits
+			 * (IPC_PEER_COMP_ACTIONS_RST_PHASE1,
+			 * IPC_PEER_COMP_ACTIONS_RST_PHASE2).
+			 * 2) Set peer CSR bit
+			 * IPC_PEER_QUERIED_IP_COMP_ACTIONS_RST_PHASE.
+			 */
+			writel(ENTRY | EXIT, isp->base + ipc->csr_in);
+			writel(QUERY, isp->base + ipc->csr_out);
+			break;
+		case ENTRY:
+		case (ENTRY | QUERY):
+			dev_dbg(&isp->pdev->dev,
+				"%s:IPC_PEER_COMP_ACTIONS_RST_PHASE1\n",
+				__func__);
+			/*
+			 * 1) Clear-by-1 CSR bits
+			 * (IPC_PEER_COMP_ACTIONS_RST_PHASE1,
+			 * IPC_PEER_QUERIED_IP_COMP_ACTIONS_RST_PHASE).
+			 * 2) Set peer CSR bit
+			 * IPC_PEER_COMP_ACTIONS_RST_PHASE1.
+			 */
+			writel(ENTRY | QUERY, isp->base + ipc->csr_in);
+			writel(ENTRY, isp->base + ipc->csr_out);
+			break;
+		case EXIT:
+		case (EXIT | QUERY):
+			dev_dbg(&isp->pdev->dev,
+				"%s: IPC_PEER_COMP_ACTIONS_RST_PHASE2\n",
+				__func__);
+			/*
+			 * Clear-by-1 CSR bit
+			 * IPC_PEER_COMP_ACTIONS_RST_PHASE2.
+			 * 1) Clear incoming doorbell.
+			 * 2) Clear-by-1 all CSR bits EXCEPT following
+			 * bits:
+			 * A. IPC_PEER_COMP_ACTIONS_RST_PHASE1.
+			 * B. IPC_PEER_COMP_ACTIONS_RST_PHASE2.
+			 * C. Possibly custom bits, depending on
+			 * their role.
+			 * 3) Set peer CSR bit
+			 * IPC_PEER_COMP_ACTIONS_RST_PHASE2.
+			 */
+			writel(EXIT, isp->base + ipc->csr_in);
+			writel(0, isp->base + ipc->db0_in);
+			writel(csr_in_clr, isp->base + ipc->csr_in);
+			writel(EXIT, isp->base + ipc->csr_out);
+
+			/*
+			 * Read csr_in again to make sure if RST_PHASE2 is done.
+			 * If csr_in is QUERY, it should be handled again.
+			 */
+			usleep_range(200, 300);
+			val = readl(isp->base + ipc->csr_in);
+			if (val & QUERY) {
+				dev_dbg(&isp->pdev->dev,
+					"%s: RST_PHASE2 retry csr_in = %x\n",
+					__func__, val);
+				break;
+			}
+			mutex_unlock(&b->ipc_mutex);
+			return 0;
+		case QUERY:
+			dev_dbg(&isp->pdev->dev,
+				"%s: %s\n", __func__,
+				"IPC_PEER_QUERIED_IP_COMP_ACTIONS_RST_PHASE");
+			/*
+			 * 1) Clear-by-1 CSR bit
+			 * IPC_PEER_QUERIED_IP_COMP_ACTIONS_RST_PHASE.
+			 * 2) Set peer CSR bit
+			 * IPC_PEER_COMP_ACTIONS_RST_PHASE1
+			 */
+			writel(QUERY, isp->base + ipc->csr_in);
+			writel(ENTRY, isp->base + ipc->csr_out);
+			break;
+		default:
+			dev_dbg_ratelimited(&isp->pdev->dev,
+					    "%s: unexpected CSR 0x%x\n",
+					    __func__, val);
+			break;
+		}
+	}
+
+	mutex_unlock(&b->ipc_mutex);
+	dev_err(&isp->pdev->dev, "Timed out while waiting for CSE\n");
+
+	return -ETIMEDOUT;
+}
+
+static void
+ipu_buttress_ipc_validity_close(struct ipu_device *isp,
+				struct ipu_buttress_ipc *ipc)
+{
+	/* Set bit 5 in CSE CSR */
+	writel(BUTTRESS_IU2CSECSR_IPC_PEER_DEASSERTED_REG_VALID_REQ,
+	       isp->base + ipc->csr_out);
+}
+
+static int
+ipu_buttress_ipc_validity_open(struct ipu_device *isp,
+			       struct ipu_buttress_ipc *ipc)
+{
+	unsigned int mask = BUTTRESS_IU2CSECSR_IPC_PEER_ACKED_REG_VALID;
+	unsigned int tout = BUTTRESS_IPC_VALIDITY_TIMEOUT;
+	void __iomem *addr;
+	int ret;
+	u32 val;
+
+	/* Set bit 3 in CSE CSR */
+	writel(BUTTRESS_IU2CSECSR_IPC_PEER_ASSERTED_REG_VALID_REQ,
+	       isp->base + ipc->csr_out);
+
+	addr = isp->base + ipc->csr_in;
+	ret = readl_poll_timeout(addr, val, val & mask, 200, tout);
+	if (ret) {
+		val = readl(addr);
+		dev_err(&isp->pdev->dev, "CSE validity timeout 0x%x\n", val);
+		ipu_buttress_ipc_validity_close(isp, ipc);
+	}
+
+	return ret;
+}
+
+static void ipu_buttress_ipc_recv(struct ipu_device *isp,
+				  struct ipu_buttress_ipc *ipc, u32 *ipc_msg)
+{
+	if (ipc_msg)
+		*ipc_msg = readl(isp->base + ipc->data0_in);
+	writel(0, isp->base + ipc->db0_in);
+}
+
+static int ipu_buttress_ipc_send_bulk(struct ipu_device *isp,
+				      enum ipu_buttress_ipc_domain ipc_domain,
+				      struct ipu_ipc_buttress_bulk_msg *msgs,
+				      u32 size)
+{
+	struct ipu_buttress *b = &isp->buttress;
+	struct ipu_buttress_ipc *ipc;
+	unsigned long tx_timeout_jiffies, rx_timeout_jiffies;
+	u32 val;
+	int ret;
+	int tout;
+	unsigned int i, retry = BUTTRESS_IPC_CMD_SEND_RETRY;
+
+	ipc = ipc_domain == IPU_BUTTRESS_IPC_CSE ? &b->cse : &b->ish;
+
+	mutex_lock(&b->ipc_mutex);
+
+	ret = ipu_buttress_ipc_validity_open(isp, ipc);
+	if (ret) {
+		dev_err(&isp->pdev->dev, "IPC validity open failed\n");
+		goto out;
+	}
+
+	tx_timeout_jiffies = msecs_to_jiffies(BUTTRESS_IPC_TX_TIMEOUT);
+	rx_timeout_jiffies = msecs_to_jiffies(BUTTRESS_IPC_RX_TIMEOUT);
+
+	for (i = 0; i < size; i++) {
+		reinit_completion(&ipc->send_complete);
+		if (msgs[i].require_resp)
+			reinit_completion(&ipc->recv_complete);
+
+		dev_dbg(&isp->pdev->dev, "bulk IPC command: 0x%x\n",
+			msgs[i].cmd);
+		writel(msgs[i].cmd, isp->base + ipc->data0_out);
+
+		val = BUTTRESS_IU2CSEDB0_BUSY | msgs[i].cmd_size;
+
+		writel(val, isp->base + ipc->db0_out);
+
+		tout = wait_for_completion_timeout(&ipc->send_complete,
+						   tx_timeout_jiffies);
+		if (!tout) {
+			dev_err(&isp->pdev->dev, "send IPC response timeout\n");
+			if (!retry--) {
+				ret = -ETIMEDOUT;
+				goto out;
+			}
+
+			/*
+			 * WORKAROUND: Sometimes CSE is not
+			 * responding on first try, try again.
+			 */
+			writel(0, isp->base + ipc->db0_out);
+			i--;
+			continue;
+		}
+
+		retry = BUTTRESS_IPC_CMD_SEND_RETRY;
+
+		if (!msgs[i].require_resp)
+			continue;
+
+		tout = wait_for_completion_timeout(&ipc->recv_complete,
+						   rx_timeout_jiffies);
+		if (!tout) {
+			dev_err(&isp->pdev->dev, "recv IPC response timeout\n");
+			ret = -ETIMEDOUT;
+			goto out;
+		}
+
+		if (ipc->nack_mask &&
+		    (ipc->recv_data & ipc->nack_mask) == ipc->nack) {
+			dev_err(&isp->pdev->dev,
+				"IPC NACK for cmd 0x%x\n", msgs[i].cmd);
+			ret = -ENODEV;
+			goto out;
+		}
+
+		if (ipc->recv_data != msgs[i].expected_resp) {
+			dev_err(&isp->pdev->dev,
+				"expected resp: 0x%x, IPC response: 0x%x ",
+				msgs[i].expected_resp, ipc->recv_data);
+			ret = -EIO;
+			goto out;
+		}
+	}
+
+	dev_dbg(&isp->pdev->dev, "bulk IPC commands done\n");
+
+out:
+	ipu_buttress_ipc_validity_close(isp, ipc);
+	mutex_unlock(&b->ipc_mutex);
+	return ret;
+}
+
+static int
+ipu_buttress_ipc_send(struct ipu_device *isp,
+		      enum ipu_buttress_ipc_domain ipc_domain,
+		      u32 ipc_msg, u32 size, bool require_resp,
+		      u32 expected_resp)
+{
+	struct ipu_ipc_buttress_bulk_msg msg = {
+		.cmd = ipc_msg,
+		.cmd_size = size,
+		.require_resp = require_resp,
+		.expected_resp = expected_resp,
+	};
+
+	return ipu_buttress_ipc_send_bulk(isp, ipc_domain, &msg, 1);
+}
+
+static irqreturn_t ipu_buttress_call_isr(struct ipu_bus_device *adev)
+{
+	irqreturn_t ret = IRQ_WAKE_THREAD;
+
+	if (!adev || !adev->adrv)
+		return IRQ_NONE;
+
+	if (adev->adrv->isr)
+		ret = adev->adrv->isr(adev);
+
+	if (ret == IRQ_WAKE_THREAD && !adev->adrv->isr_threaded)
+		ret = IRQ_NONE;
+
+	adev->adrv->wake_isr_thread = (ret == IRQ_WAKE_THREAD);
+
+	return ret;
+}
+
+irqreturn_t ipu_buttress_isr(int irq, void *isp_ptr)
+{
+	struct ipu_device *isp = isp_ptr;
+	struct ipu_bus_device *adev[] = { isp->isys, isp->psys };
+	struct ipu_buttress *b = &isp->buttress;
+	irqreturn_t ret = IRQ_NONE;
+	u32 disable_irqs = 0;
+	u32 irq_status;
+	u32 reg_irq_sts = BUTTRESS_REG_ISR_STATUS;
+	unsigned int i;
+
+	pm_runtime_get(&isp->pdev->dev);
+
+	if (!pm_runtime_active(&isp->pdev->dev)) {
+		irq_status = readl(isp->base + reg_irq_sts);
+		writel(irq_status, isp->base + BUTTRESS_REG_ISR_CLEAR);
+		pm_runtime_put(&isp->pdev->dev);
+		return IRQ_HANDLED;
+	}
+
+	irq_status = readl(isp->base + reg_irq_sts);
+	if (!irq_status) {
+		pm_runtime_put(&isp->pdev->dev);
+		return IRQ_NONE;
+	}
+
+	do {
+		writel(irq_status, isp->base + BUTTRESS_REG_ISR_CLEAR);
+
+		for (i = 0; i < ARRAY_SIZE(ipu_adev_irq_mask); i++) {
+			if (irq_status & ipu_adev_irq_mask[i]) {
+				irqreturn_t r = ipu_buttress_call_isr(adev[i]);
+
+				if (r == IRQ_WAKE_THREAD) {
+					ret = IRQ_WAKE_THREAD;
+					disable_irqs |= ipu_adev_irq_mask[i];
+				} else if (ret == IRQ_NONE &&
+					   r == IRQ_HANDLED) {
+					ret = IRQ_HANDLED;
+				}
+			}
+		}
+
+		if (irq_status & (BUTTRESS_ISR_IPC_FROM_CSE_IS_WAITING |
+				  BUTTRESS_ISR_IPC_FROM_ISH_IS_WAITING |
+				  BUTTRESS_ISR_IPC_EXEC_DONE_BY_CSE |
+				  BUTTRESS_ISR_IPC_EXEC_DONE_BY_ISH |
+				  BUTTRESS_ISR_SAI_VIOLATION) &&
+		    ret == IRQ_NONE)
+			ret = IRQ_HANDLED;
+
+		if (irq_status & BUTTRESS_ISR_IPC_FROM_CSE_IS_WAITING) {
+			dev_dbg(&isp->pdev->dev,
+				"BUTTRESS_ISR_IPC_FROM_CSE_IS_WAITING\n");
+			ipu_buttress_ipc_recv(isp, &b->cse, &b->cse.recv_data);
+			complete(&b->cse.recv_complete);
+		}
+
+		if (irq_status & BUTTRESS_ISR_IPC_FROM_ISH_IS_WAITING) {
+			dev_dbg(&isp->pdev->dev,
+				"BUTTRESS_ISR_IPC_FROM_ISH_IS_WAITING\n");
+			ipu_buttress_ipc_recv(isp, &b->ish, &b->ish.recv_data);
+			complete(&b->ish.recv_complete);
+		}
+
+		if (irq_status & BUTTRESS_ISR_IPC_EXEC_DONE_BY_CSE) {
+			dev_dbg(&isp->pdev->dev,
+				"BUTTRESS_ISR_IPC_EXEC_DONE_BY_CSE\n");
+			complete(&b->cse.send_complete);
+		}
+
+		if (irq_status & BUTTRESS_ISR_IPC_EXEC_DONE_BY_ISH) {
+			dev_dbg(&isp->pdev->dev,
+				"BUTTRESS_ISR_IPC_EXEC_DONE_BY_CSE\n");
+			complete(&b->ish.send_complete);
+		}
+
+		if (irq_status & BUTTRESS_ISR_SAI_VIOLATION &&
+		    ipu_buttress_get_secure_mode(isp)) {
+			dev_err(&isp->pdev->dev,
+				"BUTTRESS_ISR_SAI_VIOLATION\n");
+			WARN_ON(1);
+		}
+
+		irq_status = readl(isp->base + reg_irq_sts);
+	} while (irq_status && !isp->flr_done);
+
+	if (disable_irqs)
+		writel(BUTTRESS_IRQS & ~disable_irqs,
+		       isp->base + BUTTRESS_REG_ISR_ENABLE);
+
+	pm_runtime_put(&isp->pdev->dev);
+
+	return ret;
+}
+
+irqreturn_t ipu_buttress_isr_threaded(int irq, void *isp_ptr)
+{
+	struct ipu_device *isp = isp_ptr;
+	struct ipu_bus_device *adev[] = { isp->isys, isp->psys };
+	irqreturn_t ret = IRQ_NONE;
+	unsigned int i;
+
+	dev_dbg(&isp->pdev->dev, "isr: Buttress threaded interrupt handler\n");
+
+	for (i = 0; i < ARRAY_SIZE(ipu_adev_irq_mask); i++) {
+		if (adev[i] && adev[i]->adrv &&
+		    adev[i]->adrv->wake_isr_thread &&
+		    adev[i]->adrv->isr_threaded(adev[i]) == IRQ_HANDLED)
+			ret = IRQ_HANDLED;
+	}
+
+	writel(BUTTRESS_IRQS, isp->base + BUTTRESS_REG_ISR_ENABLE);
+
+	return ret;
+}
+
+int ipu_buttress_power(struct device *dev,
+		       struct ipu_buttress_ctrl *ctrl, bool on)
+{
+	struct ipu_device *isp = to_ipu_bus_device(dev)->isp;
+	u32 pwr_sts, val;
+	int ret = 0;
+
+	if (!ctrl)
+		return 0;
+
+	/* Until FLR completion nothing is expected to work */
+	if (isp->flr_done)
+		return 0;
+
+	mutex_lock(&isp->buttress.power_mutex);
+
+	if (!on) {
+		val = 0;
+		pwr_sts = ctrl->pwr_sts_off << ctrl->pwr_sts_shift;
+	} else {
+		val = BUTTRESS_FREQ_CTL_START |
+			ctrl->divisor << ctrl->divisor_shift |
+			ctrl->qos_floor << BUTTRESS_FREQ_CTL_QOS_FLOOR_SHIFT |
+			BUTTRESS_FREQ_CTL_ICCMAX_LEVEL;
+
+		pwr_sts = ctrl->pwr_sts_on << ctrl->pwr_sts_shift;
+	}
+
+	writel(val, isp->base + ctrl->freq_ctl);
+
+	ret = readl_poll_timeout(isp->base + BUTTRESS_REG_PWR_STATE,
+				 val, ((val & ctrl->pwr_sts_mask) == pwr_sts),
+				 100, BUTTRESS_POWER_TIMEOUT);
+	if (ret)
+		dev_err(&isp->pdev->dev,
+			"Change power status timeout with 0x%x\n", val);
+
+	ctrl->started = !ret && on;
+
+	mutex_unlock(&isp->buttress.power_mutex);
+
+	return ret;
+}
+
+static bool secure_mode_enable = 1;
+module_param(secure_mode_enable, bool, 0660);
+MODULE_PARM_DESC(secure_mode, "IPU secure mode enable");
+
+void ipu_buttress_set_secure_mode(struct ipu_device *isp)
+{
+	u8 retry = 100;
+	u32 val, read;
+
+	/*
+	 * HACK to disable possible secure mode. This can be
+	 * reverted when CSE is disabling the secure mode
+	 */
+	read = readl(isp->base + BUTTRESS_REG_SECURITY_CTL);
+
+	if (secure_mode_enable)
+		val = read |= BUTTRESS_SECURITY_CTL_FW_SECURE_MODE;
+	else
+		val = read & ~BUTTRESS_SECURITY_CTL_FW_SECURE_MODE;
+
+	if (val == read)
+		return;
+
+	writel(val, isp->base + BUTTRESS_REG_SECURITY_CTL);
+
+	/* In B0, for some registers in buttress, because of a hw bug, write
+	 * might not succeed at first attempt. Write twice until the
+	 * write is successful
+	 */
+	writel(val, isp->base + BUTTRESS_REG_SECURITY_CTL);
+
+	while (retry--) {
+		read = readl(isp->base + BUTTRESS_REG_SECURITY_CTL);
+		if (read == val)
+			break;
+
+		writel(val, isp->base + BUTTRESS_REG_SECURITY_CTL);
+
+		if (retry == 0)
+			dev_err(&isp->pdev->dev,
+				"update security control register failed\n");
+	}
+}
+
+bool ipu_buttress_get_secure_mode(struct ipu_device *isp)
+{
+	u32 val;
+
+	val = readl(isp->base + BUTTRESS_REG_SECURITY_CTL);
+
+	return val & BUTTRESS_SECURITY_CTL_FW_SECURE_MODE;
+}
+
+bool ipu_buttress_auth_done(struct ipu_device *isp)
+{
+	u32 val;
+
+	if (!isp->secure_mode)
+		return 1;
+
+	val = readl(isp->base + BUTTRESS_REG_SECURITY_CTL);
+
+	return (val & BUTTRESS_SECURITY_CTL_FW_SETUP_MASK) ==
+	    BUTTRESS_SECURITY_CTL_AUTH_DONE;
+}
+EXPORT_SYMBOL(ipu_buttress_auth_done);
+
+static void ipu_buttress_set_psys_ratio(struct ipu_device *isp,
+					unsigned int psys_divisor,
+					unsigned int psys_qos_floor)
+{
+	struct ipu_buttress_ctrl *ctrl = isp->psys->ctrl;
+
+	mutex_lock(&isp->buttress.power_mutex);
+
+	if (ctrl->divisor == psys_divisor && ctrl->qos_floor == psys_qos_floor)
+		goto out_mutex_unlock;
+
+	ctrl->divisor = psys_divisor;
+	ctrl->qos_floor = psys_qos_floor;
+
+	if (ctrl->started) {
+		/*
+		 * According to documentation driver initiates DVFS
+		 * transition by writing wanted ratio, floor ratio and start
+		 * bit. No need to stop PS first
+		 */
+		writel(BUTTRESS_FREQ_CTL_START |
+		       ctrl->qos_floor << BUTTRESS_FREQ_CTL_QOS_FLOOR_SHIFT |
+		       psys_divisor, isp->base + BUTTRESS_REG_PS_FREQ_CTL);
+	}
+
+out_mutex_unlock:
+	mutex_unlock(&isp->buttress.power_mutex);
+}
+
+static void ipu_buttress_set_isys_ratio(struct ipu_device *isp,
+					unsigned int isys_divisor)
+{
+	struct ipu_buttress_ctrl *ctrl = isp->isys->ctrl;
+
+	mutex_lock(&isp->buttress.power_mutex);
+
+	if (ctrl->divisor == isys_divisor)
+		goto out_mutex_unlock;
+
+	ctrl->divisor = isys_divisor;
+
+	if (ctrl->started) {
+		writel(BUTTRESS_FREQ_CTL_START |
+		       ctrl->qos_floor << BUTTRESS_FREQ_CTL_QOS_FLOOR_SHIFT |
+		       isys_divisor, isp->base + BUTTRESS_REG_IS_FREQ_CTL);
+	}
+
+out_mutex_unlock:
+	mutex_unlock(&isp->buttress.power_mutex);
+}
+
+static void ipu_buttress_set_psys_freq(struct ipu_device *isp,
+				       unsigned int freq)
+{
+	unsigned int psys_ratio = freq / BUTTRESS_PS_FREQ_STEP;
+
+	if (isp->buttress.psys_force_ratio)
+		return;
+
+	ipu_buttress_set_psys_ratio(isp, psys_ratio, psys_ratio);
+}
+
+void
+ipu_buttress_add_psys_constraint(struct ipu_device *isp,
+				 struct ipu_buttress_constraint *constraint)
+{
+	struct ipu_buttress *b = &isp->buttress;
+
+	mutex_lock(&b->cons_mutex);
+	list_add(&constraint->list, &b->constraints);
+
+	if (constraint->min_freq > b->psys_min_freq) {
+		isp->buttress.psys_min_freq = min(constraint->min_freq,
+						  b->psys_fused_freqs.max_freq);
+		ipu_buttress_set_psys_freq(isp, b->psys_min_freq);
+	}
+	mutex_unlock(&b->cons_mutex);
+}
+EXPORT_SYMBOL_GPL(ipu_buttress_add_psys_constraint);
+
+void
+ipu_buttress_remove_psys_constraint(struct ipu_device *isp,
+				    struct ipu_buttress_constraint *constraint)
+{
+	struct ipu_buttress *b = &isp->buttress;
+	struct ipu_buttress_constraint *c;
+	unsigned int min_freq = 0;
+
+	mutex_lock(&b->cons_mutex);
+	list_del(&constraint->list);
+
+	if (constraint->min_freq >= b->psys_min_freq) {
+		list_for_each_entry(c, &b->constraints, list)
+			if (c->min_freq > min_freq)
+				min_freq = c->min_freq;
+
+		b->psys_min_freq = clamp(min_freq,
+					 b->psys_fused_freqs.efficient_freq,
+					 b->psys_fused_freqs.max_freq);
+		ipu_buttress_set_psys_freq(isp, b->psys_min_freq);
+	}
+	mutex_unlock(&b->cons_mutex);
+}
+EXPORT_SYMBOL_GPL(ipu_buttress_remove_psys_constraint);
+
+int ipu_buttress_reset_authentication(struct ipu_device *isp)
+{
+	int ret;
+	u32 val;
+
+	if (!isp->secure_mode) {
+		dev_dbg(&isp->pdev->dev,
+			"Non-secure mode -> skip authentication\n");
+		return 0;
+	}
+
+	writel(BUTTRESS_FW_RESET_CTL_START, isp->base +
+	       BUTTRESS_REG_FW_RESET_CTL);
+
+	ret = readl_poll_timeout(isp->base + BUTTRESS_REG_FW_RESET_CTL, val,
+				 val & BUTTRESS_FW_RESET_CTL_DONE, 500,
+				 BUTTRESS_CSE_FWRESET_TIMEOUT);
+	if (ret) {
+		dev_err(&isp->pdev->dev,
+			"Time out while resetting authentication state\n");
+	} else {
+		dev_info(&isp->pdev->dev,
+			 "FW reset for authentication done\n");
+		writel(0, isp->base + BUTTRESS_REG_FW_RESET_CTL);
+		/* leave some time for HW restore */
+		usleep_range(800, 1000);
+	}
+
+	return ret;
+}
+
+int ipu_buttress_map_fw_image(struct ipu_bus_device *sys,
+			      const struct firmware *fw, struct sg_table *sgt)
+{
+	struct page **pages;
+	const void *addr;
+	unsigned long n_pages, i;
+	int rval;
+
+	n_pages = PAGE_ALIGN(fw->size) >> PAGE_SHIFT;
+
+	pages = kmalloc_array(n_pages, sizeof(*pages), GFP_KERNEL);
+	if (!pages)
+		return -ENOMEM;
+
+	addr = fw->data;
+	for (i = 0; i < n_pages; i++) {
+		struct page *p = vmalloc_to_page(addr);
+
+		if (!p) {
+			rval = -ENODEV;
+			goto out;
+		}
+		pages[i] = p;
+		addr += PAGE_SIZE;
+	}
+
+	rval = sg_alloc_table_from_pages(sgt, pages, n_pages, 0, fw->size,
+					 GFP_KERNEL);
+	if (rval) {
+		rval = -ENOMEM;
+		goto out;
+	}
+
+	n_pages = dma_map_sg(&sys->dev, sgt->sgl, sgt->nents, DMA_TO_DEVICE);
+	if (n_pages != sgt->nents) {
+		rval = -ENOMEM;
+		sg_free_table(sgt);
+		goto out;
+	}
+
+	dma_sync_sg_for_device(&sys->dev, sgt->sgl, sgt->nents, DMA_TO_DEVICE);
+
+out:
+	kfree(pages);
+
+	return rval;
+}
+EXPORT_SYMBOL_GPL(ipu_buttress_map_fw_image);
+
+int ipu_buttress_unmap_fw_image(struct ipu_bus_device *sys,
+				struct sg_table *sgt)
+{
+	dma_unmap_sg(&sys->dev, sgt->sgl, sgt->nents, DMA_TO_DEVICE);
+	sg_free_table(sgt);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ipu_buttress_unmap_fw_image);
+
+int ipu_buttress_authenticate(struct ipu_device *isp)
+{
+	struct ipu_psys_pdata *psys_pdata;
+	struct ipu_buttress *b = &isp->buttress;
+	u32 data, mask, done, fail;
+	int rval;
+
+	if (!isp->secure_mode) {
+		dev_dbg(&isp->pdev->dev,
+			"Non-secure mode -> skip authentication\n");
+		return 0;
+	}
+
+	psys_pdata = isp->psys->pdata;
+
+	mutex_lock(&b->auth_mutex);
+
+	if (ipu_buttress_auth_done(isp)) {
+		rval = 0;
+		goto iunit_power_off;
+	}
+
+	/*
+	 * Write address of FIT table to FW_SOURCE register
+	 * Let's use fw address. I.e. not using FIT table yet
+	 */
+	data = lower_32_bits(isp->pkg_dir_dma_addr);
+	writel(data, isp->base + BUTTRESS_REG_FW_SOURCE_BASE_LO);
+
+	data = upper_32_bits(isp->pkg_dir_dma_addr);
+	writel(data, isp->base + BUTTRESS_REG_FW_SOURCE_BASE_HI);
+
+	/*
+	 * Write boot_load into IU2CSEDATA0
+	 * Write sizeof(boot_load) | 0x2 << CLIENT_ID to
+	 * IU2CSEDB.IU2CSECMD and set IU2CSEDB.IU2CSEBUSY as
+	 */
+	dev_info(&isp->pdev->dev, "Sending BOOT_LOAD to CSE\n");
+	rval = ipu_buttress_ipc_send(isp, IPU_BUTTRESS_IPC_CSE,
+				     BUTTRESS_IU2CSEDATA0_IPC_BOOT_LOAD,
+				     1, 1,
+				     BUTTRESS_CSE2IUDATA0_IPC_BOOT_LOAD_DONE);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "CSE boot_load failed\n");
+		goto iunit_power_off;
+	}
+
+	mask = BUTTRESS_SECURITY_CTL_FW_SETUP_MASK;
+	done = BUTTRESS_SECURITY_CTL_FW_SETUP_DONE;
+	fail = BUTTRESS_SECURITY_CTL_AUTH_FAILED;
+	rval = readl_poll_timeout(isp->base + BUTTRESS_REG_SECURITY_CTL, data,
+				  ((data & mask) == done ||
+				   (data & mask) == fail), 500,
+				  BUTTRESS_CSE_BOOTLOAD_TIMEOUT);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "CSE boot_load timeout\n");
+		goto iunit_power_off;
+	}
+
+	data = readl(isp->base + BUTTRESS_REG_SECURITY_CTL) & mask;
+	if (data == fail) {
+		dev_err(&isp->pdev->dev, "CSE auth failed\n");
+		rval = -EINVAL;
+		goto iunit_power_off;
+	}
+
+	rval = readl_poll_timeout(psys_pdata->base + BOOTLOADER_STATUS_OFFSET,
+				  data, data == BOOTLOADER_MAGIC_KEY, 500,
+				  BUTTRESS_CSE_BOOTLOAD_TIMEOUT);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Expect magic number timeout 0x%x\n",
+			data);
+		goto iunit_power_off;
+	}
+
+	/*
+	 * Write authenticate_run into IU2CSEDATA0
+	 * Write sizeof(boot_load) | 0x2 << CLIENT_ID to
+	 * IU2CSEDB.IU2CSECMD and set IU2CSEDB.IU2CSEBUSY as
+	 */
+	dev_info(&isp->pdev->dev, "Sending AUTHENTICATE_RUN to CSE\n");
+	rval = ipu_buttress_ipc_send(isp, IPU_BUTTRESS_IPC_CSE,
+				     BUTTRESS_IU2CSEDATA0_IPC_AUTH_RUN,
+				     1, 1,
+				     BUTTRESS_CSE2IUDATA0_IPC_AUTH_RUN_DONE);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "CSE authenticate_run failed\n");
+		goto iunit_power_off;
+	}
+
+	done = BUTTRESS_SECURITY_CTL_AUTH_DONE;
+	rval = readl_poll_timeout(isp->base + BUTTRESS_REG_SECURITY_CTL, data,
+				  ((data & mask) == done ||
+				   (data & mask) == fail), 500,
+				  BUTTRESS_CSE_AUTHENTICATE_TIMEOUT);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "CSE authenticate timeout\n");
+		goto iunit_power_off;
+	}
+
+	data = readl(isp->base + BUTTRESS_REG_SECURITY_CTL) & mask;
+	if (data == fail) {
+		dev_err(&isp->pdev->dev, "CSE boot_load failed\n");
+		rval = -EINVAL;
+		goto iunit_power_off;
+	}
+
+	dev_info(&isp->pdev->dev, "CSE authenticate_run done\n");
+
+iunit_power_off:
+	mutex_unlock(&b->auth_mutex);
+
+	return rval;
+}
+
+static int ipu_buttress_send_tsc_request(struct ipu_device *isp)
+{
+	u32 val, mask, shift, done;
+	int ret;
+
+	mask = BUTTRESS_PWR_STATE_HH_STATUS_MASK;
+	shift = BUTTRESS_PWR_STATE_HH_STATUS_SHIFT;
+
+	writel(BUTTRESS_FABRIC_CMD_START_TSC_SYNC,
+	       isp->base + BUTTRESS_REG_FABRIC_CMD);
+
+	val = readl(isp->base + BUTTRESS_REG_PWR_STATE);
+	val = (val & mask) >> shift;
+	if (val == BUTTRESS_PWR_STATE_HH_STATE_ERR) {
+		dev_err(&isp->pdev->dev, "Start tsc sync failed\n");
+		return -EINVAL;
+	}
+
+	done = BUTTRESS_PWR_STATE_HH_STATE_DONE;
+	ret = readl_poll_timeout(isp->base + BUTTRESS_REG_PWR_STATE, val,
+				 ((val & mask) >> shift == done), 500,
+				 BUTTRESS_TSC_SYNC_TIMEOUT);
+	if (ret)
+		dev_err(&isp->pdev->dev, "Start tsc sync timeout\n");
+
+	return ret;
+}
+
+int ipu_buttress_start_tsc_sync(struct ipu_device *isp)
+{
+	unsigned int i;
+
+	for (i = 0; i < BUTTRESS_TSC_SYNC_RESET_TRIAL_MAX; i++) {
+		int ret;
+
+		ret = ipu_buttress_send_tsc_request(isp);
+		if (ret == -ETIMEDOUT) {
+			u32 val;
+			/* set tsw soft reset */
+			val = readl(isp->base + BUTTRESS_REG_TSW_CTL);
+			val = val | BUTTRESS_TSW_CTL_SOFT_RESET;
+			writel(val, isp->base + BUTTRESS_REG_TSW_CTL);
+			/* clear tsw soft reset */
+			val = val & (~BUTTRESS_TSW_CTL_SOFT_RESET);
+			writel(val, isp->base + BUTTRESS_REG_TSW_CTL);
+
+			continue;
+		}
+		return ret;
+	}
+
+	dev_err(&isp->pdev->dev, "TSC sync failed(timeout)\n");
+
+	return -ETIMEDOUT;
+}
+EXPORT_SYMBOL(ipu_buttress_start_tsc_sync);
+
+struct clk_ipu_sensor {
+	struct ipu_device *isp;
+	struct clk_hw hw;
+	unsigned int id;
+	unsigned long rate;
+};
+
+#define to_clk_ipu_sensor(_hw) container_of(_hw, struct clk_ipu_sensor, hw)
+
+int ipu_buttress_tsc_read(struct ipu_device *isp, u64 *val)
+{
+	u32 tsc_hi_1, tsc_hi_2, tsc_lo;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	tsc_hi_1 = readl(isp->base + BUTTRESS_REG_TSC_HI);
+	tsc_lo = readl(isp->base + BUTTRESS_REG_TSC_LO);
+	tsc_hi_2 = readl(isp->base + BUTTRESS_REG_TSC_HI);
+	if (tsc_hi_1 == tsc_hi_2) {
+		*val = (u64)tsc_hi_1 << 32 | tsc_lo;
+	} else {
+		/* Check if TSC has rolled over */
+		if (tsc_lo & BIT(31))
+			*val = (u64)tsc_hi_1 << 32 | tsc_lo;
+		else
+			*val = (u64)tsc_hi_2 << 32 | tsc_lo;
+	}
+	local_irq_restore(flags);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ipu_buttress_tsc_read);
+
+#ifdef CONFIG_DEBUG_FS
+
+static int ipu_buttress_reg_open(struct inode *inode, struct file *file)
+{
+	if (!inode->i_private)
+		return -EACCES;
+
+	file->private_data = inode->i_private;
+	return 0;
+}
+
+static ssize_t ipu_buttress_reg_read(struct file *file, char __user *buf,
+				     size_t count, loff_t *ppos)
+{
+	struct debugfs_reg32 *reg = file->private_data;
+	u8 tmp[11];
+	u32 val = readl((void __iomem *)reg->offset);
+	int len = scnprintf(tmp, sizeof(tmp), "0x%08x", val);
+
+	return simple_read_from_buffer(buf, len, ppos, &tmp, len);
+}
+
+static ssize_t ipu_buttress_reg_write(struct file *file,
+				      const char __user *buf,
+				      size_t count, loff_t *ppos)
+{
+	struct debugfs_reg32 *reg = file->private_data;
+	u32 val;
+	int rval;
+
+	rval = kstrtou32_from_user(buf, count, 0, &val);
+	if (rval)
+		return rval;
+
+	writel(val, (void __iomem *)reg->offset);
+
+	return count;
+}
+
+static struct debugfs_reg32 buttress_regs[] = {
+	{"IU2CSEDB0", BUTTRESS_REG_IU2CSEDB0},
+	{"IU2CSEDATA0", BUTTRESS_REG_IU2CSEDATA0},
+	{"CSE2IUDB0", BUTTRESS_REG_CSE2IUDB0},
+	{"CSE2IUDATA0", BUTTRESS_REG_CSE2IUDATA0},
+	{"CSE2IUCSR", BUTTRESS_REG_CSE2IUCSR},
+	{"IU2CSECSR", BUTTRESS_REG_IU2CSECSR},
+};
+
+static const struct file_operations ipu_buttress_reg_fops = {
+	.owner = THIS_MODULE,
+	.open = ipu_buttress_reg_open,
+	.read = ipu_buttress_reg_read,
+	.write = ipu_buttress_reg_write,
+};
+
+static int ipu_buttress_start_tsc_sync_set(void *data, u64 val)
+{
+	struct ipu_device *isp = data;
+
+	return ipu_buttress_start_tsc_sync(isp);
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(ipu_buttress_start_tsc_sync_fops, NULL,
+			ipu_buttress_start_tsc_sync_set, "%llu\n");
+
+static int ipu_buttress_tsc_get(void *data, u64 *val)
+{
+	return ipu_buttress_tsc_read(data, val);
+}
+DEFINE_SIMPLE_ATTRIBUTE(ipu_buttress_tsc_fops, ipu_buttress_tsc_get,
+			NULL, "%llu\n");
+
+static int ipu_buttress_psys_force_freq_get(void *data, u64 *val)
+{
+	struct ipu_device *isp = data;
+
+	*val = isp->buttress.psys_force_ratio * BUTTRESS_PS_FREQ_STEP;
+
+	return 0;
+}
+
+static int ipu_buttress_psys_force_freq_set(void *data, u64 val)
+{
+	struct ipu_device *isp = data;
+
+	if (val && (val < BUTTRESS_MIN_FORCE_PS_FREQ ||
+		    val > BUTTRESS_MAX_FORCE_PS_FREQ))
+		return -EINVAL;
+
+	do_div(val, BUTTRESS_PS_FREQ_STEP);
+	isp->buttress.psys_force_ratio = val;
+
+	if (isp->buttress.psys_force_ratio)
+		ipu_buttress_set_psys_ratio(isp,
+					    isp->buttress.psys_force_ratio,
+					    isp->buttress.psys_force_ratio);
+	else
+		ipu_buttress_set_psys_freq(isp, isp->buttress.psys_min_freq);
+
+	return 0;
+}
+
+static int ipu_buttress_isys_freq_get(void *data, u64 *val)
+{
+	struct ipu_device *isp = data;
+	u32 reg_val;
+	int rval;
+
+	rval = pm_runtime_get_sync(&isp->isys->dev);
+	if (rval < 0) {
+		pm_runtime_put(&isp->isys->dev);
+		dev_err(&isp->pdev->dev, "Runtime PM failed (%d)\n", rval);
+		return rval;
+	}
+
+	reg_val = readl(isp->base + BUTTRESS_REG_IS_FREQ_CTL);
+
+	pm_runtime_put(&isp->isys->dev);
+
+	*val = IPU_IS_FREQ_RATIO_BASE *
+	    (reg_val & IPU_BUTTRESS_IS_FREQ_CTL_DIVISOR_MASK);
+
+	return 0;
+}
+
+static int ipu_buttress_isys_freq_set(void *data, u64 val)
+{
+	struct ipu_device *isp = data;
+	int rval;
+
+	if (val < BUTTRESS_MIN_FORCE_IS_FREQ ||
+	    val > BUTTRESS_MAX_FORCE_IS_FREQ)
+		return -EINVAL;
+
+	rval = pm_runtime_get_sync(&isp->isys->dev);
+	if (rval < 0) {
+		pm_runtime_put(&isp->isys->dev);
+		dev_err(&isp->pdev->dev, "Runtime PM failed (%d)\n", rval);
+		return rval;
+	}
+
+	do_div(val, BUTTRESS_IS_FREQ_STEP);
+	if (val)
+		ipu_buttress_set_isys_ratio(isp, val);
+
+	pm_runtime_put(&isp->isys->dev);
+
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(ipu_buttress_psys_force_freq_fops,
+			ipu_buttress_psys_force_freq_get,
+			ipu_buttress_psys_force_freq_set, "%llu\n");
+
+DEFINE_SIMPLE_ATTRIBUTE(ipu_buttress_psys_freq_fops,
+			ipu_buttress_psys_freq_get, NULL, "%llu\n");
+
+DEFINE_SIMPLE_ATTRIBUTE(ipu_buttress_isys_freq_fops,
+			ipu_buttress_isys_freq_get,
+			ipu_buttress_isys_freq_set, "%llu\n");
+
+int ipu_buttress_debugfs_init(struct ipu_device *isp)
+{
+	struct debugfs_reg32 *reg =
+	    devm_kcalloc(&isp->pdev->dev, ARRAY_SIZE(buttress_regs),
+			 sizeof(*reg), GFP_KERNEL);
+	struct dentry *dir, *file;
+	int i;
+
+	if (!reg)
+		return -ENOMEM;
+
+	dir = debugfs_create_dir("buttress", isp->ipu_dir);
+	if (!dir)
+		return -ENOMEM;
+
+	for (i = 0; i < ARRAY_SIZE(buttress_regs); i++, reg++) {
+		reg->offset = (unsigned long)isp->base +
+		    buttress_regs[i].offset;
+		reg->name = buttress_regs[i].name;
+		file = debugfs_create_file(reg->name, 0700,
+					   dir, reg, &ipu_buttress_reg_fops);
+		if (!file)
+			goto err;
+	}
+
+	file = debugfs_create_file("start_tsc_sync", 0200, dir, isp,
+				   &ipu_buttress_start_tsc_sync_fops);
+	if (!file)
+		goto err;
+	file = debugfs_create_file("tsc", 0400, dir, isp,
+				   &ipu_buttress_tsc_fops);
+	if (!file)
+		goto err;
+	file = debugfs_create_file("psys_force_freq", 0700, dir, isp,
+				   &ipu_buttress_psys_force_freq_fops);
+	if (!file)
+		goto err;
+
+	file = debugfs_create_file("psys_freq", 0400, dir, isp,
+				   &ipu_buttress_psys_freq_fops);
+	if (!file)
+		goto err;
+
+	file = debugfs_create_file("isys_freq", 0700, dir, isp,
+				   &ipu_buttress_isys_freq_fops);
+	if (!file)
+		goto err;
+
+	return 0;
+err:
+	debugfs_remove_recursive(dir);
+	return -ENOMEM;
+}
+
+#endif /* CONFIG_DEBUG_FS */
+
+u64 ipu_buttress_tsc_ticks_to_ns(u64 ticks)
+{
+	u64 ns = ticks * 10000;
+	/*
+	 * TSC clock frequency is 19.2MHz,
+	 * converting TSC tick count to ns is calculated by:
+	 * ns = ticks * 1000 000 000 / 19.2Mhz
+	 *    = ticks * 1000 000 000 / 19200000Hz
+	 *    = ticks * 10000 / 192 ns
+	 */
+	do_div(ns, 192);
+
+	return ns;
+}
+EXPORT_SYMBOL_GPL(ipu_buttress_tsc_ticks_to_ns);
+
+static ssize_t psys_fused_min_freq_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	struct ipu_device *isp = pci_get_drvdata(to_pci_dev(dev));
+
+	return snprintf(buf, PAGE_SIZE, "%u\n",
+			isp->buttress.psys_fused_freqs.min_freq);
+}
+
+static DEVICE_ATTR_RO(psys_fused_min_freq);
+
+static ssize_t psys_fused_max_freq_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	struct ipu_device *isp = pci_get_drvdata(to_pci_dev(dev));
+
+	return snprintf(buf, PAGE_SIZE, "%u\n",
+			isp->buttress.psys_fused_freqs.max_freq);
+}
+
+static DEVICE_ATTR_RO(psys_fused_max_freq);
+
+static ssize_t psys_fused_efficient_freq_show(struct device *dev,
+					      struct device_attribute *attr,
+					      char *buf)
+{
+	struct ipu_device *isp = pci_get_drvdata(to_pci_dev(dev));
+
+	return snprintf(buf, PAGE_SIZE, "%u\n",
+			isp->buttress.psys_fused_freqs.efficient_freq);
+}
+
+static DEVICE_ATTR_RO(psys_fused_efficient_freq);
+
+int ipu_buttress_restore(struct ipu_device *isp)
+{
+	struct ipu_buttress *b = &isp->buttress;
+
+	writel(BUTTRESS_IRQS, isp->base + BUTTRESS_REG_ISR_CLEAR);
+	writel(BUTTRESS_IRQS, isp->base + BUTTRESS_REG_ISR_ENABLE);
+	writel(b->wdt_cached_value, isp->base + BUTTRESS_REG_WDT);
+
+	return 0;
+}
+
+int ipu_buttress_init(struct ipu_device *isp)
+{
+	struct ipu_buttress *b = &isp->buttress;
+	int rval, ipc_reset_retry = BUTTRESS_CSE_IPC_RESET_RETRY;
+
+	mutex_init(&b->power_mutex);
+	mutex_init(&b->auth_mutex);
+	mutex_init(&b->cons_mutex);
+	mutex_init(&b->ipc_mutex);
+	init_completion(&b->ish.send_complete);
+	init_completion(&b->cse.send_complete);
+	init_completion(&b->ish.recv_complete);
+	init_completion(&b->cse.recv_complete);
+
+	b->cse.nack = BUTTRESS_CSE2IUDATA0_IPC_NACK;
+	b->cse.nack_mask = BUTTRESS_CSE2IUDATA0_IPC_NACK_MASK;
+	b->cse.csr_in = BUTTRESS_REG_CSE2IUCSR;
+	b->cse.csr_out = BUTTRESS_REG_IU2CSECSR;
+	b->cse.db0_in = BUTTRESS_REG_CSE2IUDB0;
+	b->cse.db0_out = BUTTRESS_REG_IU2CSEDB0;
+	b->cse.data0_in = BUTTRESS_REG_CSE2IUDATA0;
+	b->cse.data0_out = BUTTRESS_REG_IU2CSEDATA0;
+
+	/* no ISH on IPU6 */
+	memset(&b->ish, 0, sizeof(b->ish));
+	INIT_LIST_HEAD(&b->constraints);
+
+	ipu_buttress_set_secure_mode(isp);
+	isp->secure_mode = ipu_buttress_get_secure_mode(isp);
+	if (isp->secure_mode != secure_mode_enable)
+		dev_warn(&isp->pdev->dev, "Unable to set secure mode\n");
+
+	dev_info(&isp->pdev->dev, "IPU in %s mode\n",
+		 isp->secure_mode ? "secure" : "non-secure");
+
+	b->wdt_cached_value = readl(isp->base + BUTTRESS_REG_WDT);
+	writel(BUTTRESS_IRQS, isp->base + BUTTRESS_REG_ISR_CLEAR);
+	writel(BUTTRESS_IRQS, isp->base + BUTTRESS_REG_ISR_ENABLE);
+
+	rval = device_create_file(&isp->pdev->dev,
+				  &dev_attr_psys_fused_min_freq);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Create min freq file failed\n");
+		goto err_mutex_destroy;
+	}
+
+	rval = device_create_file(&isp->pdev->dev,
+				  &dev_attr_psys_fused_max_freq);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Create max freq file failed\n");
+		goto err_remove_min_freq_file;
+	}
+
+	rval = device_create_file(&isp->pdev->dev,
+				  &dev_attr_psys_fused_efficient_freq);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Create efficient freq file failed\n");
+		goto err_remove_max_freq_file;
+	}
+
+	/*
+	 * We want to retry couple of time in case CSE initialization
+	 * is delayed for reason or another.
+	 */
+	do {
+		rval = ipu_buttress_ipc_reset(isp, &b->cse);
+		if (rval) {
+			dev_warn(&isp->pdev->dev,
+				 "IPC reset protocol failed, retrying\n");
+		} else {
+			dev_info(&isp->pdev->dev, "IPC reset done\n");
+			return 0;
+		}
+	} while (ipc_reset_retry--);
+
+	dev_err(&isp->pdev->dev, "IPC reset protocol failed\n");
+
+err_remove_max_freq_file:
+	device_remove_file(&isp->pdev->dev, &dev_attr_psys_fused_max_freq);
+err_remove_min_freq_file:
+	device_remove_file(&isp->pdev->dev, &dev_attr_psys_fused_min_freq);
+err_mutex_destroy:
+	mutex_destroy(&b->power_mutex);
+	mutex_destroy(&b->auth_mutex);
+	mutex_destroy(&b->cons_mutex);
+	mutex_destroy(&b->ipc_mutex);
+
+	return rval;
+}
+
+void ipu_buttress_exit(struct ipu_device *isp)
+{
+	struct ipu_buttress *b = &isp->buttress;
+
+	writel(0, isp->base + BUTTRESS_REG_ISR_ENABLE);
+
+	device_remove_file(&isp->pdev->dev,
+			   &dev_attr_psys_fused_efficient_freq);
+	device_remove_file(&isp->pdev->dev, &dev_attr_psys_fused_max_freq);
+	device_remove_file(&isp->pdev->dev, &dev_attr_psys_fused_min_freq);
+
+	mutex_destroy(&b->power_mutex);
+	mutex_destroy(&b->auth_mutex);
+	mutex_destroy(&b->cons_mutex);
+	mutex_destroy(&b->ipc_mutex);
+}
diff -ruN a/drivers/media/pci/intel/ipu-buttress.h b/drivers/media/pci/intel/ipu-buttress.h
--- a/drivers/media/pci/intel/ipu-buttress.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-buttress.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,128 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_BUTTRESS_H
+#define IPU_BUTTRESS_H
+
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include "ipu.h"
+
+#define IPU_BUTTRESS_NUM_OF_SENS_CKS	3
+#define IPU_BUTTRESS_NUM_OF_PLL_CKS	3
+#define IPU_BUTTRESS_TSC_CLK		19200000
+
+#define BUTTRESS_POWER_TIMEOUT			200000
+
+#define BUTTRESS_PS_FREQ_STEP		25U
+#define BUTTRESS_MIN_FORCE_PS_FREQ	(BUTTRESS_PS_FREQ_STEP * 8)
+#define BUTTRESS_MAX_FORCE_PS_FREQ	(BUTTRESS_PS_FREQ_STEP * 32)
+
+#define BUTTRESS_IS_FREQ_STEP		25U
+#define BUTTRESS_MIN_FORCE_IS_FREQ	(BUTTRESS_IS_FREQ_STEP * 8)
+#define BUTTRESS_MAX_FORCE_IS_FREQ	(BUTTRESS_IS_FREQ_STEP * 16)
+
+struct ipu_buttress_ctrl {
+	u32 freq_ctl, pwr_sts_shift, pwr_sts_mask, pwr_sts_on, pwr_sts_off;
+	union {
+		unsigned int divisor;
+		unsigned int ratio;
+	};
+	union {
+		unsigned int divisor_shift;
+		unsigned int ratio_shift;
+	};
+	unsigned int qos_floor;
+	bool started;
+};
+
+struct ipu_buttress_fused_freqs {
+	unsigned int min_freq;
+	unsigned int max_freq;
+	unsigned int efficient_freq;
+};
+
+struct ipu_buttress_ipc {
+	struct completion send_complete;
+	struct completion recv_complete;
+	u32 nack;
+	u32 nack_mask;
+	u32 recv_data;
+	u32 csr_out;
+	u32 csr_in;
+	u32 db0_in;
+	u32 db0_out;
+	u32 data0_out;
+	u32 data0_in;
+};
+
+struct ipu_buttress {
+	struct mutex power_mutex, auth_mutex, cons_mutex, ipc_mutex;
+	struct ipu_buttress_ipc cse;
+	struct ipu_buttress_ipc ish;
+	struct list_head constraints;
+	struct ipu_buttress_fused_freqs psys_fused_freqs;
+	unsigned int psys_min_freq;
+	u32 wdt_cached_value;
+	u8 psys_force_ratio;
+	bool force_suspend;
+};
+
+struct ipu_buttress_sensor_clk_freq {
+	unsigned int rate;
+	unsigned int val;
+};
+
+struct firmware;
+
+enum ipu_buttress_ipc_domain {
+	IPU_BUTTRESS_IPC_CSE,
+	IPU_BUTTRESS_IPC_ISH,
+};
+
+struct ipu_buttress_constraint {
+	struct list_head list;
+	unsigned int min_freq;
+};
+
+struct ipu_ipc_buttress_bulk_msg {
+	u32 cmd;
+	u32 expected_resp;
+	bool require_resp;
+	u8 cmd_size;
+};
+
+int ipu_buttress_ipc_reset(struct ipu_device *isp,
+			   struct ipu_buttress_ipc *ipc);
+int ipu_buttress_map_fw_image(struct ipu_bus_device *sys,
+			      const struct firmware *fw, struct sg_table *sgt);
+int ipu_buttress_unmap_fw_image(struct ipu_bus_device *sys,
+				struct sg_table *sgt);
+int ipu_buttress_power(struct device *dev,
+		       struct ipu_buttress_ctrl *ctrl, bool on);
+void
+ipu_buttress_add_psys_constraint(struct ipu_device *isp,
+				 struct ipu_buttress_constraint *constraint);
+void
+ipu_buttress_remove_psys_constraint(struct ipu_device *isp,
+				    struct ipu_buttress_constraint *constraint);
+void ipu_buttress_set_secure_mode(struct ipu_device *isp);
+bool ipu_buttress_get_secure_mode(struct ipu_device *isp);
+int ipu_buttress_authenticate(struct ipu_device *isp);
+int ipu_buttress_reset_authentication(struct ipu_device *isp);
+bool ipu_buttress_auth_done(struct ipu_device *isp);
+int ipu_buttress_start_tsc_sync(struct ipu_device *isp);
+int ipu_buttress_tsc_read(struct ipu_device *isp, u64 *val);
+u64 ipu_buttress_tsc_ticks_to_ns(u64 ticks);
+
+irqreturn_t ipu_buttress_isr(int irq, void *isp_ptr);
+irqreturn_t ipu_buttress_isr_threaded(int irq, void *isp_ptr);
+int ipu_buttress_debugfs_init(struct ipu_device *isp);
+int ipu_buttress_init(struct ipu_device *isp);
+void ipu_buttress_exit(struct ipu_device *isp);
+void ipu_buttress_csi_port_config(struct ipu_device *isp,
+				  u32 legacy, u32 combo);
+int ipu_buttress_restore(struct ipu_device *isp);
+
+int ipu_buttress_psys_freq_get(void *data, u64 *val);
+#endif /* IPU_BUTTRESS_H */
diff -ruN a/drivers/media/pci/intel/ipu.c b/drivers/media/pci/intel/ipu.c
--- a/drivers/media/pci/intel/ipu.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,821 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2020 Intel Corporation
+
+#include <linux/debugfs.h>
+#include <linux/device.h>
+#include <linux/interrupt.h>
+#include <linux/firmware.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/pci.h>
+#include <linux/pm_qos.h>
+#include <linux/pm_runtime.h>
+#include <linux/timer.h>
+#include <linux/sched.h>
+
+#include "ipu.h"
+#include "ipu-buttress.h"
+#include "ipu-platform.h"
+#include "ipu-platform-buttress-regs.h"
+#include "ipu-cpd.h"
+#include "ipu-pdata.h"
+#include "ipu-bus.h"
+#include "ipu-mmu.h"
+#include "ipu-platform-regs.h"
+#include "ipu-platform-isys-csi2-reg.h"
+#include "ipu-trace.h"
+
+#define IPU_PCI_BAR		0
+enum ipu_version ipu_ver;
+EXPORT_SYMBOL(ipu_ver);
+
+static struct ipu_bus_device *ipu_isys_init(struct pci_dev *pdev,
+					    struct device *parent,
+					    struct ipu_buttress_ctrl *ctrl,
+					    void __iomem *base,
+					    const struct ipu_isys_internal_pdata
+					    *ipdata,
+					    unsigned int nr)
+{
+	struct ipu_bus_device *isys;
+	struct ipu_isys_pdata *pdata;
+
+	pdata = devm_kzalloc(&pdev->dev, sizeof(*pdata), GFP_KERNEL);
+	if (!pdata)
+		return ERR_PTR(-ENOMEM);
+
+	pdata->base = base;
+	pdata->ipdata = ipdata;
+
+	/* Use 250MHz for ipu6 se */
+	if (ipu_ver == IPU_VER_6SE)
+		ctrl->ratio = IPU6SE_IS_FREQ_CTL_DEFAULT_RATIO;
+
+	isys = ipu_bus_add_device(pdev, parent, pdata, ctrl,
+				  IPU_ISYS_NAME, nr);
+	if (IS_ERR(isys))
+		return ERR_PTR(-ENOMEM);
+
+	isys->mmu = ipu_mmu_init(&pdev->dev, base, ISYS_MMID,
+				 &ipdata->hw_variant);
+	if (IS_ERR(isys->mmu))
+		return ERR_PTR(-ENOMEM);
+
+	isys->mmu->dev = &isys->dev;
+
+	return isys;
+}
+
+static struct ipu_bus_device *ipu_psys_init(struct pci_dev *pdev,
+					    struct device *parent,
+					    struct ipu_buttress_ctrl *ctrl,
+					    void __iomem *base,
+					    const struct ipu_psys_internal_pdata
+					    *ipdata, unsigned int nr)
+{
+	struct ipu_bus_device *psys;
+	struct ipu_psys_pdata *pdata;
+
+	pdata = devm_kzalloc(&pdev->dev, sizeof(*pdata), GFP_KERNEL);
+	if (!pdata)
+		return ERR_PTR(-ENOMEM);
+
+	pdata->base = base;
+	pdata->ipdata = ipdata;
+
+	psys = ipu_bus_add_device(pdev, parent, pdata, ctrl,
+				  IPU_PSYS_NAME, nr);
+	if (IS_ERR(psys))
+		return ERR_PTR(-ENOMEM);
+
+	psys->mmu = ipu_mmu_init(&pdev->dev, base, PSYS_MMID,
+				 &ipdata->hw_variant);
+	if (IS_ERR(psys->mmu))
+		return ERR_PTR(-ENOMEM);
+
+	psys->mmu->dev = &psys->dev;
+
+	return psys;
+}
+
+int ipu_fw_authenticate(void *data, u64 val)
+{
+	struct ipu_device *isp = data;
+	int ret;
+
+	if (!isp->secure_mode)
+		return -EINVAL;
+
+	ret = ipu_buttress_reset_authentication(isp);
+	if (ret) {
+		dev_err(&isp->pdev->dev, "Failed to reset authentication!\n");
+		return ret;
+	}
+
+	ret = pm_runtime_get_sync(&isp->psys->dev);
+	if (ret < 0) {
+		dev_err(&isp->pdev->dev, "Runtime PM failed (%d)\n", ret);
+		return ret;
+	}
+
+	ret = ipu_buttress_authenticate(isp);
+	if (ret) {
+		dev_err(&isp->pdev->dev, "FW authentication failed\n");
+		return ret;
+	}
+
+	pm_runtime_put(&isp->psys->dev);
+
+	return 0;
+}
+EXPORT_SYMBOL(ipu_fw_authenticate);
+DEFINE_SIMPLE_ATTRIBUTE(authenticate_fops, NULL, ipu_fw_authenticate, "%llu\n");
+
+#ifdef CONFIG_DEBUG_FS
+static int resume_ipu_bus_device(struct ipu_bus_device *adev)
+{
+	struct device *dev = &adev->dev;
+	const struct dev_pm_ops *pm = dev->driver ? dev->driver->pm : NULL;
+
+	if (!pm || !pm->resume)
+		return -EIO;
+
+	return pm->resume(dev);
+}
+
+static int suspend_ipu_bus_device(struct ipu_bus_device *adev)
+{
+	struct device *dev = &adev->dev;
+	const struct dev_pm_ops *pm = dev->driver ? dev->driver->pm : NULL;
+
+	if (!pm || !pm->suspend)
+		return -EIO;
+
+	return pm->suspend(dev);
+}
+
+static int force_suspend_get(void *data, u64 *val)
+{
+	struct ipu_device *isp = data;
+	struct ipu_buttress *b = &isp->buttress;
+
+	*val = b->force_suspend;
+	return 0;
+}
+
+static int force_suspend_set(void *data, u64 val)
+{
+	struct ipu_device *isp = data;
+	struct ipu_buttress *b = &isp->buttress;
+	int ret = 0;
+
+	if (val == b->force_suspend)
+		return 0;
+
+	if (val) {
+		b->force_suspend = 1;
+		ret = suspend_ipu_bus_device(isp->psys);
+		if (ret) {
+			dev_err(&isp->pdev->dev, "Failed to suspend psys\n");
+			return ret;
+		}
+		ret = suspend_ipu_bus_device(isp->isys);
+		if (ret) {
+			dev_err(&isp->pdev->dev, "Failed to suspend isys\n");
+			return ret;
+		}
+		ret = pci_set_power_state(isp->pdev, PCI_D3hot);
+		if (ret) {
+			dev_err(&isp->pdev->dev,
+				"Failed to suspend IUnit PCI device\n");
+			return ret;
+		}
+	} else {
+		ret = pci_set_power_state(isp->pdev, PCI_D0);
+		if (ret) {
+			dev_err(&isp->pdev->dev,
+				"Failed to suspend IUnit PCI device\n");
+			return ret;
+		}
+		ret = resume_ipu_bus_device(isp->isys);
+		if (ret) {
+			dev_err(&isp->pdev->dev, "Failed to resume isys\n");
+			return ret;
+		}
+		ret = resume_ipu_bus_device(isp->psys);
+		if (ret) {
+			dev_err(&isp->pdev->dev, "Failed to resume psys\n");
+			return ret;
+		}
+		b->force_suspend = 0;
+	}
+
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(force_suspend_fops, force_suspend_get,
+			force_suspend_set, "%llu\n");
+/*
+ * The sysfs interface for reloading cpd fw is there only for debug purpose,
+ * and it must not be used when either isys or psys is in use.
+ */
+static int cpd_fw_reload(void *data, u64 val)
+{
+	struct ipu_device *isp = data;
+	int rval = -EINVAL;
+
+	if (isp->cpd_fw_reload)
+		rval = isp->cpd_fw_reload(isp);
+
+	return rval;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(cpd_fw_fops, NULL, cpd_fw_reload, "%llu\n");
+
+static int ipu_init_debugfs(struct ipu_device *isp)
+{
+	struct dentry *file;
+	struct dentry *dir;
+
+	dir = debugfs_create_dir(pci_name(isp->pdev), NULL);
+	if (!dir)
+		return -ENOMEM;
+
+	file = debugfs_create_file("force_suspend", 0700, dir, isp,
+				   &force_suspend_fops);
+	if (!file)
+		goto err;
+	file = debugfs_create_file("authenticate", 0700, dir, isp,
+				   &authenticate_fops);
+	if (!file)
+		goto err;
+
+	file = debugfs_create_file("cpd_fw_reload", 0700, dir, isp,
+				   &cpd_fw_fops);
+	if (!file)
+		goto err;
+
+	if (ipu_trace_debugfs_add(isp, dir))
+		goto err;
+
+	isp->ipu_dir = dir;
+
+	if (ipu_buttress_debugfs_init(isp))
+		goto err;
+
+	return 0;
+err:
+	debugfs_remove_recursive(dir);
+	return -ENOMEM;
+}
+
+static void ipu_remove_debugfs(struct ipu_device *isp)
+{
+	/*
+	 * Since isys and psys debugfs dir will be created under ipu root dir,
+	 * mark its dentry to NULL to avoid duplicate removal.
+	 */
+	debugfs_remove_recursive(isp->ipu_dir);
+	isp->ipu_dir = NULL;
+}
+#endif /* CONFIG_DEBUG_FS */
+
+static int ipu_pci_config_setup(struct pci_dev *dev)
+{
+	u16 pci_command;
+	int rval;
+
+	pci_read_config_word(dev, PCI_COMMAND, &pci_command);
+	pci_command |= PCI_COMMAND_MEMORY | PCI_COMMAND_MASTER;
+	pci_write_config_word(dev, PCI_COMMAND, pci_command);
+	if (ipu_ver == IPU_VER_6EP) {
+		/* likely do nothing as msi not enabled by default */
+		pci_disable_msi(dev);
+		return 0;
+	}
+
+	rval = pci_enable_msi(dev);
+	if (rval)
+		dev_err(&dev->dev, "Failed to enable msi (%d)\n", rval);
+
+	return rval;
+}
+
+static void ipu_configure_vc_mechanism(struct ipu_device *isp)
+{
+	u32 val = readl(isp->base + BUTTRESS_REG_BTRS_CTRL);
+
+	if (IPU_BTRS_ARB_STALL_MODE_VC0 == IPU_BTRS_ARB_MODE_TYPE_STALL)
+		val |= BUTTRESS_REG_BTRS_CTRL_STALL_MODE_VC0;
+	else
+		val &= ~BUTTRESS_REG_BTRS_CTRL_STALL_MODE_VC0;
+
+	if (IPU_BTRS_ARB_STALL_MODE_VC1 == IPU_BTRS_ARB_MODE_TYPE_STALL)
+		val |= BUTTRESS_REG_BTRS_CTRL_STALL_MODE_VC1;
+	else
+		val &= ~BUTTRESS_REG_BTRS_CTRL_STALL_MODE_VC1;
+
+	writel(val, isp->base + BUTTRESS_REG_BTRS_CTRL);
+}
+
+int request_cpd_fw(const struct firmware **firmware_p, const char *name,
+		   struct device *device)
+{
+	const struct firmware *fw;
+	struct firmware *tmp;
+	int ret;
+
+	ret = request_firmware(&fw, name, device);
+	if (ret)
+		return ret;
+
+	if (is_vmalloc_addr(fw->data)) {
+		*firmware_p = fw;
+	} else {
+		tmp = kzalloc(sizeof(*tmp), GFP_KERNEL);
+		if (!tmp) {
+			release_firmware(fw);
+			return -ENOMEM;
+		}
+		tmp->size = fw->size;
+		tmp->data = vmalloc(fw->size);
+		if (!tmp->data) {
+			kfree(tmp);
+			release_firmware(fw);
+			return -ENOMEM;
+		}
+		memcpy((void *)tmp->data, fw->data, fw->size);
+		*firmware_p = tmp;
+		release_firmware(fw);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(request_cpd_fw);
+
+static int ipu_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)
+{
+	struct ipu_device *isp;
+	phys_addr_t phys;
+	void __iomem *const *iomap;
+	void __iomem *isys_base = NULL;
+	void __iomem *psys_base = NULL;
+	struct ipu_buttress_ctrl *isys_ctrl = NULL, *psys_ctrl = NULL;
+	unsigned int dma_mask = IPU_DMA_MASK;
+	int rval;
+
+	isp = devm_kzalloc(&pdev->dev, sizeof(*isp), GFP_KERNEL);
+	if (!isp)
+		return -ENOMEM;
+
+	dev_set_name(&pdev->dev, "intel-ipu");
+	isp->pdev = pdev;
+	INIT_LIST_HEAD(&isp->devices);
+
+	rval = pcim_enable_device(pdev);
+	if (rval) {
+		dev_err(&pdev->dev, "Failed to enable CI ISP device (%d)\n",
+			rval);
+		return rval;
+	}
+
+	dev_info(&pdev->dev, "Device 0x%x (rev: 0x%x)\n",
+		 pdev->device, pdev->revision);
+
+	phys = pci_resource_start(pdev, IPU_PCI_BAR);
+
+	rval = pcim_iomap_regions(pdev,
+				  1 << IPU_PCI_BAR,
+				  pci_name(pdev));
+	if (rval) {
+		dev_err(&pdev->dev, "Failed to I/O memory remapping (%d)\n",
+			rval);
+		return rval;
+	}
+	dev_info(&pdev->dev, "physical base address 0x%llx\n", phys);
+
+	iomap = pcim_iomap_table(pdev);
+	if (!iomap) {
+		dev_err(&pdev->dev, "Failed to iomap table (%d)\n", rval);
+		return -ENODEV;
+	}
+
+	isp->base = iomap[IPU_PCI_BAR];
+	dev_info(&pdev->dev, "mapped as: 0x%p\n", isp->base);
+
+	pci_set_drvdata(pdev, isp);
+	pci_set_master(pdev);
+
+	switch (id->device) {
+	case IPU6_PCI_ID:
+		ipu_ver = IPU_VER_6;
+		isp->cpd_fw_name = IPU6_FIRMWARE_NAME;
+		break;
+	case IPU6SE_PCI_ID:
+		ipu_ver = IPU_VER_6SE;
+		isp->cpd_fw_name = IPU6SE_FIRMWARE_NAME;
+		break;
+	case IPU6EP_PCI_ID:
+		ipu_ver = IPU_VER_6EP;
+		isp->cpd_fw_name = IPU6EP_FIRMWARE_NAME;
+		break;
+	default:
+		WARN(1, "Unsupported IPU device");
+		return -ENODEV;
+	}
+
+	ipu_internal_pdata_init();
+
+	isys_base = isp->base + isys_ipdata.hw_variant.offset;
+	psys_base = isp->base + psys_ipdata.hw_variant.offset;
+
+	dev_dbg(&pdev->dev, "isys_base: 0x%lx\n", (unsigned long)isys_base);
+	dev_dbg(&pdev->dev, "psys_base: 0x%lx\n", (unsigned long)psys_base);
+
+	rval = pci_set_dma_mask(pdev, DMA_BIT_MASK(dma_mask));
+	if (!rval)
+		rval = pci_set_consistent_dma_mask(pdev,
+						   DMA_BIT_MASK(dma_mask));
+	if (rval) {
+		dev_err(&pdev->dev, "Failed to set DMA mask (%d)\n", rval);
+		return rval;
+	}
+
+	dma_set_max_seg_size(&pdev->dev, UINT_MAX);
+
+	rval = ipu_pci_config_setup(pdev);
+	if (rval)
+		return rval;
+
+	rval = devm_request_threaded_irq(&pdev->dev, pdev->irq,
+					 ipu_buttress_isr,
+					 ipu_buttress_isr_threaded,
+					 IRQF_SHARED, IPU_NAME, isp);
+	if (rval) {
+		dev_err(&pdev->dev, "Requesting irq failed(%d)\n", rval);
+		return rval;
+	}
+
+	rval = ipu_buttress_init(isp);
+	if (rval)
+		return rval;
+
+	dev_info(&pdev->dev, "cpd file name: %s\n", isp->cpd_fw_name);
+
+	rval = request_cpd_fw(&isp->cpd_fw, isp->cpd_fw_name, &pdev->dev);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Requesting signed firmware failed\n");
+		return rval;
+	}
+
+	rval = ipu_cpd_validate_cpd_file(isp, isp->cpd_fw->data,
+					 isp->cpd_fw->size);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Failed to validate cpd\n");
+		goto out_ipu_bus_del_devices;
+	}
+
+	rval = ipu_trace_add(isp);
+	if (rval)
+		dev_err(&pdev->dev, "Trace support not available\n");
+
+	pm_runtime_put_noidle(&pdev->dev);
+	pm_runtime_allow(&pdev->dev);
+
+	/*
+	 * NOTE Device hierarchy below is important to ensure proper
+	 * runtime suspend and resume order.
+	 * Also registration order is important to ensure proper
+	 * suspend and resume order during system
+	 * suspend. Registration order is as follows:
+	 * isys->psys
+	 */
+	isys_ctrl = devm_kzalloc(&pdev->dev, sizeof(*isys_ctrl), GFP_KERNEL);
+	if (!isys_ctrl) {
+		rval = -ENOMEM;
+		goto out_ipu_bus_del_devices;
+	}
+
+	/* Init butress control with default values based on the HW */
+	memcpy(isys_ctrl, &isys_buttress_ctrl, sizeof(*isys_ctrl));
+
+	isp->isys = ipu_isys_init(pdev, &pdev->dev,
+				  isys_ctrl, isys_base,
+				  &isys_ipdata,
+				  0);
+	if (IS_ERR(isp->isys)) {
+		rval = PTR_ERR(isp->isys);
+		goto out_ipu_bus_del_devices;
+	}
+
+	psys_ctrl = devm_kzalloc(&pdev->dev, sizeof(*psys_ctrl), GFP_KERNEL);
+	if (!psys_ctrl) {
+		rval = -ENOMEM;
+		goto out_ipu_bus_del_devices;
+	}
+
+	/* Init butress control with default values based on the HW */
+	memcpy(psys_ctrl, &psys_buttress_ctrl, sizeof(*psys_ctrl));
+
+	isp->psys = ipu_psys_init(pdev, &isp->isys->dev,
+				  psys_ctrl, psys_base,
+				  &psys_ipdata, 0);
+	if (IS_ERR(isp->psys)) {
+		rval = PTR_ERR(isp->psys);
+		goto out_ipu_bus_del_devices;
+	}
+
+	rval = pm_runtime_get_sync(&isp->psys->dev);
+	if (rval < 0) {
+		dev_err(&isp->psys->dev, "Failed to get runtime PM\n");
+		goto out_ipu_bus_del_devices;
+	}
+
+	rval = ipu_mmu_hw_init(isp->psys->mmu);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Failed to set mmu hw\n");
+		goto out_ipu_bus_del_devices;
+	}
+
+	rval = ipu_buttress_map_fw_image(isp->psys, isp->cpd_fw,
+					 &isp->fw_sgt);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "failed to map fw image\n");
+		goto out_ipu_bus_del_devices;
+	}
+
+	isp->pkg_dir = ipu_cpd_create_pkg_dir(isp->psys,
+					      isp->cpd_fw->data,
+					      sg_dma_address(isp->fw_sgt.sgl),
+					      &isp->pkg_dir_dma_addr,
+					      &isp->pkg_dir_size);
+	if (!isp->pkg_dir) {
+		rval = -ENOMEM;
+		dev_err(&isp->pdev->dev, "failed to create pkg dir\n");
+		goto out_ipu_bus_del_devices;
+	}
+
+	rval = ipu_buttress_authenticate(isp);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "FW authentication failed(%d)\n",
+			rval);
+		goto out_ipu_bus_del_devices;
+	}
+
+	ipu_mmu_hw_cleanup(isp->psys->mmu);
+	pm_runtime_put(&isp->psys->dev);
+
+#ifdef CONFIG_DEBUG_FS
+	rval = ipu_init_debugfs(isp);
+	if (rval) {
+		dev_err(&pdev->dev, "Failed to initialize debugfs");
+		goto out_ipu_bus_del_devices;
+	}
+#endif
+
+	/* Configure the arbitration mechanisms for VC requests */
+	ipu_configure_vc_mechanism(isp);
+
+	dev_info(&pdev->dev, "IPU driver version %d.%d\n", IPU_MAJOR_VERSION,
+		 IPU_MINOR_VERSION);
+
+	return 0;
+
+out_ipu_bus_del_devices:
+	if (isp->pkg_dir) {
+		ipu_cpd_free_pkg_dir(isp->psys, isp->pkg_dir,
+				     isp->pkg_dir_dma_addr,
+				     isp->pkg_dir_size);
+		ipu_buttress_unmap_fw_image(isp->psys, &isp->fw_sgt);
+		isp->pkg_dir = NULL;
+	}
+	if (isp->psys && isp->psys->mmu)
+		ipu_mmu_cleanup(isp->psys->mmu);
+	if (isp->isys && isp->isys->mmu)
+		ipu_mmu_cleanup(isp->isys->mmu);
+	if (isp->psys)
+		pm_runtime_put(&isp->psys->dev);
+	ipu_bus_del_devices(pdev);
+	ipu_buttress_exit(isp);
+	release_firmware(isp->cpd_fw);
+
+	return rval;
+}
+
+static void ipu_pci_remove(struct pci_dev *pdev)
+{
+	struct ipu_device *isp = pci_get_drvdata(pdev);
+
+#ifdef CONFIG_DEBUG_FS
+	ipu_remove_debugfs(isp);
+#endif
+	ipu_trace_release(isp);
+
+	ipu_cpd_free_pkg_dir(isp->psys, isp->pkg_dir, isp->pkg_dir_dma_addr,
+			     isp->pkg_dir_size);
+
+	ipu_buttress_unmap_fw_image(isp->psys, &isp->fw_sgt);
+
+	isp->pkg_dir = NULL;
+	isp->pkg_dir_dma_addr = 0;
+	isp->pkg_dir_size = 0;
+
+	ipu_bus_del_devices(pdev);
+
+	pm_runtime_forbid(&pdev->dev);
+	pm_runtime_get_noresume(&pdev->dev);
+
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+
+	ipu_buttress_exit(isp);
+
+	release_firmware(isp->cpd_fw);
+
+	ipu_mmu_cleanup(isp->psys->mmu);
+	ipu_mmu_cleanup(isp->isys->mmu);
+}
+
+static void ipu_pci_reset_prepare(struct pci_dev *pdev)
+{
+	struct ipu_device *isp = pci_get_drvdata(pdev);
+
+	dev_warn(&pdev->dev, "FLR prepare\n");
+	pm_runtime_forbid(&isp->pdev->dev);
+	isp->flr_done = true;
+}
+
+static void ipu_pci_reset_done(struct pci_dev *pdev)
+{
+	struct ipu_device *isp = pci_get_drvdata(pdev);
+
+	ipu_buttress_restore(isp);
+	if (isp->secure_mode)
+		ipu_buttress_reset_authentication(isp);
+
+	ipu_bus_flr_recovery();
+	isp->ipc_reinit = true;
+	pm_runtime_allow(&isp->pdev->dev);
+
+	dev_warn(&pdev->dev, "FLR completed\n");
+}
+
+#ifdef CONFIG_PM
+
+/*
+ * PCI base driver code requires driver to provide these to enable
+ * PCI device level PM state transitions (D0<->D3)
+ */
+static int ipu_suspend(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct ipu_device *isp = pci_get_drvdata(pdev);
+
+	isp->flr_done = false;
+
+	return 0;
+}
+
+static int ipu_resume(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct ipu_device *isp = pci_get_drvdata(pdev);
+	struct ipu_buttress *b = &isp->buttress;
+	int rval;
+
+	/* Configure the arbitration mechanisms for VC requests */
+	ipu_configure_vc_mechanism(isp);
+
+	ipu_buttress_set_secure_mode(isp);
+	isp->secure_mode = ipu_buttress_get_secure_mode(isp);
+	dev_info(dev, "IPU in %s mode\n",
+		 isp->secure_mode ? "secure" : "non-secure");
+
+	ipu_buttress_restore(isp);
+
+	rval = ipu_buttress_ipc_reset(isp, &b->cse);
+	if (rval)
+		dev_err(&isp->pdev->dev, "IPC reset protocol failed!\n");
+
+	rval = pm_runtime_get_sync(&isp->psys->dev);
+	if (rval < 0) {
+		dev_err(&isp->psys->dev, "Failed to get runtime PM\n");
+		return 0;
+	}
+
+	rval = ipu_buttress_authenticate(isp);
+	if (rval)
+		dev_err(&isp->pdev->dev, "FW authentication failed(%d)\n",
+			rval);
+
+	pm_runtime_put(&isp->psys->dev);
+
+	return 0;
+}
+
+static int ipu_runtime_resume(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct ipu_device *isp = pci_get_drvdata(pdev);
+	int rval;
+
+	ipu_configure_vc_mechanism(isp);
+	ipu_buttress_restore(isp);
+
+	if (isp->ipc_reinit) {
+		struct ipu_buttress *b = &isp->buttress;
+
+		isp->ipc_reinit = false;
+		rval = ipu_buttress_ipc_reset(isp, &b->cse);
+		if (rval)
+			dev_err(&isp->pdev->dev,
+				"IPC reset protocol failed!\n");
+	}
+
+	return 0;
+}
+
+static const struct dev_pm_ops ipu_pm_ops = {
+	SET_SYSTEM_SLEEP_PM_OPS(&ipu_suspend, &ipu_resume)
+	    SET_RUNTIME_PM_OPS(&ipu_suspend,	/* Same as in suspend flow */
+			       &ipu_runtime_resume,
+			       NULL)
+};
+
+#define IPU_PM (&ipu_pm_ops)
+#else
+#define IPU_PM NULL
+#endif
+
+static const struct pci_device_id ipu_pci_tbl[] = {
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, IPU6_PCI_ID)},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, IPU6SE_PCI_ID)},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, IPU6EP_PCI_ID)},
+	{0,}
+};
+MODULE_DEVICE_TABLE(pci, ipu_pci_tbl);
+
+static const struct pci_error_handlers pci_err_handlers = {
+	.reset_prepare = ipu_pci_reset_prepare,
+	.reset_done = ipu_pci_reset_done,
+};
+
+static struct pci_driver ipu_pci_driver = {
+	.name = IPU_NAME,
+	.id_table = ipu_pci_tbl,
+	.probe = ipu_pci_probe,
+	.remove = ipu_pci_remove,
+	.driver = {
+		   .pm = IPU_PM,
+		   },
+	.err_handler = &pci_err_handlers,
+};
+
+static int __init ipu_init(void)
+{
+	int rval = ipu_bus_register();
+
+	if (rval) {
+		pr_warn("can't register ipu bus (%d)\n", rval);
+		return rval;
+	}
+
+	rval = pci_register_driver(&ipu_pci_driver);
+	if (rval) {
+		pr_warn("can't register pci driver (%d)\n", rval);
+		goto out_pci_register_driver;
+	}
+
+	return 0;
+
+out_pci_register_driver:
+	ipu_bus_unregister();
+
+	return rval;
+}
+
+static void __exit ipu_exit(void)
+{
+	pci_unregister_driver(&ipu_pci_driver);
+	ipu_bus_unregister();
+}
+
+module_init(ipu_init);
+module_exit(ipu_exit);
+
+MODULE_AUTHOR("Sakari Ailus <sakari.ailus@linux.intel.com>");
+MODULE_AUTHOR("Jouni Hgander <jouni.hogander@intel.com>");
+MODULE_AUTHOR("Antti Laakso <antti.laakso@intel.com>");
+MODULE_AUTHOR("Samu Onkalo <samu.onkalo@intel.com>");
+MODULE_AUTHOR("Jianxu Zheng <jian.xu.zheng@intel.com>");
+MODULE_AUTHOR("Tianshu Qiu <tian.shu.qiu@intel.com>");
+MODULE_AUTHOR("Renwei Wu <renwei.wu@intel.com>");
+MODULE_AUTHOR("Bingbu Cao <bingbu.cao@intel.com>");
+MODULE_AUTHOR("Yunliang Ding <yunliang.ding@intel.com>");
+MODULE_AUTHOR("Zaikuo Wang <zaikuo.wang@intel.com>");
+MODULE_AUTHOR("Leifu Zhao <leifu.zhao@intel.com>");
+MODULE_AUTHOR("Xia Wu <xia.wu@intel.com>");
+MODULE_AUTHOR("Kun Jiang <kun.jiang@intel.com>");
+MODULE_AUTHOR("Intel");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Intel ipu pci driver");
diff -ruN a/drivers/media/pci/intel/ipu-cpd.c b/drivers/media/pci/intel/ipu-cpd.c
--- a/drivers/media/pci/intel/ipu-cpd.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-cpd.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,465 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2015 - 2020 Intel Corporation
+
+#include <linux/dma-mapping.h>
+#include <linux/module.h>
+
+#include "ipu.h"
+#include "ipu-cpd.h"
+
+/* 15 entries + header*/
+#define MAX_PKG_DIR_ENT_CNT		16
+/* 2 qword per entry/header */
+#define PKG_DIR_ENT_LEN			2
+/* PKG_DIR size in bytes */
+#define PKG_DIR_SIZE			((MAX_PKG_DIR_ENT_CNT) *	\
+					 (PKG_DIR_ENT_LEN) * sizeof(u64))
+#define PKG_DIR_ID_SHIFT		48
+#define PKG_DIR_ID_MASK			0x7f
+#define PKG_DIR_VERSION_SHIFT		32
+#define PKG_DIR_SIZE_MASK		0xfffff
+/* _IUPKDR_ */
+#define PKG_DIR_HDR_MARK		0x5f4955504b44525f
+
+/* $CPD */
+#define CPD_HDR_MARK			0x44504324
+
+/* Maximum size is 2K DWORDs */
+#define MAX_MANIFEST_SIZE		(2 * 1024 * sizeof(u32))
+
+/* Maximum size is 64k */
+#define MAX_METADATA_SIZE		(64 * 1024)
+
+#define MAX_COMPONENT_ID		127
+#define MAX_COMPONENT_VERSION		0xffff
+
+#define CPD_MANIFEST_IDX	0
+#define CPD_METADATA_IDX	1
+#define CPD_MODULEDATA_IDX	2
+
+static inline struct ipu_cpd_ent *ipu_cpd_get_entries(const void *cpd)
+{
+	const struct ipu_cpd_hdr *cpd_hdr = cpd;
+
+	return (struct ipu_cpd_ent *)((u8 *)cpd + cpd_hdr->hdr_len);
+}
+
+#define ipu_cpd_get_entry(cpd, idx) (&ipu_cpd_get_entries(cpd)[idx])
+#define ipu_cpd_get_manifest(cpd) ipu_cpd_get_entry(cpd, CPD_MANIFEST_IDX)
+#define ipu_cpd_get_metadata(cpd) ipu_cpd_get_entry(cpd, CPD_METADATA_IDX)
+#define ipu_cpd_get_moduledata(cpd) ipu_cpd_get_entry(cpd, CPD_MODULEDATA_IDX)
+
+static const struct ipu_cpd_metadata_cmpnt *
+ipu_cpd_metadata_get_cmpnt(struct ipu_device *isp,
+			   const void *metadata,
+			   unsigned int metadata_size,
+			   u8 idx)
+{
+	const struct ipu_cpd_metadata_extn *extn;
+	const struct ipu_cpd_metadata_cmpnt *cmpnts;
+	int cmpnt_count;
+
+	extn = metadata;
+	cmpnts = metadata + sizeof(*extn);
+	cmpnt_count = (metadata_size - sizeof(*extn)) / sizeof(*cmpnts);
+
+	if (idx > MAX_COMPONENT_ID || idx >= cmpnt_count) {
+		dev_err(&isp->pdev->dev, "Component index out of range (%d)\n",
+			idx);
+		return ERR_PTR(-EINVAL);
+	}
+
+	return &cmpnts[idx];
+}
+
+static u32 ipu_cpd_metadata_cmpnt_version(struct ipu_device *isp,
+					  const void *metadata,
+					  unsigned int metadata_size, u8 idx)
+{
+	const struct ipu_cpd_metadata_cmpnt *cmpnt =
+	    ipu_cpd_metadata_get_cmpnt(isp, metadata,
+				       metadata_size, idx);
+
+	if (IS_ERR(cmpnt))
+		return PTR_ERR(cmpnt);
+
+	return cmpnt->ver;
+}
+
+static int ipu_cpd_metadata_get_cmpnt_id(struct ipu_device *isp,
+					 const void *metadata,
+					 unsigned int metadata_size, u8 idx)
+{
+	const struct ipu_cpd_metadata_cmpnt *cmpnt =
+	    ipu_cpd_metadata_get_cmpnt(isp, metadata,
+				       metadata_size, idx);
+
+	if (IS_ERR(cmpnt))
+		return PTR_ERR(cmpnt);
+
+	return cmpnt->id;
+}
+
+static const struct ipu6_cpd_metadata_cmpnt *
+ipu6_cpd_metadata_get_cmpnt(struct ipu_device *isp,
+			    const void *metadata,
+			    unsigned int metadata_size,
+			    u8 idx)
+{
+	const struct ipu_cpd_metadata_extn *extn = metadata;
+	const struct ipu6_cpd_metadata_cmpnt *cmpnts = metadata + sizeof(*extn);
+	int cmpnt_count;
+
+	cmpnt_count = (metadata_size - sizeof(*extn)) / sizeof(*cmpnts);
+	if (idx > MAX_COMPONENT_ID || idx >= cmpnt_count) {
+		dev_err(&isp->pdev->dev, "Component index out of range (%d)\n",
+			idx);
+		return ERR_PTR(-EINVAL);
+	}
+
+	return &cmpnts[idx];
+}
+
+static u32 ipu6_cpd_metadata_cmpnt_version(struct ipu_device *isp,
+					   const void *metadata,
+					   unsigned int metadata_size, u8 idx)
+{
+	const struct ipu6_cpd_metadata_cmpnt *cmpnt =
+	    ipu6_cpd_metadata_get_cmpnt(isp, metadata,
+					metadata_size, idx);
+
+	if (IS_ERR(cmpnt))
+		return PTR_ERR(cmpnt);
+
+	return cmpnt->ver;
+}
+
+static int ipu6_cpd_metadata_get_cmpnt_id(struct ipu_device *isp,
+					  const void *metadata,
+					  unsigned int metadata_size, u8 idx)
+{
+	const struct ipu6_cpd_metadata_cmpnt *cmpnt =
+	    ipu6_cpd_metadata_get_cmpnt(isp, metadata,
+					metadata_size, idx);
+
+	if (IS_ERR(cmpnt))
+		return PTR_ERR(cmpnt);
+
+	return cmpnt->id;
+}
+
+static int ipu_cpd_parse_module_data(struct ipu_device *isp,
+				     const void *module_data,
+				     unsigned int module_data_size,
+				     dma_addr_t dma_addr_module_data,
+				     u64 *pkg_dir,
+				     const void *metadata,
+				     unsigned int metadata_size)
+{
+	const struct ipu_cpd_module_data_hdr *module_data_hdr;
+	const struct ipu_cpd_hdr *dir_hdr;
+	const struct ipu_cpd_ent *dir_ent;
+	int i;
+	u8 len;
+
+	if (!module_data)
+		return -EINVAL;
+
+	module_data_hdr = module_data;
+	dir_hdr = module_data + module_data_hdr->hdr_len;
+	len = dir_hdr->hdr_len;
+	dir_ent = (struct ipu_cpd_ent *)(((u8 *)dir_hdr) + len);
+
+	pkg_dir[0] = PKG_DIR_HDR_MARK;
+	/* pkg_dir entry count = component count + pkg_dir header */
+	pkg_dir[1] = dir_hdr->ent_cnt + 1;
+
+	for (i = 0; i < dir_hdr->ent_cnt; i++, dir_ent++) {
+		u64 *p = &pkg_dir[PKG_DIR_ENT_LEN + i * PKG_DIR_ENT_LEN];
+		int ver, id;
+
+		*p++ = dma_addr_module_data + dir_ent->offset;
+
+		if (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP)
+			id = ipu6_cpd_metadata_get_cmpnt_id(isp, metadata,
+							    metadata_size, i);
+		else
+			id = ipu_cpd_metadata_get_cmpnt_id(isp, metadata,
+							   metadata_size, i);
+
+		if (id < 0 || id > MAX_COMPONENT_ID) {
+			dev_err(&isp->pdev->dev,
+				"Failed to parse component id\n");
+			return -EINVAL;
+		}
+
+		if (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP)
+			ver = ipu6_cpd_metadata_cmpnt_version(isp, metadata,
+							      metadata_size, i);
+		else
+			ver = ipu_cpd_metadata_cmpnt_version(isp, metadata,
+							     metadata_size, i);
+
+		if (ver < 0 || ver > MAX_COMPONENT_VERSION) {
+			dev_err(&isp->pdev->dev,
+				"Failed to parse component version\n");
+			return -EINVAL;
+		}
+
+		/*
+		 * PKG_DIR Entry (type == id)
+		 * 63:56        55      54:48   47:32   31:24   23:0
+		 * Rsvd         Rsvd    Type    Version Rsvd    Size
+		 */
+		*p = dir_ent->len | (u64)id << PKG_DIR_ID_SHIFT |
+		    (u64)ver << PKG_DIR_VERSION_SHIFT;
+	}
+
+	return 0;
+}
+
+void *ipu_cpd_create_pkg_dir(struct ipu_bus_device *adev,
+			     const void *src,
+			     dma_addr_t dma_addr_src,
+			     dma_addr_t *dma_addr, unsigned int *pkg_dir_size)
+{
+	struct ipu_device *isp = adev->isp;
+	const struct ipu_cpd_ent *ent, *man_ent, *met_ent;
+	u64 *pkg_dir;
+	unsigned int man_sz, met_sz;
+	void *pkg_dir_pos;
+	int ret;
+
+	man_ent = ipu_cpd_get_manifest(src);
+	man_sz = man_ent->len;
+
+	met_ent = ipu_cpd_get_metadata(src);
+	met_sz = met_ent->len;
+
+	*pkg_dir_size = PKG_DIR_SIZE + man_sz + met_sz;
+	pkg_dir = dma_alloc_attrs(&adev->dev, *pkg_dir_size, dma_addr,
+				  GFP_KERNEL,
+				  0);
+	if (!pkg_dir)
+		return pkg_dir;
+
+	/*
+	 * pkg_dir entry/header:
+	 * qword | 63:56 | 55   | 54:48 | 47:32 | 31:24 | 23:0
+	 * N         Address/Offset/"_IUPKDR_"
+	 * N + 1 | rsvd  | rsvd | type  | ver   | rsvd  | size
+	 *
+	 * We can ignore other fields that size in N + 1 qword as they
+	 * are 0 anyway. Just setting size for now.
+	 */
+
+	ent = ipu_cpd_get_moduledata(src);
+
+	ret = ipu_cpd_parse_module_data(isp, src + ent->offset,
+					ent->len,
+					dma_addr_src + ent->offset,
+					pkg_dir,
+					src + met_ent->offset, met_ent->len);
+	if (ret) {
+		dev_err(&isp->pdev->dev,
+			"Unable to parse module data section!\n");
+		dma_free_attrs(&isp->psys->dev, *pkg_dir_size, pkg_dir,
+			       *dma_addr,
+			       0);
+		return NULL;
+	}
+
+	/* Copy manifest after pkg_dir */
+	pkg_dir_pos = pkg_dir + PKG_DIR_ENT_LEN * MAX_PKG_DIR_ENT_CNT;
+	memcpy(pkg_dir_pos, src + man_ent->offset, man_sz);
+
+	/* Copy metadata after manifest */
+	pkg_dir_pos += man_sz;
+	memcpy(pkg_dir_pos, src + met_ent->offset, met_sz);
+
+	dma_sync_single_range_for_device(&adev->dev, *dma_addr,
+					 0, *pkg_dir_size, DMA_TO_DEVICE);
+
+	return pkg_dir;
+}
+EXPORT_SYMBOL_GPL(ipu_cpd_create_pkg_dir);
+
+void ipu_cpd_free_pkg_dir(struct ipu_bus_device *adev,
+			  u64 *pkg_dir,
+			  dma_addr_t dma_addr, unsigned int pkg_dir_size)
+{
+	dma_free_attrs(&adev->dev, pkg_dir_size, pkg_dir, dma_addr, 0);
+}
+EXPORT_SYMBOL_GPL(ipu_cpd_free_pkg_dir);
+
+static int ipu_cpd_validate_cpd(struct ipu_device *isp,
+				const void *cpd,
+				unsigned long cpd_size, unsigned long data_size)
+{
+	const struct ipu_cpd_hdr *cpd_hdr = cpd;
+	struct ipu_cpd_ent *ent;
+	unsigned int i;
+	u8 len;
+
+	len = cpd_hdr->hdr_len;
+
+	/* Ensure cpd hdr is within moduledata */
+	if (cpd_size < len) {
+		dev_err(&isp->pdev->dev, "Invalid CPD moduledata size\n");
+		return -EINVAL;
+	}
+
+	/* Sanity check for CPD header */
+	if ((cpd_size - len) / sizeof(*ent) < cpd_hdr->ent_cnt) {
+		dev_err(&isp->pdev->dev, "Invalid CPD header\n");
+		return -EINVAL;
+	}
+
+	/* Ensure that all entries are within moduledata */
+	ent = (struct ipu_cpd_ent *)(((u8 *)cpd_hdr) + len);
+	for (i = 0; i < cpd_hdr->ent_cnt; i++, ent++) {
+		if (data_size < ent->offset ||
+		    data_size - ent->offset < ent->len) {
+			dev_err(&isp->pdev->dev, "Invalid CPD entry (%d)\n", i);
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static int ipu_cpd_validate_moduledata(struct ipu_device *isp,
+				       const void *moduledata,
+				       u32 moduledata_size)
+{
+	const struct ipu_cpd_module_data_hdr *mod_hdr = moduledata;
+	int rval;
+
+	/* Ensure moduledata hdr is within moduledata */
+	if (moduledata_size < sizeof(*mod_hdr) ||
+	    moduledata_size < mod_hdr->hdr_len) {
+		dev_err(&isp->pdev->dev, "Invalid moduledata size\n");
+		return -EINVAL;
+	}
+
+	dev_info(&isp->pdev->dev, "FW version: %x\n", mod_hdr->fw_pkg_date);
+	rval = ipu_cpd_validate_cpd(isp, moduledata +
+				    mod_hdr->hdr_len,
+				    moduledata_size -
+				    mod_hdr->hdr_len, moduledata_size);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Invalid CPD in moduledata\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int ipu_cpd_validate_metadata(struct ipu_device *isp,
+				     const void *metadata, u32 meta_size)
+{
+	const struct ipu_cpd_metadata_extn *extn = metadata;
+	unsigned int size;
+
+	/* Sanity check for metadata size */
+	if (meta_size < sizeof(*extn) || meta_size > MAX_METADATA_SIZE) {
+		dev_err(&isp->pdev->dev, "%s: Invalid metadata\n", __func__);
+		return -EINVAL;
+	}
+
+	/* Validate extension and image types */
+	if (extn->extn_type != IPU_CPD_METADATA_EXTN_TYPE_IUNIT ||
+	    extn->img_type != IPU_CPD_METADATA_IMAGE_TYPE_MAIN_FIRMWARE) {
+		dev_err(&isp->pdev->dev,
+			"Invalid metadata descriptor img_type (%d)\n",
+			extn->img_type);
+		return -EINVAL;
+	}
+
+	/* Validate metadata size multiple of metadata components */
+	if (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP)
+		size = sizeof(struct ipu6_cpd_metadata_cmpnt);
+	else
+		size = sizeof(struct ipu_cpd_metadata_cmpnt);
+
+	if ((meta_size - sizeof(*extn)) % size) {
+		dev_err(&isp->pdev->dev, "%s: Invalid metadata size\n",
+			__func__);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int ipu_cpd_validate_cpd_file(struct ipu_device *isp,
+			      const void *cpd_file, unsigned long cpd_file_size)
+{
+	const struct ipu_cpd_hdr *hdr = cpd_file;
+	struct ipu_cpd_ent *ent;
+	int rval;
+
+	rval = ipu_cpd_validate_cpd(isp, cpd_file,
+				    cpd_file_size, cpd_file_size);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Invalid CPD in file\n");
+		return -EINVAL;
+	}
+
+	/* Check for CPD file marker */
+	if (hdr->hdr_mark != CPD_HDR_MARK) {
+		dev_err(&isp->pdev->dev, "Invalid CPD header\n");
+		return -EINVAL;
+	}
+
+	/* Sanity check for manifest size */
+	ent = ipu_cpd_get_manifest(cpd_file);
+	if (ent->len > MAX_MANIFEST_SIZE) {
+		dev_err(&isp->pdev->dev, "Invalid manifest size\n");
+		return -EINVAL;
+	}
+
+	/* Validate metadata */
+	ent = ipu_cpd_get_metadata(cpd_file);
+	rval = ipu_cpd_validate_metadata(isp, cpd_file + ent->offset, ent->len);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Invalid metadata\n");
+		return rval;
+	}
+
+	/* Validate moduledata */
+	ent = ipu_cpd_get_moduledata(cpd_file);
+	rval = ipu_cpd_validate_moduledata(isp, cpd_file + ent->offset,
+					   ent->len);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Invalid moduledata\n");
+		return rval;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ipu_cpd_validate_cpd_file);
+
+unsigned int ipu_cpd_pkg_dir_get_address(const u64 *pkg_dir, int pkg_dir_idx)
+{
+	return pkg_dir[++pkg_dir_idx * PKG_DIR_ENT_LEN];
+}
+EXPORT_SYMBOL_GPL(ipu_cpd_pkg_dir_get_address);
+
+unsigned int ipu_cpd_pkg_dir_get_num_entries(const u64 *pkg_dir)
+{
+	return pkg_dir[1];
+}
+EXPORT_SYMBOL_GPL(ipu_cpd_pkg_dir_get_num_entries);
+
+unsigned int ipu_cpd_pkg_dir_get_size(const u64 *pkg_dir, int pkg_dir_idx)
+{
+	return pkg_dir[++pkg_dir_idx * PKG_DIR_ENT_LEN + 1] & PKG_DIR_SIZE_MASK;
+}
+EXPORT_SYMBOL_GPL(ipu_cpd_pkg_dir_get_size);
+
+unsigned int ipu_cpd_pkg_dir_get_type(const u64 *pkg_dir, int pkg_dir_idx)
+{
+	return pkg_dir[++pkg_dir_idx * PKG_DIR_ENT_LEN + 1] >>
+	    PKG_DIR_ID_SHIFT & PKG_DIR_ID_MASK;
+}
+EXPORT_SYMBOL_GPL(ipu_cpd_pkg_dir_get_type);
diff -ruN a/drivers/media/pci/intel/ipu-cpd.h b/drivers/media/pci/intel/ipu-cpd.h
--- a/drivers/media/pci/intel/ipu-cpd.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-cpd.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,110 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2015 - 2020 Intel Corporation */
+
+#ifndef IPU_CPD_H
+#define IPU_CPD_H
+
+#define IPU_CPD_SIZE_OF_FW_ARCH_VERSION		7
+#define IPU_CPD_SIZE_OF_SYSTEM_VERSION		11
+#define IPU_CPD_SIZE_OF_COMPONENT_NAME		12
+
+#define IPU_CPD_METADATA_EXTN_TYPE_IUNIT	0x10
+
+#define IPU_CPD_METADATA_IMAGE_TYPE_RESERVED		0
+#define IPU_CPD_METADATA_IMAGE_TYPE_BOOTLOADER		1
+#define IPU_CPD_METADATA_IMAGE_TYPE_MAIN_FIRMWARE	2
+
+#define IPU_CPD_PKG_DIR_PSYS_SERVER_IDX	0
+#define IPU_CPD_PKG_DIR_ISYS_SERVER_IDX	1
+
+#define IPU_CPD_PKG_DIR_CLIENT_PG_TYPE	3
+
+#define IPU6_CPD_METADATA_HASH_KEY_SIZE          48
+#define IPU_CPD_METADATA_HASH_KEY_SIZE           32
+
+struct __packed ipu_cpd_module_data_hdr {
+	u32 hdr_len;
+	u32 endian;
+	u32 fw_pkg_date;
+	u32 hive_sdk_date;
+	u32 compiler_date;
+	u32 target_platform_type;
+	u8 sys_ver[IPU_CPD_SIZE_OF_SYSTEM_VERSION];
+	u8 fw_arch_ver[IPU_CPD_SIZE_OF_FW_ARCH_VERSION];
+	u8 rsvd[2];
+};
+
+/* ipu_cpd_hdr structure updated as the chksum and
+ * sub_partition_name is unused on host side
+ * CSE layout version 1.6 for ipu6se (hdr_len = 0x10)
+ * CSE layout version 1.7 for ipu6 (hdr_len = 0x14)
+ */
+struct __packed ipu_cpd_hdr {
+	u32 hdr_mark;
+	u32 ent_cnt;
+	u8 hdr_ver;
+	u8 ent_ver;
+	u8 hdr_len;
+};
+
+struct __packed ipu_cpd_ent {
+	u8 name[IPU_CPD_SIZE_OF_COMPONENT_NAME];
+	u32 offset;
+	u32 len;
+	u8 rsvd[4];
+};
+
+struct __packed ipu_cpd_metadata_cmpnt {
+	u32 id;
+	u32 size;
+	u32 ver;
+	u8 sha2_hash[IPU_CPD_METADATA_HASH_KEY_SIZE];
+	u32 entry_point;
+	u32 icache_base_offs;
+	u8 attrs[16];
+};
+
+struct __packed ipu6_cpd_metadata_cmpnt {
+	u32 id;
+	u32 size;
+	u32 ver;
+	u8 sha2_hash[IPU6_CPD_METADATA_HASH_KEY_SIZE];
+	u32 entry_point;
+	u32 icache_base_offs;
+	u8 attrs[16];
+};
+
+struct __packed ipu_cpd_metadata_extn {
+	u32 extn_type;
+	u32 len;
+	u32 img_type;
+	u8 rsvd[16];
+};
+
+struct __packed ipu_cpd_client_pkg_hdr {
+	u32 prog_list_offs;
+	u32 prog_list_size;
+	u32 prog_desc_offs;
+	u32 prog_desc_size;
+	u32 pg_manifest_offs;
+	u32 pg_manifest_size;
+	u32 prog_bin_offs;
+	u32 prog_bin_size;
+};
+
+void *ipu_cpd_create_pkg_dir(struct ipu_bus_device *adev,
+			     const void *src,
+			     dma_addr_t dma_addr_src,
+			     dma_addr_t *dma_addr, unsigned int *pkg_dir_size);
+void ipu_cpd_free_pkg_dir(struct ipu_bus_device *adev,
+			  u64 *pkg_dir,
+			  dma_addr_t dma_addr, unsigned int pkg_dir_size);
+int ipu_cpd_validate_cpd_file(struct ipu_device *isp,
+			      const void *cpd_file,
+			      unsigned long cpd_file_size);
+unsigned int ipu_cpd_pkg_dir_get_address(const u64 *pkg_dir, int pkg_dir_idx);
+unsigned int ipu_cpd_pkg_dir_get_num_entries(const u64 *pkg_dir);
+unsigned int ipu_cpd_pkg_dir_get_size(const u64 *pkg_dir, int pkg_dir_idx);
+unsigned int ipu_cpd_pkg_dir_get_type(const u64 *pkg_dir, int pkg_dir_idx);
+
+#endif /* IPU_CPD_H */
diff -ruN a/drivers/media/pci/intel/ipu-dma.c b/drivers/media/pci/intel/ipu-dma.c
--- a/drivers/media/pci/intel/ipu-dma.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-dma.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,406 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2021 Intel Corporation
+
+#include <asm/cacheflush.h>
+
+#include <linux/slab.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/gfp.h>
+#include <linux/highmem.h>
+#include <linux/iova.h>
+#include <linux/module.h>
+#include <linux/scatterlist.h>
+#include <linux/version.h>
+#include <linux/vmalloc.h>
+#include <linux/dma-map-ops.h>
+
+#include "ipu-dma.h"
+#include "ipu-bus.h"
+#include "ipu-mmu.h"
+
+struct vm_info {
+	struct list_head list;
+	struct page **pages;
+	void *vaddr;
+	unsigned long size;
+};
+
+static struct vm_info *get_vm_info(struct ipu_mmu *mmu, void *vaddr)
+{
+	struct vm_info *info, *save;
+
+	list_for_each_entry_safe(info, save, &mmu->vma_list, list) {
+		if (info->vaddr == vaddr)
+			return info;
+	}
+
+	return NULL;
+}
+
+/* Begin of things adapted from arch/arm/mm/dma-mapping.c */
+static void __dma_clear_buffer(struct page *page, size_t size,
+			       unsigned long attrs)
+{
+	/*
+	 * Ensure that the allocated pages are zeroed, and that any data
+	 * lurking in the kernel direct-mapped region is invalidated.
+	 */
+	void *ptr = page_address(page);
+
+	memset(ptr, 0, size);
+	if ((attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
+		clflush_cache_range(ptr, size);
+}
+
+static struct page **__dma_alloc_buffer(struct device *dev, size_t size,
+					gfp_t gfp,
+					unsigned long attrs)
+{
+	struct page **pages;
+	int count = size >> PAGE_SHIFT;
+	int array_size = count * sizeof(struct page *);
+	int i = 0;
+
+	pages = kvzalloc(array_size, GFP_KERNEL);
+	if (!pages)
+		return NULL;
+
+	gfp |= __GFP_NOWARN;
+
+	while (count) {
+		int j, order = __fls(count);
+
+		pages[i] = alloc_pages(gfp, order);
+		while (!pages[i] && order)
+			pages[i] = alloc_pages(gfp, --order);
+		if (!pages[i])
+			goto error;
+
+		if (order) {
+			split_page(pages[i], order);
+			j = 1 << order;
+			while (--j)
+				pages[i + j] = pages[i] + j;
+		}
+
+		__dma_clear_buffer(pages[i], PAGE_SIZE << order, attrs);
+		i += 1 << order;
+		count -= 1 << order;
+	}
+
+	return pages;
+error:
+	while (i--)
+		if (pages[i])
+			__free_pages(pages[i], 0);
+	kvfree(pages);
+	return NULL;
+}
+
+static int __dma_free_buffer(struct device *dev, struct page **pages,
+			     size_t size,
+			     unsigned long attrs)
+{
+	int count = size >> PAGE_SHIFT;
+	int i;
+
+	for (i = 0; i < count; i++) {
+		if (pages[i]) {
+			__dma_clear_buffer(pages[i], PAGE_SIZE, attrs);
+			__free_pages(pages[i], 0);
+		}
+	}
+
+	kvfree(pages);
+	return 0;
+}
+
+/* End of things adapted from arch/arm/mm/dma-mapping.c */
+
+static void ipu_dma_sync_single_for_cpu(struct device *dev,
+					dma_addr_t dma_handle,
+					size_t size,
+					enum dma_data_direction dir)
+{
+	struct ipu_mmu *mmu = to_ipu_bus_device(dev)->mmu;
+	unsigned long pa = ipu_mmu_iova_to_phys(mmu->dmap->mmu_info,
+						dma_handle);
+
+	clflush_cache_range(phys_to_virt(pa), size);
+}
+
+static void ipu_dma_sync_sg_for_cpu(struct device *dev,
+				    struct scatterlist *sglist,
+				    int nents, enum dma_data_direction dir)
+{
+	struct scatterlist *sg;
+	int i;
+
+	for_each_sg(sglist, sg, nents, i)
+		clflush_cache_range(page_to_virt(sg_page(sg)), sg->length);
+}
+
+static void *ipu_dma_alloc(struct device *dev, size_t size,
+			   dma_addr_t *dma_handle, gfp_t gfp,
+			   unsigned long attrs)
+{
+	struct ipu_mmu *mmu = to_ipu_bus_device(dev)->mmu;
+	struct page **pages;
+	struct iova *iova;
+	struct vm_info *info;
+	int i;
+	int rval;
+	unsigned long count;
+
+	info = kzalloc(sizeof(*info), GFP_KERNEL);
+	if (!info)
+		return NULL;
+
+	size = PAGE_ALIGN(size);
+	count = size >> PAGE_SHIFT;
+
+	iova = alloc_iova(&mmu->dmap->iovad, count,
+			  dma_get_mask(dev) >> PAGE_SHIFT, 0);
+	if (!iova)
+		goto out_kfree;
+
+	pages = __dma_alloc_buffer(dev, size, gfp, attrs);
+	if (!pages)
+		goto out_free_iova;
+
+	for (i = 0; iova->pfn_lo + i <= iova->pfn_hi; i++) {
+		rval = ipu_mmu_map(mmu->dmap->mmu_info,
+				   (iova->pfn_lo + i) << PAGE_SHIFT,
+				   page_to_phys(pages[i]), PAGE_SIZE);
+		if (rval)
+			goto out_unmap;
+	}
+
+	info->vaddr = vmap(pages, count, VM_USERMAP, PAGE_KERNEL);
+	if (!info->vaddr)
+		goto out_unmap;
+
+	*dma_handle = iova->pfn_lo << PAGE_SHIFT;
+
+	info->pages = pages;
+	info->size = size;
+	list_add(&info->list, &mmu->vma_list);
+
+	return info->vaddr;
+
+out_unmap:
+	for (i--; i >= 0; i--) {
+		ipu_mmu_unmap(mmu->dmap->mmu_info,
+			      (iova->pfn_lo + i) << PAGE_SHIFT, PAGE_SIZE);
+	}
+	__dma_free_buffer(dev, pages, size, attrs);
+
+out_free_iova:
+	__free_iova(&mmu->dmap->iovad, iova);
+out_kfree:
+	kfree(info);
+
+	return NULL;
+}
+
+static void ipu_dma_free(struct device *dev, size_t size, void *vaddr,
+			 dma_addr_t dma_handle,
+			 unsigned long attrs)
+{
+	struct ipu_mmu *mmu = to_ipu_bus_device(dev)->mmu;
+	struct page **pages;
+	struct vm_info *info;
+	struct iova *iova = find_iova(&mmu->dmap->iovad,
+				      dma_handle >> PAGE_SHIFT);
+
+	if (WARN_ON(!iova))
+		return;
+
+	info = get_vm_info(mmu, vaddr);
+	if (WARN_ON(!info))
+		return;
+
+	if (WARN_ON(!info->vaddr))
+		return;
+
+	if (WARN_ON(!info->pages))
+		return;
+
+	list_del(&info->list);
+
+	size = PAGE_ALIGN(size);
+
+	pages = info->pages;
+
+	vunmap(vaddr);
+
+	ipu_mmu_unmap(mmu->dmap->mmu_info, iova->pfn_lo << PAGE_SHIFT,
+		      (iova->pfn_hi - iova->pfn_lo + 1) << PAGE_SHIFT);
+
+	__dma_free_buffer(dev, pages, size, attrs);
+
+	mmu->tlb_invalidate(mmu);
+
+	__free_iova(&mmu->dmap->iovad, iova);
+
+	kfree(info);
+}
+
+static int ipu_dma_mmap(struct device *dev, struct vm_area_struct *vma,
+			void *addr, dma_addr_t iova, size_t size,
+			unsigned long attrs)
+{
+	struct ipu_mmu *mmu = to_ipu_bus_device(dev)->mmu;
+	struct vm_info *info;
+	size_t count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+	size_t i;
+
+	info = get_vm_info(mmu, addr);
+	if (!info)
+		return -EFAULT;
+
+	if (!info->vaddr)
+		return -EFAULT;
+
+	if (vma->vm_start & ~PAGE_MASK)
+		return -EINVAL;
+
+	if (size > info->size)
+		return -EFAULT;
+
+	for (i = 0; i < count; i++)
+		vm_insert_page(vma, vma->vm_start + (i << PAGE_SHIFT),
+			       info->pages[i]);
+
+	return 0;
+}
+
+static void ipu_dma_unmap_sg(struct device *dev,
+			     struct scatterlist *sglist,
+			     int nents, enum dma_data_direction dir,
+			     unsigned long attrs)
+{
+	struct ipu_mmu *mmu = to_ipu_bus_device(dev)->mmu;
+	struct iova *iova = find_iova(&mmu->dmap->iovad,
+				      sg_dma_address(sglist) >> PAGE_SHIFT);
+
+	if (!nents)
+		return;
+
+	if (WARN_ON(!iova))
+		return;
+
+	if ((attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
+		ipu_dma_sync_sg_for_cpu(dev, sglist, nents, DMA_BIDIRECTIONAL);
+
+	ipu_mmu_unmap(mmu->dmap->mmu_info, iova->pfn_lo << PAGE_SHIFT,
+		      (iova->pfn_hi - iova->pfn_lo + 1) << PAGE_SHIFT);
+
+	mmu->tlb_invalidate(mmu);
+
+	__free_iova(&mmu->dmap->iovad, iova);
+}
+
+static int ipu_dma_map_sg(struct device *dev, struct scatterlist *sglist,
+			  int nents, enum dma_data_direction dir,
+			  unsigned long attrs)
+{
+	struct ipu_mmu *mmu = to_ipu_bus_device(dev)->mmu;
+	struct scatterlist *sg;
+	struct iova *iova;
+	size_t size = 0;
+	u32 iova_addr;
+	int i;
+
+	for_each_sg(sglist, sg, nents, i)
+		size += PAGE_ALIGN(sg->length) >> PAGE_SHIFT;
+
+	dev_dbg(dev, "dmamap: mapping sg %d entries, %zu pages\n", nents, size);
+
+	iova = alloc_iova(&mmu->dmap->iovad, size,
+			  dma_get_mask(dev) >> PAGE_SHIFT, 0);
+	if (!iova)
+		return 0;
+
+	dev_dbg(dev, "dmamap: iova low pfn %lu, high pfn %lu\n", iova->pfn_lo,
+		iova->pfn_hi);
+
+	iova_addr = iova->pfn_lo;
+
+	for_each_sg(sglist, sg, nents, i) {
+		int rval;
+
+		dev_dbg(dev, "mapping entry %d: iova 0x%8.8x,phy 0x%16.16llx\n",
+			i, iova_addr << PAGE_SHIFT,
+			(unsigned long long)page_to_phys(sg_page(sg)));
+		rval = ipu_mmu_map(mmu->dmap->mmu_info, iova_addr << PAGE_SHIFT,
+				   page_to_phys(sg_page(sg)),
+				   PAGE_ALIGN(sg->length));
+		if (rval)
+			goto out_fail;
+		sg_dma_address(sg) = iova_addr << PAGE_SHIFT;
+#ifdef CONFIG_NEED_SG_DMA_LENGTH
+		sg_dma_len(sg) = sg->length;
+#endif /* CONFIG_NEED_SG_DMA_LENGTH */
+
+		iova_addr += PAGE_ALIGN(sg->length) >> PAGE_SHIFT;
+	}
+
+	if ((attrs & DMA_ATTR_SKIP_CPU_SYNC) == 0)
+		ipu_dma_sync_sg_for_cpu(dev, sglist, nents, DMA_BIDIRECTIONAL);
+
+	mmu->tlb_invalidate(mmu);
+
+	return nents;
+
+out_fail:
+	ipu_dma_unmap_sg(dev, sglist, i, dir, attrs);
+
+	return 0;
+}
+
+/*
+ * Create scatter-list for the already allocated DMA buffer
+ */
+static int ipu_dma_get_sgtable(struct device *dev, struct sg_table *sgt,
+			       void *cpu_addr, dma_addr_t handle, size_t size,
+			       unsigned long attrs)
+{
+	struct ipu_mmu *mmu = to_ipu_bus_device(dev)->mmu;
+	struct vm_info *info;
+	int n_pages;
+	int ret = 0;
+
+	info = get_vm_info(mmu, cpu_addr);
+	if (!info)
+		return -EFAULT;
+
+	if (!info->vaddr)
+		return -EFAULT;
+
+	if (WARN_ON(!info->pages))
+		return -ENOMEM;
+
+	n_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;
+
+	ret = sg_alloc_table_from_pages(sgt, info->pages, n_pages, 0, size,
+					GFP_KERNEL);
+	if (ret)
+		dev_dbg(dev, "IPU get sgt table fail\n");
+
+	return ret;
+}
+
+const struct dma_map_ops ipu_dma_ops = {
+	.alloc = ipu_dma_alloc,
+	.free = ipu_dma_free,
+	.mmap = ipu_dma_mmap,
+	.map_sg = ipu_dma_map_sg,
+	.unmap_sg = ipu_dma_unmap_sg,
+	.sync_single_for_cpu = ipu_dma_sync_single_for_cpu,
+	.sync_single_for_device = ipu_dma_sync_single_for_cpu,
+	.sync_sg_for_cpu = ipu_dma_sync_sg_for_cpu,
+	.sync_sg_for_device = ipu_dma_sync_sg_for_cpu,
+	.get_sgtable = ipu_dma_get_sgtable,
+};
diff -ruN a/drivers/media/pci/intel/ipu-dma.h b/drivers/media/pci/intel/ipu-dma.h
--- a/drivers/media/pci/intel/ipu-dma.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-dma.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,19 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_DMA_H
+#define IPU_DMA_H
+
+#include <linux/iova.h>
+
+struct ipu_mmu_info;
+
+struct ipu_dma_mapping {
+	struct ipu_mmu_info *mmu_info;
+	struct iova_domain iovad;
+	struct kref ref;
+};
+
+extern const struct dma_map_ops ipu_dma_ops;
+
+#endif /* IPU_DMA_H */
diff -ruN a/drivers/media/pci/intel/ipu-fw-com.c b/drivers/media/pci/intel/ipu-fw-com.c
--- a/drivers/media/pci/intel/ipu-fw-com.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-fw-com.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,496 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2020 Intel Corporation
+
+#include <asm/cacheflush.h>
+
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/dma-mapping.h>
+
+#include "ipu.h"
+#include "ipu-fw-com.h"
+#include "ipu-bus.h"
+
+/*
+ * FWCOM layer is a shared resource between FW and driver. It consist
+ * of token queues to both send and receive directions. Queue is simply
+ * an array of structures with read and write indexes to the queue.
+ * There are 1...n queues to both directions. Queues locates in
+ * system ram and are mapped to ISP MMU so that both CPU and ISP can
+ * see the same buffer. Indexes are located in ISP DMEM so that FW code
+ * can poll those with very low latency and cost. CPU access to indexes is
+ * more costly but that happens only at message sending time and
+ * interrupt trigged message handling. CPU doesn't need to poll indexes.
+ * wr_reg / rd_reg are offsets to those dmem location. They are not
+ * the indexes itself.
+ */
+
+/* Shared structure between driver and FW - do not modify */
+struct ipu_fw_sys_queue {
+	u64 host_address;
+	u32 vied_address;
+	u32 size;
+	u32 token_size;
+	u32 wr_reg;	/* reg no in subsystem's regmem */
+	u32 rd_reg;
+	u32 _align;
+};
+
+struct ipu_fw_sys_queue_res {
+	u64 host_address;
+	u32 vied_address;
+	u32 reg;
+};
+
+enum syscom_state {
+	/* Program load or explicit host setting should init to this */
+	SYSCOM_STATE_UNINIT = 0x57A7E000,
+	/* SP Syscom sets this when it is ready for use */
+	SYSCOM_STATE_READY = 0x57A7E001,
+	/* SP Syscom sets this when no more syscom accesses will happen */
+	SYSCOM_STATE_INACTIVE = 0x57A7E002
+};
+
+enum syscom_cmd {
+	/* Program load or explicit host setting should init to this */
+	SYSCOM_COMMAND_UNINIT = 0x57A7F000,
+	/* Host Syscom requests syscom to become inactive */
+	SYSCOM_COMMAND_INACTIVE = 0x57A7F001
+};
+
+/* firmware config: data that sent from the host to SP via DDR */
+/* Cell copies data into a context */
+
+struct ipu_fw_syscom_config {
+	u32 firmware_address;
+
+	u32 num_input_queues;
+	u32 num_output_queues;
+
+	/* ISP pointers to an array of ipu_fw_sys_queue structures */
+	u32 input_queue;
+	u32 output_queue;
+
+	/* ISYS / PSYS private data */
+	u32 specific_addr;
+	u32 specific_size;
+};
+
+/* End of shared structures / data */
+
+struct ipu_fw_com_context {
+	struct ipu_bus_device *adev;
+	void __iomem *dmem_addr;
+	int (*cell_ready)(struct ipu_bus_device *adev);
+	void (*cell_start)(struct ipu_bus_device *adev);
+
+	void *dma_buffer;
+	dma_addr_t dma_addr;
+	unsigned int dma_size;
+	unsigned long attrs;
+
+	unsigned int num_input_queues;
+	unsigned int num_output_queues;
+
+	struct ipu_fw_sys_queue *input_queue;	/* array of host to SP queues */
+	struct ipu_fw_sys_queue *output_queue;	/* array of SP to host */
+
+	void *config_host_addr;
+	void *specific_host_addr;
+	u64 ibuf_host_addr;
+	u64 obuf_host_addr;
+
+	u32 config_vied_addr;
+	u32 input_queue_vied_addr;
+	u32 output_queue_vied_addr;
+	u32 specific_vied_addr;
+	u32 ibuf_vied_addr;
+	u32 obuf_vied_addr;
+
+	unsigned int buttress_boot_offset;
+	void __iomem *base_addr;
+};
+
+#define FW_COM_WR_REG 0
+#define FW_COM_RD_REG 4
+
+#define REGMEM_OFFSET 0
+#define TUNIT_MAGIC_PATTERN 0x5a5a5a5a
+
+enum regmem_id {
+	/* pass pkg_dir address to SPC in non-secure mode */
+	PKG_DIR_ADDR_REG = 0,
+	/* Tunit CFG blob for secure - provided by host.*/
+	TUNIT_CFG_DWR_REG = 1,
+	/* syscom commands - modified by the host */
+	SYSCOM_COMMAND_REG = 2,
+	/* Store interrupt status - updated by SP */
+	SYSCOM_IRQ_REG = 3,
+	/* first syscom queue pointer register */
+	SYSCOM_QPR_BASE_REG = 4
+};
+
+enum message_direction {
+	DIR_RECV = 0,
+	DIR_SEND
+};
+
+#define BUTRESS_FW_BOOT_PARAMS_0 0x4000
+#define BUTTRESS_FW_BOOT_PARAM_REG(base, offset, id) ((base) \
+	+ BUTRESS_FW_BOOT_PARAMS_0 + ((offset) + (id)) * 4)
+
+enum buttress_syscom_id {
+	/* pass syscom configuration to SPC */
+	SYSCOM_CONFIG_ID		= 0,
+	/* syscom state - modified by SP */
+	SYSCOM_STATE_ID			= 1,
+	/* syscom vtl0 addr mask */
+	SYSCOM_VTL0_ADDR_MASK_ID	= 2,
+	SYSCOM_ID_MAX
+};
+
+static unsigned int num_messages(unsigned int wr, unsigned int rd,
+				 unsigned int size)
+{
+	if (wr < rd)
+		wr += size;
+	return wr - rd;
+}
+
+static unsigned int num_free(unsigned int wr, unsigned int rd,
+			     unsigned int size)
+{
+	return size - num_messages(wr, rd, size);
+}
+
+static unsigned int curr_index(void __iomem *q_dmem,
+			       enum message_direction dir)
+{
+	return readl(q_dmem +
+			 (dir == DIR_RECV ? FW_COM_RD_REG : FW_COM_WR_REG));
+}
+
+static unsigned int inc_index(void __iomem *q_dmem, struct ipu_fw_sys_queue *q,
+			      enum message_direction dir)
+{
+	unsigned int index;
+
+	index = curr_index(q_dmem, dir) + 1;
+	return index >= q->size ? 0 : index;
+}
+
+static unsigned int ipu_sys_queue_buf_size(unsigned int size,
+					   unsigned int token_size)
+{
+	return (size + 1) * token_size;
+}
+
+static void ipu_sys_queue_init(struct ipu_fw_sys_queue *q, unsigned int size,
+			       unsigned int token_size,
+			       struct ipu_fw_sys_queue_res *res)
+{
+	unsigned int buf_size;
+
+	q->size = size + 1;
+	q->token_size = token_size;
+	buf_size = ipu_sys_queue_buf_size(size, token_size);
+
+	/* acquire the shared buffer space */
+	q->host_address = res->host_address;
+	res->host_address += buf_size;
+	q->vied_address = res->vied_address;
+	res->vied_address += buf_size;
+
+	/* acquire the shared read and writer pointers */
+	q->wr_reg = res->reg;
+	res->reg++;
+	q->rd_reg = res->reg;
+	res->reg++;
+}
+
+void *ipu_fw_com_prepare(struct ipu_fw_com_cfg *cfg,
+			 struct ipu_bus_device *adev, void __iomem *base)
+{
+	struct ipu_fw_com_context *ctx;
+	struct ipu_fw_syscom_config *fw_cfg;
+	unsigned int i;
+	unsigned int sizeall, offset;
+	unsigned int sizeinput = 0, sizeoutput = 0;
+	unsigned long attrs = 0;
+	struct ipu_fw_sys_queue_res res;
+
+	/* error handling */
+	if (!cfg || !cfg->cell_start || !cfg->cell_ready)
+		return NULL;
+
+	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return NULL;
+	ctx->dmem_addr = base + cfg->dmem_addr + REGMEM_OFFSET;
+	ctx->adev = adev;
+	ctx->cell_start = cfg->cell_start;
+	ctx->cell_ready = cfg->cell_ready;
+	ctx->buttress_boot_offset = cfg->buttress_boot_offset;
+	ctx->base_addr  = base;
+
+	ctx->num_input_queues = cfg->num_input_queues;
+	ctx->num_output_queues = cfg->num_output_queues;
+
+	/*
+	 * Allocate DMA mapped memory. Allocate one big chunk.
+	 */
+	sizeall =
+	    /* Base cfg for FW */
+	    roundup(sizeof(struct ipu_fw_syscom_config), 8) +
+	    /* Descriptions of the queues */
+	    cfg->num_input_queues * sizeof(struct ipu_fw_sys_queue) +
+	    cfg->num_output_queues * sizeof(struct ipu_fw_sys_queue) +
+	    /* FW specific information structure */
+	    roundup(cfg->specific_size, 8);
+
+	for (i = 0; i < cfg->num_input_queues; i++)
+		sizeinput += ipu_sys_queue_buf_size(cfg->input[i].queue_size,
+						cfg->input[i].token_size);
+
+	for (i = 0; i < cfg->num_output_queues; i++)
+		sizeoutput += ipu_sys_queue_buf_size(cfg->output[i].queue_size,
+						 cfg->output[i].token_size);
+
+	sizeall += sizeinput + sizeoutput;
+
+	ctx->dma_buffer = dma_alloc_attrs(&ctx->adev->dev, sizeall,
+					  &ctx->dma_addr, GFP_KERNEL,
+					  attrs);
+	ctx->attrs = attrs;
+	if (!ctx->dma_buffer) {
+		dev_err(&ctx->adev->dev, "failed to allocate dma memory\n");
+		kfree(ctx);
+		return NULL;
+	}
+
+	ctx->dma_size = sizeall;
+
+	/* This is the address where FW starts to parse allocations */
+	ctx->config_host_addr = ctx->dma_buffer;
+	ctx->config_vied_addr = ctx->dma_addr;
+	fw_cfg = (struct ipu_fw_syscom_config *)ctx->config_host_addr;
+	offset = roundup(sizeof(struct ipu_fw_syscom_config), 8);
+
+	ctx->input_queue = ctx->dma_buffer + offset;
+	ctx->input_queue_vied_addr = ctx->dma_addr + offset;
+	offset += cfg->num_input_queues * sizeof(struct ipu_fw_sys_queue);
+
+	ctx->output_queue = ctx->dma_buffer + offset;
+	ctx->output_queue_vied_addr = ctx->dma_addr + offset;
+	offset += cfg->num_output_queues * sizeof(struct ipu_fw_sys_queue);
+
+	ctx->specific_host_addr = ctx->dma_buffer + offset;
+	ctx->specific_vied_addr = ctx->dma_addr + offset;
+	offset += roundup(cfg->specific_size, 8);
+
+	ctx->ibuf_host_addr = (uintptr_t)(ctx->dma_buffer + offset);
+	ctx->ibuf_vied_addr = ctx->dma_addr + offset;
+	offset += sizeinput;
+
+	ctx->obuf_host_addr = (uintptr_t)(ctx->dma_buffer + offset);
+	ctx->obuf_vied_addr = ctx->dma_addr + offset;
+	offset += sizeoutput;
+
+	/* initialize input queues */
+	res.reg = SYSCOM_QPR_BASE_REG;
+	res.host_address = ctx->ibuf_host_addr;
+	res.vied_address = ctx->ibuf_vied_addr;
+	for (i = 0; i < cfg->num_input_queues; i++) {
+		ipu_sys_queue_init(ctx->input_queue + i,
+				   cfg->input[i].queue_size,
+				   cfg->input[i].token_size, &res);
+	}
+
+	/* initialize output queues */
+	res.host_address = ctx->obuf_host_addr;
+	res.vied_address = ctx->obuf_vied_addr;
+	for (i = 0; i < cfg->num_output_queues; i++) {
+		ipu_sys_queue_init(ctx->output_queue + i,
+				   cfg->output[i].queue_size,
+				   cfg->output[i].token_size, &res);
+	}
+
+	/* copy firmware specific data */
+	if (cfg->specific_addr && cfg->specific_size) {
+		memcpy((void *)ctx->specific_host_addr,
+		       cfg->specific_addr, cfg->specific_size);
+	}
+
+	fw_cfg->num_input_queues = cfg->num_input_queues;
+	fw_cfg->num_output_queues = cfg->num_output_queues;
+	fw_cfg->input_queue = ctx->input_queue_vied_addr;
+	fw_cfg->output_queue = ctx->output_queue_vied_addr;
+	fw_cfg->specific_addr = ctx->specific_vied_addr;
+	fw_cfg->specific_size = cfg->specific_size;
+	return ctx;
+}
+EXPORT_SYMBOL_GPL(ipu_fw_com_prepare);
+
+int ipu_fw_com_open(struct ipu_fw_com_context *ctx)
+{
+	/*
+	 * Disable tunit configuration by FW.
+	 * This feature is used to configure tunit in secure mode.
+	 */
+	writel(TUNIT_MAGIC_PATTERN, ctx->dmem_addr + TUNIT_CFG_DWR_REG * 4);
+	/* Check if SP is in valid state */
+	if (!ctx->cell_ready(ctx->adev))
+		return -EIO;
+
+	/* store syscom uninitialized command */
+	writel(SYSCOM_COMMAND_UNINIT,
+	       ctx->dmem_addr + SYSCOM_COMMAND_REG * 4);
+
+	/* store syscom uninitialized state */
+	writel(SYSCOM_STATE_UNINIT,
+	       BUTTRESS_FW_BOOT_PARAM_REG(ctx->base_addr,
+					  ctx->buttress_boot_offset,
+					  SYSCOM_STATE_ID));
+
+	/* store firmware configuration address */
+	writel(ctx->config_vied_addr,
+	       BUTTRESS_FW_BOOT_PARAM_REG(ctx->base_addr,
+					  ctx->buttress_boot_offset,
+					  SYSCOM_CONFIG_ID));
+	ctx->cell_start(ctx->adev);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ipu_fw_com_open);
+
+int ipu_fw_com_close(struct ipu_fw_com_context *ctx)
+{
+	int state;
+
+	state = readl(BUTTRESS_FW_BOOT_PARAM_REG(ctx->base_addr,
+						 ctx->buttress_boot_offset,
+						 SYSCOM_STATE_ID));
+	if (state != SYSCOM_STATE_READY)
+		return -EBUSY;
+
+	/* set close request flag */
+	writel(SYSCOM_COMMAND_INACTIVE, ctx->dmem_addr +
+		   SYSCOM_COMMAND_REG * 4);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ipu_fw_com_close);
+
+int ipu_fw_com_release(struct ipu_fw_com_context *ctx, unsigned int force)
+{
+	/* check if release is forced, an verify cell state if it is not */
+	if (!force && !ctx->cell_ready(ctx->adev))
+		return -EBUSY;
+
+	dma_free_attrs(&ctx->adev->dev, ctx->dma_size,
+		       ctx->dma_buffer, ctx->dma_addr,
+		       ctx->attrs);
+	kfree(ctx);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ipu_fw_com_release);
+
+int ipu_fw_com_ready(struct ipu_fw_com_context *ctx)
+{
+	int state;
+
+	state = readl(BUTTRESS_FW_BOOT_PARAM_REG(ctx->base_addr,
+						 ctx->buttress_boot_offset,
+						 SYSCOM_STATE_ID));
+	if (state != SYSCOM_STATE_READY)
+		return -EBUSY;	/* SPC is not ready to handle messages yet */
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ipu_fw_com_ready);
+
+static bool is_index_valid(struct ipu_fw_sys_queue *q, unsigned int index)
+{
+	if (index >= q->size)
+		return false;
+	return true;
+}
+
+void *ipu_send_get_token(struct ipu_fw_com_context *ctx, int q_nbr)
+{
+	struct ipu_fw_sys_queue *q = &ctx->input_queue[q_nbr];
+	void __iomem *q_dmem = ctx->dmem_addr + q->wr_reg * 4;
+	unsigned int wr, rd;
+	unsigned int packets;
+	unsigned int index;
+
+	wr = readl(q_dmem + FW_COM_WR_REG);
+	rd = readl(q_dmem + FW_COM_RD_REG);
+
+	/* Catch indexes in dmem */
+	if (!is_index_valid(q, wr) || !is_index_valid(q, rd))
+		return NULL;
+
+	packets = num_free(wr + 1, rd, q->size);
+	if (!packets)
+		return NULL;
+
+	index = curr_index(q_dmem, DIR_SEND);
+
+	return (void *)(unsigned long)q->host_address + (index * q->token_size);
+}
+EXPORT_SYMBOL_GPL(ipu_send_get_token);
+
+void ipu_send_put_token(struct ipu_fw_com_context *ctx, int q_nbr)
+{
+	struct ipu_fw_sys_queue *q = &ctx->input_queue[q_nbr];
+	void __iomem *q_dmem = ctx->dmem_addr + q->wr_reg * 4;
+	int index = curr_index(q_dmem, DIR_SEND);
+
+	/* Increment index */
+	index = inc_index(q_dmem, q, DIR_SEND);
+
+	writel(index, q_dmem + FW_COM_WR_REG);
+}
+EXPORT_SYMBOL_GPL(ipu_send_put_token);
+
+void *ipu_recv_get_token(struct ipu_fw_com_context *ctx, int q_nbr)
+{
+	struct ipu_fw_sys_queue *q = &ctx->output_queue[q_nbr];
+	void __iomem *q_dmem = ctx->dmem_addr + q->wr_reg * 4;
+	unsigned int wr, rd;
+	unsigned int packets;
+	void *addr;
+
+	wr = readl(q_dmem + FW_COM_WR_REG);
+	rd = readl(q_dmem + FW_COM_RD_REG);
+
+	/* Catch indexes in dmem? */
+	if (!is_index_valid(q, wr) || !is_index_valid(q, rd))
+		return NULL;
+
+	packets = num_messages(wr, rd, q->size);
+	if (!packets)
+		return NULL;
+
+	addr = (void *)(unsigned long)q->host_address + (rd * q->token_size);
+
+	return addr;
+}
+EXPORT_SYMBOL_GPL(ipu_recv_get_token);
+
+void ipu_recv_put_token(struct ipu_fw_com_context *ctx, int q_nbr)
+{
+	struct ipu_fw_sys_queue *q = &ctx->output_queue[q_nbr];
+	void __iomem *q_dmem = ctx->dmem_addr + q->wr_reg * 4;
+	unsigned int rd = inc_index(q_dmem, q, DIR_RECV);
+
+	/* Release index */
+	writel(rd, q_dmem + FW_COM_RD_REG);
+}
+EXPORT_SYMBOL_GPL(ipu_recv_put_token);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Intel ipu fw comm library");
diff -ruN a/drivers/media/pci/intel/ipu-fw-com.h b/drivers/media/pci/intel/ipu-fw-com.h
--- a/drivers/media/pci/intel/ipu-fw-com.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-fw-com.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,48 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_FW_COM_H
+#define IPU_FW_COM_H
+
+struct ipu_fw_com_context;
+struct ipu_bus_device;
+
+struct ipu_fw_syscom_queue_config {
+	unsigned int queue_size;	/* tokens per queue */
+	unsigned int token_size;	/* bytes per token */
+};
+
+#define SYSCOM_BUTTRESS_FW_PARAMS_ISYS_OFFSET	0
+#define SYSCOM_BUTTRESS_FW_PARAMS_PSYS_OFFSET	7
+
+struct ipu_fw_com_cfg {
+	unsigned int num_input_queues;
+	unsigned int num_output_queues;
+	struct ipu_fw_syscom_queue_config *input;
+	struct ipu_fw_syscom_queue_config *output;
+
+	unsigned int dmem_addr;
+
+	/* firmware-specific configuration data */
+	void *specific_addr;
+	unsigned int specific_size;
+	int (*cell_ready)(struct ipu_bus_device *adev);
+	void (*cell_start)(struct ipu_bus_device *adev);
+
+	unsigned int buttress_boot_offset;
+};
+
+void *ipu_fw_com_prepare(struct ipu_fw_com_cfg *cfg,
+			 struct ipu_bus_device *adev, void __iomem *base);
+
+int ipu_fw_com_open(struct ipu_fw_com_context *ctx);
+int ipu_fw_com_ready(struct ipu_fw_com_context *ctx);
+int ipu_fw_com_close(struct ipu_fw_com_context *ctx);
+int ipu_fw_com_release(struct ipu_fw_com_context *ctx, unsigned int force);
+
+void *ipu_recv_get_token(struct ipu_fw_com_context *ctx, int q_nbr);
+void ipu_recv_put_token(struct ipu_fw_com_context *ctx, int q_nbr);
+void *ipu_send_get_token(struct ipu_fw_com_context *ctx, int q_nbr);
+void ipu_send_put_token(struct ipu_fw_com_context *ctx, int q_nbr);
+
+#endif
diff -ruN a/drivers/media/pci/intel/ipu-fw-isys.c b/drivers/media/pci/intel/ipu-fw-isys.c
--- a/drivers/media/pci/intel/ipu-fw-isys.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-fw-isys.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,600 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2021 Intel Corporation
+
+#include <asm/cacheflush.h>
+
+#include <linux/kernel.h>
+#include <linux/delay.h>
+
+#include "ipu.h"
+#include "ipu-trace.h"
+#include "ipu-platform-regs.h"
+#include "ipu-platform.h"
+#include "ipu-fw-isys.h"
+#include "ipu-fw-com.h"
+#include "ipu-isys.h"
+
+#define IPU_FW_UNSUPPORTED_DATA_TYPE	0
+static const uint32_t
+extracted_bits_per_pixel_per_mipi_data_type[N_IPU_FW_ISYS_MIPI_DATA_TYPE] = {
+	64,	/* [0x00]   IPU_FW_ISYS_MIPI_DATA_TYPE_FRAME_START_CODE */
+	64,	/* [0x01]   IPU_FW_ISYS_MIPI_DATA_TYPE_FRAME_END_CODE */
+	64,	/* [0x02]   IPU_FW_ISYS_MIPI_DATA_TYPE_LINE_START_CODE */
+	64,	/* [0x03]   IPU_FW_ISYS_MIPI_DATA_TYPE_LINE_END_CODE */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x04] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x05] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x06] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x07] */
+	64,	/* [0x08]   IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT1 */
+	64,	/* [0x09]   IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT2 */
+	64,	/* [0x0A]   IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT3 */
+	64,	/* [0x0B]   IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT4 */
+	64,	/* [0x0C]   IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT5 */
+	64,	/* [0x0D]   IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT6 */
+	64,	/* [0x0E]   IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT7 */
+	64,	/* [0x0F]   IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT8 */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x10] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x11] */
+	8,	/* [0x12]    IPU_FW_ISYS_MIPI_DATA_TYPE_EMBEDDED */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x13] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x14] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x15] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x16] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x17] */
+	12,	/* [0x18]   IPU_FW_ISYS_MIPI_DATA_TYPE_YUV420_8 */
+	15,	/* [0x19]   IPU_FW_ISYS_MIPI_DATA_TYPE_YUV420_10 */
+	12,	/* [0x1A]   IPU_FW_ISYS_MIPI_DATA_TYPE_YUV420_8_LEGACY */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x1B] */
+	12,	/* [0x1C]   IPU_FW_ISYS_MIPI_DATA_TYPE_YUV420_8_SHIFT */
+	15,	/* [0x1D]   IPU_FW_ISYS_MIPI_DATA_TYPE_YUV420_10_SHIFT */
+	16,	/* [0x1E]   IPU_FW_ISYS_MIPI_DATA_TYPE_YUV422_8 */
+	20,	/* [0x1F]   IPU_FW_ISYS_MIPI_DATA_TYPE_YUV422_10 */
+	16,	/* [0x20]   IPU_FW_ISYS_MIPI_DATA_TYPE_RGB_444 */
+	16,	/* [0x21]   IPU_FW_ISYS_MIPI_DATA_TYPE_RGB_555 */
+	16,	/* [0x22]   IPU_FW_ISYS_MIPI_DATA_TYPE_RGB_565 */
+	18,	/* [0x23]   IPU_FW_ISYS_MIPI_DATA_TYPE_RGB_666 */
+	24,	/* [0x24]   IPU_FW_ISYS_MIPI_DATA_TYPE_RGB_888 */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x25] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x26] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x27] */
+	6,	/* [0x28]    IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_6 */
+	7,	/* [0x29]    IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_7 */
+	8,	/* [0x2A]    IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_8 */
+	10,	/* [0x2B]   IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_10 */
+	12,	/* [0x2C]   IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_12 */
+	14,	/* [0x2D]   IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_14 */
+	16,	/* [0x2E]   IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_16 */
+	8,	/* [0x2F]    IPU_FW_ISYS_MIPI_DATA_TYPE_BINARY_8 */
+	8,	/* [0x30]    IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF1 */
+	8,	/* [0x31]    IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF2 */
+	8,	/* [0x32]    IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF3 */
+	8,	/* [0x33]    IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF4 */
+	8,	/* [0x34]    IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF5 */
+	8,	/* [0x35]    IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF6 */
+	8,	/* [0x36]    IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF7 */
+	8,	/* [0x37]    IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF8 */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x38] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x39] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x3A] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x3B] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x3C] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x3D] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE,	/* [0x3E] */
+	IPU_FW_UNSUPPORTED_DATA_TYPE	/* [0x3F] */
+};
+
+static const char send_msg_types[N_IPU_FW_ISYS_SEND_TYPE][32] = {
+	"STREAM_OPEN",
+	"STREAM_START",
+	"STREAM_START_AND_CAPTURE",
+	"STREAM_CAPTURE",
+	"STREAM_STOP",
+	"STREAM_FLUSH",
+	"STREAM_CLOSE"
+};
+
+static int handle_proxy_response(struct ipu_isys *isys, unsigned int req_id)
+{
+	struct ipu_fw_isys_proxy_resp_info_abi *resp;
+	int rval = -EIO;
+
+	resp = (struct ipu_fw_isys_proxy_resp_info_abi *)
+	    ipu_recv_get_token(isys->fwcom, IPU_BASE_PROXY_RECV_QUEUES);
+	if (!resp)
+		return 1;
+
+	dev_dbg(&isys->adev->dev,
+		"Proxy response: id 0x%x, error %d, details %d\n",
+		resp->request_id, resp->error_info.error,
+		resp->error_info.error_details);
+
+	if (req_id == resp->request_id)
+		rval = 0;
+
+	ipu_recv_put_token(isys->fwcom, IPU_BASE_PROXY_RECV_QUEUES);
+	return rval;
+}
+
+/* Simple blocking proxy send function */
+int ipu_fw_isys_send_proxy_token(struct ipu_isys *isys,
+				 unsigned int req_id,
+				 unsigned int index,
+				 unsigned int offset, u32 value)
+{
+	struct ipu_fw_com_context *ctx = isys->fwcom;
+	struct ipu_fw_proxy_send_queue_token *token;
+	unsigned int timeout = 1000;
+	int rval = -EBUSY;
+
+	dev_dbg(&isys->adev->dev,
+		"proxy send token: req_id 0x%x, index %d, offset 0x%x, value 0x%x\n",
+		req_id, index, offset, value);
+
+	token = ipu_send_get_token(ctx, IPU_BASE_PROXY_SEND_QUEUES);
+	if (!token)
+		goto leave;
+
+	token->request_id = req_id;
+	token->region_index = index;
+	token->offset = offset;
+	token->value = value;
+	ipu_send_put_token(ctx, IPU_BASE_PROXY_SEND_QUEUES);
+
+	/* Currently proxy doesn't support irq based service. Poll */
+	do {
+		usleep_range(100, 110);
+		rval = handle_proxy_response(isys, req_id);
+		if (!rval)
+			break;
+		if (rval == -EIO) {
+			dev_err(&isys->adev->dev,
+				"Proxy response received with unexpected id\n");
+			break;
+		}
+		timeout--;
+	} while (rval && timeout);
+
+	if (!timeout)
+		dev_err(&isys->adev->dev, "Proxy response timed out\n");
+leave:
+	return rval;
+}
+
+int
+ipu_fw_isys_complex_cmd(struct ipu_isys *isys,
+			const unsigned int stream_handle,
+			void *cpu_mapped_buf,
+			dma_addr_t dma_mapped_buf,
+			size_t size, enum ipu_fw_isys_send_type send_type)
+{
+	struct ipu_fw_com_context *ctx = isys->fwcom;
+	struct ipu_fw_send_queue_token *token;
+
+	if (send_type >= N_IPU_FW_ISYS_SEND_TYPE)
+		return -EINVAL;
+
+	dev_dbg(&isys->adev->dev, "send_token: %s\n",
+		send_msg_types[send_type]);
+
+	/*
+	 * Time to flush cache in case we have some payload. Not all messages
+	 * have that
+	 */
+	if (cpu_mapped_buf)
+		clflush_cache_range(cpu_mapped_buf, size);
+
+	token = ipu_send_get_token(ctx,
+				   stream_handle + IPU_BASE_MSG_SEND_QUEUES);
+	if (!token)
+		return -EBUSY;
+
+	token->payload = dma_mapped_buf;
+	token->buf_handle = (unsigned long)cpu_mapped_buf;
+	token->send_type = send_type;
+
+	ipu_send_put_token(ctx, stream_handle + IPU_BASE_MSG_SEND_QUEUES);
+
+	return 0;
+}
+
+int ipu_fw_isys_simple_cmd(struct ipu_isys *isys,
+			   const unsigned int stream_handle,
+			   enum ipu_fw_isys_send_type send_type)
+{
+	return ipu_fw_isys_complex_cmd(isys, stream_handle, NULL, 0, 0,
+				       send_type);
+}
+
+int ipu_fw_isys_close(struct ipu_isys *isys)
+{
+	struct device *dev = &isys->adev->dev;
+	int timeout = IPU_ISYS_TURNOFF_TIMEOUT;
+	int rval;
+	unsigned long flags;
+	void *fwcom;
+
+	/*
+	 * Stop the isys fw. Actual close takes
+	 * some time as the FW must stop its actions including code fetch
+	 * to SP icache.
+	 * spinlock to wait the interrupt handler to be finished
+	 */
+	spin_lock_irqsave(&isys->power_lock, flags);
+	rval = ipu_fw_com_close(isys->fwcom);
+	fwcom = isys->fwcom;
+	isys->fwcom = NULL;
+	spin_unlock_irqrestore(&isys->power_lock, flags);
+	if (rval)
+		dev_err(dev, "Device close failure: %d\n", rval);
+
+	/* release probably fails if the close failed. Let's try still */
+	do {
+		usleep_range(IPU_ISYS_TURNOFF_DELAY_US,
+			     2 * IPU_ISYS_TURNOFF_DELAY_US);
+		rval = ipu_fw_com_release(fwcom, 0);
+		timeout--;
+	} while (rval != 0 && timeout);
+
+	if (rval) {
+		dev_err(dev, "Device release time out %d\n", rval);
+		spin_lock_irqsave(&isys->power_lock, flags);
+		isys->fwcom = fwcom;
+		spin_unlock_irqrestore(&isys->power_lock, flags);
+	}
+
+	return rval;
+}
+
+void ipu_fw_isys_cleanup(struct ipu_isys *isys)
+{
+	int ret;
+
+	ret = ipu_fw_com_release(isys->fwcom, 1);
+	if (ret < 0)
+		dev_err(&isys->adev->dev,
+			"Device busy, fw_com release failed.");
+	isys->fwcom = NULL;
+}
+
+static void start_sp(struct ipu_bus_device *adev)
+{
+	struct ipu_isys *isys = ipu_bus_get_drvdata(adev);
+	void __iomem *spc_regs_base = isys->pdata->base +
+	    isys->pdata->ipdata->hw_variant.spc_offset;
+	u32 val = 0;
+
+	val |= IPU_ISYS_SPC_STATUS_START |
+	    IPU_ISYS_SPC_STATUS_RUN |
+	    IPU_ISYS_SPC_STATUS_CTRL_ICACHE_INVALIDATE;
+	val |= isys->icache_prefetch ? IPU_ISYS_SPC_STATUS_ICACHE_PREFETCH : 0;
+
+	writel(val, spc_regs_base + IPU_ISYS_REG_SPC_STATUS_CTRL);
+}
+
+static int query_sp(struct ipu_bus_device *adev)
+{
+	struct ipu_isys *isys = ipu_bus_get_drvdata(adev);
+	void __iomem *spc_regs_base = isys->pdata->base +
+	    isys->pdata->ipdata->hw_variant.spc_offset;
+	u32 val = readl(spc_regs_base + IPU_ISYS_REG_SPC_STATUS_CTRL);
+
+	/* return true when READY == 1, START == 0 */
+	val &= IPU_ISYS_SPC_STATUS_READY | IPU_ISYS_SPC_STATUS_START;
+
+	return val == IPU_ISYS_SPC_STATUS_READY;
+}
+
+static int ipu6_isys_fwcom_cfg_init(struct ipu_isys *isys,
+				    struct ipu_fw_com_cfg *fwcom,
+				    unsigned int num_streams)
+{
+	int i;
+	unsigned int size;
+	struct ipu_fw_syscom_queue_config *input_queue_cfg;
+	struct ipu_fw_syscom_queue_config *output_queue_cfg;
+	struct ipu6_fw_isys_fw_config *isys_fw_cfg;
+	int num_out_message_queues = 1;
+	int type_proxy = IPU_FW_ISYS_QUEUE_TYPE_PROXY;
+	int type_dev = IPU_FW_ISYS_QUEUE_TYPE_DEV;
+	int type_msg = IPU_FW_ISYS_QUEUE_TYPE_MSG;
+	int base_dev_send = IPU_BASE_DEV_SEND_QUEUES;
+	int base_msg_send = IPU_BASE_MSG_SEND_QUEUES;
+	int base_msg_recv = IPU_BASE_MSG_RECV_QUEUES;
+	int num_in_message_queues;
+	unsigned int max_streams;
+	unsigned int max_send_queues, max_sram_blocks, max_devq_size;
+
+	max_streams = IPU6_ISYS_NUM_STREAMS;
+	max_send_queues = IPU6_N_MAX_SEND_QUEUES;
+	max_sram_blocks = IPU6_NOF_SRAM_BLOCKS_MAX;
+	max_devq_size = IPU6_DEV_SEND_QUEUE_SIZE;
+	if (ipu_ver == IPU_VER_6SE) {
+		max_streams = IPU6SE_ISYS_NUM_STREAMS;
+		max_send_queues = IPU6SE_N_MAX_SEND_QUEUES;
+		max_sram_blocks = IPU6SE_NOF_SRAM_BLOCKS_MAX;
+		max_devq_size = IPU6SE_DEV_SEND_QUEUE_SIZE;
+	}
+
+	num_in_message_queues = clamp_t(unsigned int, num_streams, 1,
+					max_streams);
+	isys_fw_cfg = devm_kzalloc(&isys->adev->dev, sizeof(*isys_fw_cfg),
+				   GFP_KERNEL);
+	if (!isys_fw_cfg)
+		return -ENOMEM;
+
+	isys_fw_cfg->num_send_queues[IPU_FW_ISYS_QUEUE_TYPE_PROXY] =
+		IPU_N_MAX_PROXY_SEND_QUEUES;
+	isys_fw_cfg->num_send_queues[IPU_FW_ISYS_QUEUE_TYPE_DEV] =
+		IPU_N_MAX_DEV_SEND_QUEUES;
+	isys_fw_cfg->num_send_queues[IPU_FW_ISYS_QUEUE_TYPE_MSG] =
+		num_in_message_queues;
+	isys_fw_cfg->num_recv_queues[IPU_FW_ISYS_QUEUE_TYPE_PROXY] =
+		IPU_N_MAX_PROXY_RECV_QUEUES;
+	/* Common msg/dev return queue */
+	isys_fw_cfg->num_recv_queues[IPU_FW_ISYS_QUEUE_TYPE_DEV] = 0;
+	isys_fw_cfg->num_recv_queues[IPU_FW_ISYS_QUEUE_TYPE_MSG] =
+		num_out_message_queues;
+
+	size = sizeof(*input_queue_cfg) * max_send_queues;
+	input_queue_cfg = devm_kzalloc(&isys->adev->dev, size, GFP_KERNEL);
+	if (!input_queue_cfg)
+		return -ENOMEM;
+
+	size = sizeof(*output_queue_cfg) * IPU_N_MAX_RECV_QUEUES;
+	output_queue_cfg = devm_kzalloc(&isys->adev->dev, size, GFP_KERNEL);
+	if (!output_queue_cfg)
+		return -ENOMEM;
+
+	fwcom->input = input_queue_cfg;
+	fwcom->output = output_queue_cfg;
+
+	fwcom->num_input_queues =
+		isys_fw_cfg->num_send_queues[type_proxy] +
+		isys_fw_cfg->num_send_queues[type_dev] +
+		isys_fw_cfg->num_send_queues[type_msg];
+
+	fwcom->num_output_queues =
+		isys_fw_cfg->num_recv_queues[type_proxy] +
+		isys_fw_cfg->num_recv_queues[type_dev] +
+		isys_fw_cfg->num_recv_queues[type_msg];
+
+	/* SRAM partitioning. Equal partitioning is set. */
+	for (i = 0; i < max_sram_blocks; i++) {
+		if (i < num_in_message_queues)
+			isys_fw_cfg->buffer_partition.num_gda_pages[i] =
+				(IPU_DEVICE_GDA_NR_PAGES *
+				 IPU_DEVICE_GDA_VIRT_FACTOR) /
+				num_in_message_queues;
+		else
+			isys_fw_cfg->buffer_partition.num_gda_pages[i] = 0;
+	}
+
+	/* FW assumes proxy interface at fwcom queue 0 */
+	for (i = 0; i < isys_fw_cfg->num_send_queues[type_proxy]; i++) {
+		input_queue_cfg[i].token_size =
+			sizeof(struct ipu_fw_proxy_send_queue_token);
+		input_queue_cfg[i].queue_size = IPU_ISYS_SIZE_PROXY_SEND_QUEUE;
+	}
+
+	for (i = 0; i < isys_fw_cfg->num_send_queues[type_dev]; i++) {
+		input_queue_cfg[base_dev_send + i].token_size =
+			sizeof(struct ipu_fw_send_queue_token);
+		input_queue_cfg[base_dev_send + i].queue_size = max_devq_size;
+	}
+
+	for (i = 0; i < isys_fw_cfg->num_send_queues[type_msg]; i++) {
+		input_queue_cfg[base_msg_send + i].token_size =
+			sizeof(struct ipu_fw_send_queue_token);
+		input_queue_cfg[base_msg_send + i].queue_size =
+			IPU_ISYS_SIZE_SEND_QUEUE;
+	}
+
+	for (i = 0; i < isys_fw_cfg->num_recv_queues[type_proxy]; i++) {
+		output_queue_cfg[i].token_size =
+			sizeof(struct ipu_fw_proxy_resp_queue_token);
+		output_queue_cfg[i].queue_size = IPU_ISYS_SIZE_PROXY_RECV_QUEUE;
+	}
+	/* There is no recv DEV queue */
+	for (i = 0; i < isys_fw_cfg->num_recv_queues[type_msg]; i++) {
+		output_queue_cfg[base_msg_recv + i].token_size =
+			sizeof(struct ipu_fw_resp_queue_token);
+		output_queue_cfg[base_msg_recv + i].queue_size =
+			IPU_ISYS_SIZE_RECV_QUEUE;
+	}
+
+	fwcom->dmem_addr = isys->pdata->ipdata->hw_variant.dmem_offset;
+	fwcom->specific_addr = isys_fw_cfg;
+	fwcom->specific_size = sizeof(*isys_fw_cfg);
+
+	return 0;
+}
+
+int ipu_fw_isys_init(struct ipu_isys *isys, unsigned int num_streams)
+{
+	int retry = IPU_ISYS_OPEN_RETRY;
+
+	struct ipu_fw_com_cfg fwcom = {
+		.cell_start = start_sp,
+		.cell_ready = query_sp,
+		.buttress_boot_offset = SYSCOM_BUTTRESS_FW_PARAMS_ISYS_OFFSET,
+	};
+
+	struct device *dev = &isys->adev->dev;
+	int rval;
+
+	ipu6_isys_fwcom_cfg_init(isys, &fwcom, num_streams);
+
+	isys->fwcom = ipu_fw_com_prepare(&fwcom, isys->adev, isys->pdata->base);
+	if (!isys->fwcom) {
+		dev_err(dev, "isys fw com prepare failed\n");
+		return -EIO;
+	}
+
+	rval = ipu_fw_com_open(isys->fwcom);
+	if (rval) {
+		dev_err(dev, "isys fw com open failed %d\n", rval);
+		return rval;
+	}
+
+	do {
+		usleep_range(IPU_ISYS_OPEN_TIMEOUT_US,
+			     IPU_ISYS_OPEN_TIMEOUT_US + 10);
+		rval = ipu_fw_com_ready(isys->fwcom);
+		if (!rval)
+			break;
+		retry--;
+	} while (retry > 0);
+
+	if (!retry && rval) {
+		dev_err(dev, "isys port open ready failed %d\n", rval);
+		ipu_fw_isys_close(isys);
+	}
+
+	return rval;
+}
+
+struct ipu_fw_isys_resp_info_abi *
+ipu_fw_isys_get_resp(void *context, unsigned int queue,
+		     struct ipu_fw_isys_resp_info_abi *response)
+{
+	return (struct ipu_fw_isys_resp_info_abi *)
+	    ipu_recv_get_token(context, queue);
+}
+
+void ipu_fw_isys_put_resp(void *context, unsigned int queue)
+{
+	ipu_recv_put_token(context, queue);
+}
+
+void ipu_fw_isys_set_params(struct ipu_fw_isys_stream_cfg_data_abi *stream_cfg)
+{
+	unsigned int i;
+	unsigned int idx;
+
+	for (i = 0; i < stream_cfg->nof_input_pins; i++) {
+		idx = stream_cfg->input_pins[i].dt;
+		stream_cfg->input_pins[i].bits_per_pix =
+		    extracted_bits_per_pixel_per_mipi_data_type[idx];
+		stream_cfg->input_pins[i].mapped_dt =
+		    N_IPU_FW_ISYS_MIPI_DATA_TYPE;
+		stream_cfg->input_pins[i].mipi_decompression =
+		    IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_NO_COMPRESSION;
+		/*
+		 * CSI BE can be used to crop and change bayer order.
+		 * NOTE: currently it only crops first and last lines in height.
+		 */
+		if (stream_cfg->crop.top_offset & 1)
+			stream_cfg->input_pins[i].crop_first_and_last_lines = 1;
+		stream_cfg->input_pins[i].capture_mode =
+			IPU_FW_ISYS_CAPTURE_MODE_REGULAR;
+	}
+}
+
+void
+ipu_fw_isys_dump_stream_cfg(struct device *dev,
+			    struct ipu_fw_isys_stream_cfg_data_abi *stream_cfg)
+{
+	unsigned int i;
+
+	dev_dbg(dev, "---------------------------\n");
+	dev_dbg(dev, "IPU_FW_ISYS_STREAM_CFG_DATA\n");
+	dev_dbg(dev, "---------------------------\n");
+
+	dev_dbg(dev, "Source %d\n", stream_cfg->src);
+	dev_dbg(dev, "VC %d\n", stream_cfg->vc);
+	dev_dbg(dev, "Nof input pins %d\n", stream_cfg->nof_input_pins);
+	dev_dbg(dev, "Nof output pins %d\n", stream_cfg->nof_output_pins);
+
+	for (i = 0; i < stream_cfg->nof_input_pins; i++) {
+		dev_dbg(dev, "Input pin %d\n", i);
+		dev_dbg(dev, "Mipi data type 0x%0x\n",
+			stream_cfg->input_pins[i].dt);
+		dev_dbg(dev, "Mipi store mode %d\n",
+			stream_cfg->input_pins[i].mipi_store_mode);
+		dev_dbg(dev, "Bits per pixel %d\n",
+			stream_cfg->input_pins[i].bits_per_pix);
+		dev_dbg(dev, "Mapped data type 0x%0x\n",
+			stream_cfg->input_pins[i].mapped_dt);
+		dev_dbg(dev, "Input res width %d\n",
+			stream_cfg->input_pins[i].input_res.width);
+		dev_dbg(dev, "Input res height %d\n",
+			stream_cfg->input_pins[i].input_res.height);
+		dev_dbg(dev, "mipi decompression %d\n",
+			stream_cfg->input_pins[i].mipi_decompression);
+		dev_dbg(dev, "capture_mode %d\n",
+			stream_cfg->input_pins[i].capture_mode);
+	}
+
+	dev_dbg(dev, "Crop info\n");
+	dev_dbg(dev, "Crop.top_offset %d\n", stream_cfg->crop.top_offset);
+	dev_dbg(dev, "Crop.left_offset %d\n", stream_cfg->crop.left_offset);
+	dev_dbg(dev, "Crop.bottom_offset %d\n",
+		stream_cfg->crop.bottom_offset);
+	dev_dbg(dev, "Crop.right_offset %d\n", stream_cfg->crop.right_offset);
+	dev_dbg(dev, "----------------\n");
+
+	for (i = 0; i < stream_cfg->nof_output_pins; i++) {
+		dev_dbg(dev, "Output pin %d\n", i);
+		dev_dbg(dev, "Output input pin id %d\n",
+			stream_cfg->output_pins[i].input_pin_id);
+		dev_dbg(dev, "Output res width %d\n",
+			stream_cfg->output_pins[i].output_res.width);
+		dev_dbg(dev, "Output res height %d\n",
+			stream_cfg->output_pins[i].output_res.height);
+		dev_dbg(dev, "Stride %d\n", stream_cfg->output_pins[i].stride);
+		dev_dbg(dev, "Pin type %d\n", stream_cfg->output_pins[i].pt);
+		dev_dbg(dev, "Payload %d\n",
+			stream_cfg->output_pins[i].payload_buf_size);
+		dev_dbg(dev, "Ft %d\n", stream_cfg->output_pins[i].ft);
+		dev_dbg(dev, "Watermar in lines %d\n",
+			stream_cfg->output_pins[i].watermark_in_lines);
+		dev_dbg(dev, "Send irq %d\n",
+			stream_cfg->output_pins[i].send_irq);
+		dev_dbg(dev, "Reserve compression %d\n",
+			stream_cfg->output_pins[i].reserve_compression);
+		dev_dbg(dev, "snoopable %d\n",
+			stream_cfg->output_pins[i].snoopable);
+		dev_dbg(dev, "error_handling_enable %d\n",
+			stream_cfg->output_pins[i].error_handling_enable);
+		dev_dbg(dev, "sensor type %d\n",
+			stream_cfg->output_pins[i].sensor_type);
+		dev_dbg(dev, "----------------\n");
+	}
+
+	dev_dbg(dev, "Isl_use %d\n", stream_cfg->isl_use);
+	dev_dbg(dev, "stream sensor_type %d\n", stream_cfg->sensor_type);
+
+}
+
+void ipu_fw_isys_dump_frame_buff_set(struct device *dev,
+				     struct ipu_fw_isys_frame_buff_set_abi *buf,
+				     unsigned int outputs)
+{
+	unsigned int i;
+
+	dev_dbg(dev, "--------------------------\n");
+	dev_dbg(dev, "IPU_FW_ISYS_FRAME_BUFF_SET\n");
+	dev_dbg(dev, "--------------------------\n");
+
+	for (i = 0; i < outputs; i++) {
+		dev_dbg(dev, "Output pin %d\n", i);
+		dev_dbg(dev, "out_buf_id %llu\n",
+			buf->output_pins[i].out_buf_id);
+		dev_dbg(dev, "addr 0x%x\n", buf->output_pins[i].addr);
+		dev_dbg(dev, "compress %u\n", buf->output_pins[i].compress);
+
+		dev_dbg(dev, "----------------\n");
+	}
+
+	dev_dbg(dev, "send_irq_sof 0x%x\n", buf->send_irq_sof);
+	dev_dbg(dev, "send_irq_eof 0x%x\n", buf->send_irq_eof);
+	dev_dbg(dev, "send_resp_sof 0x%x\n", buf->send_resp_sof);
+	dev_dbg(dev, "send_resp_eof 0x%x\n", buf->send_resp_eof);
+	dev_dbg(dev, "send_irq_capture_ack 0x%x\n", buf->send_irq_capture_ack);
+	dev_dbg(dev, "send_irq_capture_done 0x%x\n",
+		buf->send_irq_capture_done);
+	dev_dbg(dev, "send_resp_capture_ack 0x%x\n",
+		buf->send_resp_capture_ack);
+	dev_dbg(dev, "send_resp_capture_done 0x%x\n",
+		buf->send_resp_capture_done);
+}
diff -ruN a/drivers/media/pci/intel/ipu-fw-isys.h b/drivers/media/pci/intel/ipu-fw-isys.h
--- a/drivers/media/pci/intel/ipu-fw-isys.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-fw-isys.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,816 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_FW_ISYS_H
+#define IPU_FW_ISYS_H
+
+#include "ipu-fw-com.h"
+
+/* Max number of Input/Output Pins */
+#define IPU_MAX_IPINS 4
+
+#define IPU_MAX_OPINS ((IPU_MAX_IPINS) + 1)
+
+#define IPU6_STREAM_ID_MAX 16
+#define IPU6_NONSECURE_STREAM_ID_MAX 12
+#define IPU6_DEV_SEND_QUEUE_SIZE (IPU6_STREAM_ID_MAX)
+#define IPU6_NOF_SRAM_BLOCKS_MAX (IPU6_STREAM_ID_MAX)
+#define IPU6_N_MAX_MSG_SEND_QUEUES (IPU6_STREAM_ID_MAX)
+#define IPU6SE_STREAM_ID_MAX 8
+#define IPU6SE_NONSECURE_STREAM_ID_MAX 4
+#define IPU6SE_DEV_SEND_QUEUE_SIZE (IPU6SE_STREAM_ID_MAX)
+#define IPU6SE_NOF_SRAM_BLOCKS_MAX (IPU6SE_STREAM_ID_MAX)
+#define IPU6SE_N_MAX_MSG_SEND_QUEUES (IPU6SE_STREAM_ID_MAX)
+
+/* Single return queue for all streams/commands type */
+#define IPU_N_MAX_MSG_RECV_QUEUES 1
+/* Single device queue for high priority commands (bypass in-order queue) */
+#define IPU_N_MAX_DEV_SEND_QUEUES 1
+/* Single dedicated send queue for proxy interface */
+#define IPU_N_MAX_PROXY_SEND_QUEUES 1
+/* Single dedicated recv queue for proxy interface */
+#define IPU_N_MAX_PROXY_RECV_QUEUES 1
+/* Send queues layout */
+#define IPU_BASE_PROXY_SEND_QUEUES 0
+#define IPU_BASE_DEV_SEND_QUEUES \
+	(IPU_BASE_PROXY_SEND_QUEUES + IPU_N_MAX_PROXY_SEND_QUEUES)
+#define IPU_BASE_MSG_SEND_QUEUES \
+	(IPU_BASE_DEV_SEND_QUEUES + IPU_N_MAX_DEV_SEND_QUEUES)
+/* Recv queues layout */
+#define IPU_BASE_PROXY_RECV_QUEUES 0
+#define IPU_BASE_MSG_RECV_QUEUES \
+	(IPU_BASE_PROXY_RECV_QUEUES + IPU_N_MAX_PROXY_RECV_QUEUES)
+#define IPU_N_MAX_RECV_QUEUES \
+	(IPU_BASE_MSG_RECV_QUEUES + IPU_N_MAX_MSG_RECV_QUEUES)
+
+#define IPU6_N_MAX_SEND_QUEUES \
+	(IPU_BASE_MSG_SEND_QUEUES + IPU6_N_MAX_MSG_SEND_QUEUES)
+#define IPU6SE_N_MAX_SEND_QUEUES \
+	(IPU_BASE_MSG_SEND_QUEUES + IPU6SE_N_MAX_MSG_SEND_QUEUES)
+
+/* Max number of supported input pins routed in ISL */
+#define IPU_MAX_IPINS_IN_ISL 2
+
+/* Max number of planes for frame formats supported by the FW */
+#define IPU_PIN_PLANES_MAX 4
+
+/**
+ * enum ipu_fw_isys_resp_type
+ */
+enum ipu_fw_isys_resp_type {
+	IPU_FW_ISYS_RESP_TYPE_STREAM_OPEN_DONE = 0,
+	IPU_FW_ISYS_RESP_TYPE_STREAM_START_ACK,
+	IPU_FW_ISYS_RESP_TYPE_STREAM_START_AND_CAPTURE_ACK,
+	IPU_FW_ISYS_RESP_TYPE_STREAM_CAPTURE_ACK,
+	IPU_FW_ISYS_RESP_TYPE_STREAM_STOP_ACK,
+	IPU_FW_ISYS_RESP_TYPE_STREAM_FLUSH_ACK,
+	IPU_FW_ISYS_RESP_TYPE_STREAM_CLOSE_ACK,
+	IPU_FW_ISYS_RESP_TYPE_PIN_DATA_READY,
+	IPU_FW_ISYS_RESP_TYPE_PIN_DATA_WATERMARK,
+	IPU_FW_ISYS_RESP_TYPE_FRAME_SOF,
+	IPU_FW_ISYS_RESP_TYPE_FRAME_EOF,
+	IPU_FW_ISYS_RESP_TYPE_STREAM_START_AND_CAPTURE_DONE,
+	IPU_FW_ISYS_RESP_TYPE_STREAM_CAPTURE_DONE,
+	IPU_FW_ISYS_RESP_TYPE_PIN_DATA_SKIPPED,
+	IPU_FW_ISYS_RESP_TYPE_STREAM_CAPTURE_SKIPPED,
+	IPU_FW_ISYS_RESP_TYPE_FRAME_SOF_DISCARDED,
+	IPU_FW_ISYS_RESP_TYPE_FRAME_EOF_DISCARDED,
+	IPU_FW_ISYS_RESP_TYPE_STATS_DATA_READY,
+	N_IPU_FW_ISYS_RESP_TYPE
+};
+
+/**
+ * enum ipu_fw_isys_send_type
+ */
+enum ipu_fw_isys_send_type {
+	IPU_FW_ISYS_SEND_TYPE_STREAM_OPEN = 0,
+	IPU_FW_ISYS_SEND_TYPE_STREAM_START,
+	IPU_FW_ISYS_SEND_TYPE_STREAM_START_AND_CAPTURE,
+	IPU_FW_ISYS_SEND_TYPE_STREAM_CAPTURE,
+	IPU_FW_ISYS_SEND_TYPE_STREAM_STOP,
+	IPU_FW_ISYS_SEND_TYPE_STREAM_FLUSH,
+	IPU_FW_ISYS_SEND_TYPE_STREAM_CLOSE,
+	N_IPU_FW_ISYS_SEND_TYPE
+};
+
+/**
+ * enum ipu_fw_isys_queue_type
+ */
+enum ipu_fw_isys_queue_type {
+	IPU_FW_ISYS_QUEUE_TYPE_PROXY = 0,
+	IPU_FW_ISYS_QUEUE_TYPE_DEV,
+	IPU_FW_ISYS_QUEUE_TYPE_MSG,
+	N_IPU_FW_ISYS_QUEUE_TYPE
+};
+
+/**
+ * enum ipu_fw_isys_stream_source: Specifies a source for a stream
+ */
+enum ipu_fw_isys_stream_source {
+	IPU_FW_ISYS_STREAM_SRC_PORT_0 = 0,
+	IPU_FW_ISYS_STREAM_SRC_PORT_1,
+	IPU_FW_ISYS_STREAM_SRC_PORT_2,
+	IPU_FW_ISYS_STREAM_SRC_PORT_3,
+	IPU_FW_ISYS_STREAM_SRC_PORT_4,
+	IPU_FW_ISYS_STREAM_SRC_PORT_5,
+	IPU_FW_ISYS_STREAM_SRC_PORT_6,
+	IPU_FW_ISYS_STREAM_SRC_PORT_7,
+	IPU_FW_ISYS_STREAM_SRC_PORT_8,
+	IPU_FW_ISYS_STREAM_SRC_PORT_9,
+	IPU_FW_ISYS_STREAM_SRC_PORT_10,
+	IPU_FW_ISYS_STREAM_SRC_PORT_11,
+	IPU_FW_ISYS_STREAM_SRC_PORT_12,
+	IPU_FW_ISYS_STREAM_SRC_PORT_13,
+	IPU_FW_ISYS_STREAM_SRC_PORT_14,
+	IPU_FW_ISYS_STREAM_SRC_PORT_15,
+	IPU_FW_ISYS_STREAM_SRC_MIPIGEN_0,
+	IPU_FW_ISYS_STREAM_SRC_MIPIGEN_1,
+	IPU_FW_ISYS_STREAM_SRC_MIPIGEN_2,
+	IPU_FW_ISYS_STREAM_SRC_MIPIGEN_3,
+	IPU_FW_ISYS_STREAM_SRC_MIPIGEN_4,
+	IPU_FW_ISYS_STREAM_SRC_MIPIGEN_5,
+	IPU_FW_ISYS_STREAM_SRC_MIPIGEN_6,
+	IPU_FW_ISYS_STREAM_SRC_MIPIGEN_7,
+	IPU_FW_ISYS_STREAM_SRC_MIPIGEN_8,
+	IPU_FW_ISYS_STREAM_SRC_MIPIGEN_9,
+	N_IPU_FW_ISYS_STREAM_SRC
+};
+
+enum ipu_fw_isys_sensor_type {
+	/* non-snoopable to PSYS */
+	IPU_FW_ISYS_VC1_SENSOR_DATA	= 0,
+	/* non-snoopable for PDAF */
+	IPU_FW_ISYS_VC1_SENSOR_PDAF,
+	/* snoopable to CPU */
+	IPU_FW_ISYS_VC0_SENSOR_METADATA,
+	/* snoopable to CPU */
+	IPU_FW_ISYS_VC0_SENSOR_DATA,
+	N_IPU_FW_ISYS_SENSOR_TYPE
+};
+
+enum ipu6se_fw_isys_sensor_info {
+	/* VC1 */
+	IPU6SE_FW_ISYS_SENSOR_DATA_1 = 1,
+	IPU6SE_FW_ISYS_SENSOR_DATA_2 = 2,
+	IPU6SE_FW_ISYS_SENSOR_DATA_3 = 3,
+	IPU6SE_FW_ISYS_SENSOR_PDAF_1 = 4,
+	IPU6SE_FW_ISYS_SENSOR_PDAF_2 = 4,
+	/* VC0 */
+	IPU6SE_FW_ISYS_SENSOR_METADATA = 5,
+	IPU6SE_FW_ISYS_SENSOR_DATA_4 = 6,
+	IPU6SE_FW_ISYS_SENSOR_DATA_5 = 7,
+	IPU6SE_FW_ISYS_SENSOR_DATA_6 = 8,
+	IPU6SE_FW_ISYS_SENSOR_DATA_7 = 9,
+	IPU6SE_FW_ISYS_SENSOR_DATA_8 = 10,
+	IPU6SE_FW_ISYS_SENSOR_DATA_9 = 11,
+	N_IPU6SE_FW_ISYS_SENSOR_INFO,
+	IPU6SE_FW_ISYS_VC1_SENSOR_DATA_START = IPU6SE_FW_ISYS_SENSOR_DATA_1,
+	IPU6SE_FW_ISYS_VC1_SENSOR_DATA_END = IPU6SE_FW_ISYS_SENSOR_DATA_3,
+	IPU6SE_FW_ISYS_VC0_SENSOR_DATA_START = IPU6SE_FW_ISYS_SENSOR_DATA_4,
+	IPU6SE_FW_ISYS_VC0_SENSOR_DATA_END = IPU6SE_FW_ISYS_SENSOR_DATA_9,
+	IPU6SE_FW_ISYS_VC1_SENSOR_PDAF_START = IPU6SE_FW_ISYS_SENSOR_PDAF_1,
+	IPU6SE_FW_ISYS_VC1_SENSOR_PDAF_END = IPU6SE_FW_ISYS_SENSOR_PDAF_2,
+};
+
+enum ipu6_fw_isys_sensor_info {
+	/* VC1 */
+	IPU6_FW_ISYS_SENSOR_DATA_1 = 1,
+	IPU6_FW_ISYS_SENSOR_DATA_2 = 2,
+	IPU6_FW_ISYS_SENSOR_DATA_3 = 3,
+	IPU6_FW_ISYS_SENSOR_DATA_4 = 4,
+	IPU6_FW_ISYS_SENSOR_DATA_5 = 5,
+	IPU6_FW_ISYS_SENSOR_DATA_6 = 6,
+	IPU6_FW_ISYS_SENSOR_DATA_7 = 7,
+	IPU6_FW_ISYS_SENSOR_DATA_8 = 8,
+	IPU6_FW_ISYS_SENSOR_DATA_9 = 9,
+	IPU6_FW_ISYS_SENSOR_DATA_10 = 10,
+	IPU6_FW_ISYS_SENSOR_PDAF_1 = 11,
+	IPU6_FW_ISYS_SENSOR_PDAF_2 = 12,
+	/* VC0 */
+	IPU6_FW_ISYS_SENSOR_METADATA = 13,
+	IPU6_FW_ISYS_SENSOR_DATA_11 = 14,
+	IPU6_FW_ISYS_SENSOR_DATA_12 = 15,
+	IPU6_FW_ISYS_SENSOR_DATA_13 = 16,
+	IPU6_FW_ISYS_SENSOR_DATA_14 = 17,
+	IPU6_FW_ISYS_SENSOR_DATA_15 = 18,
+	IPU6_FW_ISYS_SENSOR_DATA_16 = 19,
+	N_IPU6_FW_ISYS_SENSOR_INFO,
+	IPU6_FW_ISYS_VC1_SENSOR_DATA_START = IPU6_FW_ISYS_SENSOR_DATA_1,
+	IPU6_FW_ISYS_VC1_SENSOR_DATA_END = IPU6_FW_ISYS_SENSOR_DATA_10,
+	IPU6_FW_ISYS_VC0_SENSOR_DATA_START = IPU6_FW_ISYS_SENSOR_DATA_11,
+	IPU6_FW_ISYS_VC0_SENSOR_DATA_END = IPU6_FW_ISYS_SENSOR_DATA_16,
+	IPU6_FW_ISYS_VC1_SENSOR_PDAF_START = IPU6_FW_ISYS_SENSOR_PDAF_1,
+	IPU6_FW_ISYS_VC1_SENSOR_PDAF_END = IPU6_FW_ISYS_SENSOR_PDAF_2,
+};
+
+#define IPU_FW_ISYS_STREAM_SRC_CSI2_PORT0 IPU_FW_ISYS_STREAM_SRC_PORT_0
+#define IPU_FW_ISYS_STREAM_SRC_CSI2_PORT1 IPU_FW_ISYS_STREAM_SRC_PORT_1
+#define IPU_FW_ISYS_STREAM_SRC_CSI2_PORT2 IPU_FW_ISYS_STREAM_SRC_PORT_2
+#define IPU_FW_ISYS_STREAM_SRC_CSI2_PORT3 IPU_FW_ISYS_STREAM_SRC_PORT_3
+
+#define IPU_FW_ISYS_STREAM_SRC_CSI2_3PH_PORTA IPU_FW_ISYS_STREAM_SRC_PORT_4
+#define IPU_FW_ISYS_STREAM_SRC_CSI2_3PH_PORTB IPU_FW_ISYS_STREAM_SRC_PORT_5
+#define IPU_FW_ISYS_STREAM_SRC_CSI2_3PH_CPHY_PORT0 IPU_FW_ISYS_STREAM_SRC_PORT_6
+#define IPU_FW_ISYS_STREAM_SRC_CSI2_3PH_CPHY_PORT1 IPU_FW_ISYS_STREAM_SRC_PORT_7
+#define IPU_FW_ISYS_STREAM_SRC_CSI2_3PH_CPHY_PORT2 IPU_FW_ISYS_STREAM_SRC_PORT_8
+#define IPU_FW_ISYS_STREAM_SRC_CSI2_3PH_CPHY_PORT3 IPU_FW_ISYS_STREAM_SRC_PORT_9
+
+#define IPU_FW_ISYS_STREAM_SRC_MIPIGEN_PORT0 IPU_FW_ISYS_STREAM_SRC_MIPIGEN_0
+#define IPU_FW_ISYS_STREAM_SRC_MIPIGEN_PORT1 IPU_FW_ISYS_STREAM_SRC_MIPIGEN_1
+
+/**
+ * enum ipu_fw_isys_mipi_vc: MIPI csi2 spec
+ * supports up to 4 virtual per physical channel
+ */
+enum ipu_fw_isys_mipi_vc {
+	IPU_FW_ISYS_MIPI_VC_0 = 0,
+	IPU_FW_ISYS_MIPI_VC_1,
+	IPU_FW_ISYS_MIPI_VC_2,
+	IPU_FW_ISYS_MIPI_VC_3,
+	N_IPU_FW_ISYS_MIPI_VC
+};
+
+/**
+ *  Supported Pixel Frame formats. Expandable if needed
+ */
+enum ipu_fw_isys_frame_format_type {
+	IPU_FW_ISYS_FRAME_FORMAT_NV11 = 0, /* 12 bit YUV 411, Y, UV plane */
+	IPU_FW_ISYS_FRAME_FORMAT_NV12,	/* 12 bit YUV 420, Y, UV plane */
+	IPU_FW_ISYS_FRAME_FORMAT_NV12_16, /* 16 bit YUV 420, Y, UV plane */
+	IPU_FW_ISYS_FRAME_FORMAT_NV12_TILEY, /* 12 bit YUV 420,
+					      * Intel proprietary tiled format,
+					      * TileY
+					      */
+	IPU_FW_ISYS_FRAME_FORMAT_NV16,	/* 16 bit YUV 422, Y, UV plane */
+	IPU_FW_ISYS_FRAME_FORMAT_NV21,	/* 12 bit YUV 420, Y, VU plane */
+	IPU_FW_ISYS_FRAME_FORMAT_NV61,	/* 16 bit YUV 422, Y, VU plane */
+	IPU_FW_ISYS_FRAME_FORMAT_YV12,	/* 12 bit YUV 420, Y, V, U plane */
+	IPU_FW_ISYS_FRAME_FORMAT_YV16,	/* 16 bit YUV 422, Y, V, U plane */
+	IPU_FW_ISYS_FRAME_FORMAT_YUV420, /* 12 bit YUV 420, Y, U, V plane */
+	IPU_FW_ISYS_FRAME_FORMAT_YUV420_10, /* yuv420, 10 bits per subpixel */
+	IPU_FW_ISYS_FRAME_FORMAT_YUV420_12, /* yuv420, 12 bits per subpixel */
+	IPU_FW_ISYS_FRAME_FORMAT_YUV420_14, /* yuv420, 14 bits per subpixel */
+	IPU_FW_ISYS_FRAME_FORMAT_YUV420_16, /* yuv420, 16 bits per subpixel */
+	IPU_FW_ISYS_FRAME_FORMAT_YUV422, /* 16 bit YUV 422, Y, U, V plane */
+	IPU_FW_ISYS_FRAME_FORMAT_YUV422_16, /* yuv422, 16 bits per subpixel */
+	IPU_FW_ISYS_FRAME_FORMAT_UYVY,	/* 16 bit YUV 422, UYVY interleaved */
+	IPU_FW_ISYS_FRAME_FORMAT_YUYV,	/* 16 bit YUV 422, YUYV interleaved */
+	IPU_FW_ISYS_FRAME_FORMAT_YUV444, /* 24 bit YUV 444, Y, U, V plane */
+	IPU_FW_ISYS_FRAME_FORMAT_YUV_LINE, /* Internal format, 2 y lines
+					    * followed by a uvinterleaved line
+					    */
+	IPU_FW_ISYS_FRAME_FORMAT_RAW8,	/* RAW8, 1 plane */
+	IPU_FW_ISYS_FRAME_FORMAT_RAW10,	/* RAW10, 1 plane */
+	IPU_FW_ISYS_FRAME_FORMAT_RAW12,	/* RAW12, 1 plane */
+	IPU_FW_ISYS_FRAME_FORMAT_RAW14,	/* RAW14, 1 plane */
+	IPU_FW_ISYS_FRAME_FORMAT_RAW16,	/* RAW16, 1 plane */
+	IPU_FW_ISYS_FRAME_FORMAT_RGB565, /* 16 bit RGB, 1 plane. Each 3 sub
+					  * pixels are packed into one 16 bit
+					  * value, 5 bits for R, 6 bits
+					  *   for G and 5 bits for B.
+					  */
+
+	IPU_FW_ISYS_FRAME_FORMAT_PLANAR_RGB888,	/* 24 bit RGB, 3 planes */
+	IPU_FW_ISYS_FRAME_FORMAT_RGBA888,	/* 32 bit RGBA, 1 plane,
+						 * A=Alpha (alpha is unused)
+						 */
+	IPU_FW_ISYS_FRAME_FORMAT_QPLANE6,	/* Internal, for advanced ISP */
+	IPU_FW_ISYS_FRAME_FORMAT_BINARY_8, /* byte stream, used for jpeg. */
+	N_IPU_FW_ISYS_FRAME_FORMAT
+};
+
+/* Temporary for driver compatibility */
+#define IPU_FW_ISYS_FRAME_FORMAT_RAW		(IPU_FW_ISYS_FRAME_FORMAT_RAW16)
+
+enum ipu_fw_isys_mipi_compression_type {
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_NO_COMPRESSION = 0,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_10_8_10_TYPE1,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_10_8_10_TYPE2,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_10_7_10_TYPE1,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_10_7_10_TYPE2,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_10_6_10_TYPE1,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_10_6_10_TYPE2,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_12_8_12_TYPE1,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_12_8_12_TYPE2,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_12_7_12_TYPE1,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_12_7_12_TYPE2,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_12_6_12_TYPE1,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_12_6_12_TYPE2,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_12_10_12_TYPE1,
+	IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_12_10_12_TYPE2,
+	N_IPU_FW_ISYS_MIPI_COMPRESSION_TYPE,
+};
+
+/**
+ *  Supported MIPI data type. Keep in sync array in ipu_fw_isys_private.c
+ */
+enum ipu_fw_isys_mipi_data_type {
+	/** SYNCHRONIZATION SHORT PACKET DATA TYPES */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_FRAME_START_CODE = 0x00,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_FRAME_END_CODE = 0x01,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_LINE_START_CODE = 0x02,	/* Optional */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_LINE_END_CODE = 0x03,	/* Optional */
+	/** Reserved 0x04-0x07 */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x04 = 0x04,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x05 = 0x05,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x06 = 0x06,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x07 = 0x07,
+	/** GENERIC SHORT PACKET DATA TYPES */
+	/** They are used to keep the timing information for
+	 * the opening/closing of shutters,
+	 *  triggering of flashes and etc.
+	 */
+	/* Generic Short Packet Codes 1 - 8 */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT1 = 0x08,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT2 = 0x09,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT3 = 0x0A,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT4 = 0x0B,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT5 = 0x0C,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT6 = 0x0D,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT7 = 0x0E,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_GENERIC_SHORT8 = 0x0F,
+	/** GENERIC LONG PACKET DATA TYPES */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_NULL = 0x10,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_BLANKING_DATA = 0x11,
+	/* Embedded 8-bit non Image Data */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_EMBEDDED = 0x12,
+	/** Reserved 0x13-0x17 */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x13 = 0x13,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x14 = 0x14,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x15 = 0x15,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x16 = 0x16,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x17 = 0x17,
+	/** YUV DATA TYPES */
+	/* 8 bits per subpixel */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_YUV420_8 = 0x18,
+	/* 10 bits per subpixel */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_YUV420_10 = 0x19,
+	/* 8 bits per subpixel */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_YUV420_8_LEGACY = 0x1A,
+	/** Reserved 0x1B */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x1B = 0x1B,
+	/* YUV420 8-bit Chroma Shifted Pixel Sampling) */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_YUV420_8_SHIFT = 0x1C,
+	/* YUV420 8-bit (Chroma Shifted Pixel Sampling) */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_YUV420_10_SHIFT = 0x1D,
+	/* UYVY..UVYV, 8 bits per subpixel */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_YUV422_8 = 0x1E,
+	/* UYVY..UVYV, 10 bits per subpixel */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_YUV422_10 = 0x1F,
+	/** RGB DATA TYPES */
+	/* BGR..BGR, 4 bits per subpixel */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RGB_444 = 0x20,
+	/* BGR..BGR, 5 bits per subpixel */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RGB_555 = 0x21,
+	/* BGR..BGR, 5 bits B and R, 6 bits G */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RGB_565 = 0x22,
+	/* BGR..BGR, 6 bits per subpixel */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RGB_666 = 0x23,
+	/* BGR..BGR, 8 bits per subpixel */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RGB_888 = 0x24,
+	/** Reserved 0x25-0x27 */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x25 = 0x25,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x26 = 0x26,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x27 = 0x27,
+	/** RAW DATA TYPES */
+	/* RAW data, 6 - 14 bits per pixel */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_6 = 0x28,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_7 = 0x29,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_8 = 0x2A,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_10 = 0x2B,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_12 = 0x2C,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_14 = 0x2D,
+	/** Reserved 0x2E-2F are used with assigned meaning */
+	/* RAW data, 16 bits per pixel, not specified in CSI-MIPI standard */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RAW_16 = 0x2E,
+	/* Binary byte stream, which is target at JPEG,
+	 * not specified in CSI-MIPI standard
+	 */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_BINARY_8 = 0x2F,
+
+	/** USER DEFINED 8-BIT DATA TYPES */
+	/** For example, the data transmitter (e.g. the SoC sensor)
+	 * can keep the JPEG data as
+	 *  the User Defined Data Type 4 and the MPEG data as the
+	 *  User Defined Data Type 7.
+	 */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF1 = 0x30,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF2 = 0x31,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF3 = 0x32,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF4 = 0x33,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF5 = 0x34,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF6 = 0x35,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF7 = 0x36,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_USER_DEF8 = 0x37,
+	/** Reserved 0x38-0x3F */
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x38 = 0x38,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x39 = 0x39,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x3A = 0x3A,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x3B = 0x3B,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x3C = 0x3C,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x3D = 0x3D,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x3E = 0x3E,
+	IPU_FW_ISYS_MIPI_DATA_TYPE_RESERVED_0x3F = 0x3F,
+
+	/* Keep always last and max value */
+	N_IPU_FW_ISYS_MIPI_DATA_TYPE = 0x40
+};
+
+/** enum ipu_fw_isys_pin_type: output pin buffer types.
+ * Buffers can be queued and de-queued to hand them over between IA and ISYS
+ */
+enum ipu_fw_isys_pin_type {
+	/* Captured as MIPI packets */
+	IPU_FW_ISYS_PIN_TYPE_MIPI = 0,
+	/* Captured through the RAW path */
+	IPU_FW_ISYS_PIN_TYPE_RAW_NS = 1,
+	/* Captured through the SoC path */
+	IPU_FW_ISYS_PIN_TYPE_RAW_SOC = 3,
+	/* Reserved for future use, maybe short packets */
+	IPU_FW_ISYS_PIN_TYPE_METADATA_0 = 4,
+	/* Reserved for future use */
+	IPU_FW_ISYS_PIN_TYPE_METADATA_1 = 5,
+	/* Keep always last and max value */
+	N_IPU_FW_ISYS_PIN_TYPE
+};
+
+/**
+ * enum ipu_fw_isys_mipi_store_mode. Describes if long MIPI packets reach
+ * MIPI SRAM with the long packet header or
+ * if not, then only option is to capture it with pin type MIPI.
+ */
+enum ipu_fw_isys_mipi_store_mode {
+	IPU_FW_ISYS_MIPI_STORE_MODE_NORMAL = 0,
+	IPU_FW_ISYS_MIPI_STORE_MODE_DISCARD_LONG_HEADER,
+	N_IPU_FW_ISYS_MIPI_STORE_MODE
+};
+
+/**
+ * ISYS capture mode and sensor enums
+ * Used for Tobii sensor, if doubt, use default value 0
+ */
+
+enum ipu_fw_isys_capture_mode {
+	IPU_FW_ISYS_CAPTURE_MODE_REGULAR = 0,
+	IPU_FW_ISYS_CAPTURE_MODE_BURST,
+	N_IPU_FW_ISYS_CAPTURE_MODE,
+};
+
+enum ipu_fw_isys_sensor_mode {
+	IPU_FW_ISYS_SENSOR_MODE_NORMAL = 0,
+	IPU_FW_ISYS_SENSOR_MODE_TOBII,
+	N_IPU_FW_ISYS_SENSOR_MODE,
+};
+
+/**
+ * enum ipu_fw_isys_error. Describes the error type detected by the FW
+ */
+enum ipu_fw_isys_error {
+	IPU_FW_ISYS_ERROR_NONE = 0,	/* No details */
+	IPU_FW_ISYS_ERROR_FW_INTERNAL_CONSISTENCY,	/* enum */
+	IPU_FW_ISYS_ERROR_HW_CONSISTENCY,	/* enum */
+	IPU_FW_ISYS_ERROR_DRIVER_INVALID_COMMAND_SEQUENCE,	/* enum */
+	IPU_FW_ISYS_ERROR_DRIVER_INVALID_DEVICE_CONFIGURATION,	/* enum */
+	IPU_FW_ISYS_ERROR_DRIVER_INVALID_STREAM_CONFIGURATION,	/* enum */
+	IPU_FW_ISYS_ERROR_DRIVER_INVALID_FRAME_CONFIGURATION,	/* enum */
+	IPU_FW_ISYS_ERROR_INSUFFICIENT_RESOURCES,	/* enum */
+	IPU_FW_ISYS_ERROR_HW_REPORTED_STR2MMIO,	/* HW code */
+	IPU_FW_ISYS_ERROR_HW_REPORTED_SIG2CIO,	/* HW code */
+	IPU_FW_ISYS_ERROR_SENSOR_FW_SYNC,	/* enum */
+	IPU_FW_ISYS_ERROR_STREAM_IN_SUSPENSION,	/* FW code */
+	IPU_FW_ISYS_ERROR_RESPONSE_QUEUE_FULL,	/* FW code */
+	N_IPU_FW_ISYS_ERROR
+};
+
+/**
+ * enum ipu_fw_proxy_error. Describes the error type for
+ * the proxy detected by the FW
+ */
+enum ipu_fw_proxy_error {
+	IPU_FW_PROXY_ERROR_NONE = 0,
+	IPU_FW_PROXY_ERROR_INVALID_WRITE_REGION,
+	IPU_FW_PROXY_ERROR_INVALID_WRITE_OFFSET,
+	N_IPU_FW_PROXY_ERROR
+};
+
+struct ipu_isys;
+
+struct ipu6_fw_isys_buffer_partition_abi {
+	u32 num_gda_pages[IPU6_STREAM_ID_MAX];
+};
+
+struct ipu6_fw_isys_fw_config {
+	struct ipu6_fw_isys_buffer_partition_abi buffer_partition;
+	u32 num_send_queues[N_IPU_FW_ISYS_QUEUE_TYPE];
+	u32 num_recv_queues[N_IPU_FW_ISYS_QUEUE_TYPE];
+};
+
+/**
+ * struct ipu_fw_isys_resolution_abi: Generic resolution structure.
+ * @Width
+ * @Height
+ */
+struct ipu_fw_isys_resolution_abi {
+	u32 width;
+	u32 height;
+};
+
+/**
+ * struct ipu_fw_isys_output_pin_payload_abi
+ * @out_buf_id: Points to output pin buffer - buffer identifier
+ * @addr: Points to output pin buffer - CSS Virtual Address
+ * @compress: Request frame compression (1), or  not (0)
+ */
+struct ipu_fw_isys_output_pin_payload_abi {
+	u64 out_buf_id;
+	u32 addr;
+	u32 compress;
+};
+
+/**
+ * struct ipu_fw_isys_output_pin_info_abi
+ * @output_res: output pin resolution
+ * @stride: output stride in Bytes (not valid for statistics)
+ * @watermark_in_lines: pin watermark level in lines
+ * @payload_buf_size: minimum size in Bytes of all buffers that will be
+ *			supplied for capture on this pin
+ * @send_irq: assert if pin event should trigger irq
+ * @pt: pin type -real format "enum ipu_fw_isys_pin_type"
+ * @ft: frame format type -real format "enum ipu_fw_isys_frame_format_type"
+ * @input_pin_id: related input pin id
+ * @reserve_compression: reserve compression resources for pin
+ */
+struct ipu_fw_isys_output_pin_info_abi {
+	struct ipu_fw_isys_resolution_abi output_res;
+	u32 stride;
+	u32 watermark_in_lines;
+	u32 payload_buf_size;
+	u32 ts_offsets[IPU_PIN_PLANES_MAX];
+	u32 s2m_pixel_soc_pixel_remapping;
+	u32 csi_be_soc_pixel_remapping;
+	u8 send_irq;
+	u8 input_pin_id;
+	u8 pt;
+	u8 ft;
+	u8 reserved;
+	u8 reserve_compression;
+	u8 snoopable;
+	u8 error_handling_enable;
+	u32 sensor_type;
+};
+
+/**
+ * struct ipu_fw_isys_param_pin_abi
+ * @param_buf_id: Points to param port buffer - buffer identifier
+ * @addr: Points to param pin buffer - CSS Virtual Address
+ */
+struct ipu_fw_isys_param_pin_abi {
+	u64 param_buf_id;
+	u32 addr;
+};
+
+/**
+ * struct ipu_fw_isys_input_pin_info_abi
+ * @input_res: input resolution
+ * @dt: mipi data type ((enum ipu_fw_isys_mipi_data_type)
+ * @mipi_store_mode: defines if legacy long packet header will be stored or
+ *		     discarded if discarded, output pin type for this
+ *		     input pin can only be MIPI
+ *		     (enum ipu_fw_isys_mipi_store_mode)
+ * @bits_per_pix: native bits per pixel
+ * @mapped_dt: actual data type from sensor
+ * @mipi_decompression: defines which compression will be in mipi backend
+
+ * @crop_first_and_last_lines    Control whether to crop the
+ *                              first and last line of the
+ *                              input image. Crop done by HW
+ *                              device.
+ * @capture_mode: mode of capture, regular or burst, default value is regular
+ */
+struct ipu_fw_isys_input_pin_info_abi {
+	struct ipu_fw_isys_resolution_abi input_res;
+	u8 dt;
+	u8 mipi_store_mode;
+	u8 bits_per_pix;
+	u8 mapped_dt;
+	u8 mipi_decompression;
+	u8 crop_first_and_last_lines;
+	u8 capture_mode;
+};
+
+/**
+ * struct ipu_fw_isys_cropping_abi - cropping coordinates
+ */
+struct ipu_fw_isys_cropping_abi {
+	s32 top_offset;
+	s32 left_offset;
+	s32 bottom_offset;
+	s32 right_offset;
+};
+
+/**
+ * struct ipu_fw_isys_stream_cfg_data_abi
+ * ISYS stream configuration data structure
+ * @crop: defines cropping resolution for the
+ * maximum number of input pins which can be cropped,
+ * it is directly mapped to the HW devices
+ * @input_pins: input pin descriptors
+ * @output_pins: output pin descriptors
+ * @compfmt: de-compression setting for User Defined Data
+ * @nof_input_pins: number of input pins
+ * @nof_output_pins: number of output pins
+ * @send_irq_sof_discarded: send irq on discarded frame sof response
+ *		- if '1' it will override the send_resp_sof_discarded
+ *		  and send the response
+ *		- if '0' the send_resp_sof_discarded will determine
+ *		  whether to send the response
+ * @send_irq_eof_discarded: send irq on discarded frame eof response
+ *		- if '1' it will override the send_resp_eof_discarded
+ *		  and send the response
+ *		- if '0' the send_resp_eof_discarded will determine
+ *		  whether to send the response
+ * @send_resp_sof_discarded: send response for discarded frame sof detected,
+ *			     used only when send_irq_sof_discarded is '0'
+ * @send_resp_eof_discarded: send response for discarded frame eof detected,
+ *			     used only when send_irq_eof_discarded is '0'
+ * @src: Stream source index e.g. MIPI_generator_0, CSI2-rx_1
+ * @vc: MIPI Virtual Channel (up to 4 virtual per physical channel)
+ * @isl_use: indicates whether stream requires ISL and how
+ * @sensor_type: type of connected sensor, tobii or others, default is 0
+ */
+struct ipu_fw_isys_stream_cfg_data_abi {
+	struct ipu_fw_isys_cropping_abi crop;
+	struct ipu_fw_isys_input_pin_info_abi input_pins[IPU_MAX_IPINS];
+	struct ipu_fw_isys_output_pin_info_abi output_pins[IPU_MAX_OPINS];
+	u32 compfmt;
+	u8 nof_input_pins;
+	u8 nof_output_pins;
+	u8 send_irq_sof_discarded;
+	u8 send_irq_eof_discarded;
+	u8 send_resp_sof_discarded;
+	u8 send_resp_eof_discarded;
+	u8 src;
+	u8 vc;
+	u8 isl_use;
+	u8 sensor_type;
+};
+
+/**
+ * struct ipu_fw_isys_frame_buff_set - frame buffer set
+ * @output_pins: output pin addresses
+ * @send_irq_sof: send irq on frame sof response
+ *		- if '1' it will override the send_resp_sof and
+ *		  send the response
+ *		- if '0' the send_resp_sof will determine whether to
+ *		  send the response
+ * @send_irq_eof: send irq on frame eof response
+ *		- if '1' it will override the send_resp_eof and
+ *		  send the response
+ *		- if '0' the send_resp_eof will determine whether to
+ *		  send the response
+ * @send_resp_sof: send response for frame sof detected,
+ *		   used only when send_irq_sof is '0'
+ * @send_resp_eof: send response for frame eof detected,
+ *		   used only when send_irq_eof is '0'
+ * @send_resp_capture_ack: send response for capture ack event
+ * @send_resp_capture_done: send response for capture done event
+ */
+struct ipu_fw_isys_frame_buff_set_abi {
+	struct ipu_fw_isys_output_pin_payload_abi output_pins[IPU_MAX_OPINS];
+	u8 send_irq_sof;
+	u8 send_irq_eof;
+	u8 send_irq_capture_ack;
+	u8 send_irq_capture_done;
+	u8 send_resp_sof;
+	u8 send_resp_eof;
+	u8 send_resp_capture_ack;
+	u8 send_resp_capture_done;
+	u8 reserved;
+};
+
+/**
+ * struct ipu_fw_isys_error_info_abi
+ * @error: error code if something went wrong
+ * @error_details: depending on error code, it may contain additional error info
+ */
+struct ipu_fw_isys_error_info_abi {
+	enum ipu_fw_isys_error error;
+	u32 error_details;
+};
+
+/**
+ * struct ipu_fw_isys_resp_info_comm
+ * @pin: this var is only valid for pin event related responses,
+ *     contains pin addresses
+ * @error_info: error information from the FW
+ * @timestamp: Time information for event if available
+ * @stream_handle: stream id the response corresponds to
+ * @type: response type (enum ipu_fw_isys_resp_type)
+ * @pin_id: pin id that the pin payload corresponds to
+ */
+struct ipu_fw_isys_resp_info_abi {
+	u64 buf_id;
+	struct ipu_fw_isys_output_pin_payload_abi pin;
+	struct ipu_fw_isys_error_info_abi error_info;
+	u32 timestamp[2];
+	u8 stream_handle;
+	u8 type;
+	u8 pin_id;
+	u16 reserved;
+};
+
+/**
+ * struct ipu_fw_isys_proxy_error_info_comm
+ * @proxy_error: error code if something went wrong
+ * @proxy_error_details: depending on error code, it may contain additional
+ *			error info
+ */
+struct ipu_fw_isys_proxy_error_info_abi {
+	enum ipu_fw_proxy_error error;
+	u32 error_details;
+};
+
+struct ipu_fw_isys_proxy_resp_info_abi {
+	u32 request_id;
+	struct ipu_fw_isys_proxy_error_info_abi error_info;
+};
+
+/**
+ * struct ipu_fw_proxy_write_queue_token
+ * @request_id: update id for the specific proxy write request
+ * @region_index: Region id for the proxy write request
+ * @offset: Offset of the write request according to the base address
+ *	    of the region
+ * @value: Value that is requested to be written with the proxy write request
+ */
+struct ipu_fw_proxy_write_queue_token {
+	u32 request_id;
+	u32 region_index;
+	u32 offset;
+	u32 value;
+};
+
+/* From here on type defines not coming from the ISYSAPI interface */
+
+/**
+ * struct ipu_fw_resp_queue_token
+ */
+struct ipu_fw_resp_queue_token {
+	struct ipu_fw_isys_resp_info_abi resp_info;
+};
+
+/**
+ * struct ipu_fw_send_queue_token
+ */
+struct ipu_fw_send_queue_token {
+	u64 buf_handle;
+	u32 payload;
+	u16 send_type;
+	u16 stream_id;
+};
+
+/**
+ * struct ipu_fw_proxy_resp_queue_token
+ */
+struct ipu_fw_proxy_resp_queue_token {
+	struct ipu_fw_isys_proxy_resp_info_abi proxy_resp_info;
+};
+
+/**
+ * struct ipu_fw_proxy_send_queue_token
+ */
+struct ipu_fw_proxy_send_queue_token {
+	u32 request_id;
+	u32 region_index;
+	u32 offset;
+	u32 value;
+};
+
+void ipu_fw_isys_set_params(struct ipu_fw_isys_stream_cfg_data_abi *stream_cfg);
+
+void ipu_fw_isys_dump_stream_cfg(struct device *dev,
+				 struct ipu_fw_isys_stream_cfg_data_abi
+				 *stream_cfg);
+void ipu_fw_isys_dump_frame_buff_set(struct device *dev,
+				     struct ipu_fw_isys_frame_buff_set_abi *buf,
+				     unsigned int outputs);
+int ipu_fw_isys_init(struct ipu_isys *isys, unsigned int num_streams);
+int ipu_fw_isys_close(struct ipu_isys *isys);
+int ipu_fw_isys_simple_cmd(struct ipu_isys *isys,
+			   const unsigned int stream_handle,
+			   enum ipu_fw_isys_send_type send_type);
+int ipu_fw_isys_complex_cmd(struct ipu_isys *isys,
+			    const unsigned int stream_handle,
+			    void *cpu_mapped_buf,
+			    dma_addr_t dma_mapped_buf,
+			    size_t size, enum ipu_fw_isys_send_type send_type);
+int ipu_fw_isys_send_proxy_token(struct ipu_isys *isys,
+				 unsigned int req_id,
+				 unsigned int index,
+				 unsigned int offset, u32 value);
+void ipu_fw_isys_cleanup(struct ipu_isys *isys);
+struct ipu_fw_isys_resp_info_abi *
+ipu_fw_isys_get_resp(void *context, unsigned int queue,
+		     struct ipu_fw_isys_resp_info_abi *response);
+void ipu_fw_isys_put_resp(void *context, unsigned int queue);
+#endif
diff -ruN a/drivers/media/pci/intel/ipu-fw-psys.c b/drivers/media/pci/intel/ipu-fw-psys.c
--- a/drivers/media/pci/intel/ipu-fw-psys.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-fw-psys.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,430 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2016 - 2020 Intel Corporation
+
+#include <linux/delay.h>
+
+#include <uapi/linux/ipu-psys.h>
+
+#include "ipu-fw-com.h"
+#include "ipu-fw-psys.h"
+#include "ipu-psys.h"
+
+int ipu_fw_psys_pg_start(struct ipu_psys_kcmd *kcmd)
+{
+	kcmd->kpg->pg->state = IPU_FW_PSYS_PROCESS_GROUP_STARTED;
+	return 0;
+}
+
+int ipu_fw_psys_pg_disown(struct ipu_psys_kcmd *kcmd)
+{
+	struct ipu_fw_psys_cmd *psys_cmd;
+	int ret = 0;
+
+	psys_cmd = ipu_send_get_token(kcmd->fh->psys->fwcom, 0);
+	if (!psys_cmd) {
+		dev_err(&kcmd->fh->psys->adev->dev,
+			"%s failed to get token!\n", __func__);
+		kcmd->pg_user = NULL;
+		ret = -ENODATA;
+		goto out;
+	}
+	psys_cmd->command = IPU_FW_PSYS_PROCESS_GROUP_CMD_START;
+	psys_cmd->msg = 0;
+	psys_cmd->context_handle = kcmd->kpg->pg->ipu_virtual_address;
+	ipu_send_put_token(kcmd->fh->psys->fwcom, 0);
+
+out:
+	return ret;
+}
+
+int ipu_fw_psys_ppg_suspend(struct ipu_psys_kcmd *kcmd)
+{
+	struct ipu_fw_psys_cmd *psys_cmd;
+	int ret = 0;
+
+	/* ppg suspend cmd uses QUEUE_DEVICE_ID instead of QUEUE_COMMAND_ID */
+	psys_cmd = ipu_send_get_token(kcmd->fh->psys->fwcom, 1);
+	if (!psys_cmd) {
+		dev_err(&kcmd->fh->psys->adev->dev,
+			"%s failed to get token!\n", __func__);
+		kcmd->pg_user = NULL;
+		ret = -ENODATA;
+		goto out;
+	}
+	psys_cmd->command = IPU_FW_PSYS_PROCESS_GROUP_CMD_SUSPEND;
+	psys_cmd->msg = 0;
+	psys_cmd->context_handle = kcmd->kpg->pg->ipu_virtual_address;
+	ipu_send_put_token(kcmd->fh->psys->fwcom, 1);
+
+out:
+	return ret;
+}
+
+int ipu_fw_psys_ppg_resume(struct ipu_psys_kcmd *kcmd)
+{
+	struct ipu_fw_psys_cmd *psys_cmd;
+	int ret = 0;
+
+	psys_cmd = ipu_send_get_token(kcmd->fh->psys->fwcom, 0);
+	if (!psys_cmd) {
+		dev_err(&kcmd->fh->psys->adev->dev,
+			"%s failed to get token!\n", __func__);
+		kcmd->pg_user = NULL;
+		ret = -ENODATA;
+		goto out;
+	}
+	psys_cmd->command = IPU_FW_PSYS_PROCESS_GROUP_CMD_RESUME;
+	psys_cmd->msg = 0;
+	psys_cmd->context_handle = kcmd->kpg->pg->ipu_virtual_address;
+	ipu_send_put_token(kcmd->fh->psys->fwcom, 0);
+
+out:
+	return ret;
+}
+
+int ipu_fw_psys_pg_abort(struct ipu_psys_kcmd *kcmd)
+{
+	struct ipu_fw_psys_cmd *psys_cmd;
+	int ret = 0;
+
+	psys_cmd = ipu_send_get_token(kcmd->fh->psys->fwcom, 0);
+	if (!psys_cmd) {
+		dev_err(&kcmd->fh->psys->adev->dev,
+			"%s failed to get token!\n", __func__);
+		kcmd->pg_user = NULL;
+		ret = -ENODATA;
+		goto out;
+	}
+	psys_cmd->command = IPU_FW_PSYS_PROCESS_GROUP_CMD_STOP;
+	psys_cmd->msg = 0;
+	psys_cmd->context_handle = kcmd->kpg->pg->ipu_virtual_address;
+	ipu_send_put_token(kcmd->fh->psys->fwcom, 0);
+
+out:
+	return ret;
+}
+
+int ipu_fw_psys_pg_submit(struct ipu_psys_kcmd *kcmd)
+{
+	kcmd->kpg->pg->state = IPU_FW_PSYS_PROCESS_GROUP_BLOCKED;
+	return 0;
+}
+
+int ipu_fw_psys_rcv_event(struct ipu_psys *psys,
+			  struct ipu_fw_psys_event *event)
+{
+	void *rcv;
+
+	rcv = ipu_recv_get_token(psys->fwcom, 0);
+	if (!rcv)
+		return 0;
+
+	memcpy(event, rcv, sizeof(*event));
+	ipu_recv_put_token(psys->fwcom, 0);
+	return 1;
+}
+
+int ipu_fw_psys_terminal_set(struct ipu_fw_psys_terminal *terminal,
+			     int terminal_idx,
+			     struct ipu_psys_kcmd *kcmd,
+			     u32 buffer, unsigned int size)
+{
+	u32 type;
+	u32 buffer_state;
+
+	type = terminal->terminal_type;
+
+	switch (type) {
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_CACHED_IN:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_CACHED_OUT:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SPATIAL_IN:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SPATIAL_OUT:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SLICED_IN:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SLICED_OUT:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PROGRAM:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PROGRAM_CONTROL_INIT:
+		buffer_state = IPU_FW_PSYS_BUFFER_UNDEFINED;
+		break;
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_STREAM:
+	case IPU_FW_PSYS_TERMINAL_TYPE_DATA_IN:
+	case IPU_FW_PSYS_TERMINAL_TYPE_STATE_IN:
+		buffer_state = IPU_FW_PSYS_BUFFER_FULL;
+		break;
+	case IPU_FW_PSYS_TERMINAL_TYPE_DATA_OUT:
+	case IPU_FW_PSYS_TERMINAL_TYPE_STATE_OUT:
+		buffer_state = IPU_FW_PSYS_BUFFER_EMPTY;
+		break;
+	default:
+		dev_err(&kcmd->fh->psys->adev->dev,
+			"unknown terminal type: 0x%x\n", type);
+		return -EAGAIN;
+	}
+
+	if (type == IPU_FW_PSYS_TERMINAL_TYPE_DATA_IN ||
+	    type == IPU_FW_PSYS_TERMINAL_TYPE_DATA_OUT) {
+		struct ipu_fw_psys_data_terminal *dterminal =
+		    (struct ipu_fw_psys_data_terminal *)terminal;
+		dterminal->connection_type = IPU_FW_PSYS_CONNECTION_MEMORY;
+		dterminal->frame.data_bytes = size;
+		if (!ipu_fw_psys_pg_get_protocol(kcmd))
+			dterminal->frame.data = buffer;
+		else
+			dterminal->frame.data_index = terminal_idx;
+		dterminal->frame.buffer_state = buffer_state;
+	} else {
+		struct ipu_fw_psys_param_terminal *pterminal =
+		    (struct ipu_fw_psys_param_terminal *)terminal;
+		if (!ipu_fw_psys_pg_get_protocol(kcmd))
+			pterminal->param_payload.buffer = buffer;
+		else
+			pterminal->param_payload.terminal_index = terminal_idx;
+	}
+	return 0;
+}
+
+void ipu_fw_psys_pg_dump(struct ipu_psys *psys,
+			 struct ipu_psys_kcmd *kcmd, const char *note)
+{
+	ipu6_fw_psys_pg_dump(psys, kcmd, note);
+}
+
+int ipu_fw_psys_pg_get_id(struct ipu_psys_kcmd *kcmd)
+{
+	return kcmd->kpg->pg->ID;
+}
+
+int ipu_fw_psys_pg_get_terminal_count(struct ipu_psys_kcmd *kcmd)
+{
+	return kcmd->kpg->pg->terminal_count;
+}
+
+int ipu_fw_psys_pg_get_size(struct ipu_psys_kcmd *kcmd)
+{
+	return kcmd->kpg->pg->size;
+}
+
+int ipu_fw_psys_pg_set_ipu_vaddress(struct ipu_psys_kcmd *kcmd,
+				    dma_addr_t vaddress)
+{
+	kcmd->kpg->pg->ipu_virtual_address = vaddress;
+	return 0;
+}
+
+struct ipu_fw_psys_terminal *ipu_fw_psys_pg_get_terminal(struct ipu_psys_kcmd
+							 *kcmd, int index)
+{
+	struct ipu_fw_psys_terminal *terminal;
+	u16 *terminal_offset_table;
+
+	terminal_offset_table =
+	    (uint16_t *)((char *)kcmd->kpg->pg +
+			  kcmd->kpg->pg->terminals_offset);
+	terminal = (struct ipu_fw_psys_terminal *)
+	    ((char *)kcmd->kpg->pg + terminal_offset_table[index]);
+	return terminal;
+}
+
+void ipu_fw_psys_pg_set_token(struct ipu_psys_kcmd *kcmd, u64 token)
+{
+	kcmd->kpg->pg->token = (u64)token;
+}
+
+u64 ipu_fw_psys_pg_get_token(struct ipu_psys_kcmd *kcmd)
+{
+	return kcmd->kpg->pg->token;
+}
+
+int ipu_fw_psys_pg_get_protocol(struct ipu_psys_kcmd *kcmd)
+{
+	return kcmd->kpg->pg->protocol_version;
+}
+
+int ipu_fw_psys_ppg_set_buffer_set(struct ipu_psys_kcmd *kcmd,
+				   struct ipu_fw_psys_terminal *terminal,
+				   int terminal_idx, u32 buffer)
+{
+	u32 type;
+	u32 buffer_state;
+	u32 *buffer_ptr;
+	struct ipu_fw_psys_buffer_set *buf_set = kcmd->kbuf_set->buf_set;
+
+	type = terminal->terminal_type;
+
+	switch (type) {
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_CACHED_IN:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_CACHED_OUT:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SPATIAL_IN:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SPATIAL_OUT:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SLICED_IN:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SLICED_OUT:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PROGRAM:
+	case IPU_FW_PSYS_TERMINAL_TYPE_PROGRAM_CONTROL_INIT:
+		buffer_state = IPU_FW_PSYS_BUFFER_UNDEFINED;
+		break;
+	case IPU_FW_PSYS_TERMINAL_TYPE_PARAM_STREAM:
+	case IPU_FW_PSYS_TERMINAL_TYPE_DATA_IN:
+	case IPU_FW_PSYS_TERMINAL_TYPE_STATE_IN:
+		buffer_state = IPU_FW_PSYS_BUFFER_FULL;
+		break;
+	case IPU_FW_PSYS_TERMINAL_TYPE_DATA_OUT:
+	case IPU_FW_PSYS_TERMINAL_TYPE_STATE_OUT:
+		buffer_state = IPU_FW_PSYS_BUFFER_EMPTY;
+		break;
+	default:
+		dev_err(&kcmd->fh->psys->adev->dev,
+			"unknown terminal type: 0x%x\n", type);
+		return -EAGAIN;
+	}
+
+	buffer_ptr = (u32 *)((char *)buf_set + sizeof(*buf_set) +
+			      terminal_idx * sizeof(*buffer_ptr));
+
+	*buffer_ptr = buffer;
+
+	if (type == IPU_FW_PSYS_TERMINAL_TYPE_DATA_IN ||
+	    type == IPU_FW_PSYS_TERMINAL_TYPE_DATA_OUT) {
+		struct ipu_fw_psys_data_terminal *dterminal =
+		    (struct ipu_fw_psys_data_terminal *)terminal;
+		dterminal->frame.buffer_state = buffer_state;
+	}
+
+	return 0;
+}
+
+size_t ipu_fw_psys_ppg_get_buffer_set_size(struct ipu_psys_kcmd *kcmd)
+{
+	return (sizeof(struct ipu_fw_psys_buffer_set) +
+		kcmd->kpg->pg->terminal_count * sizeof(u32));
+}
+
+int
+ipu_fw_psys_ppg_buffer_set_vaddress(struct ipu_fw_psys_buffer_set *buf_set,
+				    u32 vaddress)
+{
+	buf_set->ipu_virtual_address = vaddress;
+	return 0;
+}
+
+int ipu_fw_psys_ppg_buffer_set_set_kernel_enable_bitmap(
+		struct ipu_fw_psys_buffer_set *buf_set,
+		u32 *kernel_enable_bitmap)
+{
+	memcpy(buf_set->kernel_enable_bitmap, (u8 *)kernel_enable_bitmap,
+	       sizeof(buf_set->kernel_enable_bitmap));
+	return 0;
+}
+
+struct ipu_fw_psys_buffer_set *
+ipu_fw_psys_ppg_create_buffer_set(struct ipu_psys_kcmd *kcmd,
+				  void *kaddr, u32 frame_counter)
+{
+	struct ipu_fw_psys_buffer_set *buffer_set = NULL;
+	unsigned int i;
+
+	buffer_set = (struct ipu_fw_psys_buffer_set *)kaddr;
+
+	/*
+	 * Set base struct members
+	 */
+	buffer_set->ipu_virtual_address = 0;
+	buffer_set->process_group_handle = kcmd->kpg->pg->ipu_virtual_address;
+	buffer_set->frame_counter = frame_counter;
+	buffer_set->terminal_count = kcmd->kpg->pg->terminal_count;
+
+	/*
+	 * Initialize adjacent buffer addresses
+	 */
+	for (i = 0; i < buffer_set->terminal_count; i++) {
+		u32 *buffer =
+		    (u32 *)((char *)buffer_set +
+			     sizeof(*buffer_set) + sizeof(u32) * i);
+
+		*buffer = 0;
+	}
+
+	return buffer_set;
+}
+
+int ipu_fw_psys_ppg_enqueue_bufs(struct ipu_psys_kcmd *kcmd)
+{
+	struct ipu_fw_psys_cmd *psys_cmd;
+	unsigned int queue_id;
+	int ret = 0;
+	unsigned int size;
+
+	if (ipu_ver == IPU_VER_6SE)
+		size = IPU6SE_FW_PSYS_N_PSYS_CMD_QUEUE_ID;
+	else
+		size = IPU6_FW_PSYS_N_PSYS_CMD_QUEUE_ID;
+	queue_id = kcmd->kpg->pg->base_queue_id;
+
+	if (queue_id >= size)
+		return -EINVAL;
+
+	psys_cmd = ipu_send_get_token(kcmd->fh->psys->fwcom, queue_id);
+	if (!psys_cmd) {
+		dev_err(&kcmd->fh->psys->adev->dev,
+			"%s failed to get token!\n", __func__);
+		kcmd->pg_user = NULL;
+		return -ENODATA;
+	}
+
+	psys_cmd->command = IPU_FW_PSYS_PROCESS_GROUP_CMD_RUN;
+	psys_cmd->msg = 0;
+	psys_cmd->context_handle = kcmd->kbuf_set->buf_set->ipu_virtual_address;
+
+	ipu_send_put_token(kcmd->fh->psys->fwcom, queue_id);
+
+	return ret;
+}
+
+u8 ipu_fw_psys_ppg_get_base_queue_id(struct ipu_psys_kcmd *kcmd)
+{
+	return kcmd->kpg->pg->base_queue_id;
+}
+
+void ipu_fw_psys_ppg_set_base_queue_id(struct ipu_psys_kcmd *kcmd, u8 queue_id)
+{
+	kcmd->kpg->pg->base_queue_id = queue_id;
+}
+
+int ipu_fw_psys_open(struct ipu_psys *psys)
+{
+	int retry = IPU_PSYS_OPEN_RETRY, retval;
+
+	retval = ipu_fw_com_open(psys->fwcom);
+	if (retval) {
+		dev_err(&psys->adev->dev, "fw com open failed.\n");
+		return retval;
+	}
+
+	do {
+		usleep_range(IPU_PSYS_OPEN_TIMEOUT_US,
+			     IPU_PSYS_OPEN_TIMEOUT_US + 10);
+		retval = ipu_fw_com_ready(psys->fwcom);
+		if (!retval) {
+			dev_dbg(&psys->adev->dev, "psys port open ready!\n");
+			break;
+		}
+	} while (retry-- > 0);
+
+	if (!retry && retval) {
+		dev_err(&psys->adev->dev, "psys port open ready failed %d\n",
+			retval);
+		ipu_fw_com_close(psys->fwcom);
+		return retval;
+	}
+	return 0;
+}
+
+int ipu_fw_psys_close(struct ipu_psys *psys)
+{
+	int retval;
+
+	retval = ipu_fw_com_close(psys->fwcom);
+	if (retval) {
+		dev_err(&psys->adev->dev, "fw com close failed.\n");
+		return retval;
+	}
+	return retval;
+}
diff -ruN a/drivers/media/pci/intel/ipu-fw-psys.h b/drivers/media/pci/intel/ipu-fw-psys.h
--- a/drivers/media/pci/intel/ipu-fw-psys.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-fw-psys.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,382 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2016 - 2020 Intel Corporation */
+
+#ifndef IPU_FW_PSYS_H
+#define IPU_FW_PSYS_H
+
+#include "ipu6-platform-resources.h"
+#include "ipu6se-platform-resources.h"
+#include "ipu6ep-platform-resources.h"
+
+#define IPU_FW_PSYS_CMD_QUEUE_SIZE 0x20
+#define IPU_FW_PSYS_EVENT_QUEUE_SIZE 0x40
+
+#define IPU_FW_PSYS_CMD_BITS 64
+#define IPU_FW_PSYS_EVENT_BITS 128
+
+enum {
+	IPU_FW_PSYS_EVENT_TYPE_SUCCESS = 0,
+	IPU_FW_PSYS_EVENT_TYPE_UNKNOWN_ERROR = 1,
+	IPU_FW_PSYS_EVENT_TYPE_RET_REM_OBJ_NOT_FOUND = 2,
+	IPU_FW_PSYS_EVENT_TYPE_RET_REM_OBJ_TOO_BIG = 3,
+	IPU_FW_PSYS_EVENT_TYPE_RET_REM_OBJ_DDR_TRANS_ERR = 4,
+	IPU_FW_PSYS_EVENT_TYPE_RET_REM_OBJ_NULL_PKG_DIR_ADDR = 5,
+	IPU_FW_PSYS_EVENT_TYPE_PROC_GRP_LOAD_FRAME_ERR = 6,
+	IPU_FW_PSYS_EVENT_TYPE_PROC_GRP_LOAD_FRAGMENT_ERR = 7,
+	IPU_FW_PSYS_EVENT_TYPE_PROC_GRP_PROCESS_COUNT_ZERO = 8,
+	IPU_FW_PSYS_EVENT_TYPE_PROC_GRP_PROCESS_INIT_ERR = 9,
+	IPU_FW_PSYS_EVENT_TYPE_PROC_GRP_ABORT = 10,
+	IPU_FW_PSYS_EVENT_TYPE_PROC_GRP_NULL = 11,
+	IPU_FW_PSYS_EVENT_TYPE_PROC_GRP_VALIDATION_ERR = 12,
+	IPU_FW_PSYS_EVENT_TYPE_PROC_GRP_INVALID_FRAME = 13
+};
+
+enum {
+	IPU_FW_PSYS_EVENT_QUEUE_MAIN_ID,
+	IPU_FW_PSYS_N_PSYS_EVENT_QUEUE_ID
+};
+
+enum {
+	IPU_FW_PSYS_PROCESS_GROUP_ERROR = 0,
+	IPU_FW_PSYS_PROCESS_GROUP_CREATED,
+	IPU_FW_PSYS_PROCESS_GROUP_READY,
+	IPU_FW_PSYS_PROCESS_GROUP_BLOCKED,
+	IPU_FW_PSYS_PROCESS_GROUP_STARTED,
+	IPU_FW_PSYS_PROCESS_GROUP_RUNNING,
+	IPU_FW_PSYS_PROCESS_GROUP_STALLED,
+	IPU_FW_PSYS_PROCESS_GROUP_STOPPED,
+	IPU_FW_PSYS_N_PROCESS_GROUP_STATES
+};
+
+enum {
+	IPU_FW_PSYS_CONNECTION_MEMORY = 0,
+	IPU_FW_PSYS_CONNECTION_MEMORY_STREAM,
+	IPU_FW_PSYS_CONNECTION_STREAM,
+	IPU_FW_PSYS_N_CONNECTION_TYPES
+};
+
+enum {
+	IPU_FW_PSYS_BUFFER_NULL = 0,
+	IPU_FW_PSYS_BUFFER_UNDEFINED,
+	IPU_FW_PSYS_BUFFER_EMPTY,
+	IPU_FW_PSYS_BUFFER_NONEMPTY,
+	IPU_FW_PSYS_BUFFER_FULL,
+	IPU_FW_PSYS_N_BUFFER_STATES
+};
+
+enum {
+	IPU_FW_PSYS_TERMINAL_TYPE_DATA_IN = 0,
+	IPU_FW_PSYS_TERMINAL_TYPE_DATA_OUT,
+	IPU_FW_PSYS_TERMINAL_TYPE_PARAM_STREAM,
+	IPU_FW_PSYS_TERMINAL_TYPE_PARAM_CACHED_IN,
+	IPU_FW_PSYS_TERMINAL_TYPE_PARAM_CACHED_OUT,
+	IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SPATIAL_IN,
+	IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SPATIAL_OUT,
+	IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SLICED_IN,
+	IPU_FW_PSYS_TERMINAL_TYPE_PARAM_SLICED_OUT,
+	IPU_FW_PSYS_TERMINAL_TYPE_STATE_IN,
+	IPU_FW_PSYS_TERMINAL_TYPE_STATE_OUT,
+	IPU_FW_PSYS_TERMINAL_TYPE_PROGRAM,
+	IPU_FW_PSYS_TERMINAL_TYPE_PROGRAM_CONTROL_INIT,
+	IPU_FW_PSYS_N_TERMINAL_TYPES
+};
+
+enum {
+	IPU_FW_PSYS_COL_DIMENSION = 0,
+	IPU_FW_PSYS_ROW_DIMENSION = 1,
+	IPU_FW_PSYS_N_DATA_DIMENSION = 2
+};
+
+enum {
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_NOP = 0,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_SUBMIT,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_ATTACH,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_DETACH,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_START,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_DISOWN,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_RUN,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_STOP,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_SUSPEND,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_RESUME,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_ABORT,
+	IPU_FW_PSYS_PROCESS_GROUP_CMD_RESET,
+	IPU_FW_PSYS_N_PROCESS_GROUP_CMDS
+};
+
+enum {
+	IPU_FW_PSYS_PROCESS_GROUP_PROTOCOL_LEGACY = 0,
+	IPU_FW_PSYS_PROCESS_GROUP_PROTOCOL_PPG,
+	IPU_FW_PSYS_PROCESS_GROUP_N_PROTOCOLS
+};
+
+struct __packed ipu_fw_psys_process_group {
+	u64 token;
+	u64 private_token;
+	u32 routing_bitmap[IPU_FW_PSYS_RBM_NOF_ELEMS];
+	u32 kernel_bitmap[IPU_FW_PSYS_KBM_NOF_ELEMS];
+	u32 size;
+	u32 psys_server_init_cycles;
+	u32 pg_load_start_ts;
+	u32 pg_load_cycles;
+	u32 pg_init_cycles;
+	u32 pg_processing_cycles;
+	u32 pg_next_frame_init_cycles;
+	u32 pg_complete_cycles;
+	u32 ID;
+	u32 state;
+	u32 ipu_virtual_address;
+	u32 resource_bitmap;
+	u16 fragment_count;
+	u16 fragment_state;
+	u16 fragment_limit;
+	u16 processes_offset;
+	u16 terminals_offset;
+	u8 process_count;
+	u8 terminal_count;
+	u8 subgraph_count;
+	u8 protocol_version;
+	u8 base_queue_id;
+	u8 num_queues;
+	u8 mask_irq;
+	u8 error_handling_enable;
+	u8 padding[IPU_FW_PSYS_N_PADDING_UINT8_IN_PROCESS_GROUP_STRUCT];
+};
+
+struct ipu_fw_psys_srv_init {
+	void *host_ddr_pkg_dir;
+	u32 ddr_pkg_dir_address;
+	u32 pkg_dir_size;
+
+	u32 icache_prefetch_sp;
+	u32 icache_prefetch_isp;
+};
+
+struct __packed ipu_fw_psys_cmd {
+	u16 command;
+	u16 msg;
+	u32 context_handle;
+};
+
+struct __packed ipu_fw_psys_event {
+	u16 status;
+	u16 command;
+	u32 context_handle;
+	u64 token;
+};
+
+struct ipu_fw_psys_terminal {
+	u32 terminal_type;
+	s16 parent_offset;
+	u16 size;
+	u16 tm_index;
+	u8 ID;
+	u8 padding[IPU_FW_PSYS_N_PADDING_UINT8_IN_TERMINAL_STRUCT];
+};
+
+struct ipu_fw_psys_param_payload {
+	u64 host_buffer;
+	u32 buffer;
+	u32 terminal_index;
+};
+
+struct ipu_fw_psys_param_terminal {
+	struct ipu_fw_psys_terminal base;
+	struct ipu_fw_psys_param_payload param_payload;
+	u16 param_section_desc_offset;
+	u8 padding[IPU_FW_PSYS_N_PADDING_UINT8_IN_PARAM_TERMINAL_STRUCT];
+};
+
+struct ipu_fw_psys_frame {
+	u32 buffer_state;
+	u32 access_type;
+	u32 pointer_state;
+	u32 access_scope;
+	u32 data;
+	u32 data_index;
+	u32 data_bytes;
+	u8 padding[IPU_FW_PSYS_N_PADDING_UINT8_IN_FRAME_STRUCT];
+};
+
+struct ipu_fw_psys_frame_descriptor {
+	u32 frame_format_type;
+	u32 plane_count;
+	u32 plane_offsets[IPU_FW_PSYS_N_FRAME_PLANES];
+	u32 stride[1];
+	u32 ts_offsets[IPU_FW_PSYS_N_FRAME_PLANES];
+	u16 dimension[2];
+	u16 size;
+	u8 bpp;
+	u8 bpe;
+	u8 is_compressed;
+	u8 padding[IPU_FW_PSYS_N_PADDING_UINT8_IN_FRAME_DESC_STRUCT];
+};
+
+struct ipu_fw_psys_stream {
+	u64 dummy;
+};
+
+struct ipu_fw_psys_data_terminal {
+	struct ipu_fw_psys_terminal base;
+	struct ipu_fw_psys_frame_descriptor frame_descriptor;
+	struct ipu_fw_psys_frame frame;
+	struct ipu_fw_psys_stream stream;
+	u32 reserved;
+	u32 connection_type;
+	u16 fragment_descriptor_offset;
+	u8 kernel_id;
+	u8 subgraph_id;
+	u8 padding[IPU_FW_PSYS_N_PADDING_UINT8_IN_DATA_TERMINAL_STRUCT];
+};
+
+struct ipu_fw_psys_buffer_set {
+	u64 token;
+	u32 kernel_enable_bitmap[IPU_FW_PSYS_KERNEL_BITMAP_NOF_ELEMS];
+	u32 terminal_enable_bitmap[IPU_FW_PSYS_KERNEL_BITMAP_NOF_ELEMS];
+	u32 routing_enable_bitmap[IPU_FW_PSYS_KERNEL_BITMAP_NOF_ELEMS];
+	u32 rbm[IPU_FW_PSYS_RBM_NOF_ELEMS];
+	u32 ipu_virtual_address;
+	u32 process_group_handle;
+	u16 terminal_count;
+	u8 frame_counter;
+	u8 padding[IPU_FW_PSYS_N_PADDING_UINT8_IN_BUFFER_SET_STRUCT];
+};
+
+struct ipu_fw_psys_program_group_manifest {
+	u32 kernel_bitmap[IPU_FW_PSYS_KERNEL_BITMAP_NOF_ELEMS];
+	u32 ID;
+	u16 program_manifest_offset;
+	u16 terminal_manifest_offset;
+	u16 private_data_offset;
+	u16 rbm_manifest_offset;
+	u16 size;
+	u8 alignment;
+	u8 kernel_count;
+	u8 program_count;
+	u8 terminal_count;
+	u8 subgraph_count;
+	u8 reserved[5];
+};
+
+struct ipu_fw_generic_program_manifest {
+	u16 *dev_chn_size;
+	u16 *dev_chn_offset;
+	u16 *ext_mem_size;
+	u16 *ext_mem_offset;
+	u8 cell_id;
+	u8 cells[IPU_FW_PSYS_PROCESS_MAX_CELLS];
+	u8 cell_type_id;
+	u8 *is_dfm_relocatable;
+	u32 *dfm_port_bitmap;
+	u32 *dfm_active_port_bitmap;
+};
+
+struct ipu_fw_resource_definitions {
+	u32 num_cells;
+	u32 num_cells_type;
+	const u8 *cells;
+	u32 num_dev_channels;
+	const u16 *dev_channels;
+
+	u32 num_ext_mem_types;
+	u32 num_ext_mem_ids;
+	const u16 *ext_mem_ids;
+
+	u32 num_dfm_ids;
+	const u16 *dfms;
+
+	u32 cell_mem_row;
+	const u8 *cell_mem;
+};
+
+struct ipu6_psys_hw_res_variant {
+	unsigned int queue_num;
+	unsigned int cell_num;
+	int (*set_proc_dev_chn)(struct ipu_fw_psys_process *ptr, u16 offset,
+				u16 value);
+	int (*set_proc_dfm_bitmap)(struct ipu_fw_psys_process *ptr,
+				   u16 id, u32 bitmap, u32 active_bitmap);
+	int (*set_proc_ext_mem)(struct ipu_fw_psys_process *ptr,
+				u16 type_id, u16 mem_id, u16 offset);
+	int (*get_pgm_by_proc)(struct ipu_fw_generic_program_manifest *gen_pm,
+			       const struct ipu_fw_psys_program_group_manifest
+			       *pg_manifest,
+			       struct ipu_fw_psys_process *process);
+};
+struct ipu_psys_kcmd;
+struct ipu_psys;
+int ipu_fw_psys_pg_start(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_disown(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_abort(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_submit(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_ppg_suspend(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_ppg_resume(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_load_cycles(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_init_cycles(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_processing_cycles(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_server_init_cycles(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_next_frame_init_cycles(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_complete_cycles(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_rcv_event(struct ipu_psys *psys,
+			  struct ipu_fw_psys_event *event);
+int ipu_fw_psys_terminal_set(struct ipu_fw_psys_terminal *terminal,
+			     int terminal_idx,
+			     struct ipu_psys_kcmd *kcmd,
+			     u32 buffer, unsigned int size);
+void ipu_fw_psys_pg_dump(struct ipu_psys *psys,
+			 struct ipu_psys_kcmd *kcmd, const char *note);
+int ipu_fw_psys_pg_get_id(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_get_terminal_count(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_get_size(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_pg_set_ipu_vaddress(struct ipu_psys_kcmd *kcmd,
+				    dma_addr_t vaddress);
+struct ipu_fw_psys_terminal *ipu_fw_psys_pg_get_terminal(struct ipu_psys_kcmd
+							 *kcmd, int index);
+void ipu_fw_psys_pg_set_token(struct ipu_psys_kcmd *kcmd, u64 token);
+u64 ipu_fw_psys_pg_get_token(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_ppg_set_buffer_set(struct ipu_psys_kcmd *kcmd,
+				   struct ipu_fw_psys_terminal *terminal,
+				   int terminal_idx, u32 buffer);
+size_t ipu_fw_psys_ppg_get_buffer_set_size(struct ipu_psys_kcmd *kcmd);
+int
+ipu_fw_psys_ppg_buffer_set_vaddress(struct ipu_fw_psys_buffer_set *buf_set,
+				    u32 vaddress);
+int ipu_fw_psys_ppg_buffer_set_set_kernel_enable_bitmap(
+	struct ipu_fw_psys_buffer_set *buf_set, u32 *kernel_enable_bitmap);
+struct ipu_fw_psys_buffer_set *
+ipu_fw_psys_ppg_create_buffer_set(struct ipu_psys_kcmd *kcmd,
+				  void *kaddr, u32 frame_counter);
+int ipu_fw_psys_ppg_enqueue_bufs(struct ipu_psys_kcmd *kcmd);
+u8 ipu_fw_psys_ppg_get_base_queue_id(struct ipu_psys_kcmd *kcmd);
+void ipu_fw_psys_ppg_set_base_queue_id(struct ipu_psys_kcmd *kcmd, u8 queue_id);
+int ipu_fw_psys_pg_get_protocol(struct ipu_psys_kcmd *kcmd);
+int ipu_fw_psys_open(struct ipu_psys *psys);
+int ipu_fw_psys_close(struct ipu_psys *psys);
+
+/* common resource interface for both abi and api mode */
+int ipu_fw_psys_set_process_cell_id(struct ipu_fw_psys_process *ptr, u8 index,
+				    u8 value);
+u8 ipu_fw_psys_get_process_cell_id(struct ipu_fw_psys_process *ptr, u8 index);
+int ipu_fw_psys_clear_process_cell(struct ipu_fw_psys_process *ptr);
+int ipu_fw_psys_set_proc_dev_chn(struct ipu_fw_psys_process *ptr, u16 offset,
+				 u16 value);
+int ipu_fw_psys_set_process_ext_mem(struct ipu_fw_psys_process *ptr,
+				    u16 type_id, u16 mem_id, u16 offset);
+int ipu_fw_psys_get_program_manifest_by_process(
+	struct ipu_fw_generic_program_manifest *gen_pm,
+	const struct ipu_fw_psys_program_group_manifest *pg_manifest,
+	struct ipu_fw_psys_process *process);
+int ipu6_fw_psys_set_proc_dev_chn(struct ipu_fw_psys_process *ptr, u16 offset,
+				  u16 value);
+int ipu6_fw_psys_set_proc_dfm_bitmap(struct ipu_fw_psys_process *ptr,
+				     u16 id, u32 bitmap,
+				     u32 active_bitmap);
+int ipu6_fw_psys_set_process_ext_mem(struct ipu_fw_psys_process *ptr,
+				     u16 type_id, u16 mem_id, u16 offset);
+int ipu6_fw_psys_get_program_manifest_by_process(
+	struct ipu_fw_generic_program_manifest *gen_pm,
+	const struct ipu_fw_psys_program_group_manifest *pg_manifest,
+	struct ipu_fw_psys_process *process);
+void ipu6_fw_psys_pg_dump(struct ipu_psys *psys,
+			  struct ipu_psys_kcmd *kcmd, const char *note);
+void ipu6_psys_hw_res_variant_init(void);
+#endif /* IPU_FW_PSYS_H */
diff -ruN a/drivers/media/pci/intel/ipu.h b/drivers/media/pci/intel/ipu.h
--- a/drivers/media/pci/intel/ipu.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,108 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_H
+#define IPU_H
+
+#include <linux/ioport.h>
+#include <linux/list.h>
+#include <uapi/linux/media.h>
+#include <linux/version.h>
+
+#include "ipu-pdata.h"
+#include "ipu-bus.h"
+#include "ipu-buttress.h"
+#include "ipu-trace.h"
+
+#define IPU6_PCI_ID	0x9a19
+#define IPU6SE_PCI_ID	0x4e19
+#define IPU6EP_PCI_ID	0x465d
+
+enum ipu_version {
+	IPU_VER_INVALID = 0,
+	IPU_VER_6,
+	IPU_VER_6SE,
+	IPU_VER_6EP,
+};
+
+/*
+ * IPU version definitions to reflect the IPU driver changes.
+ * Both ISYS and PSYS share the same version.
+ */
+#define IPU_MAJOR_VERSION 1
+#define IPU_MINOR_VERSION 0
+#define IPU_DRIVER_VERSION (IPU_MAJOR_VERSION << 16 | IPU_MINOR_VERSION)
+
+/* processing system frequency: 25Mhz x ratio, Legal values [8,32] */
+#define PS_FREQ_CTL_DEFAULT_RATIO	0x12
+
+/* input system frequency: 1600Mhz / divisor. Legal values [2,8] */
+#define IS_FREQ_SOURCE			1600000000
+#define IS_FREQ_CTL_DIVISOR		0x4
+
+/*
+ * ISYS DMA can overshoot. For higher resolutions over allocation is one line
+ * but it must be at minimum 1024 bytes. Value could be different in
+ * different versions / generations thus provide it via platform data.
+ */
+#define IPU_ISYS_OVERALLOC_MIN		1024
+
+/*
+ * Physical pages in GDA is 128, page size is 2K for IPU6, 1K for others.
+ */
+#define IPU_DEVICE_GDA_NR_PAGES		128
+
+/*
+ * Virtualization factor to calculate the available virtual pages.
+ */
+#define IPU_DEVICE_GDA_VIRT_FACTOR	32
+
+struct pci_dev;
+struct list_head;
+struct firmware;
+
+#define NR_OF_MMU_RESOURCES			2
+
+struct ipu_device {
+	struct pci_dev *pdev;
+	struct list_head devices;
+	struct ipu_bus_device *isys;
+	struct ipu_bus_device *psys;
+	struct ipu_buttress buttress;
+
+	const struct firmware *cpd_fw;
+	const char *cpd_fw_name;
+	u64 *pkg_dir;
+	dma_addr_t pkg_dir_dma_addr;
+	unsigned int pkg_dir_size;
+	struct sg_table fw_sgt;
+
+	void __iomem *base;
+#ifdef CONFIG_DEBUG_FS
+	struct dentry *ipu_dir;
+#endif
+	struct ipu_trace *trace;
+	bool flr_done;
+	bool ipc_reinit;
+	bool secure_mode;
+
+	int (*cpd_fw_reload)(struct ipu_device *isp);
+};
+
+#define IPU_DMA_MASK	39
+#define IPU_LIB_CALL_TIMEOUT_MS		2000
+#define IPU_PSYS_CMD_TIMEOUT_MS	2000
+#define IPU_PSYS_OPEN_TIMEOUT_US	   50
+#define IPU_PSYS_OPEN_RETRY (10000 / IPU_PSYS_OPEN_TIMEOUT_US)
+
+int ipu_fw_authenticate(void *data, u64 val);
+void ipu_configure_spc(struct ipu_device *isp,
+		       const struct ipu_hw_variants *hw_variant,
+		       int pkg_dir_idx, void __iomem *base, u64 *pkg_dir,
+		       dma_addr_t pkg_dir_dma_addr);
+int request_cpd_fw(const struct firmware **firmware_p, const char *name,
+		   struct device *device);
+extern enum ipu_version ipu_ver;
+void ipu_internal_pdata_init(void);
+
+#endif /* IPU_H */
diff -ruN a/drivers/media/pci/intel/ipu-isys.c b/drivers/media/pci/intel/ipu-isys.c
--- a/drivers/media/pci/intel/ipu-isys.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,1447 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2021 Intel Corporation
+
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/firmware.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/pm_runtime.h>
+#include <linux/string.h>
+#include <linux/sched.h>
+#include <linux/version.h>
+
+#include <media/ipu-isys.h>
+#include <media/v4l2-mc.h>
+#include <media/v4l2-subdev.h>
+#include <media/v4l2-fwnode.h>
+#include <media/v4l2-ctrls.h>
+#include <media/v4l2-device.h>
+#include <media/v4l2-event.h>
+#include <media/v4l2-ioctl.h>
+#include <media/v4l2-async.h>
+#include "ipu.h"
+#include "ipu-bus.h"
+#include "ipu-cpd.h"
+#include "ipu-mmu.h"
+#include "ipu-dma.h"
+#include "ipu-isys.h"
+#include "ipu-isys-csi2.h"
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+#include "ipu-isys-tpg.h"
+#endif
+#include "ipu-isys-video.h"
+#include "ipu-platform-regs.h"
+#include "ipu-buttress.h"
+#include "ipu-platform.h"
+#include "ipu-platform-buttress-regs.h"
+
+#define ISYS_PM_QOS_VALUE	300
+
+#define IPU_BUTTRESS_FABIC_CONTROL	    0x68
+#define GDA_ENABLE_IWAKE_INDEX		    2
+#define GDA_IWAKE_THRESHOLD_INDEX           1
+#define GDA_IRQ_CRITICAL_THRESHOLD_INDEX    0
+
+/* LTR & DID value are 10 bit at most */
+#define LTR_DID_VAL_MAX		1023
+#define LTR_DEFAULT_VALUE	0x70503C19
+#define FILL_TIME_DEFAULT_VALUE 0xFFF0783C
+#define LTR_DID_PKGC_2R		20
+#define LTR_DID_PKGC_8		100
+#define LTR_SCALE_DEFAULT	5
+#define LTR_SCALE_1024NS	2
+#define REG_PKGC_PMON_CFG	0xB00
+
+#define VAL_PKGC_PMON_CFG_RESET 0x38
+#define VAL_PKGC_PMON_CFG_START 0x7
+
+#define IS_PIXEL_BUFFER_PAGES		0x80
+/* BIOS provides the driver the LTR and threshold information in IPU,
+ * IS pixel buffer is 256KB, MaxSRAMSize is 200KB on IPU6.
+ */
+#define IPU6_MAX_SRAM_SIZE			(200 << 10)
+/* IS pixel buffer is 128KB, MaxSRAMSize is 96KB on IPU6SE.
+ */
+#define IPU6SE_MAX_SRAM_SIZE			(96 << 10)
+/* When iwake mode is disabled the critical threshold is statically set to 75%
+ * of the IS pixel buffer criticalThreshold = (128 * 3) / 4
+ */
+#define CRITICAL_THRESHOLD_IWAKE_DISABLE	(IS_PIXEL_BUFFER_PAGES * 3 / 4)
+
+union fabric_ctrl {
+	struct {
+		u16 ltr_val   : 10;
+		u16 ltr_scale : 3;
+		u16 RSVD1     : 3;
+		u16 did_val   : 10;
+		u16 did_scale : 3;
+		u16 RSVD2     : 1;
+		u16 keep_power_in_D0   : 1;
+		u16 keep_power_override : 1;
+	} bits;
+	u32 value;
+};
+
+enum ltr_did_type {
+	LTR_IWAKE_ON,
+	LTR_IWAKE_OFF,
+	LTR_ISYS_ON,
+	LTR_ISYS_OFF,
+	LTR_TYPE_MAX
+};
+
+static int
+isys_complete_ext_device_registration(struct ipu_isys *isys,
+				      struct v4l2_subdev *sd,
+				      struct ipu_isys_csi2_config *csi2)
+{
+	unsigned int i;
+	int rval;
+
+	v4l2_set_subdev_hostdata(sd, csi2);
+
+	for (i = 0; i < sd->entity.num_pads; i++) {
+		if (sd->entity.pads[i].flags & MEDIA_PAD_FL_SOURCE)
+			break;
+	}
+
+	if (i == sd->entity.num_pads) {
+		dev_warn(&isys->adev->dev,
+			 "no source pad in external entity\n");
+		rval = -ENOENT;
+		goto skip_unregister_subdev;
+	}
+
+	rval = media_create_pad_link(&sd->entity, i,
+				     &isys->csi2[csi2->port].asd.sd.entity,
+				     0, 0);
+	if (rval) {
+		dev_warn(&isys->adev->dev, "can't create link\n");
+		goto skip_unregister_subdev;
+	}
+
+	isys->csi2[csi2->port].nlanes = csi2->nlanes;
+	return 0;
+
+skip_unregister_subdev:
+	v4l2_device_unregister_subdev(sd);
+	return rval;
+}
+
+static void isys_unregister_subdevices(struct ipu_isys *isys)
+{
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	const struct ipu_isys_internal_tpg_pdata *tpg =
+	    &isys->pdata->ipdata->tpg;
+#endif
+	const struct ipu_isys_internal_csi2_pdata *csi2 =
+	    &isys->pdata->ipdata->csi2;
+	unsigned int i;
+
+	ipu_isys_csi2_be_cleanup(&isys->csi2_be);
+	for (i = 0; i < NR_OF_CSI2_BE_SOC_DEV; i++)
+		ipu_isys_csi2_be_soc_cleanup(&isys->csi2_be_soc[i]);
+
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	for (i = 0; i < tpg->ntpgs; i++)
+		ipu_isys_tpg_cleanup(&isys->tpg[i]);
+#endif
+
+	for (i = 0; i < csi2->nports; i++)
+		ipu_isys_csi2_cleanup(&isys->csi2[i]);
+}
+
+static int isys_register_subdevices(struct ipu_isys *isys)
+{
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	const struct ipu_isys_internal_tpg_pdata *tpg =
+	    &isys->pdata->ipdata->tpg;
+#endif
+	const struct ipu_isys_internal_csi2_pdata *csi2 =
+	    &isys->pdata->ipdata->csi2;
+	struct ipu_isys_csi2_be_soc *csi2_be_soc;
+	unsigned int i, k;
+	int rval;
+
+	isys->csi2 = devm_kcalloc(&isys->adev->dev, csi2->nports,
+				  sizeof(*isys->csi2), GFP_KERNEL);
+	if (!isys->csi2) {
+		rval = -ENOMEM;
+		goto fail;
+	}
+
+	for (i = 0; i < csi2->nports; i++) {
+		rval = ipu_isys_csi2_init(&isys->csi2[i], isys,
+					  isys->pdata->base +
+					  csi2->offsets[i], i);
+		if (rval)
+			goto fail;
+
+		isys->isr_csi2_bits |= IPU_ISYS_UNISPART_IRQ_CSI2(i);
+	}
+
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	isys->tpg = devm_kcalloc(&isys->adev->dev, tpg->ntpgs,
+				 sizeof(*isys->tpg), GFP_KERNEL);
+	if (!isys->tpg) {
+		rval = -ENOMEM;
+		goto fail;
+	}
+
+	for (i = 0; i < tpg->ntpgs; i++) {
+		rval = ipu_isys_tpg_init(&isys->tpg[i], isys,
+					 isys->pdata->base +
+					 tpg->offsets[i],
+					 tpg->sels ? (isys->pdata->base +
+						      tpg->sels[i]) : NULL, i);
+		if (rval)
+			goto fail;
+	}
+#endif
+
+	for (k = 0; k < NR_OF_CSI2_BE_SOC_DEV; k++) {
+		rval = ipu_isys_csi2_be_soc_init(&isys->csi2_be_soc[k],
+						 isys, k);
+		if (rval) {
+			dev_info(&isys->adev->dev,
+				 "can't register csi2 soc be device %d\n", k);
+			goto fail;
+		}
+	}
+
+	rval = ipu_isys_csi2_be_init(&isys->csi2_be, isys);
+	if (rval) {
+		dev_info(&isys->adev->dev,
+			 "can't register raw csi2 be device\n");
+		goto fail;
+	}
+
+	for (i = 0; i < csi2->nports; i++) {
+		rval = media_create_pad_link(&isys->csi2[i].asd.sd.entity,
+					     CSI2_PAD_SOURCE,
+					     &isys->csi2_be.asd.sd.entity,
+					     CSI2_BE_PAD_SINK, 0);
+		if (rval) {
+			dev_info(&isys->adev->dev,
+				 "can't create link csi2 <=> csi2_be\n");
+			goto fail;
+		}
+		for (k = 0; k < NR_OF_CSI2_BE_SOC_DEV; k++) {
+			csi2_be_soc = &isys->csi2_be_soc[k];
+			rval =
+			    media_create_pad_link(&isys->csi2[i].asd.sd.entity,
+						  CSI2_PAD_SOURCE,
+						  &csi2_be_soc->asd.sd.entity,
+						  CSI2_BE_SOC_PAD_SINK, 0);
+			if (rval) {
+				dev_info(&isys->adev->dev,
+					 "can't create link csi2->be_soc\n");
+				goto fail;
+			}
+		}
+	}
+
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	for (i = 0; i < tpg->ntpgs; i++) {
+		rval = media_create_pad_link(&isys->tpg[i].asd.sd.entity,
+					     TPG_PAD_SOURCE,
+					     &isys->csi2_be.asd.sd.entity,
+					     CSI2_BE_PAD_SINK, 0);
+		if (rval) {
+			dev_info(&isys->adev->dev,
+				 "can't create link between tpg and csi2_be\n");
+			goto fail;
+		}
+
+		for (k = 0; k < NR_OF_CSI2_BE_SOC_DEV; k++) {
+			csi2_be_soc = &isys->csi2_be_soc[k];
+			rval =
+			    media_create_pad_link(&isys->tpg[i].asd.sd.entity,
+						  TPG_PAD_SOURCE,
+						  &csi2_be_soc->asd.sd.entity,
+						  CSI2_BE_SOC_PAD_SINK, 0);
+			if (rval) {
+				dev_info(&isys->adev->dev,
+					 "can't create link tpg->be_soc\n");
+				goto fail;
+			}
+		}
+	}
+#endif
+
+	return 0;
+
+fail:
+	isys_unregister_subdevices(isys);
+	return rval;
+}
+
+/* read ltrdid threshold values from BIOS or system configuration */
+static void get_lut_ltrdid(struct ipu_isys *isys, struct ltr_did *pltr_did)
+{
+	struct isys_iwake_watermark *iwake_watermark = isys->iwake_watermark;
+	/* default values*/
+	struct ltr_did ltrdid_default;
+
+	ltrdid_default.lut_ltr.value = LTR_DEFAULT_VALUE;
+	ltrdid_default.lut_fill_time.value = FILL_TIME_DEFAULT_VALUE;
+
+	if (iwake_watermark->ltrdid.lut_ltr.value)
+		*pltr_did = iwake_watermark->ltrdid;
+	else
+		*pltr_did = ltrdid_default;
+}
+
+static int set_iwake_register(struct ipu_isys *isys, u32 index, u32 value)
+{
+	int ret = 0;
+	u32 req_id = index;
+	u32 offset = 0;
+
+	ret = ipu_fw_isys_send_proxy_token(isys, req_id, index, offset, value);
+	if (ret)
+		dev_err(&isys->adev->dev, "write %d failed %d", index, ret);
+
+	return ret;
+}
+
+/*
+ * When input system is powered up and before enabling any new sensor capture,
+ * or after disabling any sensor capture the following values need to be set:
+ * LTR_value = LTR(usec) from calculation;
+ * LTR_scale = 2;
+ * DID_value = DID(usec) from calculation;
+ * DID_scale = 2;
+ *
+ * When input system is powered down, the LTR and DID values
+ * must be returned to the default values:
+ * LTR_value = 1023;
+ * LTR_scale = 5;
+ * DID_value = 1023;
+ * DID_scale = 2;
+ */
+static void set_iwake_ltrdid(struct ipu_isys *isys,
+			     u16 ltr,
+			     u16 did,
+			     enum ltr_did_type use)
+{
+	/* did_scale will set to 2= 1us */
+	u16 ltr_val, ltr_scale, did_val;
+	union fabric_ctrl fc;
+	struct ipu_device *isp = isys->adev->isp;
+
+	switch (use) {
+	case LTR_IWAKE_ON:
+		ltr_val = min_t(u16, ltr, (u16)LTR_DID_VAL_MAX);
+		did_val = min_t(u16, did, (u16)LTR_DID_VAL_MAX);
+		ltr_scale = (ltr == LTR_DID_VAL_MAX &&
+				did == LTR_DID_VAL_MAX) ?
+				LTR_SCALE_DEFAULT : LTR_SCALE_1024NS;
+		break;
+	case LTR_ISYS_ON:
+	case LTR_IWAKE_OFF:
+		ltr_val = LTR_DID_PKGC_2R;
+		did_val = LTR_DID_PKGC_2R;
+		ltr_scale = LTR_SCALE_1024NS;
+		break;
+	case LTR_ISYS_OFF:
+		ltr_val   = LTR_DID_VAL_MAX;
+		did_val   = LTR_DID_VAL_MAX;
+		ltr_scale = LTR_SCALE_DEFAULT;
+		break;
+	default:
+		return;
+	}
+
+	fc.value = readl(isp->base + IPU_BUTTRESS_FABIC_CONTROL);
+	fc.bits.ltr_val = ltr_val;
+	fc.bits.ltr_scale = ltr_scale;
+	fc.bits.did_val = did_val;
+	fc.bits.did_scale = 2;
+	dev_dbg(&isys->adev->dev,
+		"%s ltr: %d  did: %d", __func__, ltr_val, did_val);
+	writel(fc.value, isp->base + IPU_BUTTRESS_FABIC_CONTROL);
+}
+
+/* SW driver may clear register GDA_ENABLE_IWAKE before the FW configures the
+ * stream for debug purposes. Otherwise SW should not access this register.
+ */
+static int enable_iwake(struct ipu_isys *isys, bool enable)
+{
+	int ret = 0;
+	struct isys_iwake_watermark *iwake_watermark = isys->iwake_watermark;
+
+	mutex_lock(&iwake_watermark->mutex);
+	if (iwake_watermark->iwake_enabled == enable) {
+		mutex_unlock(&iwake_watermark->mutex);
+		return ret;
+	}
+	ret = set_iwake_register(isys, GDA_ENABLE_IWAKE_INDEX, enable);
+	if (!ret)
+		iwake_watermark->iwake_enabled = enable;
+	mutex_unlock(&iwake_watermark->mutex);
+	return ret;
+}
+
+void update_watermark_setting(struct ipu_isys *isys)
+{
+	struct isys_iwake_watermark *iwake_watermark = isys->iwake_watermark;
+	struct list_head *stream_node;
+	struct video_stream_watermark *p_watermark;
+	struct ltr_did ltrdid;
+	u16 calc_fill_time_us = 0;
+	u16 ltr = 0;
+	u16 did = 0;
+	u32 iwake_threshold, iwake_critical_threshold;
+	u64 threshold_bytes;
+	u64 isys_pb_datarate_mbs = 0;
+	u16 sram_granulrity_shift =
+		(ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) ?
+		IPU6_SRAM_GRANULRITY_SHIFT : IPU6SE_SRAM_GRANULRITY_SHIFT;
+	int max_sram_size =
+		(ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) ?
+		IPU6_MAX_SRAM_SIZE : IPU6SE_MAX_SRAM_SIZE;
+
+	mutex_lock(&iwake_watermark->mutex);
+	if (iwake_watermark->force_iwake_disable) {
+		set_iwake_ltrdid(isys, 0, 0, LTR_IWAKE_OFF);
+		set_iwake_register(isys, GDA_IRQ_CRITICAL_THRESHOLD_INDEX,
+				   CRITICAL_THRESHOLD_IWAKE_DISABLE);
+		mutex_unlock(&iwake_watermark->mutex);
+		return;
+	}
+
+	if (list_empty(&iwake_watermark->video_list)) {
+		isys_pb_datarate_mbs = 0;
+	} else {
+		list_for_each(stream_node, &iwake_watermark->video_list)
+		{
+			p_watermark = list_entry(stream_node,
+						 struct video_stream_watermark,
+						 stream_node);
+			isys_pb_datarate_mbs += p_watermark->stream_data_rate;
+		}
+	}
+	mutex_unlock(&iwake_watermark->mutex);
+
+	if (!isys_pb_datarate_mbs) {
+		enable_iwake(isys, false);
+		set_iwake_ltrdid(isys, 0, 0, LTR_IWAKE_OFF);
+		mutex_lock(&iwake_watermark->mutex);
+		set_iwake_register(isys, GDA_IRQ_CRITICAL_THRESHOLD_INDEX,
+				   CRITICAL_THRESHOLD_IWAKE_DISABLE);
+		mutex_unlock(&iwake_watermark->mutex);
+	} else {
+		/* should enable iwake by default according to FW */
+		enable_iwake(isys, true);
+		calc_fill_time_us = (u16)(max_sram_size / isys_pb_datarate_mbs);
+		get_lut_ltrdid(isys, &ltrdid);
+
+		if (calc_fill_time_us <= ltrdid.lut_fill_time.bits.th0)
+			ltr = 0;
+		else if (calc_fill_time_us <= ltrdid.lut_fill_time.bits.th1)
+			ltr = ltrdid.lut_ltr.bits.val0;
+		else if (calc_fill_time_us <= ltrdid.lut_fill_time.bits.th2)
+			ltr = ltrdid.lut_ltr.bits.val1;
+		else if (calc_fill_time_us <= ltrdid.lut_fill_time.bits.th3)
+			ltr = ltrdid.lut_ltr.bits.val2;
+		else
+			ltr = ltrdid.lut_ltr.bits.val3;
+
+		did = calc_fill_time_us - ltr;
+
+		threshold_bytes = did * isys_pb_datarate_mbs;
+		/* calculate iwake threshold with 2KB granularity pages */
+		iwake_threshold =
+			max_t(u32, 1, threshold_bytes >> sram_granulrity_shift);
+
+		iwake_threshold = min_t(u32, iwake_threshold, max_sram_size);
+
+		/* set the critical threshold to halfway between
+		 * iwake threshold and the full buffer.
+		 */
+		iwake_critical_threshold = iwake_threshold +
+			(IS_PIXEL_BUFFER_PAGES - iwake_threshold) / 2;
+
+		dev_dbg(&isys->adev->dev, "%s threshold: %u  critical: %u",
+			__func__, iwake_threshold, iwake_critical_threshold);
+		set_iwake_ltrdid(isys, ltr, did, LTR_IWAKE_ON);
+		mutex_lock(&iwake_watermark->mutex);
+		set_iwake_register(isys,
+				   GDA_IWAKE_THRESHOLD_INDEX, iwake_threshold);
+
+		set_iwake_register(isys,
+				   GDA_IRQ_CRITICAL_THRESHOLD_INDEX,
+				   iwake_critical_threshold);
+		mutex_unlock(&iwake_watermark->mutex);
+
+		writel(VAL_PKGC_PMON_CFG_RESET,
+		       isys->adev->isp->base + REG_PKGC_PMON_CFG);
+		writel(VAL_PKGC_PMON_CFG_START,
+		       isys->adev->isp->base + REG_PKGC_PMON_CFG);
+	}
+}
+
+static int isys_iwake_watermark_init(struct ipu_isys *isys)
+{
+	struct isys_iwake_watermark *iwake_watermark;
+
+	if (isys->iwake_watermark)
+		return 0;
+
+	iwake_watermark = devm_kzalloc(&isys->adev->dev,
+				       sizeof(*iwake_watermark), GFP_KERNEL);
+	if (!iwake_watermark)
+		return -ENOMEM;
+	INIT_LIST_HEAD(&iwake_watermark->video_list);
+	mutex_init(&iwake_watermark->mutex);
+
+	iwake_watermark->ltrdid.lut_ltr.value = 0;
+	isys->iwake_watermark = iwake_watermark;
+	iwake_watermark->isys = isys;
+	iwake_watermark->iwake_enabled = false;
+	iwake_watermark->force_iwake_disable = false;
+	return 0;
+}
+
+static int isys_iwake_watermark_cleanup(struct ipu_isys *isys)
+{
+	struct isys_iwake_watermark *iwake_watermark = isys->iwake_watermark;
+
+	if (!iwake_watermark)
+		return -EINVAL;
+	mutex_lock(&iwake_watermark->mutex);
+	list_del(&iwake_watermark->video_list);
+	mutex_unlock(&iwake_watermark->mutex);
+	mutex_destroy(&iwake_watermark->mutex);
+	isys->iwake_watermark = NULL;
+	return 0;
+}
+
+/* The .bound() notifier callback when a match is found */
+static int isys_notifier_bound(struct v4l2_async_notifier *notifier,
+			       struct v4l2_subdev *sd,
+			       struct v4l2_async_subdev *asd)
+{
+	struct ipu_isys *isys = container_of(notifier,
+					struct ipu_isys, notifier);
+	struct sensor_async_subdev *s_asd = container_of(asd,
+					struct sensor_async_subdev, asd);
+
+	dev_info(&isys->adev->dev, "bind %s nlanes is %d port is %d\n",
+		 sd->name, s_asd->csi2.nlanes, s_asd->csi2.port);
+	isys_complete_ext_device_registration(isys, sd, &s_asd->csi2);
+
+	return v4l2_device_register_subdev_nodes(&isys->v4l2_dev);
+}
+
+static void isys_notifier_unbind(struct v4l2_async_notifier *notifier,
+				 struct v4l2_subdev *sd,
+				 struct v4l2_async_subdev *asd)
+{
+	struct ipu_isys *isys = container_of(notifier,
+					struct ipu_isys, notifier);
+
+	dev_info(&isys->adev->dev, "unbind %s\n", sd->name);
+}
+
+static int isys_notifier_complete(struct v4l2_async_notifier *notifier)
+{
+	struct ipu_isys *isys = container_of(notifier,
+					struct ipu_isys, notifier);
+
+	dev_info(&isys->adev->dev, "All sensor registration completed.\n");
+
+	return v4l2_device_register_subdev_nodes(&isys->v4l2_dev);
+}
+
+static const struct v4l2_async_notifier_operations isys_async_ops = {
+	.bound = isys_notifier_bound,
+	.unbind = isys_notifier_unbind,
+	.complete = isys_notifier_complete,
+};
+
+static int isys_fwnode_parse(struct device *dev,
+			     struct v4l2_fwnode_endpoint *vep,
+			     struct v4l2_async_subdev *asd)
+{
+	struct sensor_async_subdev *s_asd =
+			container_of(asd, struct sensor_async_subdev, asd);
+
+	s_asd->csi2.port = vep->base.port;
+	s_asd->csi2.nlanes = vep->bus.mipi_csi2.num_data_lanes;
+
+	return 0;
+}
+
+static int isys_notifier_init(struct ipu_isys *isys)
+{
+	struct ipu_device *isp = isys->adev->isp;
+	size_t asd_struct_size = sizeof(struct sensor_async_subdev);
+	int ret;
+
+	v4l2_async_notifier_init(&isys->notifier);
+	ret = v4l2_async_notifier_parse_fwnode_endpoints(&isp->pdev->dev,
+							 &isys->notifier,
+							 asd_struct_size,
+							 isys_fwnode_parse);
+
+	if (ret < 0) {
+		dev_err(&isys->adev->dev,
+			"v4l2 parse_fwnode_endpoints() failed: %d\n", ret);
+		return ret;
+	}
+
+	if (list_empty(&isys->notifier.asd_list)) {
+		/* isys probe could continue with async subdevs missing */
+		dev_warn(&isys->adev->dev, "no subdev found in graph\n");
+		return 0;
+	}
+
+	isys->notifier.ops = &isys_async_ops;
+	ret = v4l2_async_notifier_register(&isys->v4l2_dev, &isys->notifier);
+	if (ret) {
+		dev_err(&isys->adev->dev,
+			"failed to register async notifier : %d\n", ret);
+		v4l2_async_notifier_cleanup(&isys->notifier);
+	}
+
+	return ret;
+}
+
+static void isys_notifier_cleanup(struct ipu_isys *isys)
+{
+	v4l2_async_notifier_unregister(&isys->notifier);
+	v4l2_async_notifier_cleanup(&isys->notifier);
+}
+
+static struct media_device_ops isys_mdev_ops = {
+	.link_notify = v4l2_pipeline_link_notify,
+};
+
+static int isys_register_devices(struct ipu_isys *isys)
+{
+	int rval;
+
+	isys->media_dev.dev = &isys->adev->dev;
+	isys->media_dev.ops = &isys_mdev_ops;
+	strlcpy(isys->media_dev.model,
+		IPU_MEDIA_DEV_MODEL_NAME, sizeof(isys->media_dev.model));
+	snprintf(isys->media_dev.bus_info, sizeof(isys->media_dev.bus_info),
+		 "pci:%s", dev_name(isys->adev->dev.parent->parent));
+	strlcpy(isys->v4l2_dev.name, isys->media_dev.model,
+		sizeof(isys->v4l2_dev.name));
+
+	media_device_init(&isys->media_dev);
+
+	rval = media_device_register(&isys->media_dev);
+	if (rval < 0) {
+		dev_info(&isys->adev->dev, "can't register media device\n");
+		goto out_media_device_unregister;
+	}
+
+	isys->v4l2_dev.mdev = &isys->media_dev;
+
+	rval = v4l2_device_register(&isys->adev->dev, &isys->v4l2_dev);
+	if (rval < 0) {
+		dev_info(&isys->adev->dev, "can't register v4l2 device\n");
+		goto out_media_device_unregister;
+	}
+
+	rval = isys_register_subdevices(isys);
+	if (rval)
+		goto out_v4l2_device_unregister;
+
+	rval = isys_notifier_init(isys);
+	if (rval)
+		goto out_isys_unregister_subdevices;
+
+	rval = v4l2_device_register_subdev_nodes(&isys->v4l2_dev);
+	if (rval)
+		goto out_isys_notifier_cleanup;
+
+	return 0;
+
+out_isys_notifier_cleanup:
+	isys_notifier_cleanup(isys);
+
+out_isys_unregister_subdevices:
+	isys_unregister_subdevices(isys);
+
+out_v4l2_device_unregister:
+	v4l2_device_unregister(&isys->v4l2_dev);
+
+out_media_device_unregister:
+	media_device_unregister(&isys->media_dev);
+	media_device_cleanup(&isys->media_dev);
+
+	return rval;
+}
+
+static void isys_unregister_devices(struct ipu_isys *isys)
+{
+	isys_unregister_subdevices(isys);
+	v4l2_device_unregister(&isys->v4l2_dev);
+	media_device_unregister(&isys->media_dev);
+	media_device_cleanup(&isys->media_dev);
+}
+
+#ifdef CONFIG_PM
+static int isys_runtime_pm_resume(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	struct ipu_device *isp = adev->isp;
+	struct ipu_isys *isys = ipu_bus_get_drvdata(adev);
+	unsigned long flags;
+	int ret;
+
+	if (!isys)
+		return 0;
+
+	ret = ipu_mmu_hw_init(adev->mmu);
+	if (ret)
+		return ret;
+
+	ipu_trace_restore(dev);
+
+	cpu_latency_qos_update_request(&isys->pm_qos, ISYS_PM_QOS_VALUE);
+
+	ret = ipu_buttress_start_tsc_sync(isp);
+	if (ret)
+		return ret;
+
+	spin_lock_irqsave(&isys->power_lock, flags);
+	isys->power = 1;
+	spin_unlock_irqrestore(&isys->power_lock, flags);
+
+	if (isys->short_packet_source == IPU_ISYS_SHORT_PACKET_FROM_TUNIT) {
+		mutex_lock(&isys->short_packet_tracing_mutex);
+		isys->short_packet_tracing_count = 0;
+		mutex_unlock(&isys->short_packet_tracing_mutex);
+	}
+	isys_setup_hw(isys);
+
+	set_iwake_ltrdid(isys, 0, 0, LTR_ISYS_ON);
+	return 0;
+}
+
+static int isys_runtime_pm_suspend(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	struct ipu_isys *isys = ipu_bus_get_drvdata(adev);
+	unsigned long flags;
+
+	if (!isys)
+		return 0;
+
+	spin_lock_irqsave(&isys->power_lock, flags);
+	isys->power = 0;
+	spin_unlock_irqrestore(&isys->power_lock, flags);
+
+	ipu_trace_stop(dev);
+	mutex_lock(&isys->mutex);
+	isys->reset_needed = false;
+	mutex_unlock(&isys->mutex);
+
+	cpu_latency_qos_update_request(&isys->pm_qos, PM_QOS_DEFAULT_VALUE);
+
+	ipu_mmu_hw_cleanup(adev->mmu);
+
+	set_iwake_ltrdid(isys, 0, 0, LTR_ISYS_OFF);
+	return 0;
+}
+
+static int isys_suspend(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	struct ipu_isys *isys = ipu_bus_get_drvdata(adev);
+
+	/* If stream is open, refuse to suspend */
+	if (isys->stream_opened)
+		return -EBUSY;
+
+	return 0;
+}
+
+static int isys_resume(struct device *dev)
+{
+	return 0;
+}
+
+static const struct dev_pm_ops isys_pm_ops = {
+	.runtime_suspend = isys_runtime_pm_suspend,
+	.runtime_resume = isys_runtime_pm_resume,
+	.suspend = isys_suspend,
+	.resume = isys_resume,
+};
+
+#define ISYS_PM_OPS (&isys_pm_ops)
+#else
+#define ISYS_PM_OPS NULL
+#endif
+
+static void isys_remove(struct ipu_bus_device *adev)
+{
+	struct ipu_isys *isys = ipu_bus_get_drvdata(adev);
+	struct ipu_device *isp = adev->isp;
+	struct isys_fw_msgs *fwmsg, *safe;
+
+	dev_info(&adev->dev, "removed\n");
+#ifdef CONFIG_DEBUG_FS
+	if (isp->ipu_dir)
+		debugfs_remove_recursive(isys->debugfsdir);
+#endif
+
+	list_for_each_entry_safe(fwmsg, safe, &isys->framebuflist, head) {
+		dma_free_attrs(&adev->dev, sizeof(struct isys_fw_msgs),
+			       fwmsg, fwmsg->dma_addr,
+			       0);
+	}
+
+	list_for_each_entry_safe(fwmsg, safe, &isys->framebuflist_fw, head) {
+		dma_free_attrs(&adev->dev, sizeof(struct isys_fw_msgs),
+			       fwmsg, fwmsg->dma_addr,
+			       0
+		    );
+	}
+
+	isys_iwake_watermark_cleanup(isys);
+
+	ipu_trace_uninit(&adev->dev);
+	isys_notifier_cleanup(isys);
+	isys_unregister_devices(isys);
+
+	cpu_latency_qos_remove_request(&isys->pm_qos);
+
+	if (!isp->secure_mode) {
+		ipu_cpd_free_pkg_dir(adev, isys->pkg_dir,
+				     isys->pkg_dir_dma_addr,
+				     isys->pkg_dir_size);
+		ipu_buttress_unmap_fw_image(adev, &isys->fw_sgt);
+		release_firmware(isys->fw);
+	}
+
+	mutex_destroy(&isys->stream_mutex);
+	mutex_destroy(&isys->mutex);
+
+	if (isys->short_packet_source == IPU_ISYS_SHORT_PACKET_FROM_TUNIT) {
+		u32 trace_size = IPU_ISYS_SHORT_PACKET_TRACE_BUFFER_SIZE;
+
+		dma_free_coherent(&adev->dev, trace_size,
+				  isys->short_packet_trace_buffer,
+				  isys->short_packet_trace_buffer_dma_addr);
+	}
+}
+
+#ifdef CONFIG_DEBUG_FS
+static int ipu_isys_icache_prefetch_get(void *data, u64 *val)
+{
+	struct ipu_isys *isys = data;
+
+	*val = isys->icache_prefetch;
+	return 0;
+}
+
+static int ipu_isys_icache_prefetch_set(void *data, u64 val)
+{
+	struct ipu_isys *isys = data;
+
+	if (val != !!val)
+		return -EINVAL;
+
+	isys->icache_prefetch = val;
+
+	return 0;
+}
+
+static int isys_iwake_control_get(void *data, u64 *val)
+{
+	struct ipu_isys *isys = data;
+	struct isys_iwake_watermark *iwake_watermark = isys->iwake_watermark;
+
+	mutex_lock(&iwake_watermark->mutex);
+	*val = isys->iwake_watermark->force_iwake_disable;
+	mutex_unlock(&iwake_watermark->mutex);
+	return 0;
+}
+
+static int isys_iwake_control_set(void *data, u64 val)
+{
+	struct ipu_isys *isys = data;
+	struct isys_iwake_watermark *iwake_watermark;
+
+	if (val != !!val)
+		return -EINVAL;
+	/* If stream is open, refuse to set iwake */
+	if (isys->stream_opened)
+		return -EBUSY;
+
+	iwake_watermark = isys->iwake_watermark;
+	mutex_lock(&iwake_watermark->mutex);
+	isys->iwake_watermark->force_iwake_disable = !!val;
+	mutex_unlock(&iwake_watermark->mutex);
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(isys_icache_prefetch_fops,
+			ipu_isys_icache_prefetch_get,
+			ipu_isys_icache_prefetch_set, "%llu\n");
+
+DEFINE_SIMPLE_ATTRIBUTE(isys_iwake_control_fops,
+			isys_iwake_control_get,
+			isys_iwake_control_set, "%llu\n");
+
+static int ipu_isys_init_debugfs(struct ipu_isys *isys)
+{
+	struct dentry *file;
+	struct dentry *dir;
+#ifdef IPU_ISYS_GPC
+	int ret;
+#endif
+
+	dir = debugfs_create_dir("isys", isys->adev->isp->ipu_dir);
+	if (IS_ERR(dir))
+		return -ENOMEM;
+
+	file = debugfs_create_file("icache_prefetch", 0600,
+				   dir, isys, &isys_icache_prefetch_fops);
+	if (IS_ERR(file))
+		goto err;
+
+	file = debugfs_create_file("iwake_disable", 0600,
+				   dir, isys, &isys_iwake_control_fops);
+	if (IS_ERR(file))
+		goto err;
+
+	isys->debugfsdir = dir;
+
+#ifdef IPU_ISYS_GPC
+	ret = ipu_isys_gpc_init_debugfs(isys);
+	if (ret)
+		return ret;
+#endif
+
+	return 0;
+err:
+	debugfs_remove_recursive(dir);
+	return -ENOMEM;
+}
+#endif
+
+static int alloc_fw_msg_bufs(struct ipu_isys *isys, int amount)
+{
+	dma_addr_t dma_addr;
+	struct isys_fw_msgs *addr;
+	unsigned int i;
+	unsigned long flags;
+
+	for (i = 0; i < amount; i++) {
+		addr = dma_alloc_attrs(&isys->adev->dev,
+				       sizeof(struct isys_fw_msgs),
+				       &dma_addr, GFP_KERNEL,
+				       0);
+		if (!addr)
+			break;
+		addr->dma_addr = dma_addr;
+
+		spin_lock_irqsave(&isys->listlock, flags);
+		list_add(&addr->head, &isys->framebuflist);
+		spin_unlock_irqrestore(&isys->listlock, flags);
+	}
+	if (i == amount)
+		return 0;
+	spin_lock_irqsave(&isys->listlock, flags);
+	while (!list_empty(&isys->framebuflist)) {
+		addr = list_first_entry(&isys->framebuflist,
+					struct isys_fw_msgs, head);
+		list_del(&addr->head);
+		spin_unlock_irqrestore(&isys->listlock, flags);
+		dma_free_attrs(&isys->adev->dev,
+			       sizeof(struct isys_fw_msgs),
+			       addr, addr->dma_addr,
+			       0);
+		spin_lock_irqsave(&isys->listlock, flags);
+	}
+	spin_unlock_irqrestore(&isys->listlock, flags);
+	return -ENOMEM;
+}
+
+struct isys_fw_msgs *ipu_get_fw_msg_buf(struct ipu_isys_pipeline *ip)
+{
+	struct ipu_isys_video *pipe_av =
+	    container_of(ip, struct ipu_isys_video, ip);
+	struct ipu_isys *isys;
+	struct isys_fw_msgs *msg;
+	unsigned long flags;
+
+	isys = pipe_av->isys;
+
+	spin_lock_irqsave(&isys->listlock, flags);
+	if (list_empty(&isys->framebuflist)) {
+		spin_unlock_irqrestore(&isys->listlock, flags);
+		dev_dbg(&isys->adev->dev, "Frame list empty - Allocate more");
+
+		alloc_fw_msg_bufs(isys, 5);
+
+		spin_lock_irqsave(&isys->listlock, flags);
+		if (list_empty(&isys->framebuflist)) {
+			spin_unlock_irqrestore(&isys->listlock, flags);
+			dev_err(&isys->adev->dev, "Frame list empty");
+			return NULL;
+		}
+	}
+	msg = list_last_entry(&isys->framebuflist, struct isys_fw_msgs, head);
+	list_move(&msg->head, &isys->framebuflist_fw);
+	spin_unlock_irqrestore(&isys->listlock, flags);
+	memset(&msg->fw_msg, 0, sizeof(msg->fw_msg));
+
+	return msg;
+}
+
+void ipu_cleanup_fw_msg_bufs(struct ipu_isys *isys)
+{
+	struct isys_fw_msgs *fwmsg, *fwmsg0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&isys->listlock, flags);
+	list_for_each_entry_safe(fwmsg, fwmsg0, &isys->framebuflist_fw, head)
+		list_move(&fwmsg->head, &isys->framebuflist);
+	spin_unlock_irqrestore(&isys->listlock, flags);
+}
+
+void ipu_put_fw_mgs_buf(struct ipu_isys *isys, u64 data)
+{
+	struct isys_fw_msgs *msg;
+	unsigned long flags;
+	u64 *ptr = (u64 *)(unsigned long)data;
+
+	if (!ptr)
+		return;
+
+	spin_lock_irqsave(&isys->listlock, flags);
+	msg = container_of(ptr, struct isys_fw_msgs, fw_msg.dummy);
+	list_move(&msg->head, &isys->framebuflist);
+	spin_unlock_irqrestore(&isys->listlock, flags);
+}
+
+static int isys_probe(struct ipu_bus_device *adev)
+{
+	struct ipu_isys *isys;
+	struct ipu_device *isp = adev->isp;
+	const struct firmware *fw;
+	int rval = 0;
+
+	isys = devm_kzalloc(&adev->dev, sizeof(*isys), GFP_KERNEL);
+	if (!isys)
+		return -ENOMEM;
+
+	rval = ipu_mmu_hw_init(adev->mmu);
+	if (rval)
+		return rval;
+
+	/* By default, short packet is captured from T-Unit. */
+	isys->short_packet_source = IPU_ISYS_SHORT_PACKET_FROM_RECEIVER;
+	isys->adev = adev;
+	isys->pdata = adev->pdata;
+
+	/* initial streamID for different sensor types */
+	if (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) {
+		isys->sensor_info.vc1_data_start =
+			IPU6_FW_ISYS_VC1_SENSOR_DATA_START;
+		isys->sensor_info.vc1_data_end =
+			IPU6_FW_ISYS_VC1_SENSOR_DATA_END;
+		isys->sensor_info.vc0_data_start =
+			IPU6_FW_ISYS_VC0_SENSOR_DATA_START;
+		isys->sensor_info.vc0_data_end =
+			IPU6_FW_ISYS_VC0_SENSOR_DATA_END;
+		isys->sensor_info.vc1_pdaf_start =
+			IPU6_FW_ISYS_VC1_SENSOR_PDAF_START;
+		isys->sensor_info.vc1_pdaf_end =
+			IPU6_FW_ISYS_VC1_SENSOR_PDAF_END;
+		isys->sensor_info.sensor_metadata =
+			IPU6_FW_ISYS_SENSOR_METADATA;
+
+		isys->sensor_types[IPU_FW_ISYS_VC1_SENSOR_DATA] =
+			IPU6_FW_ISYS_VC1_SENSOR_DATA_START;
+		isys->sensor_types[IPU_FW_ISYS_VC1_SENSOR_PDAF] =
+			IPU6_FW_ISYS_VC1_SENSOR_PDAF_START;
+		isys->sensor_types[IPU_FW_ISYS_VC0_SENSOR_DATA] =
+			IPU6_FW_ISYS_VC0_SENSOR_DATA_START;
+	} else if (ipu_ver == IPU_VER_6SE) {
+		isys->sensor_info.vc1_data_start =
+			IPU6SE_FW_ISYS_VC1_SENSOR_DATA_START;
+		isys->sensor_info.vc1_data_end =
+			IPU6SE_FW_ISYS_VC1_SENSOR_DATA_END;
+		isys->sensor_info.vc0_data_start =
+			IPU6SE_FW_ISYS_VC0_SENSOR_DATA_START;
+		isys->sensor_info.vc0_data_end =
+			IPU6SE_FW_ISYS_VC0_SENSOR_DATA_END;
+		isys->sensor_info.vc1_pdaf_start =
+			IPU6SE_FW_ISYS_VC1_SENSOR_PDAF_START;
+		isys->sensor_info.vc1_pdaf_end =
+			IPU6SE_FW_ISYS_VC1_SENSOR_PDAF_END;
+		isys->sensor_info.sensor_metadata =
+			IPU6SE_FW_ISYS_SENSOR_METADATA;
+
+		isys->sensor_types[IPU_FW_ISYS_VC1_SENSOR_DATA] =
+			IPU6SE_FW_ISYS_VC1_SENSOR_DATA_START;
+		isys->sensor_types[IPU_FW_ISYS_VC1_SENSOR_PDAF] =
+			IPU6SE_FW_ISYS_VC1_SENSOR_PDAF_START;
+		isys->sensor_types[IPU_FW_ISYS_VC0_SENSOR_DATA] =
+			IPU6SE_FW_ISYS_VC0_SENSOR_DATA_START;
+	}
+
+	INIT_LIST_HEAD(&isys->requests);
+
+	spin_lock_init(&isys->lock);
+	spin_lock_init(&isys->power_lock);
+	isys->power = 0;
+
+	mutex_init(&isys->mutex);
+	mutex_init(&isys->stream_mutex);
+	mutex_init(&isys->lib_mutex);
+
+	spin_lock_init(&isys->listlock);
+	INIT_LIST_HEAD(&isys->framebuflist);
+	INIT_LIST_HEAD(&isys->framebuflist_fw);
+
+	dev_dbg(&adev->dev, "isys probe %p %p\n", adev, &adev->dev);
+	ipu_bus_set_drvdata(adev, isys);
+
+	isys->line_align = IPU_ISYS_2600_MEM_LINE_ALIGN;
+	isys->icache_prefetch = 0;
+
+#ifndef CONFIG_PM
+	isys_setup_hw(isys);
+#endif
+
+	if (!isp->secure_mode) {
+		fw = isp->cpd_fw;
+		rval = ipu_buttress_map_fw_image(adev, fw, &isys->fw_sgt);
+		if (rval)
+			goto release_firmware;
+
+		isys->pkg_dir =
+		    ipu_cpd_create_pkg_dir(adev, isp->cpd_fw->data,
+					   sg_dma_address(isys->fw_sgt.sgl),
+					   &isys->pkg_dir_dma_addr,
+					   &isys->pkg_dir_size);
+		if (!isys->pkg_dir) {
+			rval = -ENOMEM;
+			goto remove_shared_buffer;
+		}
+	}
+
+#ifdef CONFIG_DEBUG_FS
+	/* Debug fs failure is not fatal. */
+	ipu_isys_init_debugfs(isys);
+#endif
+
+	ipu_trace_init(adev->isp, isys->pdata->base, &adev->dev,
+		       isys_trace_blocks);
+
+	cpu_latency_qos_add_request(&isys->pm_qos, PM_QOS_DEFAULT_VALUE);
+	alloc_fw_msg_bufs(isys, 20);
+
+	rval = isys_register_devices(isys);
+	if (rval)
+		goto out_remove_pkg_dir_shared_buffer;
+	rval = isys_iwake_watermark_init(isys);
+	if (rval)
+		goto out_unregister_devices;
+
+	ipu_mmu_hw_cleanup(adev->mmu);
+
+	return 0;
+
+out_unregister_devices:
+	isys_iwake_watermark_cleanup(isys);
+	isys_unregister_devices(isys);
+out_remove_pkg_dir_shared_buffer:
+	if (!isp->secure_mode)
+		ipu_cpd_free_pkg_dir(adev, isys->pkg_dir,
+				     isys->pkg_dir_dma_addr,
+				     isys->pkg_dir_size);
+remove_shared_buffer:
+	if (!isp->secure_mode)
+		ipu_buttress_unmap_fw_image(adev, &isys->fw_sgt);
+release_firmware:
+	if (!isp->secure_mode)
+		release_firmware(isys->fw);
+	ipu_trace_uninit(&adev->dev);
+
+	mutex_destroy(&isys->mutex);
+	mutex_destroy(&isys->stream_mutex);
+
+	if (isys->short_packet_source == IPU_ISYS_SHORT_PACKET_FROM_TUNIT)
+		mutex_destroy(&isys->short_packet_tracing_mutex);
+
+	ipu_mmu_hw_cleanup(adev->mmu);
+
+	return rval;
+}
+
+struct fwmsg {
+	int type;
+	char *msg;
+	bool valid_ts;
+};
+
+static const struct fwmsg fw_msg[] = {
+	{IPU_FW_ISYS_RESP_TYPE_STREAM_OPEN_DONE, "STREAM_OPEN_DONE", 0},
+	{IPU_FW_ISYS_RESP_TYPE_STREAM_CLOSE_ACK, "STREAM_CLOSE_ACK", 0},
+	{IPU_FW_ISYS_RESP_TYPE_STREAM_START_ACK, "STREAM_START_ACK", 0},
+	{IPU_FW_ISYS_RESP_TYPE_STREAM_START_AND_CAPTURE_ACK,
+	 "STREAM_START_AND_CAPTURE_ACK", 0},
+	{IPU_FW_ISYS_RESP_TYPE_STREAM_STOP_ACK, "STREAM_STOP_ACK", 0},
+	{IPU_FW_ISYS_RESP_TYPE_STREAM_FLUSH_ACK, "STREAM_FLUSH_ACK", 0},
+	{IPU_FW_ISYS_RESP_TYPE_PIN_DATA_READY, "PIN_DATA_READY", 1},
+	{IPU_FW_ISYS_RESP_TYPE_STREAM_CAPTURE_ACK, "STREAM_CAPTURE_ACK", 0},
+	{IPU_FW_ISYS_RESP_TYPE_STREAM_START_AND_CAPTURE_DONE,
+	 "STREAM_START_AND_CAPTURE_DONE", 1},
+	{IPU_FW_ISYS_RESP_TYPE_STREAM_CAPTURE_DONE, "STREAM_CAPTURE_DONE", 1},
+	{IPU_FW_ISYS_RESP_TYPE_FRAME_SOF, "FRAME_SOF", 1},
+	{IPU_FW_ISYS_RESP_TYPE_FRAME_EOF, "FRAME_EOF", 1},
+	{IPU_FW_ISYS_RESP_TYPE_STATS_DATA_READY, "STATS_READY", 1},
+	{-1, "UNKNOWN MESSAGE", 0},
+};
+
+static int resp_type_to_index(int type)
+{
+	unsigned int i;
+
+	for (i = 0; i < ARRAY_SIZE(fw_msg); i++)
+		if (fw_msg[i].type == type)
+			return i;
+
+	return i - 1;
+}
+
+int isys_isr_one(struct ipu_bus_device *adev)
+{
+	struct ipu_isys *isys = ipu_bus_get_drvdata(adev);
+	struct ipu_fw_isys_resp_info_abi resp_data;
+	struct ipu_fw_isys_resp_info_abi *resp;
+	struct ipu_isys_pipeline *pipe;
+	u64 ts;
+	unsigned int i;
+
+	if (!isys->fwcom)
+		return 0;
+
+	resp = ipu_fw_isys_get_resp(isys->fwcom, IPU_BASE_MSG_RECV_QUEUES,
+				    &resp_data);
+	if (!resp)
+		return 1;
+
+	ts = (u64)resp->timestamp[1] << 32 | resp->timestamp[0];
+
+	if (resp->error_info.error == IPU_FW_ISYS_ERROR_STREAM_IN_SUSPENSION)
+		/* Suspension is kind of special case: not enough buffers */
+		dev_dbg(&adev->dev,
+			"hostlib: error resp %02d %s, stream %u, error SUSPENSION, details %d, timestamp 0x%16.16llx, pin %d\n",
+			resp->type,
+			fw_msg[resp_type_to_index(resp->type)].msg,
+			resp->stream_handle,
+			resp->error_info.error_details,
+			fw_msg[resp_type_to_index(resp->type)].valid_ts ?
+			ts : 0, resp->pin_id);
+	else if (resp->error_info.error)
+		dev_dbg(&adev->dev,
+			"hostlib: error resp %02d %s, stream %u, error %d, details %d, timestamp 0x%16.16llx, pin %d\n",
+			resp->type,
+			fw_msg[resp_type_to_index(resp->type)].msg,
+			resp->stream_handle,
+			resp->error_info.error, resp->error_info.error_details,
+			fw_msg[resp_type_to_index(resp->type)].valid_ts ?
+			ts : 0, resp->pin_id);
+	else
+		dev_dbg(&adev->dev,
+			"hostlib: resp %02d %s, stream %u, timestamp 0x%16.16llx, pin %d\n",
+			resp->type,
+			fw_msg[resp_type_to_index(resp->type)].msg,
+			resp->stream_handle,
+			fw_msg[resp_type_to_index(resp->type)].valid_ts ?
+			ts : 0, resp->pin_id);
+
+	if (resp->stream_handle >= IPU_ISYS_MAX_STREAMS) {
+		dev_err(&adev->dev, "bad stream handle %u\n",
+			resp->stream_handle);
+		goto leave;
+	}
+
+	pipe = isys->pipes[resp->stream_handle];
+	if (!pipe) {
+		dev_err(&adev->dev, "no pipeline for stream %u\n",
+			resp->stream_handle);
+		goto leave;
+	}
+	pipe->error = resp->error_info.error;
+
+	switch (resp->type) {
+	case IPU_FW_ISYS_RESP_TYPE_STREAM_OPEN_DONE:
+		ipu_put_fw_mgs_buf(ipu_bus_get_drvdata(adev), resp->buf_id);
+		complete(&pipe->stream_open_completion);
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_STREAM_CLOSE_ACK:
+		complete(&pipe->stream_close_completion);
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_STREAM_START_ACK:
+		complete(&pipe->stream_start_completion);
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_STREAM_START_AND_CAPTURE_ACK:
+		ipu_put_fw_mgs_buf(ipu_bus_get_drvdata(adev), resp->buf_id);
+		complete(&pipe->stream_start_completion);
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_STREAM_STOP_ACK:
+		complete(&pipe->stream_stop_completion);
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_STREAM_FLUSH_ACK:
+		complete(&pipe->stream_stop_completion);
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_PIN_DATA_READY:
+		if (resp->pin_id < IPU_ISYS_OUTPUT_PINS &&
+		    pipe->output_pins[resp->pin_id].pin_ready)
+			pipe->output_pins[resp->pin_id].pin_ready(pipe, resp);
+		else
+			dev_err(&adev->dev,
+				"%d:No data pin ready handler for pin id %d\n",
+				resp->stream_handle, resp->pin_id);
+		if (pipe->csi2)
+			ipu_isys_csi2_error(pipe->csi2);
+
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_STREAM_CAPTURE_ACK:
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_STREAM_START_AND_CAPTURE_DONE:
+	case IPU_FW_ISYS_RESP_TYPE_STREAM_CAPTURE_DONE:
+		if (pipe->interlaced) {
+			struct ipu_isys_buffer *ib, *ib_safe;
+			struct list_head list;
+			unsigned long flags;
+			unsigned int *ts = resp->timestamp;
+
+			if (pipe->isys->short_packet_source ==
+			    IPU_ISYS_SHORT_PACKET_FROM_TUNIT)
+				pipe->cur_field =
+				    ipu_isys_csi2_get_current_field(pipe, ts);
+
+			/*
+			 * Move the pending buffers to a local temp list.
+			 * Then we do not need to handle the lock during
+			 * the loop.
+			 */
+			spin_lock_irqsave(&pipe->short_packet_queue_lock,
+					  flags);
+			list_cut_position(&list,
+					  &pipe->pending_interlaced_bufs,
+					  pipe->pending_interlaced_bufs.prev);
+			spin_unlock_irqrestore(&pipe->short_packet_queue_lock,
+					       flags);
+
+			list_for_each_entry_safe(ib, ib_safe, &list, head) {
+				struct vb2_buffer *vb;
+
+				vb = ipu_isys_buffer_to_vb2_buffer(ib);
+				to_vb2_v4l2_buffer(vb)->field = pipe->cur_field;
+				list_del(&ib->head);
+
+				ipu_isys_queue_buf_done(ib);
+			}
+		}
+		for (i = 0; i < IPU_NUM_CAPTURE_DONE; i++)
+			if (pipe->capture_done[i])
+				pipe->capture_done[i] (pipe, resp);
+
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_FRAME_SOF:
+		if (pipe->csi2)
+			ipu_isys_csi2_sof_event(pipe->csi2);
+
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+#ifdef IPU_TPG_FRAME_SYNC
+		if (pipe->tpg)
+			ipu_isys_tpg_sof_event(pipe->tpg);
+#endif
+#endif
+		pipe->seq[pipe->seq_index].sequence =
+		    atomic_read(&pipe->sequence) - 1;
+		pipe->seq[pipe->seq_index].timestamp = ts;
+		dev_dbg(&adev->dev,
+			"sof: handle %d: (index %u), timestamp 0x%16.16llx\n",
+			resp->stream_handle,
+			pipe->seq[pipe->seq_index].sequence, ts);
+		pipe->seq_index = (pipe->seq_index + 1)
+		    % IPU_ISYS_MAX_PARALLEL_SOF;
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_FRAME_EOF:
+		if (pipe->csi2)
+			ipu_isys_csi2_eof_event(pipe->csi2);
+
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+#ifdef IPU_TPG_FRAME_SYNC
+		if (pipe->tpg)
+			ipu_isys_tpg_eof_event(pipe->tpg);
+#endif
+#endif
+
+		dev_dbg(&adev->dev,
+			"eof: handle %d: (index %u), timestamp 0x%16.16llx\n",
+			resp->stream_handle,
+			pipe->seq[pipe->seq_index].sequence, ts);
+		break;
+	case IPU_FW_ISYS_RESP_TYPE_STATS_DATA_READY:
+		break;
+	default:
+		dev_err(&adev->dev, "%d:unknown response type %u\n",
+			resp->stream_handle, resp->type);
+		break;
+	}
+
+leave:
+	ipu_fw_isys_put_resp(isys->fwcom, IPU_BASE_MSG_RECV_QUEUES);
+	return 0;
+}
+
+static struct ipu_bus_driver isys_driver = {
+	.probe = isys_probe,
+	.remove = isys_remove,
+	.isr = isys_isr,
+	.wanted = IPU_ISYS_NAME,
+	.drv = {
+		.name = IPU_ISYS_NAME,
+		.owner = THIS_MODULE,
+		.pm = ISYS_PM_OPS,
+	},
+};
+
+module_ipu_bus_driver(isys_driver);
+
+static const struct pci_device_id ipu_pci_tbl[] = {
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, IPU6_PCI_ID)},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, IPU6SE_PCI_ID)},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, IPU6EP_PCI_ID)},
+	{0,}
+};
+MODULE_DEVICE_TABLE(pci, ipu_pci_tbl);
+
+MODULE_AUTHOR("Sakari Ailus <sakari.ailus@linux.intel.com>");
+MODULE_AUTHOR("Samu Onkalo <samu.onkalo@intel.com>");
+MODULE_AUTHOR("Jouni Hgander <jouni.hogander@intel.com>");
+MODULE_AUTHOR("Jouni Ukkonen <jouni.ukkonen@intel.com>");
+MODULE_AUTHOR("Jianxu Zheng <jian.xu.zheng@intel.com>");
+MODULE_AUTHOR("Tianshu Qiu <tian.shu.qiu@intel.com>");
+MODULE_AUTHOR("Renwei Wu <renwei.wu@intel.com>");
+MODULE_AUTHOR("Bingbu Cao <bingbu.cao@intel.com>");
+MODULE_AUTHOR("Yunliang Ding <yunliang.ding@intel.com>");
+MODULE_AUTHOR("Zaikuo Wang <zaikuo.wang@intel.com>");
+MODULE_AUTHOR("Leifu Zhao <leifu.zhao@intel.com>");
+MODULE_AUTHOR("Xia Wu <xia.wu@intel.com>");
+MODULE_AUTHOR("Kun Jiang <kun.jiang@intel.com>");
+MODULE_AUTHOR("Yu Xia <yu.y.xia@intel.com>");
+MODULE_AUTHOR("Jerry Hu <jerry.w.hu@intel.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Intel ipu input system driver");
diff -ruN a/drivers/media/pci/intel/ipu-isys-csi2-be.c b/drivers/media/pci/intel/ipu-isys-csi2-be.c
--- a/drivers/media/pci/intel/ipu-isys-csi2-be.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-csi2-be.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,325 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2014 - 2020 Intel Corporation
+
+#include <linux/device.h>
+#include <linux/module.h>
+
+#include <media/ipu-isys.h>
+#include <media/media-entity.h>
+#include <media/v4l2-device.h>
+
+#include "ipu.h"
+#include "ipu-bus.h"
+#include "ipu-isys.h"
+#include "ipu-isys-csi2-be.h"
+#include "ipu-isys-subdev.h"
+#include "ipu-isys-video.h"
+
+/*
+ * Raw bayer format pixel order MUST BE MAINTAINED in groups of four codes.
+ * Otherwise pixel order calculation below WILL BREAK!
+ */
+static const u32 csi2_be_supported_codes_pad[] = {
+	MEDIA_BUS_FMT_SBGGR12_1X12,
+	MEDIA_BUS_FMT_SGBRG12_1X12,
+	MEDIA_BUS_FMT_SGRBG12_1X12,
+	MEDIA_BUS_FMT_SRGGB12_1X12,
+	MEDIA_BUS_FMT_SBGGR10_1X10,
+	MEDIA_BUS_FMT_SGBRG10_1X10,
+	MEDIA_BUS_FMT_SGRBG10_1X10,
+	MEDIA_BUS_FMT_SRGGB10_1X10,
+	MEDIA_BUS_FMT_SBGGR8_1X8,
+	MEDIA_BUS_FMT_SGBRG8_1X8,
+	MEDIA_BUS_FMT_SGRBG8_1X8,
+	MEDIA_BUS_FMT_SRGGB8_1X8,
+	0,
+};
+
+static const u32 *csi2_be_supported_codes[] = {
+	csi2_be_supported_codes_pad,
+	csi2_be_supported_codes_pad,
+};
+
+static struct v4l2_subdev_internal_ops csi2_be_sd_internal_ops = {
+	.open = ipu_isys_subdev_open,
+	.close = ipu_isys_subdev_close,
+};
+
+static const struct v4l2_subdev_core_ops csi2_be_sd_core_ops = {
+};
+
+static const struct v4l2_ctrl_config compression_ctrl_cfg = {
+	.ops = NULL,
+	.id = V4L2_CID_IPU_ISYS_COMPRESSION,
+	.name = "ISYS CSI-BE compression",
+	.type = V4L2_CTRL_TYPE_BOOLEAN,
+	.min = 0,
+	.max = 1,
+	.step = 1,
+	.def = 0,
+};
+
+static int set_stream(struct v4l2_subdev *sd, int enable)
+{
+	return 0;
+}
+
+static const struct v4l2_subdev_video_ops csi2_be_sd_video_ops = {
+	.s_stream = set_stream,
+};
+
+static int __subdev_link_validate(struct v4l2_subdev *sd,
+				  struct media_link *link,
+				  struct v4l2_subdev_format *source_fmt,
+				  struct v4l2_subdev_format *sink_fmt)
+{
+	struct ipu_isys_pipeline *ip = container_of(sd->entity.pipe,
+						    struct ipu_isys_pipeline,
+						    pipe);
+
+	ip->csi2_be = to_ipu_isys_csi2_be(sd);
+	return ipu_isys_subdev_link_validate(sd, link, source_fmt, sink_fmt);
+}
+
+static int get_supported_code_index(u32 code)
+{
+	int i;
+
+	for (i = 0; csi2_be_supported_codes_pad[i]; i++) {
+		if (csi2_be_supported_codes_pad[i] == code)
+			return i;
+	}
+	return -EINVAL;
+}
+
+static int ipu_isys_csi2_be_set_sel(struct v4l2_subdev *sd,
+				    struct v4l2_subdev_state *state,
+				    struct v4l2_subdev_selection *sel)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+	struct media_pad *pad = &asd->sd.entity.pads[sel->pad];
+
+	if (sel->target == V4L2_SEL_TGT_CROP &&
+	    pad->flags & MEDIA_PAD_FL_SOURCE &&
+	    asd->valid_tgts[CSI2_BE_PAD_SOURCE].crop) {
+		struct v4l2_mbus_framefmt *ffmt =
+			__ipu_isys_get_ffmt(sd, state, sel->pad, sel->which);
+		struct v4l2_rect *r = __ipu_isys_get_selection
+		    (sd, state, sel->target, CSI2_BE_PAD_SINK, sel->which);
+
+		if (get_supported_code_index(ffmt->code) < 0) {
+			/* Non-bayer formats can't be single line cropped */
+			sel->r.left &= ~1;
+			sel->r.top &= ~1;
+
+			/* Non-bayer formats can't pe padded at all */
+			sel->r.width = clamp(sel->r.width,
+					     IPU_ISYS_MIN_WIDTH, r->width);
+		} else {
+			sel->r.width = clamp(sel->r.width,
+					     IPU_ISYS_MIN_WIDTH,
+					     IPU_ISYS_MAX_WIDTH);
+		}
+
+		/*
+		 * Vertical padding is not supported, height is
+		 * restricted by sink pad resolution.
+		 */
+		sel->r.height = clamp(sel->r.height, IPU_ISYS_MIN_HEIGHT,
+				      r->height);
+		*__ipu_isys_get_selection(sd, state, sel->target, sel->pad,
+					  sel->which) = sel->r;
+		ipu_isys_subdev_fmt_propagate
+		    (sd, state, NULL, &sel->r,
+		     IPU_ISYS_SUBDEV_PROP_TGT_SOURCE_CROP,
+		     sel->pad, sel->which);
+		return 0;
+	}
+	return ipu_isys_subdev_set_sel(sd, state, sel);
+}
+
+static const struct v4l2_subdev_pad_ops csi2_be_sd_pad_ops = {
+	.link_validate = __subdev_link_validate,
+	.get_fmt = ipu_isys_subdev_get_ffmt,
+	.set_fmt = ipu_isys_subdev_set_ffmt,
+	.get_selection = ipu_isys_subdev_get_sel,
+	.set_selection = ipu_isys_csi2_be_set_sel,
+	.enum_mbus_code = ipu_isys_subdev_enum_mbus_code,
+};
+
+static struct v4l2_subdev_ops csi2_be_sd_ops = {
+	.core = &csi2_be_sd_core_ops,
+	.video = &csi2_be_sd_video_ops,
+	.pad = &csi2_be_sd_pad_ops,
+};
+
+static struct media_entity_operations csi2_be_entity_ops = {
+	.link_validate = v4l2_subdev_link_validate,
+};
+
+static void csi2_be_set_ffmt(struct v4l2_subdev *sd,
+			     struct v4l2_subdev_state *state,
+			     struct v4l2_subdev_format *fmt)
+{
+	struct ipu_isys_csi2 *csi2 = to_ipu_isys_csi2(sd);
+	struct v4l2_mbus_framefmt *ffmt =
+		__ipu_isys_get_ffmt(sd, state, fmt->pad, fmt->which);
+
+	switch (fmt->pad) {
+	case CSI2_BE_PAD_SINK:
+		if (fmt->format.field != V4L2_FIELD_ALTERNATE)
+			fmt->format.field = V4L2_FIELD_NONE;
+		*ffmt = fmt->format;
+
+		ipu_isys_subdev_fmt_propagate
+		    (sd, state, &fmt->format, NULL,
+		     IPU_ISYS_SUBDEV_PROP_TGT_SINK_FMT, fmt->pad, fmt->which);
+		return;
+	case CSI2_BE_PAD_SOURCE: {
+		struct v4l2_mbus_framefmt *sink_ffmt =
+			__ipu_isys_get_ffmt(sd, state, CSI2_BE_PAD_SINK,
+					    fmt->which);
+		struct v4l2_rect *r =
+			__ipu_isys_get_selection(sd, state, V4L2_SEL_TGT_CROP,
+						 CSI2_BE_PAD_SOURCE,
+						 fmt->which);
+		struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+		u32 code = sink_ffmt->code;
+		int idx = get_supported_code_index(code);
+
+		if (asd->valid_tgts[CSI2_BE_PAD_SOURCE].crop && idx >= 0) {
+			int crop_info = 0;
+
+			if (r->top & 1)
+				crop_info |= CSI2_BE_CROP_VER;
+			if (r->left & 1)
+				crop_info |= CSI2_BE_CROP_HOR;
+			code = csi2_be_supported_codes_pad
+				[((idx & CSI2_BE_CROP_MASK) ^ crop_info)
+				+ (idx & ~CSI2_BE_CROP_MASK)];
+		}
+		ffmt->width = r->width;
+		ffmt->height = r->height;
+		ffmt->code = code;
+		ffmt->field = sink_ffmt->field;
+		return;
+	}
+	default:
+		dev_err(&csi2->isys->adev->dev, "Unknown pad type\n");
+		WARN_ON(1);
+	}
+}
+
+void ipu_isys_csi2_be_cleanup(struct ipu_isys_csi2_be *csi2_be)
+{
+	v4l2_ctrl_handler_free(&csi2_be->av.ctrl_handler);
+	v4l2_device_unregister_subdev(&csi2_be->asd.sd);
+	ipu_isys_subdev_cleanup(&csi2_be->asd);
+	ipu_isys_video_cleanup(&csi2_be->av);
+}
+
+int ipu_isys_csi2_be_init(struct ipu_isys_csi2_be *csi2_be,
+			  struct ipu_isys *isys)
+{
+	struct v4l2_subdev_format fmt = {
+		.which = V4L2_SUBDEV_FORMAT_ACTIVE,
+		.pad = CSI2_BE_PAD_SINK,
+		.format = {
+			   .width = 4096,
+			   .height = 3072,
+			  },
+	};
+	struct v4l2_subdev_selection sel = {
+		.which = V4L2_SUBDEV_FORMAT_ACTIVE,
+		.pad = CSI2_BE_PAD_SOURCE,
+		.target = V4L2_SEL_TGT_CROP,
+		.r = {
+		      .width = fmt.format.width,
+		      .height = fmt.format.height,
+		     },
+	};
+	int rval;
+
+	csi2_be->asd.sd.entity.ops = &csi2_be_entity_ops;
+	csi2_be->asd.isys = isys;
+
+	rval = ipu_isys_subdev_init(&csi2_be->asd, &csi2_be_sd_ops, 0,
+				    NR_OF_CSI2_BE_PADS,
+				    NR_OF_CSI2_BE_SOURCE_PADS,
+				    NR_OF_CSI2_BE_SINK_PADS, 0);
+	if (rval)
+		goto fail;
+
+	csi2_be->asd.pad[CSI2_BE_PAD_SINK].flags = MEDIA_PAD_FL_SINK
+	    | MEDIA_PAD_FL_MUST_CONNECT;
+	csi2_be->asd.pad[CSI2_BE_PAD_SOURCE].flags = MEDIA_PAD_FL_SOURCE;
+	csi2_be->asd.valid_tgts[CSI2_BE_PAD_SOURCE].crop = true;
+	csi2_be->asd.set_ffmt = csi2_be_set_ffmt;
+
+	BUILD_BUG_ON(ARRAY_SIZE(csi2_be_supported_codes) != NR_OF_CSI2_BE_PADS);
+	csi2_be->asd.supported_codes = csi2_be_supported_codes;
+	csi2_be->asd.be_mode = IPU_BE_RAW;
+	csi2_be->asd.isl_mode = IPU_ISL_CSI2_BE;
+
+	ipu_isys_subdev_set_ffmt(&csi2_be->asd.sd, NULL, &fmt);
+	ipu_isys_csi2_be_set_sel(&csi2_be->asd.sd, NULL, &sel);
+
+	csi2_be->asd.sd.internal_ops = &csi2_be_sd_internal_ops;
+	snprintf(csi2_be->asd.sd.name, sizeof(csi2_be->asd.sd.name),
+		 IPU_ISYS_ENTITY_PREFIX " CSI2 BE");
+	snprintf(csi2_be->av.vdev.name, sizeof(csi2_be->av.vdev.name),
+		 IPU_ISYS_ENTITY_PREFIX " CSI2 BE capture");
+	csi2_be->av.aq.css_pin_type = IPU_FW_ISYS_PIN_TYPE_RAW_NS;
+	v4l2_set_subdevdata(&csi2_be->asd.sd, &csi2_be->asd);
+	rval = v4l2_device_register_subdev(&isys->v4l2_dev, &csi2_be->asd.sd);
+	if (rval) {
+		dev_info(&isys->adev->dev, "can't register v4l2 subdev\n");
+		goto fail;
+	}
+
+	csi2_be->av.isys = isys;
+	csi2_be->av.pfmts = ipu_isys_pfmts;
+	csi2_be->av.try_fmt_vid_mplane =
+	    ipu_isys_video_try_fmt_vid_mplane_default;
+	csi2_be->av.prepare_fw_stream =
+	    ipu_isys_prepare_fw_cfg_default;
+	csi2_be->av.aq.buf_prepare = ipu_isys_buf_prepare;
+	csi2_be->av.aq.fill_frame_buff_set_pin =
+	    ipu_isys_buffer_to_fw_frame_buff_pin;
+	csi2_be->av.aq.link_fmt_validate = ipu_isys_link_fmt_validate;
+	csi2_be->av.aq.vbq.buf_struct_size =
+	    sizeof(struct ipu_isys_video_buffer);
+
+	/* create v4l2 ctrl for csi-be video node */
+	rval = v4l2_ctrl_handler_init(&csi2_be->av.ctrl_handler, 0);
+	if (rval) {
+		dev_err(&isys->adev->dev,
+			"failed to init v4l2 ctrl handler for csi2_be\n");
+		goto fail;
+	}
+
+	csi2_be->av.compression_ctrl =
+		v4l2_ctrl_new_custom(&csi2_be->av.ctrl_handler,
+				     &compression_ctrl_cfg, NULL);
+	if (!csi2_be->av.compression_ctrl) {
+		dev_err(&isys->adev->dev,
+			"failed to create CSI-BE cmprs ctrl\n");
+		goto fail;
+	}
+	csi2_be->av.compression = 0;
+	csi2_be->av.vdev.ctrl_handler = &csi2_be->av.ctrl_handler;
+
+	rval = ipu_isys_video_init(&csi2_be->av, &csi2_be->asd.sd.entity,
+				   CSI2_BE_PAD_SOURCE, MEDIA_PAD_FL_SINK, 0);
+	if (rval) {
+		dev_info(&isys->adev->dev, "can't init video node\n");
+		goto fail;
+	}
+
+	return 0;
+
+fail:
+	ipu_isys_csi2_be_cleanup(csi2_be);
+
+	return rval;
+}
diff -ruN a/drivers/media/pci/intel/ipu-isys-csi2-be.h b/drivers/media/pci/intel/ipu-isys-csi2-be.h
--- a/drivers/media/pci/intel/ipu-isys-csi2-be.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-csi2-be.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,66 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2014 - 2020 Intel Corporation */
+
+#ifndef IPU_ISYS_CSI2_BE_H
+#define IPU_ISYS_CSI2_BE_H
+
+#include <media/media-entity.h>
+#include <media/v4l2-device.h>
+
+#include "ipu-isys-queue.h"
+#include "ipu-isys-subdev.h"
+#include "ipu-isys-video.h"
+#include "ipu-platform-isys.h"
+
+struct ipu_isys_csi2_be_pdata;
+struct ipu_isys;
+
+#define CSI2_BE_PAD_SINK		0
+#define CSI2_BE_PAD_SOURCE		1
+
+#define NR_OF_CSI2_BE_PADS		2
+#define NR_OF_CSI2_BE_SOURCE_PADS	1
+#define NR_OF_CSI2_BE_SINK_PADS		1
+
+#define NR_OF_CSI2_BE_SOC_SOURCE_PADS	1
+#define NR_OF_CSI2_BE_SOC_SINK_PADS	1
+#define CSI2_BE_SOC_PAD_SINK 0
+#define CSI2_BE_SOC_PAD_SOURCE 1
+#define NR_OF_CSI2_BE_SOC_PADS \
+	(NR_OF_CSI2_BE_SOC_SOURCE_PADS + NR_OF_CSI2_BE_SOC_SINK_PADS)
+
+#define CSI2_BE_CROP_HOR	BIT(0)
+#define CSI2_BE_CROP_VER	BIT(1)
+#define CSI2_BE_CROP_MASK	(CSI2_BE_CROP_VER | CSI2_BE_CROP_HOR)
+
+/*
+ * struct ipu_isys_csi2_be
+ */
+struct ipu_isys_csi2_be {
+	struct ipu_isys_csi2_be_pdata *pdata;
+	struct ipu_isys_subdev asd;
+	struct ipu_isys_video av;
+};
+
+struct ipu_isys_csi2_be_soc {
+	struct ipu_isys_csi2_be_pdata *pdata;
+	struct ipu_isys_subdev asd;
+	struct ipu_isys_video av;
+};
+
+#define to_ipu_isys_csi2_be(sd)	\
+	container_of(to_ipu_isys_subdev(sd), \
+	struct ipu_isys_csi2_be, asd)
+
+#define to_ipu_isys_csi2_be_soc(sd)	\
+	container_of(to_ipu_isys_subdev(sd), \
+	struct ipu_isys_csi2_be_soc, asd)
+
+int ipu_isys_csi2_be_init(struct ipu_isys_csi2_be *csi2_be,
+			  struct ipu_isys *isys);
+void ipu_isys_csi2_be_cleanup(struct ipu_isys_csi2_be *csi2_be);
+int ipu_isys_csi2_be_soc_init(struct ipu_isys_csi2_be_soc *csi2_be_soc,
+			      struct ipu_isys *isys, int index);
+void ipu_isys_csi2_be_soc_cleanup(struct ipu_isys_csi2_be_soc *csi2_be);
+
+#endif /* IPU_ISYS_CSI2_BE_H */
diff -ruN a/drivers/media/pci/intel/ipu-isys-csi2-be-soc.c b/drivers/media/pci/intel/ipu-isys-csi2-be-soc.c
--- a/drivers/media/pci/intel/ipu-isys-csi2-be-soc.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-csi2-be-soc.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,341 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2014 - 2021 Intel Corporation
+
+#include <linux/device.h>
+#include <linux/module.h>
+
+#include <media/ipu-isys.h>
+#include <media/media-entity.h>
+#include <media/v4l2-device.h>
+
+#include "ipu.h"
+#include "ipu-bus.h"
+#include "ipu-isys.h"
+#include "ipu-isys-csi2-be.h"
+#include "ipu-isys-subdev.h"
+#include "ipu-isys-video.h"
+
+/*
+ * Raw bayer format pixel order MUST BE MAINTAINED in groups of four codes.
+ * Otherwise pixel order calculation below WILL BREAK!
+ */
+static const u32 csi2_be_soc_supported_codes_pad[] = {
+	MEDIA_BUS_FMT_Y10_1X10,
+	MEDIA_BUS_FMT_RGB565_1X16,
+	MEDIA_BUS_FMT_RGB888_1X24,
+	MEDIA_BUS_FMT_UYVY8_1X16,
+	MEDIA_BUS_FMT_YUYV8_1X16,
+	MEDIA_BUS_FMT_SBGGR12_1X12,
+	MEDIA_BUS_FMT_SGBRG12_1X12,
+	MEDIA_BUS_FMT_SGRBG12_1X12,
+	MEDIA_BUS_FMT_SRGGB12_1X12,
+	MEDIA_BUS_FMT_SBGGR10_1X10,
+	MEDIA_BUS_FMT_SGBRG10_1X10,
+	MEDIA_BUS_FMT_SGRBG10_1X10,
+	MEDIA_BUS_FMT_SRGGB10_1X10,
+	MEDIA_BUS_FMT_SBGGR8_1X8,
+	MEDIA_BUS_FMT_SGBRG8_1X8,
+	MEDIA_BUS_FMT_SGRBG8_1X8,
+	MEDIA_BUS_FMT_SRGGB8_1X8,
+	0,
+};
+
+/*
+ * Raw bayer format pixel order MUST BE MAINTAINED in groups of four codes.
+ * Otherwise pixel order calculation below WILL BREAK!
+ */
+static const u32 csi2_be_soc_supported_raw_bayer_codes_pad[] = {
+	MEDIA_BUS_FMT_SBGGR12_1X12,
+	MEDIA_BUS_FMT_SGBRG12_1X12,
+	MEDIA_BUS_FMT_SGRBG12_1X12,
+	MEDIA_BUS_FMT_SRGGB12_1X12,
+	MEDIA_BUS_FMT_SBGGR10_1X10,
+	MEDIA_BUS_FMT_SGBRG10_1X10,
+	MEDIA_BUS_FMT_SGRBG10_1X10,
+	MEDIA_BUS_FMT_SRGGB10_1X10,
+	MEDIA_BUS_FMT_SBGGR8_1X8,
+	MEDIA_BUS_FMT_SGBRG8_1X8,
+	MEDIA_BUS_FMT_SGRBG8_1X8,
+	MEDIA_BUS_FMT_SRGGB8_1X8,
+	0,
+};
+
+static int get_supported_code_index(u32 code)
+{
+	int i;
+
+	for (i = 0; csi2_be_soc_supported_raw_bayer_codes_pad[i]; i++) {
+		if (csi2_be_soc_supported_raw_bayer_codes_pad[i] == code)
+			return i;
+	}
+	return -EINVAL;
+}
+
+static const u32 *csi2_be_soc_supported_codes[NR_OF_CSI2_BE_SOC_PADS];
+
+static struct v4l2_subdev_internal_ops csi2_be_soc_sd_internal_ops = {
+	.open = ipu_isys_subdev_open,
+	.close = ipu_isys_subdev_close,
+};
+
+static const struct v4l2_subdev_core_ops csi2_be_soc_sd_core_ops = {
+};
+
+static const struct v4l2_ctrl_config compression_ctrl_cfg = {
+	.ops = NULL,
+	.id = V4L2_CID_IPU_ISYS_COMPRESSION,
+	.name = "ISYS BE-SOC compression",
+	.type = V4L2_CTRL_TYPE_BOOLEAN,
+	.min = 0,
+	.max = 1,
+	.step = 1,
+	.def = 0,
+};
+
+static int set_stream(struct v4l2_subdev *sd, int enable)
+{
+	return 0;
+}
+
+static const struct v4l2_subdev_video_ops csi2_be_soc_sd_video_ops = {
+	.s_stream = set_stream,
+};
+
+static int
+__subdev_link_validate(struct v4l2_subdev *sd, struct media_link *link,
+		       struct v4l2_subdev_format *source_fmt,
+		       struct v4l2_subdev_format *sink_fmt)
+{
+	struct ipu_isys_pipeline *ip = container_of(sd->entity.pipe,
+						    struct ipu_isys_pipeline,
+						    pipe);
+
+	ip->csi2_be_soc = to_ipu_isys_csi2_be_soc(sd);
+	return ipu_isys_subdev_link_validate(sd, link, source_fmt, sink_fmt);
+}
+
+static int
+ipu_isys_csi2_be_soc_set_sel(struct v4l2_subdev *sd,
+			     struct v4l2_subdev_state *state,
+			     struct v4l2_subdev_selection *sel)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+	struct media_pad *pad = &asd->sd.entity.pads[sel->pad];
+
+	if (sel->target == V4L2_SEL_TGT_CROP &&
+	    pad->flags & MEDIA_PAD_FL_SOURCE &&
+	    asd->valid_tgts[sel->pad].crop) {
+		enum isys_subdev_prop_tgt tgt =
+		    IPU_ISYS_SUBDEV_PROP_TGT_SOURCE_CROP;
+		struct v4l2_mbus_framefmt *ffmt =
+			__ipu_isys_get_ffmt(sd, state, sel->pad, sel->which);
+
+		if (get_supported_code_index(ffmt->code) < 0) {
+			/* Non-bayer formats can't be odd lines cropped */
+			sel->r.left &= ~1;
+			sel->r.top &= ~1;
+		}
+
+		sel->r.width = clamp(sel->r.width, IPU_ISYS_MIN_WIDTH,
+				     IPU_ISYS_MAX_WIDTH);
+
+		sel->r.height = clamp(sel->r.height, IPU_ISYS_MIN_HEIGHT,
+				      IPU_ISYS_MAX_HEIGHT);
+
+		*__ipu_isys_get_selection(sd, state, sel->target, sel->pad,
+					  sel->which) = sel->r;
+		ipu_isys_subdev_fmt_propagate(sd, state, NULL, &sel->r,
+					      tgt, sel->pad, sel->which);
+		return 0;
+	}
+	return -EINVAL;
+}
+
+static const struct v4l2_subdev_pad_ops csi2_be_soc_sd_pad_ops = {
+	.link_validate = __subdev_link_validate,
+	.get_fmt = ipu_isys_subdev_get_ffmt,
+	.set_fmt = ipu_isys_subdev_set_ffmt,
+	.get_selection = ipu_isys_subdev_get_sel,
+	.set_selection = ipu_isys_csi2_be_soc_set_sel,
+	.enum_mbus_code = ipu_isys_subdev_enum_mbus_code,
+};
+
+static struct v4l2_subdev_ops csi2_be_soc_sd_ops = {
+	.core = &csi2_be_soc_sd_core_ops,
+	.video = &csi2_be_soc_sd_video_ops,
+	.pad = &csi2_be_soc_sd_pad_ops,
+};
+
+static struct media_entity_operations csi2_be_soc_entity_ops = {
+	.link_validate = v4l2_subdev_link_validate,
+};
+
+static void csi2_be_soc_set_ffmt(struct v4l2_subdev *sd,
+				 struct v4l2_subdev_state *state,
+				 struct v4l2_subdev_format *fmt)
+{
+	struct v4l2_mbus_framefmt *ffmt =
+		__ipu_isys_get_ffmt(sd, state, fmt->pad,
+				    fmt->which);
+
+	if (sd->entity.pads[fmt->pad].flags & MEDIA_PAD_FL_SINK) {
+		if (fmt->format.field != V4L2_FIELD_ALTERNATE)
+			fmt->format.field = V4L2_FIELD_NONE;
+		*ffmt = fmt->format;
+
+		ipu_isys_subdev_fmt_propagate(sd, state, &fmt->format,
+					      NULL,
+					      IPU_ISYS_SUBDEV_PROP_TGT_SINK_FMT,
+					      fmt->pad, fmt->which);
+	} else if (sd->entity.pads[fmt->pad].flags & MEDIA_PAD_FL_SOURCE) {
+		struct v4l2_mbus_framefmt *sink_ffmt;
+		struct v4l2_rect *r = __ipu_isys_get_selection(sd, state,
+			V4L2_SEL_TGT_CROP, fmt->pad, fmt->which);
+		struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+		u32 code;
+		int idx;
+
+		sink_ffmt = __ipu_isys_get_ffmt(sd, state, 0, fmt->which);
+		code = sink_ffmt->code;
+		idx = get_supported_code_index(code);
+
+		if (asd->valid_tgts[fmt->pad].crop && idx >= 0) {
+			int crop_info = 0;
+
+			/* Only croping odd line at top side. */
+			if (r->top & 1)
+				crop_info |= CSI2_BE_CROP_VER;
+
+			code = csi2_be_soc_supported_raw_bayer_codes_pad
+				[((idx & CSI2_BE_CROP_MASK) ^ crop_info)
+				+ (idx & ~CSI2_BE_CROP_MASK)];
+
+		}
+		ffmt->code = code;
+		ffmt->width = r->width;
+		ffmt->height = r->height;
+		ffmt->field = sink_ffmt->field;
+
+	}
+}
+
+void ipu_isys_csi2_be_soc_cleanup(struct ipu_isys_csi2_be_soc *csi2_be_soc)
+{
+	v4l2_device_unregister_subdev(&csi2_be_soc->asd.sd);
+	ipu_isys_subdev_cleanup(&csi2_be_soc->asd);
+	v4l2_ctrl_handler_free(&csi2_be_soc->av.ctrl_handler);
+	ipu_isys_video_cleanup(&csi2_be_soc->av);
+}
+
+int ipu_isys_csi2_be_soc_init(struct ipu_isys_csi2_be_soc *csi2_be_soc,
+			      struct ipu_isys *isys, int index)
+{
+	struct v4l2_subdev_format fmt = {
+		.which = V4L2_SUBDEV_FORMAT_ACTIVE,
+		.pad = CSI2_BE_SOC_PAD_SINK,
+		.format = {
+			   .width = 4096,
+			   .height = 3072,
+			   },
+	};
+	int rval, i;
+
+	csi2_be_soc->asd.sd.entity.ops = &csi2_be_soc_entity_ops;
+	csi2_be_soc->asd.isys = isys;
+
+	rval = ipu_isys_subdev_init(&csi2_be_soc->asd,
+				    &csi2_be_soc_sd_ops, 0,
+				    NR_OF_CSI2_BE_SOC_PADS,
+				    NR_OF_CSI2_BE_SOC_SOURCE_PADS,
+				    NR_OF_CSI2_BE_SOC_SINK_PADS, 0);
+	if (rval)
+		goto fail;
+
+	csi2_be_soc->asd.pad[CSI2_BE_SOC_PAD_SINK].flags = MEDIA_PAD_FL_SINK;
+	csi2_be_soc->asd.pad[CSI2_BE_SOC_PAD_SOURCE].flags =
+		MEDIA_PAD_FL_SOURCE;
+	csi2_be_soc->asd.valid_tgts[CSI2_BE_SOC_PAD_SOURCE].crop = true;
+
+	for (i = 0; i < NR_OF_CSI2_BE_SOC_PADS; i++)
+		csi2_be_soc_supported_codes[i] =
+			csi2_be_soc_supported_codes_pad;
+	csi2_be_soc->asd.supported_codes = csi2_be_soc_supported_codes;
+	csi2_be_soc->asd.be_mode = IPU_BE_SOC;
+	csi2_be_soc->asd.isl_mode = IPU_ISL_OFF;
+	csi2_be_soc->asd.set_ffmt = csi2_be_soc_set_ffmt;
+
+	fmt.pad = CSI2_BE_SOC_PAD_SINK;
+	ipu_isys_subdev_set_ffmt(&csi2_be_soc->asd.sd, NULL, &fmt);
+	fmt.pad = CSI2_BE_SOC_PAD_SOURCE;
+	ipu_isys_subdev_set_ffmt(&csi2_be_soc->asd.sd, NULL, &fmt);
+	csi2_be_soc->asd.sd.internal_ops = &csi2_be_soc_sd_internal_ops;
+
+	snprintf(csi2_be_soc->asd.sd.name, sizeof(csi2_be_soc->asd.sd.name),
+		 IPU_ISYS_ENTITY_PREFIX " CSI2 BE SOC %d", index);
+	v4l2_set_subdevdata(&csi2_be_soc->asd.sd, &csi2_be_soc->asd);
+
+	rval = v4l2_device_register_subdev(&isys->v4l2_dev,
+					   &csi2_be_soc->asd.sd);
+	if (rval) {
+		dev_info(&isys->adev->dev, "can't register v4l2 subdev\n");
+		goto fail;
+	}
+
+	snprintf(csi2_be_soc->av.vdev.name, sizeof(csi2_be_soc->av.vdev.name),
+		 IPU_ISYS_ENTITY_PREFIX " BE SOC capture %d", index);
+
+	/*
+	 * Pin type could be overwritten for YUV422 to I420 case, at
+	 * set_format phase
+	 */
+	csi2_be_soc->av.aq.css_pin_type = IPU_FW_ISYS_PIN_TYPE_RAW_SOC;
+	csi2_be_soc->av.isys = isys;
+	csi2_be_soc->av.pfmts = ipu_isys_pfmts_be_soc;
+	csi2_be_soc->av.try_fmt_vid_mplane =
+		ipu_isys_video_try_fmt_vid_mplane_default;
+	csi2_be_soc->av.prepare_fw_stream =
+		ipu_isys_prepare_fw_cfg_default;
+	csi2_be_soc->av.aq.buf_prepare = ipu_isys_buf_prepare;
+	csi2_be_soc->av.aq.fill_frame_buff_set_pin =
+		ipu_isys_buffer_to_fw_frame_buff_pin;
+	csi2_be_soc->av.aq.link_fmt_validate = ipu_isys_link_fmt_validate;
+	csi2_be_soc->av.aq.vbq.buf_struct_size =
+		sizeof(struct ipu_isys_video_buffer);
+
+	/* create v4l2 ctrl for be-soc video node */
+	rval = v4l2_ctrl_handler_init(&csi2_be_soc->av.ctrl_handler, 0);
+	if (rval) {
+		dev_err(&isys->adev->dev,
+			"failed to init v4l2 ctrl handler for be_soc\n");
+		goto fail;
+	}
+
+	csi2_be_soc->av.compression_ctrl =
+		v4l2_ctrl_new_custom(&csi2_be_soc->av.ctrl_handler,
+				     &compression_ctrl_cfg, NULL);
+	if (!csi2_be_soc->av.compression_ctrl) {
+		dev_err(&isys->adev->dev,
+			"failed to create BE-SOC cmprs ctrl\n");
+		goto fail;
+	}
+	csi2_be_soc->av.compression = 0;
+	csi2_be_soc->av.vdev.ctrl_handler =
+		&csi2_be_soc->av.ctrl_handler;
+
+	rval = ipu_isys_video_init(&csi2_be_soc->av,
+				   &csi2_be_soc->asd.sd.entity,
+				   CSI2_BE_SOC_PAD_SOURCE,
+				   MEDIA_PAD_FL_SINK,
+				   MEDIA_LNK_FL_DYNAMIC);
+	if (rval) {
+		dev_info(&isys->adev->dev, "can't init video node\n");
+		goto fail;
+	}
+
+	return 0;
+
+fail:
+	ipu_isys_csi2_be_soc_cleanup(csi2_be_soc);
+
+	return rval;
+}
diff -ruN a/drivers/media/pci/intel/ipu-isys-csi2.c b/drivers/media/pci/intel/ipu-isys-csi2.c
--- a/drivers/media/pci/intel/ipu-isys-csi2.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-csi2.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,662 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2020 Intel Corporation
+
+#include <linux/device.h>
+#include <linux/module.h>
+#include <linux/version.h>
+
+#include <media/ipu-isys.h>
+#include <media/media-entity.h>
+#include <media/v4l2-device.h>
+#include <media/v4l2-event.h>
+
+#include "ipu.h"
+#include "ipu-bus.h"
+#include "ipu-buttress.h"
+#include "ipu-isys.h"
+#include "ipu-isys-subdev.h"
+#include "ipu-isys-video.h"
+#include "ipu-platform-regs.h"
+
+static const u32 csi2_supported_codes_pad_sink[] = {
+	MEDIA_BUS_FMT_Y10_1X10,
+	MEDIA_BUS_FMT_RGB565_1X16,
+	MEDIA_BUS_FMT_RGB888_1X24,
+	MEDIA_BUS_FMT_UYVY8_1X16,
+	MEDIA_BUS_FMT_YUYV8_1X16,
+	MEDIA_BUS_FMT_YUYV10_1X20,
+	MEDIA_BUS_FMT_SBGGR10_1X10,
+	MEDIA_BUS_FMT_SGBRG10_1X10,
+	MEDIA_BUS_FMT_SGRBG10_1X10,
+	MEDIA_BUS_FMT_SRGGB10_1X10,
+	MEDIA_BUS_FMT_SBGGR10_DPCM8_1X8,
+	MEDIA_BUS_FMT_SGBRG10_DPCM8_1X8,
+	MEDIA_BUS_FMT_SGRBG10_DPCM8_1X8,
+	MEDIA_BUS_FMT_SRGGB10_DPCM8_1X8,
+	MEDIA_BUS_FMT_SBGGR12_1X12,
+	MEDIA_BUS_FMT_SGBRG12_1X12,
+	MEDIA_BUS_FMT_SGRBG12_1X12,
+	MEDIA_BUS_FMT_SRGGB12_1X12,
+	MEDIA_BUS_FMT_SBGGR8_1X8,
+	MEDIA_BUS_FMT_SGBRG8_1X8,
+	MEDIA_BUS_FMT_SGRBG8_1X8,
+	MEDIA_BUS_FMT_SRGGB8_1X8,
+	0,
+};
+
+static const u32 csi2_supported_codes_pad_source[] = {
+	MEDIA_BUS_FMT_Y10_1X10,
+	MEDIA_BUS_FMT_RGB565_1X16,
+	MEDIA_BUS_FMT_RGB888_1X24,
+	MEDIA_BUS_FMT_UYVY8_1X16,
+	MEDIA_BUS_FMT_YUYV8_1X16,
+	MEDIA_BUS_FMT_YUYV10_1X20,
+	MEDIA_BUS_FMT_SBGGR10_1X10,
+	MEDIA_BUS_FMT_SGBRG10_1X10,
+	MEDIA_BUS_FMT_SGRBG10_1X10,
+	MEDIA_BUS_FMT_SRGGB10_1X10,
+	MEDIA_BUS_FMT_SBGGR12_1X12,
+	MEDIA_BUS_FMT_SGBRG12_1X12,
+	MEDIA_BUS_FMT_SGRBG12_1X12,
+	MEDIA_BUS_FMT_SRGGB12_1X12,
+	MEDIA_BUS_FMT_SBGGR8_1X8,
+	MEDIA_BUS_FMT_SGBRG8_1X8,
+	MEDIA_BUS_FMT_SGRBG8_1X8,
+	MEDIA_BUS_FMT_SRGGB8_1X8,
+	0,
+};
+
+static const u32 *csi2_supported_codes[NR_OF_CSI2_PADS];
+
+static struct v4l2_subdev_internal_ops csi2_sd_internal_ops = {
+	.open = ipu_isys_subdev_open,
+	.close = ipu_isys_subdev_close,
+};
+
+int ipu_isys_csi2_get_link_freq(struct ipu_isys_csi2 *csi2, __s64 *link_freq)
+{
+	struct ipu_isys_pipeline *pipe = container_of(csi2->asd.sd.entity.pipe,
+						      struct ipu_isys_pipeline,
+						      pipe);
+	struct v4l2_subdev *ext_sd =
+	    media_entity_to_v4l2_subdev(pipe->external->entity);
+	struct v4l2_ext_control c = {.id = V4L2_CID_LINK_FREQ, };
+	struct v4l2_ext_controls cs = {.count = 1,
+		.controls = &c,
+	};
+	struct v4l2_querymenu qm = {.id = c.id, };
+	int rval;
+
+	if (!ext_sd) {
+		WARN_ON(1);
+		return -ENODEV;
+	}
+	rval = v4l2_g_ext_ctrls(ext_sd->ctrl_handler,
+				ext_sd->devnode,
+				ext_sd->v4l2_dev->mdev,
+				&cs);
+	if (rval) {
+		dev_info(&csi2->isys->adev->dev, "can't get link frequency\n");
+		return rval;
+	}
+
+	qm.index = c.value;
+
+	rval = v4l2_querymenu(ext_sd->ctrl_handler, &qm);
+	if (rval) {
+		dev_info(&csi2->isys->adev->dev, "can't get menu item\n");
+		return rval;
+	}
+
+	dev_dbg(&csi2->isys->adev->dev, "%s: link frequency %lld\n", __func__,
+		qm.value);
+
+	if (!qm.value)
+		return -EINVAL;
+	*link_freq = qm.value;
+	return 0;
+}
+
+static int subscribe_event(struct v4l2_subdev *sd, struct v4l2_fh *fh,
+			   struct v4l2_event_subscription *sub)
+{
+	struct ipu_isys_csi2 *csi2 = to_ipu_isys_csi2(sd);
+
+	dev_dbg(&csi2->isys->adev->dev, "subscribe event(type %u id %u)\n",
+		sub->type, sub->id);
+
+	switch (sub->type) {
+	case V4L2_EVENT_FRAME_SYNC:
+		return v4l2_event_subscribe(fh, sub, 10, NULL);
+	case V4L2_EVENT_CTRL:
+		return v4l2_ctrl_subscribe_event(fh, sub);
+	default:
+		return -EINVAL;
+	}
+}
+
+static const struct v4l2_subdev_core_ops csi2_sd_core_ops = {
+	.subscribe_event = subscribe_event,
+	.unsubscribe_event = v4l2_event_subdev_unsubscribe,
+};
+
+/*
+ * The input system CSI2+ receiver has several
+ * parameters affecting the receiver timings. These depend
+ * on the MIPI bus frequency F in Hz (sensor transmitter rate)
+ * as follows:
+ *	register value = (A/1e9 + B * UI) / COUNT_ACC
+ * where
+ *	UI = 1 / (2 * F) in seconds
+ *	COUNT_ACC = counter accuracy in seconds
+ *	For legacy IPU,  COUNT_ACC = 0.125 ns
+ *
+ * A and B are coefficients from the table below,
+ * depending whether the register minimum or maximum value is
+ * calculated.
+ *				       Minimum     Maximum
+ * Clock lane			       A     B     A     B
+ * reg_rx_csi_dly_cnt_termen_clane     0     0    38     0
+ * reg_rx_csi_dly_cnt_settle_clane    95    -8   300   -16
+ * Data lanes
+ * reg_rx_csi_dly_cnt_termen_dlane0    0     0    35     4
+ * reg_rx_csi_dly_cnt_settle_dlane0   85    -2   145    -6
+ * reg_rx_csi_dly_cnt_termen_dlane1    0     0    35     4
+ * reg_rx_csi_dly_cnt_settle_dlane1   85    -2   145    -6
+ * reg_rx_csi_dly_cnt_termen_dlane2    0     0    35     4
+ * reg_rx_csi_dly_cnt_settle_dlane2   85    -2   145    -6
+ * reg_rx_csi_dly_cnt_termen_dlane3    0     0    35     4
+ * reg_rx_csi_dly_cnt_settle_dlane3   85    -2   145    -6
+ *
+ * We use the minimum values of both A and B.
+ */
+
+#define DIV_SHIFT	8
+
+static uint32_t calc_timing(s32 a, int32_t b, int64_t link_freq, int32_t accinv)
+{
+	return accinv * a + (accinv * b * (500000000 >> DIV_SHIFT)
+			     / (int32_t)(link_freq >> DIV_SHIFT));
+}
+
+static int
+ipu_isys_csi2_calc_timing(struct ipu_isys_csi2 *csi2,
+			  struct ipu_isys_csi2_timing *timing, uint32_t accinv)
+{
+	__s64 link_freq;
+	int rval;
+
+	rval = ipu_isys_csi2_get_link_freq(csi2, &link_freq);
+	if (rval)
+		return rval;
+
+	timing->ctermen = calc_timing(CSI2_CSI_RX_DLY_CNT_TERMEN_CLANE_A,
+				      CSI2_CSI_RX_DLY_CNT_TERMEN_CLANE_B,
+				      link_freq, accinv);
+	timing->csettle = calc_timing(CSI2_CSI_RX_DLY_CNT_SETTLE_CLANE_A,
+				      CSI2_CSI_RX_DLY_CNT_SETTLE_CLANE_B,
+				      link_freq, accinv);
+	dev_dbg(&csi2->isys->adev->dev, "ctermen %u\n", timing->ctermen);
+	dev_dbg(&csi2->isys->adev->dev, "csettle %u\n", timing->csettle);
+
+	timing->dtermen = calc_timing(CSI2_CSI_RX_DLY_CNT_TERMEN_DLANE_A,
+				      CSI2_CSI_RX_DLY_CNT_TERMEN_DLANE_B,
+				      link_freq, accinv);
+	timing->dsettle = calc_timing(CSI2_CSI_RX_DLY_CNT_SETTLE_DLANE_A,
+				      CSI2_CSI_RX_DLY_CNT_SETTLE_DLANE_B,
+				      link_freq, accinv);
+	dev_dbg(&csi2->isys->adev->dev, "dtermen %u\n", timing->dtermen);
+	dev_dbg(&csi2->isys->adev->dev, "dsettle %u\n", timing->dsettle);
+
+	return 0;
+}
+
+#define CSI2_ACCINV	8
+
+static int set_stream(struct v4l2_subdev *sd, int enable)
+{
+	struct ipu_isys_csi2 *csi2 = to_ipu_isys_csi2(sd);
+	struct ipu_isys_pipeline *ip = container_of(sd->entity.pipe,
+						    struct ipu_isys_pipeline,
+						    pipe);
+	struct ipu_isys_csi2_config *cfg;
+	struct v4l2_subdev *ext_sd;
+	struct ipu_isys_csi2_timing timing = {0};
+	unsigned int nlanes;
+	int rval;
+
+	dev_dbg(&csi2->isys->adev->dev, "csi2 s_stream %d\n", enable);
+
+	if (!ip->external->entity) {
+		WARN_ON(1);
+		return -ENODEV;
+	}
+	ext_sd = media_entity_to_v4l2_subdev(ip->external->entity);
+	cfg = v4l2_get_subdev_hostdata(ext_sd);
+
+	if (!enable) {
+		ipu_isys_csi2_set_stream(sd, timing, 0, enable);
+		return 0;
+	}
+
+	ip->has_sof = true;
+
+	nlanes = cfg->nlanes;
+
+	dev_dbg(&csi2->isys->adev->dev, "lane nr %d.\n", nlanes);
+
+	rval = ipu_isys_csi2_calc_timing(csi2, &timing, CSI2_ACCINV);
+	if (rval)
+		return rval;
+
+	rval = ipu_isys_csi2_set_stream(sd, timing, nlanes, enable);
+
+	return rval;
+}
+
+static void csi2_capture_done(struct ipu_isys_pipeline *ip,
+			      struct ipu_fw_isys_resp_info_abi *info)
+{
+	if (ip->interlaced && ip->isys->short_packet_source ==
+	    IPU_ISYS_SHORT_PACKET_FROM_RECEIVER) {
+		struct ipu_isys_buffer *ib;
+		unsigned long flags;
+
+		spin_lock_irqsave(&ip->short_packet_queue_lock, flags);
+		if (!list_empty(&ip->short_packet_active)) {
+			ib = list_last_entry(&ip->short_packet_active,
+					     struct ipu_isys_buffer, head);
+			list_move(&ib->head, &ip->short_packet_incoming);
+		}
+		spin_unlock_irqrestore(&ip->short_packet_queue_lock, flags);
+	}
+}
+
+static int csi2_link_validate(struct media_link *link)
+{
+	struct ipu_isys_csi2 *csi2;
+	struct ipu_isys_pipeline *ip;
+	int rval;
+
+	if (!link->sink->entity ||
+	    !link->sink->entity->pipe || !link->source->entity)
+		return -EINVAL;
+	csi2 =
+	    to_ipu_isys_csi2(media_entity_to_v4l2_subdev(link->sink->entity));
+	ip = to_ipu_isys_pipeline(link->sink->entity->pipe);
+	csi2->receiver_errors = 0;
+	ip->csi2 = csi2;
+	ipu_isys_video_add_capture_done(to_ipu_isys_pipeline
+					(link->sink->entity->pipe),
+					csi2_capture_done);
+
+	rval = v4l2_subdev_link_validate(link);
+	if (rval)
+		return rval;
+
+	if (!v4l2_ctrl_g_ctrl(csi2->store_csi2_header)) {
+		struct media_pad *remote_pad =
+		    media_entity_remote_pad(&csi2->asd.pad[CSI2_PAD_SOURCE]);
+
+		if (remote_pad &&
+		    is_media_entity_v4l2_subdev(remote_pad->entity)) {
+			dev_err(&csi2->isys->adev->dev,
+				"CSI2 BE requires CSI2 headers.\n");
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static const struct v4l2_subdev_video_ops csi2_sd_video_ops = {
+	.s_stream = set_stream,
+};
+
+static int ipu_isys_csi2_get_fmt(struct v4l2_subdev *sd,
+				 struct v4l2_subdev_state *state,
+				 struct v4l2_subdev_format *fmt)
+{
+	return ipu_isys_subdev_get_ffmt(sd, state, fmt);
+}
+
+static int ipu_isys_csi2_set_fmt(struct v4l2_subdev *sd,
+				 struct v4l2_subdev_state *state,
+				 struct v4l2_subdev_format *fmt)
+{
+	return ipu_isys_subdev_set_ffmt(sd, state, fmt);
+}
+
+static int __subdev_link_validate(struct v4l2_subdev *sd,
+				  struct media_link *link,
+				  struct v4l2_subdev_format *source_fmt,
+				  struct v4l2_subdev_format *sink_fmt)
+{
+	struct ipu_isys_pipeline *ip = container_of(sd->entity.pipe,
+						    struct ipu_isys_pipeline,
+						    pipe);
+
+	if (source_fmt->format.field == V4L2_FIELD_ALTERNATE)
+		ip->interlaced = true;
+
+	return ipu_isys_subdev_link_validate(sd, link, source_fmt, sink_fmt);
+}
+
+static const struct v4l2_subdev_pad_ops csi2_sd_pad_ops = {
+	.link_validate = __subdev_link_validate,
+	.get_fmt = ipu_isys_csi2_get_fmt,
+	.set_fmt = ipu_isys_csi2_set_fmt,
+	.enum_mbus_code = ipu_isys_subdev_enum_mbus_code,
+};
+
+static struct v4l2_subdev_ops csi2_sd_ops = {
+	.core = &csi2_sd_core_ops,
+	.video = &csi2_sd_video_ops,
+	.pad = &csi2_sd_pad_ops,
+};
+
+static struct media_entity_operations csi2_entity_ops = {
+	.link_validate = csi2_link_validate,
+};
+
+static void csi2_set_ffmt(struct v4l2_subdev *sd,
+			  struct v4l2_subdev_state *state,
+			  struct v4l2_subdev_format *fmt)
+{
+	enum isys_subdev_prop_tgt tgt = IPU_ISYS_SUBDEV_PROP_TGT_SINK_FMT;
+	struct v4l2_mbus_framefmt *ffmt =
+		__ipu_isys_get_ffmt(sd, state, fmt->pad,
+				    fmt->which);
+
+	if (fmt->format.field != V4L2_FIELD_ALTERNATE)
+		fmt->format.field = V4L2_FIELD_NONE;
+
+	if (fmt->pad == CSI2_PAD_SINK) {
+		*ffmt = fmt->format;
+		ipu_isys_subdev_fmt_propagate(sd, state, &fmt->format, NULL,
+					      tgt, fmt->pad, fmt->which);
+		return;
+	}
+
+	if (sd->entity.pads[fmt->pad].flags & MEDIA_PAD_FL_SOURCE) {
+		ffmt->width = fmt->format.width;
+		ffmt->height = fmt->format.height;
+		ffmt->field = fmt->format.field;
+		ffmt->code =
+		    ipu_isys_subdev_code_to_uncompressed(fmt->format.code);
+		return;
+	}
+
+	WARN_ON(1);
+}
+
+static const struct ipu_isys_pixelformat *
+csi2_try_fmt(struct ipu_isys_video *av,
+	     struct v4l2_pix_format_mplane *mpix)
+{
+	struct media_link *link = list_first_entry(&av->vdev.entity.links,
+						   struct media_link, list);
+	struct v4l2_subdev *sd =
+	    media_entity_to_v4l2_subdev(link->source->entity);
+	struct ipu_isys_csi2 *csi2;
+
+	if (!sd)
+		return NULL;
+
+	csi2 = to_ipu_isys_csi2(sd);
+
+	return ipu_isys_video_try_fmt_vid_mplane(av, mpix,
+				v4l2_ctrl_g_ctrl(csi2->store_csi2_header));
+}
+
+void ipu_isys_csi2_cleanup(struct ipu_isys_csi2 *csi2)
+{
+	if (!csi2->isys)
+		return;
+
+	v4l2_device_unregister_subdev(&csi2->asd.sd);
+	ipu_isys_subdev_cleanup(&csi2->asd);
+	ipu_isys_video_cleanup(&csi2->av);
+	csi2->isys = NULL;
+}
+
+static void csi_ctrl_init(struct v4l2_subdev *sd)
+{
+	struct ipu_isys_csi2 *csi2 = to_ipu_isys_csi2(sd);
+
+	static const struct v4l2_ctrl_config cfg = {
+		.id = V4L2_CID_IPU_STORE_CSI2_HEADER,
+		.name = "Store CSI-2 Headers",
+		.type = V4L2_CTRL_TYPE_BOOLEAN,
+		.min = 0,
+		.max = 1,
+		.step = 1,
+		.def = 1,
+	};
+
+	csi2->store_csi2_header = v4l2_ctrl_new_custom(&csi2->asd.ctrl_handler,
+						       &cfg, NULL);
+}
+
+int ipu_isys_csi2_init(struct ipu_isys_csi2 *csi2,
+		       struct ipu_isys *isys,
+		       void __iomem *base, unsigned int index)
+{
+	struct v4l2_subdev_format fmt = {
+		.which = V4L2_SUBDEV_FORMAT_ACTIVE,
+		.pad = CSI2_PAD_SINK,
+		.format = {
+			   .width = 4096,
+			   .height = 3072,
+			  },
+	};
+	int i, rval, src;
+
+	dev_dbg(&isys->adev->dev, "csi-%d base = 0x%lx\n", index,
+		(unsigned long)base);
+	csi2->isys = isys;
+	csi2->base = base;
+	csi2->index = index;
+
+	csi2->asd.sd.entity.ops = &csi2_entity_ops;
+	csi2->asd.ctrl_init = csi_ctrl_init;
+	csi2->asd.isys = isys;
+	init_completion(&csi2->eof_completion);
+	rval = ipu_isys_subdev_init(&csi2->asd, &csi2_sd_ops, 0,
+				    NR_OF_CSI2_PADS,
+				    NR_OF_CSI2_SOURCE_PADS,
+				    NR_OF_CSI2_SINK_PADS,
+				    0);
+	if (rval)
+		goto fail;
+
+	csi2->asd.pad[CSI2_PAD_SINK].flags = MEDIA_PAD_FL_SINK
+		| MEDIA_PAD_FL_MUST_CONNECT;
+	csi2->asd.pad[CSI2_PAD_SOURCE].flags = MEDIA_PAD_FL_SOURCE;
+
+	src = index;
+	csi2->asd.source = IPU_FW_ISYS_STREAM_SRC_CSI2_PORT0 + src;
+	csi2_supported_codes[CSI2_PAD_SINK] = csi2_supported_codes_pad_sink;
+
+	for (i = 0; i < NR_OF_CSI2_SOURCE_PADS; i++)
+		csi2_supported_codes[i + 1] = csi2_supported_codes_pad_source;
+	csi2->asd.supported_codes = csi2_supported_codes;
+	csi2->asd.set_ffmt = csi2_set_ffmt;
+
+	csi2->asd.sd.flags |= V4L2_SUBDEV_FL_HAS_EVENTS;
+	csi2->asd.sd.internal_ops = &csi2_sd_internal_ops;
+	snprintf(csi2->asd.sd.name, sizeof(csi2->asd.sd.name),
+		 IPU_ISYS_ENTITY_PREFIX " CSI-2 %u", index);
+	v4l2_set_subdevdata(&csi2->asd.sd, &csi2->asd);
+
+	rval = v4l2_device_register_subdev(&isys->v4l2_dev, &csi2->asd.sd);
+	if (rval) {
+		dev_info(&isys->adev->dev, "can't register v4l2 subdev\n");
+		goto fail;
+	}
+
+	mutex_lock(&csi2->asd.mutex);
+	__ipu_isys_subdev_set_ffmt(&csi2->asd.sd, NULL, &fmt);
+	mutex_unlock(&csi2->asd.mutex);
+
+	snprintf(csi2->av.vdev.name, sizeof(csi2->av.vdev.name),
+		 IPU_ISYS_ENTITY_PREFIX " CSI-2 %u capture", index);
+	csi2->av.isys = isys;
+	csi2->av.aq.css_pin_type = IPU_FW_ISYS_PIN_TYPE_MIPI;
+	csi2->av.pfmts = ipu_isys_pfmts_packed;
+	csi2->av.try_fmt_vid_mplane = csi2_try_fmt;
+	csi2->av.prepare_fw_stream =
+		ipu_isys_prepare_fw_cfg_default;
+	csi2->av.packed = true;
+	csi2->av.line_header_length =
+		IPU_ISYS_CSI2_LONG_PACKET_HEADER_SIZE;
+	csi2->av.line_footer_length =
+		IPU_ISYS_CSI2_LONG_PACKET_FOOTER_SIZE;
+	csi2->av.aq.buf_prepare = ipu_isys_buf_prepare;
+	csi2->av.aq.fill_frame_buff_set_pin =
+	ipu_isys_buffer_to_fw_frame_buff_pin;
+	csi2->av.aq.link_fmt_validate =
+		ipu_isys_link_fmt_validate;
+	csi2->av.aq.vbq.buf_struct_size =
+		sizeof(struct ipu_isys_video_buffer);
+
+	rval = ipu_isys_video_init(&csi2->av,
+				   &csi2->asd.sd.entity,
+				   CSI2_PAD_SOURCE,
+				   MEDIA_PAD_FL_SINK, 0);
+	if (rval) {
+		dev_info(&isys->adev->dev, "can't init video node\n");
+		goto fail;
+	}
+
+	return 0;
+
+fail:
+	ipu_isys_csi2_cleanup(csi2);
+
+	return rval;
+}
+
+void ipu_isys_csi2_sof_event(struct ipu_isys_csi2 *csi2)
+{
+	struct ipu_isys_pipeline *ip = NULL;
+	struct v4l2_event ev = {
+		.type = V4L2_EVENT_FRAME_SYNC,
+	};
+	struct video_device *vdev = csi2->asd.sd.devnode;
+	unsigned long flags;
+	unsigned int i;
+
+	spin_lock_irqsave(&csi2->isys->lock, flags);
+	csi2->in_frame = true;
+
+	for (i = 0; i < IPU_ISYS_MAX_STREAMS; i++) {
+		if (csi2->isys->pipes[i] &&
+		    csi2->isys->pipes[i]->csi2 == csi2) {
+			ip = csi2->isys->pipes[i];
+			break;
+		}
+	}
+
+	/* Pipe already vanished */
+	if (!ip) {
+		spin_unlock_irqrestore(&csi2->isys->lock, flags);
+		return;
+	}
+
+	ev.u.frame_sync.frame_sequence = atomic_inc_return(&ip->sequence) - 1;
+	spin_unlock_irqrestore(&csi2->isys->lock, flags);
+
+	v4l2_event_queue(vdev, &ev);
+	dev_dbg(&csi2->isys->adev->dev,
+		"sof_event::csi2-%i sequence: %i\n",
+		csi2->index, ev.u.frame_sync.frame_sequence);
+}
+
+void ipu_isys_csi2_eof_event(struct ipu_isys_csi2 *csi2)
+{
+	struct ipu_isys_pipeline *ip = NULL;
+	unsigned long flags;
+	unsigned int i;
+	u32 frame_sequence;
+
+	spin_lock_irqsave(&csi2->isys->lock, flags);
+	csi2->in_frame = false;
+	if (csi2->wait_for_sync)
+		complete(&csi2->eof_completion);
+
+	for (i = 0; i < IPU_ISYS_MAX_STREAMS; i++) {
+		if (csi2->isys->pipes[i] &&
+		    csi2->isys->pipes[i]->csi2 == csi2) {
+			ip = csi2->isys->pipes[i];
+			break;
+		}
+	}
+
+	if (ip) {
+		frame_sequence = atomic_read(&ip->sequence);
+		spin_unlock_irqrestore(&csi2->isys->lock, flags);
+
+		dev_dbg(&csi2->isys->adev->dev,
+			"eof_event::csi2-%i sequence: %i\n",
+			csi2->index, frame_sequence);
+		return;
+	}
+
+	spin_unlock_irqrestore(&csi2->isys->lock, flags);
+}
+
+/* Call this function only _after_ the sensor has been stopped */
+void ipu_isys_csi2_wait_last_eof(struct ipu_isys_csi2 *csi2)
+{
+	unsigned long flags, tout;
+
+	spin_lock_irqsave(&csi2->isys->lock, flags);
+
+	if (!csi2->in_frame) {
+		spin_unlock_irqrestore(&csi2->isys->lock, flags);
+		return;
+	}
+
+	reinit_completion(&csi2->eof_completion);
+	csi2->wait_for_sync = true;
+	spin_unlock_irqrestore(&csi2->isys->lock, flags);
+	tout = wait_for_completion_timeout(&csi2->eof_completion,
+					   IPU_EOF_TIMEOUT_JIFFIES);
+	if (!tout)
+		dev_err(&csi2->isys->adev->dev,
+			"csi2-%d: timeout at sync to eof\n",
+			csi2->index);
+	csi2->wait_for_sync = false;
+}
+
+struct ipu_isys_buffer *
+ipu_isys_csi2_get_short_packet_buffer(struct ipu_isys_pipeline *ip,
+				      struct ipu_isys_buffer_list *bl)
+{
+	struct ipu_isys_buffer *ib;
+	struct ipu_isys_private_buffer *pb;
+	struct ipu_isys_mipi_packet_header *ph;
+	unsigned long flags;
+
+	spin_lock_irqsave(&ip->short_packet_queue_lock, flags);
+	if (list_empty(&ip->short_packet_incoming)) {
+		spin_unlock_irqrestore(&ip->short_packet_queue_lock, flags);
+		return NULL;
+	}
+	ib = list_last_entry(&ip->short_packet_incoming,
+			     struct ipu_isys_buffer, head);
+	pb = ipu_isys_buffer_to_private_buffer(ib);
+	ph = (struct ipu_isys_mipi_packet_header *)pb->buffer;
+
+	/* Fill the packet header with magic number. */
+	ph->word_count = 0xffff;
+	ph->dtype = 0xff;
+
+	dma_sync_single_for_cpu(&ip->isys->adev->dev, pb->dma_addr,
+				sizeof(*ph), DMA_BIDIRECTIONAL);
+	spin_unlock_irqrestore(&ip->short_packet_queue_lock, flags);
+	list_move(&ib->head, &bl->head);
+
+	return ib;
+}
diff -ruN a/drivers/media/pci/intel/ipu-isys-csi2.h b/drivers/media/pci/intel/ipu-isys-csi2.h
--- a/drivers/media/pci/intel/ipu-isys-csi2.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-csi2.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,164 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_ISYS_CSI2_H
+#define IPU_ISYS_CSI2_H
+
+#include <media/media-entity.h>
+#include <media/v4l2-device.h>
+
+#include "ipu-isys-queue.h"
+#include "ipu-isys-subdev.h"
+#include "ipu-isys-video.h"
+#include "ipu-platform-isys.h"
+
+struct ipu_isys_csi2_timing;
+struct ipu_isys_csi2_pdata;
+struct ipu_isys;
+
+#define NR_OF_CSI2_SINK_PADS		1
+#define CSI2_PAD_SINK			0
+#define NR_OF_CSI2_SOURCE_PADS		1
+#define CSI2_PAD_SOURCE			1
+#define NR_OF_CSI2_PADS	(NR_OF_CSI2_SINK_PADS + NR_OF_CSI2_SOURCE_PADS)
+
+#define IPU_ISYS_SHORT_PACKET_BUFFER_NUM	VIDEO_MAX_FRAME
+#define IPU_ISYS_SHORT_PACKET_WIDTH	32
+#define IPU_ISYS_SHORT_PACKET_FRAME_PACKETS	2
+#define IPU_ISYS_SHORT_PACKET_EXTRA_PACKETS	64
+#define IPU_ISYS_SHORT_PACKET_UNITSIZE	8
+#define IPU_ISYS_SHORT_PACKET_GENERAL_DT	0
+#define IPU_ISYS_SHORT_PACKET_PT		0
+#define IPU_ISYS_SHORT_PACKET_FT		0
+
+#define IPU_ISYS_SHORT_PACKET_STRIDE \
+	(IPU_ISYS_SHORT_PACKET_WIDTH * \
+	IPU_ISYS_SHORT_PACKET_UNITSIZE)
+#define IPU_ISYS_SHORT_PACKET_NUM(num_lines) \
+	((num_lines) * 2 + IPU_ISYS_SHORT_PACKET_FRAME_PACKETS + \
+	IPU_ISYS_SHORT_PACKET_EXTRA_PACKETS)
+#define IPU_ISYS_SHORT_PACKET_PKT_LINES(num_lines) \
+	DIV_ROUND_UP(IPU_ISYS_SHORT_PACKET_NUM(num_lines) * \
+	IPU_ISYS_SHORT_PACKET_UNITSIZE, \
+	IPU_ISYS_SHORT_PACKET_STRIDE)
+#define IPU_ISYS_SHORT_PACKET_BUF_SIZE(num_lines) \
+	(IPU_ISYS_SHORT_PACKET_WIDTH * \
+	IPU_ISYS_SHORT_PACKET_PKT_LINES(num_lines) * \
+	IPU_ISYS_SHORT_PACKET_UNITSIZE)
+
+#define IPU_ISYS_SHORT_PACKET_TRACE_MSG_NUMBER	256
+#define IPU_ISYS_SHORT_PACKET_TRACE_MSG_SIZE	16
+#define IPU_ISYS_SHORT_PACKET_TRACE_BUFFER_SIZE \
+	(IPU_ISYS_SHORT_PACKET_TRACE_MSG_NUMBER * \
+	IPU_ISYS_SHORT_PACKET_TRACE_MSG_SIZE)
+
+#define IPU_ISYS_SHORT_PACKET_FROM_RECEIVER	0
+#define IPU_ISYS_SHORT_PACKET_FROM_TUNIT		1
+
+#define IPU_ISYS_SHORT_PACKET_TRACE_MAX_TIMESHIFT 100
+#define IPU_ISYS_SHORT_PACKET_TRACE_EVENT_MASK	0x2082
+#define IPU_SKEW_CAL_LIMIT_HZ (1500000000ul / 2)
+
+#define CSI2_CSI_RX_DLY_CNT_TERMEN_CLANE_A		0
+#define CSI2_CSI_RX_DLY_CNT_TERMEN_CLANE_B		0
+#define CSI2_CSI_RX_DLY_CNT_SETTLE_CLANE_A		95
+#define CSI2_CSI_RX_DLY_CNT_SETTLE_CLANE_B		-8
+
+#define CSI2_CSI_RX_DLY_CNT_TERMEN_DLANE_A		0
+#define CSI2_CSI_RX_DLY_CNT_TERMEN_DLANE_B		0
+#define CSI2_CSI_RX_DLY_CNT_SETTLE_DLANE_A		85
+#define CSI2_CSI_RX_DLY_CNT_SETTLE_DLANE_B		-2
+
+#define IPU_EOF_TIMEOUT 300
+#define IPU_EOF_TIMEOUT_JIFFIES msecs_to_jiffies(IPU_EOF_TIMEOUT)
+
+/*
+ * struct ipu_isys_csi2
+ *
+ * @nlanes: number of lanes in the receiver
+ */
+struct ipu_isys_csi2 {
+	struct ipu_isys_csi2_pdata *pdata;
+	struct ipu_isys *isys;
+	struct ipu_isys_subdev asd;
+	struct ipu_isys_video av;
+	struct completion eof_completion;
+
+	void __iomem *base;
+	u32 receiver_errors;
+	unsigned int nlanes;
+	unsigned int index;
+	atomic_t sof_sequence;
+	bool in_frame;
+	bool wait_for_sync;
+
+	struct v4l2_ctrl *store_csi2_header;
+};
+
+struct ipu_isys_csi2_timing {
+	u32 ctermen;
+	u32 csettle;
+	u32 dtermen;
+	u32 dsettle;
+};
+
+/*
+ * This structure defines the MIPI packet header output
+ * from IPU MIPI receiver. Due to hardware conversion,
+ * this structure is not the same as defined in CSI-2 spec.
+ */
+struct ipu_isys_mipi_packet_header {
+	u32 word_count:16, dtype:13, sync:2, stype:1;
+	u32 sid:4, port_id:4, reserved:23, odd_even:1;
+} __packed;
+
+/*
+ * This structure defines the trace message content
+ * for CSI2 receiver monitor messages.
+ */
+struct ipu_isys_csi2_monitor_message {
+	u64 fe:1,
+	    fs:1,
+	    pe:1,
+	    ps:1,
+	    le:1,
+	    ls:1,
+	    reserved1:2,
+	    sequence:2,
+	    reserved2:2,
+	    flash_shutter:4,
+	    error_cause:12,
+	    fifo_overrun:1,
+	    crc_error:2,
+	    reserved3:1,
+	    timestamp_l:16,
+	    port:4, vc:2, reserved4:2, frame_sync:4, reserved5:4;
+	u64 reserved6:3,
+	    cmd:2, reserved7:1, monitor_id:7, reserved8:1, timestamp_h:50;
+} __packed;
+
+#define to_ipu_isys_csi2(sd) container_of(to_ipu_isys_subdev(sd), \
+					struct ipu_isys_csi2, asd)
+
+int ipu_isys_csi2_get_link_freq(struct ipu_isys_csi2 *csi2, __s64 *link_freq);
+int ipu_isys_csi2_init(struct ipu_isys_csi2 *csi2,
+		       struct ipu_isys *isys,
+		       void __iomem *base, unsigned int index);
+void ipu_isys_csi2_cleanup(struct ipu_isys_csi2 *csi2);
+struct ipu_isys_buffer *
+ipu_isys_csi2_get_short_packet_buffer(struct ipu_isys_pipeline *ip,
+				      struct ipu_isys_buffer_list *bl);
+void ipu_isys_csi2_sof_event(struct ipu_isys_csi2 *csi2);
+void ipu_isys_csi2_eof_event(struct ipu_isys_csi2 *csi2);
+void ipu_isys_csi2_wait_last_eof(struct ipu_isys_csi2 *csi2);
+
+/* interface for platform specific */
+int ipu_isys_csi2_set_stream(struct v4l2_subdev *sd,
+			     struct ipu_isys_csi2_timing timing,
+			     unsigned int nlanes, int enable);
+unsigned int ipu_isys_csi2_get_current_field(struct ipu_isys_pipeline *ip,
+					     unsigned int *timestamp);
+void ipu_isys_csi2_isr(struct ipu_isys_csi2 *csi2);
+void ipu_isys_csi2_error(struct ipu_isys_csi2 *csi2);
+
+#endif /* IPU_ISYS_CSI2_H */
diff -ruN a/drivers/media/pci/intel/ipu-isys.h b/drivers/media/pci/intel/ipu-isys.h
--- a/drivers/media/pci/intel/ipu-isys.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,236 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_ISYS_H
+#define IPU_ISYS_H
+
+#include <linux/pm_qos.h>
+#include <linux/spinlock.h>
+
+#include <media/v4l2-device.h>
+#include <media/media-device.h>
+
+#include <uapi/linux/ipu-isys.h>
+
+#include "ipu.h"
+#include "ipu-isys-media.h"
+#include "ipu-isys-csi2.h"
+#include "ipu-isys-csi2-be.h"
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+#include "ipu-isys-tpg.h"
+#endif
+#include "ipu-isys-video.h"
+#include "ipu-pdata.h"
+#include "ipu-fw-isys.h"
+#include "ipu-platform-isys.h"
+
+#define IPU_ISYS_2600_MEM_LINE_ALIGN	64
+
+/* for TPG */
+#define IPU_ISYS_FREQ		533000000UL
+
+/*
+ * Current message queue configuration. These must be big enough
+ * so that they never gets full. Queues are located in system memory
+ */
+#define IPU_ISYS_SIZE_RECV_QUEUE 40
+#define IPU_ISYS_SIZE_SEND_QUEUE 40
+#define IPU_ISYS_SIZE_PROXY_RECV_QUEUE 5
+#define IPU_ISYS_SIZE_PROXY_SEND_QUEUE 5
+#define IPU_ISYS_NUM_RECV_QUEUE 1
+
+/*
+ * Device close takes some time from last ack message to actual stopping
+ * of the SP processor. As long as the SP processor runs we can't proceed with
+ * clean up of resources.
+ */
+#define IPU_ISYS_OPEN_TIMEOUT_US		1000
+#define IPU_ISYS_OPEN_RETRY		1000
+#define IPU_ISYS_TURNOFF_DELAY_US		1000
+#define IPU_ISYS_TURNOFF_TIMEOUT		1000
+#define IPU_LIB_CALL_TIMEOUT_JIFFIES \
+	msecs_to_jiffies(IPU_LIB_CALL_TIMEOUT_MS)
+
+#define IPU_ISYS_CSI2_LONG_PACKET_HEADER_SIZE	32
+#define IPU_ISYS_CSI2_LONG_PACKET_FOOTER_SIZE	32
+
+#define IPU_ISYS_MIN_WIDTH		1U
+#define IPU_ISYS_MIN_HEIGHT		1U
+#define IPU_ISYS_MAX_WIDTH		16384U
+#define IPU_ISYS_MAX_HEIGHT		16384U
+
+#define NR_OF_CSI2_BE_SOC_DEV 8
+
+/* the threshold granularity is 2KB on IPU6 */
+#define IPU6_SRAM_GRANULRITY_SHIFT	11
+#define IPU6_SRAM_GRANULRITY_SIZE	2048
+/* the threshold granularity is 1KB on IPU6SE */
+#define IPU6SE_SRAM_GRANULRITY_SHIFT	10
+#define IPU6SE_SRAM_GRANULRITY_SIZE	1024
+
+struct task_struct;
+
+struct ltr_did {
+	union {
+		u32 value;
+		struct {
+			u8 val0;
+			u8 val1;
+			u8 val2;
+			u8 val3;
+		} bits;
+	} lut_ltr;
+	union {
+		u32 value;
+		struct {
+			u8 th0;
+			u8 th1;
+			u8 th2;
+			u8 th3;
+		} bits;
+	} lut_fill_time;
+};
+
+struct isys_iwake_watermark {
+	bool iwake_enabled;
+	bool force_iwake_disable;
+	u32 iwake_threshold;
+	u64 isys_pixelbuffer_datarate;
+	struct ltr_did ltrdid;
+	struct mutex mutex; /* protect whole struct */
+	struct ipu_isys *isys;
+	struct list_head video_list;
+};
+struct ipu_isys_sensor_info {
+	unsigned int vc1_data_start;
+	unsigned int vc1_data_end;
+	unsigned int vc0_data_start;
+	unsigned int vc0_data_end;
+	unsigned int vc1_pdaf_start;
+	unsigned int vc1_pdaf_end;
+	unsigned int sensor_metadata;
+};
+
+/*
+ * struct ipu_isys
+ *
+ * @media_dev: Media device
+ * @v4l2_dev: V4L2 device
+ * @adev: ISYS bus device
+ * @power: Is ISYS powered on or not?
+ * @isr_bits: Which bits does the ISR handle?
+ * @power_lock: Serialise access to power (power state in general)
+ * @csi2_rx_ctrl_cached: cached shared value between all CSI2 receivers
+ * @lock: serialise access to pipes
+ * @pipes: pipelines per stream ID
+ * @fwcom: fw communication layer private pointer
+ *         or optional external library private pointer
+ * @line_align: line alignment in memory
+ * @reset_needed: Isys requires d0i0->i3 transition
+ * @video_opened: total number of opened file handles on video nodes
+ * @mutex: serialise access isys video open/release related operations
+ * @stream_mutex: serialise stream start and stop, queueing requests
+ * @lib_mutex: optional external library mutex
+ * @pdata: platform data pointer
+ * @csi2: CSI-2 receivers
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+ * @tpg: test pattern generators
+#endif
+ * @csi2_be: CSI-2 back-ends
+ * @fw: ISYS firmware binary (unsecure firmware)
+ * @fw_sgt: fw scatterlist
+ * @pkg_dir: host pointer to pkg_dir
+ * @pkg_dir_dma_addr: I/O virtual address for pkg_dir
+ * @pkg_dir_size: size of pkg_dir in bytes
+ * @short_packet_source: select short packet capture mode
+ */
+struct ipu_isys {
+	struct media_device media_dev;
+	struct v4l2_device v4l2_dev;
+	struct ipu_bus_device *adev;
+
+	int power;
+	spinlock_t power_lock;	/* Serialise access to power */
+	u32 isr_csi2_bits;
+	u32 csi2_rx_ctrl_cached;
+	spinlock_t lock;	/* Serialise access to pipes */
+	struct ipu_isys_pipeline *pipes[IPU_ISYS_MAX_STREAMS];
+	void *fwcom;
+	unsigned int line_align;
+	bool reset_needed;
+	bool icache_prefetch;
+	bool csi2_cse_ipc_not_supported;
+	unsigned int video_opened;
+	unsigned int stream_opened;
+	struct ipu_isys_sensor_info sensor_info;
+	unsigned int sensor_types[N_IPU_FW_ISYS_SENSOR_TYPE];
+
+#ifdef CONFIG_DEBUG_FS
+	struct dentry *debugfsdir;
+#endif
+	struct mutex mutex;	/* Serialise isys video open/release related */
+	struct mutex stream_mutex;	/* Stream start, stop, queueing reqs */
+	struct mutex lib_mutex;	/* Serialise optional external library mutex */
+
+	struct ipu_isys_pdata *pdata;
+
+	struct ipu_isys_csi2 *csi2;
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	struct ipu_isys_tpg *tpg;
+#endif
+	struct ipu_isys_csi2_be csi2_be;
+	struct ipu_isys_csi2_be_soc csi2_be_soc[NR_OF_CSI2_BE_SOC_DEV];
+	const struct firmware *fw;
+	struct sg_table fw_sgt;
+
+	u64 *pkg_dir;
+	dma_addr_t pkg_dir_dma_addr;
+	unsigned int pkg_dir_size;
+
+	struct list_head requests;
+	struct pm_qos_request pm_qos;
+	unsigned int short_packet_source;
+	struct ipu_isys_csi2_monitor_message *short_packet_trace_buffer;
+	dma_addr_t short_packet_trace_buffer_dma_addr;
+	unsigned int short_packet_tracing_count;
+	struct mutex short_packet_tracing_mutex;	/* For tracing count */
+	u64 tsc_timer_base;
+	u64 tunit_timer_base;
+	spinlock_t listlock;	/* Protect framebuflist */
+	struct list_head framebuflist;
+	struct list_head framebuflist_fw;
+	struct v4l2_async_notifier notifier;
+	struct isys_iwake_watermark *iwake_watermark;
+
+};
+
+void update_watermark_setting(struct ipu_isys *isys);
+
+struct isys_fw_msgs {
+	union {
+		u64 dummy;
+		struct ipu_fw_isys_frame_buff_set_abi frame;
+		struct ipu_fw_isys_stream_cfg_data_abi stream;
+	} fw_msg;
+	struct list_head head;
+	dma_addr_t dma_addr;
+};
+
+#define to_frame_msg_buf(a) (&(a)->fw_msg.frame)
+#define to_stream_cfg_msg_buf(a) (&(a)->fw_msg.stream)
+#define to_dma_addr(a) ((a)->dma_addr)
+
+struct isys_fw_msgs *ipu_get_fw_msg_buf(struct ipu_isys_pipeline *ip);
+void ipu_put_fw_mgs_buf(struct ipu_isys *isys, u64 data);
+void ipu_cleanup_fw_msg_bufs(struct ipu_isys *isys);
+
+extern const struct v4l2_ioctl_ops ipu_isys_ioctl_ops;
+
+void isys_setup_hw(struct ipu_isys *isys);
+int isys_isr_one(struct ipu_bus_device *adev);
+irqreturn_t isys_isr(struct ipu_bus_device *adev);
+#ifdef IPU_ISYS_GPC
+int ipu_isys_gpc_init_debugfs(struct ipu_isys *isys);
+#endif
+
+#endif /* IPU_ISYS_H */
diff -ruN a/drivers/media/pci/intel/ipu-isys-media.h b/drivers/media/pci/intel/ipu-isys-media.h
--- a/drivers/media/pci/intel/ipu-isys-media.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-media.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,77 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2016 - 2020 Intel Corporation */
+
+#ifndef IPU_ISYS_MEDIA_H
+#define IPU_ISYS_MEDIA_H
+
+#include <linux/slab.h>
+#include <media/media-entity.h>
+
+struct __packed media_request_cmd {
+	__u32 cmd;
+	__u32 request;
+	__u32 flags;
+};
+
+struct __packed media_event_request_complete {
+	__u32 id;
+};
+
+#define MEDIA_EVENT_TYPE_REQUEST_COMPLETE	1
+
+struct __packed media_event {
+	__u32 type;
+	__u32 sequence;
+	__u32 reserved[4];
+
+	union {
+		struct media_event_request_complete req_complete;
+	};
+};
+
+enum media_device_request_state {
+	MEDIA_DEVICE_REQUEST_STATE_IDLE,
+	MEDIA_DEVICE_REQUEST_STATE_QUEUED,
+	MEDIA_DEVICE_REQUEST_STATE_DELETED,
+	MEDIA_DEVICE_REQUEST_STATE_COMPLETE,
+};
+
+struct media_kevent {
+	struct list_head list;
+	struct media_event ev;
+};
+
+struct media_device_request {
+	u32 id;
+	struct media_device *mdev;
+	struct file *filp;
+	struct media_kevent *kev;
+	struct kref kref;
+	struct list_head list;
+	struct list_head fh_list;
+	enum media_device_request_state state;
+	struct list_head data;
+	u32 flags;
+};
+
+static inline struct media_device_request *
+media_device_request_find(struct media_device *mdev, u16 reqid)
+{
+	return NULL;
+}
+
+static inline void media_device_request_get(struct media_device_request *req)
+{
+}
+
+static inline void media_device_request_put(struct media_device_request *req)
+{
+}
+
+static inline void
+media_device_request_complete(struct media_device *mdev,
+			      struct media_device_request *req)
+{
+}
+
+#endif /* IPU_ISYS_MEDIA_H */
diff -ruN a/drivers/media/pci/intel/ipu-isys-queue.c b/drivers/media/pci/intel/ipu-isys-queue.c
--- a/drivers/media/pci/intel/ipu-isys-queue.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-queue.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,1063 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2020 Intel Corporation
+
+#include <linux/completion.h>
+#include <linux/device.h>
+#include <linux/module.h>
+#include <linux/string.h>
+
+#include <media/media-entity.h>
+#include <media/videobuf2-dma-contig.h>
+#include <media/v4l2-ioctl.h>
+
+#include "ipu.h"
+#include "ipu-bus.h"
+#include "ipu-buttress.h"
+#include "ipu-isys.h"
+#include "ipu-isys-csi2.h"
+#include "ipu-isys-video.h"
+
+static bool wall_clock_ts_on;
+module_param(wall_clock_ts_on, bool, 0660);
+MODULE_PARM_DESC(wall_clock_ts_on, "Timestamp based on REALTIME clock");
+
+static int queue_setup(struct vb2_queue *q,
+		       unsigned int *num_buffers, unsigned int *num_planes,
+		       unsigned int sizes[],
+		       struct device *alloc_devs[])
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(q);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+	bool use_fmt = false;
+	unsigned int i;
+
+	/* num_planes == 0: we're being called through VIDIOC_REQBUFS */
+	if (!*num_planes) {
+		use_fmt = true;
+		*num_planes = av->mpix.num_planes;
+	}
+
+	for (i = 0; i < *num_planes; i++) {
+		if (use_fmt)
+			sizes[i] = av->mpix.plane_fmt[i].sizeimage;
+		alloc_devs[i] = aq->dev;
+		dev_dbg(&av->isys->adev->dev,
+			"%s: queue setup: plane %d size %u\n",
+			av->vdev.name, i, sizes[i]);
+	}
+
+	return 0;
+}
+
+static void ipu_isys_queue_lock(struct vb2_queue *q)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(q);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+
+	dev_dbg(&av->isys->adev->dev, "%s: queue lock\n", av->vdev.name);
+	mutex_lock(&av->mutex);
+}
+
+static void ipu_isys_queue_unlock(struct vb2_queue *q)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(q);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+
+	dev_dbg(&av->isys->adev->dev, "%s: queue unlock\n", av->vdev.name);
+	mutex_unlock(&av->mutex);
+}
+
+static int buf_init(struct vb2_buffer *vb)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+
+	dev_dbg(&av->isys->adev->dev, "buffer: %s: %s\n", av->vdev.name,
+		__func__);
+
+	if (aq->buf_init)
+		return aq->buf_init(vb);
+
+	return 0;
+}
+
+int ipu_isys_buf_prepare(struct vb2_buffer *vb)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+
+	dev_dbg(&av->isys->adev->dev,
+		"buffer: %s: configured size %u, buffer size %lu\n",
+		av->vdev.name,
+		av->mpix.plane_fmt[0].sizeimage, vb2_plane_size(vb, 0));
+
+	if (av->mpix.plane_fmt[0].sizeimage > vb2_plane_size(vb, 0))
+		return -EINVAL;
+
+	vb2_set_plane_payload(vb, 0, av->mpix.plane_fmt[0].bytesperline *
+			      av->mpix.height);
+	vb->planes[0].data_offset = av->line_header_length / BITS_PER_BYTE;
+
+	return 0;
+}
+
+static int buf_prepare(struct vb2_buffer *vb)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+	int rval;
+
+	if (av->isys->adev->isp->flr_done)
+		return -EIO;
+
+	rval = aq->buf_prepare(vb);
+	return rval;
+}
+
+static void buf_finish(struct vb2_buffer *vb)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+
+	dev_dbg(&av->isys->adev->dev, "buffer: %s: %s\n", av->vdev.name,
+		__func__);
+
+}
+
+static void buf_cleanup(struct vb2_buffer *vb)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+
+	dev_dbg(&av->isys->adev->dev, "buffer: %s: %s\n", av->vdev.name,
+		__func__);
+
+	if (aq->buf_cleanup)
+		return aq->buf_cleanup(vb);
+}
+
+/*
+ * Queue a buffer list back to incoming or active queues. The buffers
+ * are removed from the buffer list.
+ */
+void ipu_isys_buffer_list_queue(struct ipu_isys_buffer_list *bl,
+				unsigned long op_flags,
+				enum vb2_buffer_state state)
+{
+	struct ipu_isys_buffer *ib, *ib_safe;
+	unsigned long flags;
+	bool first = true;
+
+	if (!bl)
+		return;
+
+	WARN_ON(!bl->nbufs);
+	WARN_ON(op_flags & IPU_ISYS_BUFFER_LIST_FL_ACTIVE &&
+		op_flags & IPU_ISYS_BUFFER_LIST_FL_INCOMING);
+
+	list_for_each_entry_safe(ib, ib_safe, &bl->head, head) {
+		struct ipu_isys_video *av;
+
+		if (ib->type == IPU_ISYS_VIDEO_BUFFER) {
+			struct vb2_buffer *vb =
+			    ipu_isys_buffer_to_vb2_buffer(ib);
+			struct ipu_isys_queue *aq =
+			    vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+
+			av = ipu_isys_queue_to_video(aq);
+			spin_lock_irqsave(&aq->lock, flags);
+			list_del(&ib->head);
+			if (op_flags & IPU_ISYS_BUFFER_LIST_FL_ACTIVE)
+				list_add(&ib->head, &aq->active);
+			else if (op_flags & IPU_ISYS_BUFFER_LIST_FL_INCOMING)
+				list_add_tail(&ib->head, &aq->incoming);
+			spin_unlock_irqrestore(&aq->lock, flags);
+
+			if (op_flags & IPU_ISYS_BUFFER_LIST_FL_SET_STATE)
+				vb2_buffer_done(vb, state);
+		} else if (ib->type == IPU_ISYS_SHORT_PACKET_BUFFER) {
+			struct ipu_isys_private_buffer *pb =
+			    ipu_isys_buffer_to_private_buffer(ib);
+			struct ipu_isys_pipeline *ip = pb->ip;
+
+			av = container_of(ip, struct ipu_isys_video, ip);
+			spin_lock_irqsave(&ip->short_packet_queue_lock, flags);
+			list_del(&ib->head);
+			if (op_flags & IPU_ISYS_BUFFER_LIST_FL_ACTIVE)
+				list_add(&ib->head, &ip->short_packet_active);
+			else if (op_flags & IPU_ISYS_BUFFER_LIST_FL_INCOMING)
+				list_add(&ib->head, &ip->short_packet_incoming);
+			spin_unlock_irqrestore(&ip->short_packet_queue_lock,
+					       flags);
+		} else {
+			WARN_ON(1);
+			return;
+		}
+
+		if (first) {
+			dev_dbg(&av->isys->adev->dev,
+				"queue buf list %p flags %lx, s %d, %d bufs\n",
+				bl, op_flags, state, bl->nbufs);
+			first = false;
+		}
+
+		bl->nbufs--;
+	}
+
+	WARN_ON(bl->nbufs);
+}
+
+/*
+ * flush_firmware_streamon_fail() - Flush in cases where requests may
+ * have been queued to firmware and the *firmware streamon fails for a
+ * reason or another.
+ */
+static void flush_firmware_streamon_fail(struct ipu_isys_pipeline *ip)
+{
+	struct ipu_isys_video *pipe_av =
+	    container_of(ip, struct ipu_isys_video, ip);
+	struct ipu_isys_queue *aq;
+	unsigned long flags;
+
+	lockdep_assert_held(&pipe_av->mutex);
+
+	list_for_each_entry(aq, &ip->queues, node) {
+		struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+		struct ipu_isys_buffer *ib, *ib_safe;
+
+		spin_lock_irqsave(&aq->lock, flags);
+		list_for_each_entry_safe(ib, ib_safe, &aq->active, head) {
+			struct vb2_buffer *vb =
+			    ipu_isys_buffer_to_vb2_buffer(ib);
+
+			list_del(&ib->head);
+			if (av->streaming) {
+				dev_dbg(&av->isys->adev->dev,
+					"%s: queue buffer %u back to incoming\n",
+					av->vdev.name,
+					vb->index);
+				/* Queue already streaming, return to driver. */
+				list_add(&ib->head, &aq->incoming);
+				continue;
+			}
+			/* Queue not yet streaming, return to user. */
+			dev_dbg(&av->isys->adev->dev,
+				"%s: return %u back to videobuf2\n",
+				av->vdev.name,
+				vb->index);
+			vb2_buffer_done(ipu_isys_buffer_to_vb2_buffer(ib),
+					VB2_BUF_STATE_QUEUED);
+		}
+		spin_unlock_irqrestore(&aq->lock, flags);
+	}
+}
+
+/*
+ * Attempt obtaining a buffer list from the incoming queues, a list of
+ * buffers that contains one entry from each video buffer queue. If
+ * all queues have no buffers, the buffers that were already dequeued
+ * are returned to their queues.
+ */
+static int buffer_list_get(struct ipu_isys_pipeline *ip,
+			   struct ipu_isys_buffer_list *bl)
+{
+	struct ipu_isys_queue *aq;
+	struct ipu_isys_buffer *ib;
+	unsigned long flags;
+	int ret = 0;
+
+	bl->nbufs = 0;
+	INIT_LIST_HEAD(&bl->head);
+
+	list_for_each_entry(aq, &ip->queues, node) {
+		struct ipu_isys_buffer *ib;
+
+		spin_lock_irqsave(&aq->lock, flags);
+		if (list_empty(&aq->incoming)) {
+			spin_unlock_irqrestore(&aq->lock, flags);
+			ret = -ENODATA;
+			goto error;
+		}
+
+		ib = list_last_entry(&aq->incoming,
+				     struct ipu_isys_buffer, head);
+		if (ib->req) {
+			spin_unlock_irqrestore(&aq->lock, flags);
+			ret = -ENODATA;
+			goto error;
+		}
+
+		dev_dbg(&ip->isys->adev->dev, "buffer: %s: buffer %u\n",
+			ipu_isys_queue_to_video(aq)->vdev.name,
+			ipu_isys_buffer_to_vb2_buffer(ib)->index
+		    );
+		list_del(&ib->head);
+		list_add(&ib->head, &bl->head);
+		spin_unlock_irqrestore(&aq->lock, flags);
+
+		bl->nbufs++;
+	}
+
+	list_for_each_entry(ib, &bl->head, head) {
+		struct vb2_buffer *vb = ipu_isys_buffer_to_vb2_buffer(ib);
+
+		aq = vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+		if (aq->prepare_frame_buff_set)
+			aq->prepare_frame_buff_set(vb);
+	}
+
+	/* Get short packet buffer. */
+	if (ip->interlaced && ip->isys->short_packet_source ==
+	    IPU_ISYS_SHORT_PACKET_FROM_RECEIVER) {
+		ib = ipu_isys_csi2_get_short_packet_buffer(ip, bl);
+		if (!ib) {
+			ret = -ENODATA;
+			dev_err(&ip->isys->adev->dev,
+				"No more short packet buffers. Driver bug?");
+			WARN_ON(1);
+			goto error;
+		}
+		bl->nbufs++;
+	}
+
+	dev_dbg(&ip->isys->adev->dev, "get buffer list %p, %u buffers\n", bl,
+		bl->nbufs);
+	return ret;
+
+error:
+	if (!list_empty(&bl->head))
+		ipu_isys_buffer_list_queue(bl,
+					   IPU_ISYS_BUFFER_LIST_FL_INCOMING, 0);
+	return ret;
+}
+
+void
+ipu_isys_buffer_to_fw_frame_buff_pin(struct vb2_buffer *vb,
+				     struct ipu_fw_isys_frame_buff_set_abi *set)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+	struct ipu_isys_video *av = container_of(aq, struct ipu_isys_video, aq);
+
+	if (av->compression)
+		set->output_pins[aq->fw_output].compress = 1;
+
+	set->output_pins[aq->fw_output].addr =
+	    vb2_dma_contig_plane_dma_addr(vb, 0);
+	set->output_pins[aq->fw_output].out_buf_id =
+	    vb->index + 1;
+}
+
+/*
+ * Convert a buffer list to a isys fw ABI framebuffer set. The
+ * buffer list is not modified.
+ */
+#define IPU_ISYS_FRAME_NUM_THRESHOLD  (30)
+void
+ipu_isys_buffer_to_fw_frame_buff(struct ipu_fw_isys_frame_buff_set_abi *set,
+				 struct ipu_isys_pipeline *ip,
+				 struct ipu_isys_buffer_list *bl)
+{
+	struct ipu_isys_buffer *ib;
+
+	WARN_ON(!bl->nbufs);
+
+	set->send_irq_sof = 1;
+	set->send_resp_sof = 1;
+	set->send_irq_eof = 0;
+	set->send_resp_eof = 0;
+
+	if (ip->streaming)
+		set->send_irq_capture_ack = 0;
+	else
+		set->send_irq_capture_ack = 1;
+	set->send_irq_capture_done = 0;
+
+	set->send_resp_capture_ack = 1;
+	set->send_resp_capture_done = 1;
+	if (!ip->interlaced &&
+	    atomic_read(&ip->sequence) >= IPU_ISYS_FRAME_NUM_THRESHOLD) {
+		set->send_resp_capture_ack = 0;
+		set->send_resp_capture_done = 0;
+	}
+
+	list_for_each_entry(ib, &bl->head, head) {
+		if (ib->type == IPU_ISYS_VIDEO_BUFFER) {
+			struct vb2_buffer *vb =
+			    ipu_isys_buffer_to_vb2_buffer(ib);
+			struct ipu_isys_queue *aq =
+			    vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+
+			if (aq->fill_frame_buff_set_pin)
+				aq->fill_frame_buff_set_pin(vb, set);
+		} else if (ib->type == IPU_ISYS_SHORT_PACKET_BUFFER) {
+			struct ipu_isys_private_buffer *pb =
+			    ipu_isys_buffer_to_private_buffer(ib);
+			struct ipu_fw_isys_output_pin_payload_abi *output_pin =
+			    &set->output_pins[ip->short_packet_output_pin];
+
+			output_pin->addr = pb->dma_addr;
+			output_pin->out_buf_id = pb->index + 1;
+		} else {
+			WARN_ON(1);
+		}
+	}
+}
+
+/* Start streaming for real. The buffer list must be available. */
+static int ipu_isys_stream_start(struct ipu_isys_pipeline *ip,
+				 struct ipu_isys_buffer_list *bl, bool error)
+{
+	struct ipu_isys_video *pipe_av =
+	    container_of(ip, struct ipu_isys_video, ip);
+	struct ipu_isys_buffer_list __bl;
+	int rval;
+
+	mutex_lock(&pipe_av->isys->stream_mutex);
+
+	rval = ipu_isys_video_set_streaming(pipe_av, 1, bl);
+	if (rval) {
+		mutex_unlock(&pipe_av->isys->stream_mutex);
+		goto out_requeue;
+	}
+
+	ip->streaming = 1;
+
+	mutex_unlock(&pipe_av->isys->stream_mutex);
+
+	bl = &__bl;
+
+	do {
+		struct ipu_fw_isys_frame_buff_set_abi *buf = NULL;
+		struct isys_fw_msgs *msg;
+		enum ipu_fw_isys_send_type send_type =
+		    IPU_FW_ISYS_SEND_TYPE_STREAM_CAPTURE;
+
+		rval = buffer_list_get(ip, bl);
+		if (rval == -EINVAL)
+			goto out_requeue;
+		else if (rval < 0)
+			break;
+
+		msg = ipu_get_fw_msg_buf(ip);
+		if (!msg)
+			return -ENOMEM;
+
+		buf = to_frame_msg_buf(msg);
+
+		ipu_isys_buffer_to_fw_frame_buff(buf, ip, bl);
+
+		ipu_fw_isys_dump_frame_buff_set(&pipe_av->isys->adev->dev, buf,
+						ip->nr_output_pins);
+
+		ipu_isys_buffer_list_queue(bl,
+					   IPU_ISYS_BUFFER_LIST_FL_ACTIVE, 0);
+
+		rval = ipu_fw_isys_complex_cmd(pipe_av->isys,
+					       ip->stream_handle,
+					       buf, to_dma_addr(msg),
+					       sizeof(*buf),
+					       send_type);
+		ipu_put_fw_mgs_buf(pipe_av->isys, (uintptr_t)buf);
+	} while (!WARN_ON(rval));
+
+	return 0;
+
+out_requeue:
+	if (bl && bl->nbufs)
+		ipu_isys_buffer_list_queue(bl,
+					   IPU_ISYS_BUFFER_LIST_FL_INCOMING |
+					   (error ?
+					    IPU_ISYS_BUFFER_LIST_FL_SET_STATE :
+					    0),
+					   error ? VB2_BUF_STATE_ERROR :
+					   VB2_BUF_STATE_QUEUED);
+	flush_firmware_streamon_fail(ip);
+
+	return rval;
+}
+
+static void __buf_queue(struct vb2_buffer *vb, bool force)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+	struct ipu_isys_buffer *ib = vb2_buffer_to_ipu_isys_buffer(vb);
+	struct ipu_isys_pipeline *ip =
+	    to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	struct ipu_isys_buffer_list bl;
+
+	struct ipu_fw_isys_frame_buff_set_abi *buf = NULL;
+	struct isys_fw_msgs *msg;
+
+	struct ipu_isys_video *pipe_av =
+	    container_of(ip, struct ipu_isys_video, ip);
+	unsigned long flags;
+	unsigned int i;
+	int rval;
+
+	dev_dbg(&av->isys->adev->dev, "buffer: %s: buf_queue %u\n",
+		av->vdev.name,
+		vb->index
+	    );
+
+	for (i = 0; i < vb->num_planes; i++)
+		dev_dbg(&av->isys->adev->dev, "iova: plane %u iova 0x%x\n", i,
+			(u32)vb2_dma_contig_plane_dma_addr(vb, i));
+
+	spin_lock_irqsave(&aq->lock, flags);
+	list_add(&ib->head, &aq->incoming);
+	spin_unlock_irqrestore(&aq->lock, flags);
+
+	if (ib->req)
+		return;
+
+	if (!pipe_av || !vb->vb2_queue->streaming) {
+		dev_dbg(&av->isys->adev->dev,
+			"not pipe_av set, adding to incoming\n");
+		return;
+	}
+
+	mutex_unlock(&av->mutex);
+	mutex_lock(&pipe_av->mutex);
+
+	if (!force && ip->nr_streaming != ip->nr_queues) {
+		dev_dbg(&av->isys->adev->dev,
+			"not streaming yet, adding to incoming\n");
+		goto out;
+	}
+
+	/*
+	 * We just put one buffer to the incoming list of this queue
+	 * (above). Let's see whether all queues in the pipeline would
+	 * have a buffer.
+	 */
+	rval = buffer_list_get(ip, &bl);
+	if (rval < 0) {
+		if (rval == -EINVAL) {
+			dev_err(&av->isys->adev->dev,
+				"error: should not happen\n");
+			WARN_ON(1);
+		} else {
+			dev_dbg(&av->isys->adev->dev,
+				"not enough buffers available\n");
+		}
+		goto out;
+	}
+
+	msg = ipu_get_fw_msg_buf(ip);
+	if (!msg) {
+		rval = -ENOMEM;
+		goto out;
+	}
+	buf = to_frame_msg_buf(msg);
+
+	ipu_isys_buffer_to_fw_frame_buff(buf, ip, &bl);
+
+	ipu_fw_isys_dump_frame_buff_set(&pipe_av->isys->adev->dev, buf,
+					ip->nr_output_pins);
+
+	if (!ip->streaming) {
+		dev_dbg(&av->isys->adev->dev,
+			"got a buffer to start streaming!\n");
+		rval = ipu_isys_stream_start(ip, &bl, true);
+		if (rval)
+			dev_err(&av->isys->adev->dev,
+				"stream start failed.\n");
+		goto out;
+	}
+
+	/*
+	 * We must queue the buffers in the buffer list to the
+	 * appropriate video buffer queues BEFORE passing them to the
+	 * firmware since we could get a buffer event back before we
+	 * have queued them ourselves to the active queue.
+	 */
+	ipu_isys_buffer_list_queue(&bl, IPU_ISYS_BUFFER_LIST_FL_ACTIVE, 0);
+
+	rval = ipu_fw_isys_complex_cmd(pipe_av->isys,
+				       ip->stream_handle,
+				       buf, to_dma_addr(msg),
+				       sizeof(*buf),
+				       IPU_FW_ISYS_SEND_TYPE_STREAM_CAPTURE);
+	ipu_put_fw_mgs_buf(pipe_av->isys, (uintptr_t)buf);
+	if (!WARN_ON(rval < 0))
+		dev_dbg(&av->isys->adev->dev, "queued buffer\n");
+
+out:
+	mutex_unlock(&pipe_av->mutex);
+	mutex_lock(&av->mutex);
+}
+
+static void buf_queue(struct vb2_buffer *vb)
+{
+	__buf_queue(vb, false);
+}
+
+int ipu_isys_link_fmt_validate(struct ipu_isys_queue *aq)
+{
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+	struct v4l2_subdev_format fmt = { 0 };
+	struct media_pad *pad = media_entity_remote_pad(av->vdev.entity.pads);
+	struct v4l2_subdev *sd;
+	int rval;
+
+	if (!pad) {
+		dev_dbg(&av->isys->adev->dev,
+			"video node %s pad not connected\n", av->vdev.name);
+		return -ENOTCONN;
+	}
+
+	sd = media_entity_to_v4l2_subdev(pad->entity);
+
+	fmt.which = V4L2_SUBDEV_FORMAT_ACTIVE;
+	fmt.pad = pad->index;
+	rval = v4l2_subdev_call(sd, pad, get_fmt, NULL, &fmt);
+	if (rval)
+		return rval;
+
+	if (fmt.format.width != av->mpix.width ||
+	    fmt.format.height != av->mpix.height) {
+		dev_dbg(&av->isys->adev->dev,
+			"wrong width or height %ux%u (%ux%u expected)\n",
+			av->mpix.width, av->mpix.height,
+			fmt.format.width, fmt.format.height);
+		return -EINVAL;
+	}
+
+	if (fmt.format.field != av->mpix.field) {
+		dev_dbg(&av->isys->adev->dev,
+			"wrong field value 0x%8.8x (0x%8.8x expected)\n",
+			av->mpix.field, fmt.format.field);
+		return -EINVAL;
+	}
+
+	if (fmt.format.code != av->pfmt->code) {
+		dev_dbg(&av->isys->adev->dev,
+			"wrong media bus code 0x%8.8x (0x%8.8x expected)\n",
+			av->pfmt->code, fmt.format.code);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* Return buffers back to videobuf2. */
+static void return_buffers(struct ipu_isys_queue *aq,
+			   enum vb2_buffer_state state)
+{
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+	int reset_needed = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&aq->lock, flags);
+	while (!list_empty(&aq->incoming)) {
+		struct ipu_isys_buffer *ib = list_first_entry(&aq->incoming,
+							      struct
+							      ipu_isys_buffer,
+							      head);
+		struct vb2_buffer *vb = ipu_isys_buffer_to_vb2_buffer(ib);
+
+		list_del(&ib->head);
+		spin_unlock_irqrestore(&aq->lock, flags);
+
+		vb2_buffer_done(vb, state);
+
+		dev_dbg(&av->isys->adev->dev,
+			"%s: stop_streaming incoming %u\n",
+			ipu_isys_queue_to_video(vb2_queue_to_ipu_isys_queue
+						(vb->vb2_queue))->vdev.name,
+			vb->index);
+
+		spin_lock_irqsave(&aq->lock, flags);
+	}
+
+	/*
+	 * Something went wrong (FW crash / HW hang / not all buffers
+	 * returned from isys) if there are still buffers queued in active
+	 * queue. We have to clean up places a bit.
+	 */
+	while (!list_empty(&aq->active)) {
+		struct ipu_isys_buffer *ib = list_first_entry(&aq->active,
+							      struct
+							      ipu_isys_buffer,
+							      head);
+		struct vb2_buffer *vb = ipu_isys_buffer_to_vb2_buffer(ib);
+
+		list_del(&ib->head);
+		spin_unlock_irqrestore(&aq->lock, flags);
+
+		vb2_buffer_done(vb, state);
+
+		dev_warn(&av->isys->adev->dev, "%s: cleaning active queue %u\n",
+			 ipu_isys_queue_to_video(vb2_queue_to_ipu_isys_queue
+						 (vb->vb2_queue))->vdev.name,
+			 vb->index);
+
+		spin_lock_irqsave(&aq->lock, flags);
+		reset_needed = 1;
+	}
+
+	spin_unlock_irqrestore(&aq->lock, flags);
+
+	if (reset_needed) {
+		mutex_lock(&av->isys->mutex);
+		av->isys->reset_needed = true;
+		mutex_unlock(&av->isys->mutex);
+	}
+}
+
+static int start_streaming(struct vb2_queue *q, unsigned int count)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(q);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+	struct ipu_isys_video *pipe_av;
+	struct ipu_isys_pipeline *ip;
+	struct ipu_isys_buffer_list __bl, *bl = NULL;
+	bool first;
+	int rval;
+
+	dev_dbg(&av->isys->adev->dev,
+		"stream: %s: width %u, height %u, css pixelformat %u\n",
+		av->vdev.name, av->mpix.width, av->mpix.height,
+		av->pfmt->css_pixelformat);
+
+	mutex_lock(&av->isys->stream_mutex);
+
+	first = !av->vdev.entity.pipe;
+
+	if (first) {
+		rval = ipu_isys_video_prepare_streaming(av, 1);
+		if (rval)
+			goto out_return_buffers;
+	}
+
+	mutex_unlock(&av->isys->stream_mutex);
+
+	rval = aq->link_fmt_validate(aq);
+	if (rval) {
+		dev_dbg(&av->isys->adev->dev,
+			"%s: link format validation failed (%d)\n",
+			av->vdev.name, rval);
+		goto out_unprepare_streaming;
+	}
+
+	ip = to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	pipe_av = container_of(ip, struct ipu_isys_video, ip);
+	mutex_unlock(&av->mutex);
+
+	mutex_lock(&pipe_av->mutex);
+	ip->nr_streaming++;
+	dev_dbg(&av->isys->adev->dev, "queue %u of %u\n", ip->nr_streaming,
+		ip->nr_queues);
+	list_add(&aq->node, &ip->queues);
+	if (ip->nr_streaming != ip->nr_queues)
+		goto out;
+
+	if (list_empty(&av->isys->requests)) {
+		bl = &__bl;
+		rval = buffer_list_get(ip, bl);
+		if (rval == -EINVAL) {
+			goto out_stream_start;
+		} else if (rval < 0) {
+			dev_dbg(&av->isys->adev->dev,
+				"no request available, postponing streamon\n");
+			goto out;
+		}
+	}
+
+	rval = ipu_isys_stream_start(ip, bl, false);
+	if (rval)
+		goto out_stream_start;
+
+out:
+	mutex_unlock(&pipe_av->mutex);
+	mutex_lock(&av->mutex);
+
+	return 0;
+
+out_stream_start:
+	list_del(&aq->node);
+	ip->nr_streaming--;
+	mutex_unlock(&pipe_av->mutex);
+	mutex_lock(&av->mutex);
+
+out_unprepare_streaming:
+	mutex_lock(&av->isys->stream_mutex);
+	if (first)
+		ipu_isys_video_prepare_streaming(av, 0);
+
+out_return_buffers:
+	mutex_unlock(&av->isys->stream_mutex);
+	return_buffers(aq, VB2_BUF_STATE_QUEUED);
+
+	return rval;
+}
+
+static void stop_streaming(struct vb2_queue *q)
+{
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(q);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+	struct ipu_isys_pipeline *ip =
+	    to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	struct ipu_isys_video *pipe_av =
+	    container_of(ip, struct ipu_isys_video, ip);
+
+	if (pipe_av != av) {
+		mutex_unlock(&av->mutex);
+		mutex_lock(&pipe_av->mutex);
+	}
+
+	mutex_lock(&av->isys->stream_mutex);
+	if (ip->nr_streaming == ip->nr_queues && ip->streaming)
+		ipu_isys_video_set_streaming(av, 0, NULL);
+	if (ip->nr_streaming == 1)
+		ipu_isys_video_prepare_streaming(av, 0);
+	mutex_unlock(&av->isys->stream_mutex);
+
+	ip->nr_streaming--;
+	list_del(&aq->node);
+	ip->streaming = 0;
+
+	if (pipe_av != av) {
+		mutex_unlock(&pipe_av->mutex);
+		mutex_lock(&av->mutex);
+	}
+
+	return_buffers(aq, VB2_BUF_STATE_ERROR);
+}
+
+static unsigned int
+get_sof_sequence_by_timestamp(struct ipu_isys_pipeline *ip,
+			      struct ipu_fw_isys_resp_info_abi *info)
+{
+	struct ipu_isys *isys =
+	    container_of(ip, struct ipu_isys_video, ip)->isys;
+	u64 time = (u64)info->timestamp[1] << 32 | info->timestamp[0];
+	unsigned int i;
+
+	/*
+	 * The timestamp is invalid as no TSC in some FPGA platform,
+	 * so get the sequence from pipeline directly in this case.
+	 */
+	if (time == 0)
+		return atomic_read(&ip->sequence) - 1;
+
+	for (i = 0; i < IPU_ISYS_MAX_PARALLEL_SOF; i++)
+		if (time == ip->seq[i].timestamp) {
+			dev_dbg(&isys->adev->dev,
+				"sof: using seq nr %u for ts 0x%16.16llx\n",
+				ip->seq[i].sequence, time);
+			return ip->seq[i].sequence;
+		}
+
+	dev_dbg(&isys->adev->dev, "SOF: looking for 0x%16.16llx\n", time);
+	for (i = 0; i < IPU_ISYS_MAX_PARALLEL_SOF; i++)
+		dev_dbg(&isys->adev->dev,
+			"SOF: sequence %u, timestamp value 0x%16.16llx\n",
+			ip->seq[i].sequence, ip->seq[i].timestamp);
+	dev_dbg(&isys->adev->dev, "SOF sequence number not found\n");
+
+	return 0;
+}
+
+static u64 get_sof_ns_delta(struct ipu_isys_video *av,
+			    struct ipu_fw_isys_resp_info_abi *info)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(&av->isys->adev->dev);
+	struct ipu_device *isp = adev->isp;
+	u64 delta, tsc_now;
+
+	if (!ipu_buttress_tsc_read(isp, &tsc_now))
+		delta = tsc_now -
+		    ((u64)info->timestamp[1] << 32 | info->timestamp[0]);
+	else
+		delta = 0;
+
+	return ipu_buttress_tsc_ticks_to_ns(delta);
+}
+
+void
+ipu_isys_buf_calc_sequence_time(struct ipu_isys_buffer *ib,
+				struct ipu_fw_isys_resp_info_abi *info)
+{
+	struct vb2_buffer *vb = ipu_isys_buffer_to_vb2_buffer(ib);
+	struct vb2_v4l2_buffer *vbuf = to_vb2_v4l2_buffer(vb);
+	struct ipu_isys_queue *aq = vb2_queue_to_ipu_isys_queue(vb->vb2_queue);
+	struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+	struct device *dev = &av->isys->adev->dev;
+	struct ipu_isys_pipeline *ip =
+	    to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	u64 ns;
+	u32 sequence;
+
+	if (ip->has_sof) {
+		ns = (wall_clock_ts_on) ? ktime_get_real_ns() : ktime_get_ns();
+		ns -= get_sof_ns_delta(av, info);
+		sequence = get_sof_sequence_by_timestamp(ip, info);
+	} else {
+		ns = ((wall_clock_ts_on) ? ktime_get_real_ns() :
+		      ktime_get_ns());
+		sequence = (atomic_inc_return(&ip->sequence) - 1)
+		    / ip->nr_queues;
+	}
+
+	vbuf->vb2_buf.timestamp = ns;
+	vbuf->sequence = sequence;
+
+	dev_dbg(dev, "buf: %s: buffer done, CPU-timestamp:%lld, sequence:%d\n",
+		av->vdev.name, ktime_get_ns(), sequence);
+	dev_dbg(dev, "index:%d, vbuf timestamp:%lld, endl\n",
+		vb->index, vbuf->vb2_buf.timestamp);
+}
+
+void ipu_isys_queue_buf_done(struct ipu_isys_buffer *ib)
+{
+	struct vb2_buffer *vb = ipu_isys_buffer_to_vb2_buffer(ib);
+
+	if (atomic_read(&ib->str2mmio_flag)) {
+		vb2_buffer_done(vb, VB2_BUF_STATE_ERROR);
+		/*
+		 * Operation on buffer is ended with error and will be reported
+		 * to the userspace when it is de-queued
+		 */
+		atomic_set(&ib->str2mmio_flag, 0);
+	} else {
+		vb2_buffer_done(vb, VB2_BUF_STATE_DONE);
+	}
+}
+
+void ipu_isys_queue_buf_ready(struct ipu_isys_pipeline *ip,
+			      struct ipu_fw_isys_resp_info_abi *info)
+{
+	struct ipu_isys *isys =
+	    container_of(ip, struct ipu_isys_video, ip)->isys;
+	struct ipu_isys_queue *aq = ip->output_pins[info->pin_id].aq;
+	struct ipu_isys_buffer *ib;
+	struct vb2_buffer *vb;
+	unsigned long flags;
+	bool first = true;
+	struct vb2_v4l2_buffer *buf;
+
+	dev_dbg(&isys->adev->dev, "buffer: %s: received buffer %8.8x\n",
+		ipu_isys_queue_to_video(aq)->vdev.name, info->pin.addr);
+
+	spin_lock_irqsave(&aq->lock, flags);
+	if (list_empty(&aq->active)) {
+		spin_unlock_irqrestore(&aq->lock, flags);
+		dev_err(&isys->adev->dev, "active queue empty\n");
+		return;
+	}
+
+	list_for_each_entry_reverse(ib, &aq->active, head) {
+		dma_addr_t addr;
+
+		vb = ipu_isys_buffer_to_vb2_buffer(ib);
+		addr = vb2_dma_contig_plane_dma_addr(vb, 0);
+
+		if (info->pin.addr != addr) {
+			if (first)
+				dev_err(&isys->adev->dev,
+					"WARN: buffer address %pad expected!\n",
+					&addr);
+			first = false;
+			continue;
+		}
+
+		if (info->error_info.error ==
+		    IPU_FW_ISYS_ERROR_HW_REPORTED_STR2MMIO) {
+			/*
+			 * Check for error message:
+			 * 'IPU_FW_ISYS_ERROR_HW_REPORTED_STR2MMIO'
+			 */
+			atomic_set(&ib->str2mmio_flag, 1);
+		}
+		dev_dbg(&isys->adev->dev, "buffer: found buffer %pad\n", &addr);
+
+		buf = to_vb2_v4l2_buffer(vb);
+		buf->field = V4L2_FIELD_NONE;
+
+		list_del(&ib->head);
+		spin_unlock_irqrestore(&aq->lock, flags);
+
+		ipu_isys_buf_calc_sequence_time(ib, info);
+
+		/*
+		 * For interlaced buffers, the notification to user space
+		 * is postponed to capture_done event since the field
+		 * information is available only at that time.
+		 */
+		if (ip->interlaced) {
+			spin_lock_irqsave(&ip->short_packet_queue_lock, flags);
+			list_add(&ib->head, &ip->pending_interlaced_bufs);
+			spin_unlock_irqrestore(&ip->short_packet_queue_lock,
+					       flags);
+		} else {
+			ipu_isys_queue_buf_done(ib);
+		}
+
+		return;
+	}
+
+	dev_err(&isys->adev->dev,
+		"WARNING: cannot find a matching video buffer!\n");
+
+	spin_unlock_irqrestore(&aq->lock, flags);
+}
+
+void
+ipu_isys_queue_short_packet_ready(struct ipu_isys_pipeline *ip,
+				  struct ipu_fw_isys_resp_info_abi *info)
+{
+	struct ipu_isys *isys =
+	    container_of(ip, struct ipu_isys_video, ip)->isys;
+	unsigned long flags;
+
+	dev_dbg(&isys->adev->dev, "receive short packet buffer %8.8x\n",
+		info->pin.addr);
+	spin_lock_irqsave(&ip->short_packet_queue_lock, flags);
+	ip->cur_field = ipu_isys_csi2_get_current_field(ip, info->timestamp);
+	spin_unlock_irqrestore(&ip->short_packet_queue_lock, flags);
+}
+
+struct vb2_ops ipu_isys_queue_ops = {
+	.queue_setup = queue_setup,
+	.wait_prepare = ipu_isys_queue_unlock,
+	.wait_finish = ipu_isys_queue_lock,
+	.buf_init = buf_init,
+	.buf_prepare = buf_prepare,
+	.buf_finish = buf_finish,
+	.buf_cleanup = buf_cleanup,
+	.start_streaming = start_streaming,
+	.stop_streaming = stop_streaming,
+	.buf_queue = buf_queue,
+};
+
+int ipu_isys_queue_init(struct ipu_isys_queue *aq)
+{
+	struct ipu_isys *isys = ipu_isys_queue_to_video(aq)->isys;
+	int rval;
+
+	if (!aq->vbq.io_modes)
+		aq->vbq.io_modes = VB2_USERPTR | VB2_MMAP | VB2_DMABUF;
+	aq->vbq.drv_priv = aq;
+	aq->vbq.ops = &ipu_isys_queue_ops;
+	aq->vbq.mem_ops = &vb2_dma_contig_memops;
+	aq->vbq.timestamp_flags = (wall_clock_ts_on) ?
+	    V4L2_BUF_FLAG_TIMESTAMP_UNKNOWN : V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;
+
+	rval = vb2_queue_init(&aq->vbq);
+	if (rval)
+		return rval;
+
+	aq->dev = &isys->adev->dev;
+	aq->vbq.dev = &isys->adev->dev;
+	spin_lock_init(&aq->lock);
+	INIT_LIST_HEAD(&aq->active);
+	INIT_LIST_HEAD(&aq->incoming);
+
+	return 0;
+}
+
+void ipu_isys_queue_cleanup(struct ipu_isys_queue *aq)
+{
+	vb2_queue_release(&aq->vbq);
+}
diff -ruN a/drivers/media/pci/intel/ipu-isys-queue.h b/drivers/media/pci/intel/ipu-isys-queue.h
--- a/drivers/media/pci/intel/ipu-isys-queue.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-queue.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,142 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_ISYS_QUEUE_H
+#define IPU_ISYS_QUEUE_H
+
+#include <linux/list.h>
+#include <linux/spinlock.h>
+
+#include <media/videobuf2-v4l2.h>
+
+#include "ipu-isys-media.h"
+
+struct ipu_isys_video;
+struct ipu_isys_pipeline;
+struct ipu_fw_isys_resp_info_abi;
+struct ipu_fw_isys_frame_buff_set_abi;
+
+enum ipu_isys_buffer_type {
+	IPU_ISYS_VIDEO_BUFFER,
+	IPU_ISYS_SHORT_PACKET_BUFFER,
+};
+
+struct ipu_isys_queue {
+	struct list_head node;	/* struct ipu_isys_pipeline.queues */
+	struct vb2_queue vbq;
+	struct device *dev;
+	/*
+	 * @lock: serialise access to queued and pre_streamon_queued
+	 */
+	spinlock_t lock;
+	struct list_head active;
+	struct list_head incoming;
+	u32 css_pin_type;
+	unsigned int fw_output;
+	int (*buf_init)(struct vb2_buffer *vb);
+	void (*buf_cleanup)(struct vb2_buffer *vb);
+	int (*buf_prepare)(struct vb2_buffer *vb);
+	void (*prepare_frame_buff_set)(struct vb2_buffer *vb);
+	void (*fill_frame_buff_set_pin)(struct vb2_buffer *vb,
+					struct ipu_fw_isys_frame_buff_set_abi *
+					set);
+	int (*link_fmt_validate)(struct ipu_isys_queue *aq);
+};
+
+struct ipu_isys_buffer {
+	struct list_head head;
+	enum ipu_isys_buffer_type type;
+	struct list_head req_head;
+	struct media_device_request *req;
+	atomic_t str2mmio_flag;
+};
+
+struct ipu_isys_video_buffer {
+	struct vb2_v4l2_buffer vb_v4l2;
+	struct ipu_isys_buffer ib;
+};
+
+struct ipu_isys_private_buffer {
+	struct ipu_isys_buffer ib;
+	struct ipu_isys_pipeline *ip;
+	unsigned int index;
+	unsigned int bytesused;
+	dma_addr_t dma_addr;
+	void *buffer;
+};
+
+#define IPU_ISYS_BUFFER_LIST_FL_INCOMING	BIT(0)
+#define IPU_ISYS_BUFFER_LIST_FL_ACTIVE	BIT(1)
+#define IPU_ISYS_BUFFER_LIST_FL_SET_STATE	BIT(2)
+
+struct ipu_isys_buffer_list {
+	struct list_head head;
+	unsigned int nbufs;
+};
+
+#define vb2_queue_to_ipu_isys_queue(__vb2) \
+	container_of(__vb2, struct ipu_isys_queue, vbq)
+
+#define ipu_isys_to_isys_video_buffer(__ib) \
+	container_of(__ib, struct ipu_isys_video_buffer, ib)
+
+#define vb2_buffer_to_ipu_isys_video_buffer(__vb) \
+	container_of(to_vb2_v4l2_buffer(__vb), \
+	struct ipu_isys_video_buffer, vb_v4l2)
+
+#define ipu_isys_buffer_to_vb2_buffer(__ib) \
+	(&ipu_isys_to_isys_video_buffer(__ib)->vb_v4l2.vb2_buf)
+
+#define vb2_buffer_to_ipu_isys_buffer(__vb) \
+	(&vb2_buffer_to_ipu_isys_video_buffer(__vb)->ib)
+
+#define ipu_isys_buffer_to_private_buffer(__ib) \
+	container_of(__ib, struct ipu_isys_private_buffer, ib)
+
+struct ipu_isys_request {
+	struct media_device_request req;
+	/* serialise access to buffers */
+	spinlock_t lock;
+	struct list_head buffers;	/* struct ipu_isys_buffer.head */
+	bool dispatched;
+	/*
+	 * struct ipu_isys.requests;
+	 * struct ipu_isys_pipeline.struct.*
+	 */
+	struct list_head head;
+};
+
+#define to_ipu_isys_request(__req) \
+	container_of(__req, struct ipu_isys_request, req)
+
+int ipu_isys_buf_prepare(struct vb2_buffer *vb);
+
+void ipu_isys_buffer_list_queue(struct ipu_isys_buffer_list *bl,
+				unsigned long op_flags,
+				enum vb2_buffer_state state);
+struct ipu_isys_request *
+ipu_isys_next_queued_request(struct ipu_isys_pipeline *ip);
+void
+ipu_isys_buffer_to_fw_frame_buff_pin(struct vb2_buffer *vb,
+				     struct ipu_fw_isys_frame_buff_set_abi *
+				     set);
+void
+ipu_isys_buffer_to_fw_frame_buff(struct ipu_fw_isys_frame_buff_set_abi *set,
+				 struct ipu_isys_pipeline *ip,
+				 struct ipu_isys_buffer_list *bl);
+int ipu_isys_link_fmt_validate(struct ipu_isys_queue *aq);
+
+void
+ipu_isys_buf_calc_sequence_time(struct ipu_isys_buffer *ib,
+				struct ipu_fw_isys_resp_info_abi *info);
+void ipu_isys_queue_buf_done(struct ipu_isys_buffer *ib);
+void ipu_isys_queue_buf_ready(struct ipu_isys_pipeline *ip,
+			      struct ipu_fw_isys_resp_info_abi *info);
+void
+ipu_isys_queue_short_packet_ready(struct ipu_isys_pipeline *ip,
+				  struct ipu_fw_isys_resp_info_abi *inf);
+
+int ipu_isys_queue_init(struct ipu_isys_queue *aq);
+void ipu_isys_queue_cleanup(struct ipu_isys_queue *aq);
+
+#endif /* IPU_ISYS_QUEUE_H */
diff -ruN a/drivers/media/pci/intel/ipu-isys-subdev.c b/drivers/media/pci/intel/ipu-isys-subdev.c
--- a/drivers/media/pci/intel/ipu-isys-subdev.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-subdev.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,656 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2020 Intel Corporation
+
+#include <linux/types.h>
+#include <linux/videodev2.h>
+
+#include <media/media-entity.h>
+
+#include <uapi/linux/media-bus-format.h>
+
+#include "ipu-isys.h"
+#include "ipu-isys-video.h"
+#include "ipu-isys-subdev.h"
+
+unsigned int ipu_isys_mbus_code_to_bpp(u32 code)
+{
+	switch (code) {
+	case MEDIA_BUS_FMT_RGB888_1X24:
+		return 24;
+	case MEDIA_BUS_FMT_YUYV10_1X20:
+		return 20;
+	case MEDIA_BUS_FMT_Y10_1X10:
+	case MEDIA_BUS_FMT_RGB565_1X16:
+	case MEDIA_BUS_FMT_UYVY8_1X16:
+	case MEDIA_BUS_FMT_YUYV8_1X16:
+		return 16;
+	case MEDIA_BUS_FMT_SBGGR12_1X12:
+	case MEDIA_BUS_FMT_SGBRG12_1X12:
+	case MEDIA_BUS_FMT_SGRBG12_1X12:
+	case MEDIA_BUS_FMT_SRGGB12_1X12:
+		return 12;
+	case MEDIA_BUS_FMT_SBGGR10_1X10:
+	case MEDIA_BUS_FMT_SGBRG10_1X10:
+	case MEDIA_BUS_FMT_SGRBG10_1X10:
+	case MEDIA_BUS_FMT_SRGGB10_1X10:
+		return 10;
+	case MEDIA_BUS_FMT_SBGGR8_1X8:
+	case MEDIA_BUS_FMT_SGBRG8_1X8:
+	case MEDIA_BUS_FMT_SGRBG8_1X8:
+	case MEDIA_BUS_FMT_SRGGB8_1X8:
+	case MEDIA_BUS_FMT_SBGGR10_DPCM8_1X8:
+	case MEDIA_BUS_FMT_SGBRG10_DPCM8_1X8:
+	case MEDIA_BUS_FMT_SGRBG10_DPCM8_1X8:
+	case MEDIA_BUS_FMT_SRGGB10_DPCM8_1X8:
+		return 8;
+	default:
+		WARN_ON(1);
+		return -EINVAL;
+	}
+}
+
+unsigned int ipu_isys_mbus_code_to_mipi(u32 code)
+{
+	switch (code) {
+	case MEDIA_BUS_FMT_RGB565_1X16:
+		return IPU_ISYS_MIPI_CSI2_TYPE_RGB565;
+	case MEDIA_BUS_FMT_RGB888_1X24:
+		return IPU_ISYS_MIPI_CSI2_TYPE_RGB888;
+	case MEDIA_BUS_FMT_YUYV10_1X20:
+		return IPU_ISYS_MIPI_CSI2_TYPE_YUV422_10;
+	case MEDIA_BUS_FMT_UYVY8_1X16:
+	case MEDIA_BUS_FMT_YUYV8_1X16:
+		return IPU_ISYS_MIPI_CSI2_TYPE_YUV422_8;
+	case MEDIA_BUS_FMT_SBGGR12_1X12:
+	case MEDIA_BUS_FMT_SGBRG12_1X12:
+	case MEDIA_BUS_FMT_SGRBG12_1X12:
+	case MEDIA_BUS_FMT_SRGGB12_1X12:
+		return IPU_ISYS_MIPI_CSI2_TYPE_RAW12;
+	case MEDIA_BUS_FMT_Y10_1X10:
+	case MEDIA_BUS_FMT_SBGGR10_1X10:
+	case MEDIA_BUS_FMT_SGBRG10_1X10:
+	case MEDIA_BUS_FMT_SGRBG10_1X10:
+	case MEDIA_BUS_FMT_SRGGB10_1X10:
+		return IPU_ISYS_MIPI_CSI2_TYPE_RAW10;
+	case MEDIA_BUS_FMT_SBGGR8_1X8:
+	case MEDIA_BUS_FMT_SGBRG8_1X8:
+	case MEDIA_BUS_FMT_SGRBG8_1X8:
+	case MEDIA_BUS_FMT_SRGGB8_1X8:
+		return IPU_ISYS_MIPI_CSI2_TYPE_RAW8;
+	case MEDIA_BUS_FMT_SBGGR10_DPCM8_1X8:
+	case MEDIA_BUS_FMT_SGBRG10_DPCM8_1X8:
+	case MEDIA_BUS_FMT_SGRBG10_DPCM8_1X8:
+	case MEDIA_BUS_FMT_SRGGB10_DPCM8_1X8:
+		return IPU_ISYS_MIPI_CSI2_TYPE_USER_DEF(1);
+	default:
+		WARN_ON(1);
+		return -EINVAL;
+	}
+}
+
+enum ipu_isys_subdev_pixelorder ipu_isys_subdev_get_pixelorder(u32 code)
+{
+	switch (code) {
+	case MEDIA_BUS_FMT_SBGGR12_1X12:
+	case MEDIA_BUS_FMT_SBGGR10_1X10:
+	case MEDIA_BUS_FMT_SBGGR8_1X8:
+	case MEDIA_BUS_FMT_SBGGR10_DPCM8_1X8:
+		return IPU_ISYS_SUBDEV_PIXELORDER_BGGR;
+	case MEDIA_BUS_FMT_SGBRG12_1X12:
+	case MEDIA_BUS_FMT_SGBRG10_1X10:
+	case MEDIA_BUS_FMT_SGBRG8_1X8:
+	case MEDIA_BUS_FMT_SGBRG10_DPCM8_1X8:
+		return IPU_ISYS_SUBDEV_PIXELORDER_GBRG;
+	case MEDIA_BUS_FMT_SGRBG12_1X12:
+	case MEDIA_BUS_FMT_SGRBG10_1X10:
+	case MEDIA_BUS_FMT_SGRBG8_1X8:
+	case MEDIA_BUS_FMT_SGRBG10_DPCM8_1X8:
+		return IPU_ISYS_SUBDEV_PIXELORDER_GRBG;
+	case MEDIA_BUS_FMT_SRGGB12_1X12:
+	case MEDIA_BUS_FMT_SRGGB10_1X10:
+	case MEDIA_BUS_FMT_SRGGB8_1X8:
+	case MEDIA_BUS_FMT_SRGGB10_DPCM8_1X8:
+		return IPU_ISYS_SUBDEV_PIXELORDER_RGGB;
+	default:
+		WARN_ON(1);
+		return -EINVAL;
+	}
+}
+
+u32 ipu_isys_subdev_code_to_uncompressed(u32 sink_code)
+{
+	switch (sink_code) {
+	case MEDIA_BUS_FMT_SBGGR10_DPCM8_1X8:
+		return MEDIA_BUS_FMT_SBGGR10_1X10;
+	case MEDIA_BUS_FMT_SGBRG10_DPCM8_1X8:
+		return MEDIA_BUS_FMT_SGBRG10_1X10;
+	case MEDIA_BUS_FMT_SGRBG10_DPCM8_1X8:
+		return MEDIA_BUS_FMT_SGRBG10_1X10;
+	case MEDIA_BUS_FMT_SRGGB10_DPCM8_1X8:
+		return MEDIA_BUS_FMT_SRGGB10_1X10;
+	default:
+		return sink_code;
+	}
+}
+
+struct v4l2_mbus_framefmt *__ipu_isys_get_ffmt(struct v4l2_subdev *sd,
+					       struct v4l2_subdev_state *state,
+					       unsigned int pad,
+					       unsigned int which)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+
+	if (which == V4L2_SUBDEV_FORMAT_ACTIVE)
+		return &asd->ffmt[pad];
+	else
+		return v4l2_subdev_get_try_format(sd, state, pad);
+}
+
+struct v4l2_rect *__ipu_isys_get_selection(struct v4l2_subdev *sd,
+					   struct v4l2_subdev_state *state,
+					   unsigned int target,
+					   unsigned int pad, unsigned int which)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+
+	if (which == V4L2_SUBDEV_FORMAT_ACTIVE) {
+		switch (target) {
+		case V4L2_SEL_TGT_CROP:
+			return &asd->crop[pad];
+		case V4L2_SEL_TGT_COMPOSE:
+			return &asd->compose[pad];
+		}
+	} else {
+		switch (target) {
+		case V4L2_SEL_TGT_CROP:
+			return v4l2_subdev_get_try_crop(sd, state, pad);
+		case V4L2_SEL_TGT_COMPOSE:
+			return v4l2_subdev_get_try_compose(sd, state, pad);
+		}
+	}
+	WARN_ON(1);
+	return NULL;
+}
+
+static int target_valid(struct v4l2_subdev *sd, unsigned int target,
+			unsigned int pad)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+
+	switch (target) {
+	case V4L2_SEL_TGT_CROP:
+		return asd->valid_tgts[pad].crop;
+	case V4L2_SEL_TGT_COMPOSE:
+		return asd->valid_tgts[pad].compose;
+	default:
+		return 0;
+	}
+}
+
+int ipu_isys_subdev_fmt_propagate(struct v4l2_subdev *sd,
+				  struct v4l2_subdev_state *state,
+				  struct v4l2_mbus_framefmt *ffmt,
+				  struct v4l2_rect *r,
+				  enum isys_subdev_prop_tgt tgt,
+				  unsigned int pad, unsigned int which)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+	struct v4l2_mbus_framefmt **ffmts = NULL;
+	struct v4l2_rect **crops = NULL;
+	struct v4l2_rect **compose = NULL;
+	unsigned int i;
+	int rval = 0;
+
+	if (tgt == IPU_ISYS_SUBDEV_PROP_TGT_NR_OF)
+		return 0;
+
+	if (WARN_ON(pad >= sd->entity.num_pads))
+		return -EINVAL;
+
+	ffmts = kcalloc(sd->entity.num_pads,
+			sizeof(*ffmts), GFP_KERNEL);
+	if (!ffmts) {
+		rval = -ENOMEM;
+		goto out_subdev_fmt_propagate;
+	}
+	crops = kcalloc(sd->entity.num_pads,
+			sizeof(*crops), GFP_KERNEL);
+	if (!crops) {
+		rval = -ENOMEM;
+		goto out_subdev_fmt_propagate;
+	}
+	compose = kcalloc(sd->entity.num_pads,
+			  sizeof(*compose), GFP_KERNEL);
+	if (!compose) {
+		rval = -ENOMEM;
+		goto out_subdev_fmt_propagate;
+	}
+
+	for (i = 0; i < sd->entity.num_pads; i++) {
+		ffmts[i] = __ipu_isys_get_ffmt(sd, state, i, which);
+		crops[i] = __ipu_isys_get_selection(sd, state, V4L2_SEL_TGT_CROP,
+						    i, which);
+		compose[i] = __ipu_isys_get_selection(sd, state,
+						      V4L2_SEL_TGT_COMPOSE,
+						      i, which);
+	}
+
+	switch (tgt) {
+	case IPU_ISYS_SUBDEV_PROP_TGT_SINK_FMT:
+		crops[pad]->left = 0;
+		crops[pad]->top = 0;
+		crops[pad]->width = ffmt->width;
+		crops[pad]->height = ffmt->height;
+		rval = ipu_isys_subdev_fmt_propagate(sd, state, ffmt, crops[pad],
+						     tgt + 1, pad, which);
+		goto out_subdev_fmt_propagate;
+	case IPU_ISYS_SUBDEV_PROP_TGT_SINK_CROP:
+		if (WARN_ON(sd->entity.pads[pad].flags & MEDIA_PAD_FL_SOURCE))
+			goto out_subdev_fmt_propagate;
+
+		compose[pad]->left = 0;
+		compose[pad]->top = 0;
+		compose[pad]->width = r->width;
+		compose[pad]->height = r->height;
+		rval = ipu_isys_subdev_fmt_propagate(sd, state, ffmt,
+						     compose[pad], tgt + 1,
+						     pad, which);
+		goto out_subdev_fmt_propagate;
+	case IPU_ISYS_SUBDEV_PROP_TGT_SINK_COMPOSE:
+		if (WARN_ON(sd->entity.pads[pad].flags & MEDIA_PAD_FL_SOURCE)) {
+			rval = -EINVAL;
+			goto out_subdev_fmt_propagate;
+		}
+
+		for (i = 1; i < sd->entity.num_pads; i++) {
+			if (!(sd->entity.pads[i].flags &
+					MEDIA_PAD_FL_SOURCE))
+				continue;
+
+			compose[i]->left = 0;
+			compose[i]->top = 0;
+			compose[i]->width = r->width;
+			compose[i]->height = r->height;
+			rval = ipu_isys_subdev_fmt_propagate(sd, state,
+							     ffmt,
+							     compose[i],
+							     tgt + 1, i,
+							     which);
+			if (rval)
+				goto out_subdev_fmt_propagate;
+		}
+		goto out_subdev_fmt_propagate;
+	case IPU_ISYS_SUBDEV_PROP_TGT_SOURCE_COMPOSE:
+		if (WARN_ON(sd->entity.pads[pad].flags & MEDIA_PAD_FL_SINK)) {
+			rval = -EINVAL;
+			goto out_subdev_fmt_propagate;
+		}
+
+		crops[pad]->left = 0;
+		crops[pad]->top = 0;
+		crops[pad]->width = r->width;
+		crops[pad]->height = r->height;
+		rval = ipu_isys_subdev_fmt_propagate(sd, state, ffmt,
+						     crops[pad], tgt + 1,
+						     pad, which);
+		goto out_subdev_fmt_propagate;
+	case IPU_ISYS_SUBDEV_PROP_TGT_SOURCE_CROP:{
+			struct v4l2_subdev_format fmt = {
+				.which = which,
+				.pad = pad,
+				.format = {
+					.width = r->width,
+					.height = r->height,
+					/*
+					 * Either use the code from sink pad
+					 * or the current one.
+					 */
+					.code = ffmt ? ffmt->code :
+						       ffmts[pad]->code,
+					.field = ffmt ? ffmt->field :
+							ffmts[pad]->field,
+				},
+			};
+
+			asd->set_ffmt(sd, state, &fmt);
+			goto out_subdev_fmt_propagate;
+		}
+	}
+
+out_subdev_fmt_propagate:
+	kfree(ffmts);
+	kfree(crops);
+	kfree(compose);
+	return rval;
+}
+
+int ipu_isys_subdev_set_ffmt_default(struct v4l2_subdev *sd,
+				     struct v4l2_subdev_state *state,
+				     struct v4l2_subdev_format *fmt)
+{
+	struct v4l2_mbus_framefmt *ffmt =
+		__ipu_isys_get_ffmt(sd, state, fmt->pad, fmt->which);
+
+	/* No propagation for non-zero pads. */
+	if (fmt->pad) {
+		struct v4l2_mbus_framefmt *sink_ffmt =
+			__ipu_isys_get_ffmt(sd, state, 0, fmt->which);
+
+		ffmt->width = sink_ffmt->width;
+		ffmt->height = sink_ffmt->height;
+		ffmt->code = sink_ffmt->code;
+		ffmt->field = sink_ffmt->field;
+
+		return 0;
+	}
+
+	ffmt->width = fmt->format.width;
+	ffmt->height = fmt->format.height;
+	ffmt->code = fmt->format.code;
+	ffmt->field = fmt->format.field;
+
+	return ipu_isys_subdev_fmt_propagate(sd, state, &fmt->format, NULL,
+					     IPU_ISYS_SUBDEV_PROP_TGT_SINK_FMT,
+					     fmt->pad, fmt->which);
+}
+
+int __ipu_isys_subdev_set_ffmt(struct v4l2_subdev *sd,
+			       struct v4l2_subdev_state *state,
+			       struct v4l2_subdev_format *fmt)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+	struct v4l2_mbus_framefmt *ffmt =
+		__ipu_isys_get_ffmt(sd, state, fmt->pad, fmt->which);
+	u32 code = asd->supported_codes[fmt->pad][0];
+	unsigned int i;
+
+	WARN_ON(!mutex_is_locked(&asd->mutex));
+
+	fmt->format.width = clamp(fmt->format.width, IPU_ISYS_MIN_WIDTH,
+				  IPU_ISYS_MAX_WIDTH);
+	fmt->format.height = clamp(fmt->format.height,
+				   IPU_ISYS_MIN_HEIGHT, IPU_ISYS_MAX_HEIGHT);
+
+	for (i = 0; asd->supported_codes[fmt->pad][i]; i++) {
+		if (asd->supported_codes[fmt->pad][i] == fmt->format.code) {
+			code = asd->supported_codes[fmt->pad][i];
+			break;
+		}
+	}
+
+	fmt->format.code = code;
+
+	asd->set_ffmt(sd, state, fmt);
+
+	fmt->format = *ffmt;
+
+	return 0;
+}
+
+int ipu_isys_subdev_set_ffmt(struct v4l2_subdev *sd,
+			     struct v4l2_subdev_state *state,
+			     struct v4l2_subdev_format *fmt)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+	int rval;
+
+	mutex_lock(&asd->mutex);
+	rval = __ipu_isys_subdev_set_ffmt(sd, state, fmt);
+	mutex_unlock(&asd->mutex);
+
+	return rval;
+}
+
+int ipu_isys_subdev_get_ffmt(struct v4l2_subdev *sd,
+			     struct v4l2_subdev_state *state,
+			     struct v4l2_subdev_format *fmt)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+
+	mutex_lock(&asd->mutex);
+	fmt->format = *__ipu_isys_get_ffmt(sd, state, fmt->pad,
+					   fmt->which);
+	mutex_unlock(&asd->mutex);
+
+	return 0;
+}
+
+int ipu_isys_subdev_set_sel(struct v4l2_subdev *sd,
+			    struct v4l2_subdev_state *state,
+			    struct v4l2_subdev_selection *sel)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+	struct media_pad *pad = &asd->sd.entity.pads[sel->pad];
+	struct v4l2_rect *r, __r = { 0 };
+	unsigned int tgt;
+
+	if (!target_valid(sd, sel->target, sel->pad))
+		return -EINVAL;
+
+	switch (sel->target) {
+	case V4L2_SEL_TGT_CROP:
+		if (pad->flags & MEDIA_PAD_FL_SINK) {
+			struct v4l2_mbus_framefmt *ffmt =
+				__ipu_isys_get_ffmt(sd, state, sel->pad,
+						    sel->which);
+
+			__r.width = ffmt->width;
+			__r.height = ffmt->height;
+			r = &__r;
+			tgt = IPU_ISYS_SUBDEV_PROP_TGT_SINK_CROP;
+		} else {
+			/* 0 is the sink pad. */
+			r = __ipu_isys_get_selection(sd, state, sel->target, 0,
+						     sel->which);
+			tgt = IPU_ISYS_SUBDEV_PROP_TGT_SOURCE_CROP;
+		}
+
+		break;
+	case V4L2_SEL_TGT_COMPOSE:
+		if (pad->flags & MEDIA_PAD_FL_SINK) {
+			r = __ipu_isys_get_selection(sd, state, V4L2_SEL_TGT_CROP,
+						     sel->pad, sel->which);
+			tgt = IPU_ISYS_SUBDEV_PROP_TGT_SINK_COMPOSE;
+		} else {
+			r = __ipu_isys_get_selection(sd, state,
+						     V4L2_SEL_TGT_COMPOSE, 0,
+						     sel->which);
+			tgt = IPU_ISYS_SUBDEV_PROP_TGT_SOURCE_COMPOSE;
+		}
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	sel->r.width = clamp(sel->r.width, IPU_ISYS_MIN_WIDTH, r->width);
+	sel->r.height = clamp(sel->r.height, IPU_ISYS_MIN_HEIGHT, r->height);
+	*__ipu_isys_get_selection(sd, state, sel->target, sel->pad,
+				  sel->which) = sel->r;
+	return ipu_isys_subdev_fmt_propagate(sd, state, NULL, &sel->r, tgt,
+					     sel->pad, sel->which);
+}
+
+int ipu_isys_subdev_get_sel(struct v4l2_subdev *sd,
+			    struct v4l2_subdev_state *state,
+			    struct v4l2_subdev_selection *sel)
+{
+	if (!target_valid(sd, sel->target, sel->pad))
+		return -EINVAL;
+
+	sel->r = *__ipu_isys_get_selection(sd, state, sel->target,
+					   sel->pad, sel->which);
+
+	return 0;
+}
+
+int ipu_isys_subdev_enum_mbus_code(struct v4l2_subdev *sd,
+				   struct v4l2_subdev_state *state,
+				   struct v4l2_subdev_mbus_code_enum *code)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+	const u32 *supported_codes = asd->supported_codes[code->pad];
+	u32 index;
+
+	for (index = 0; supported_codes[index]; index++) {
+		if (index == code->index) {
+			code->code = supported_codes[index];
+			return 0;
+		}
+	}
+
+	return -EINVAL;
+}
+
+/*
+ * Besides validating the link, figure out the external pad and the
+ * ISYS FW ABI source.
+ */
+int ipu_isys_subdev_link_validate(struct v4l2_subdev *sd,
+				  struct media_link *link,
+				  struct v4l2_subdev_format *source_fmt,
+				  struct v4l2_subdev_format *sink_fmt)
+{
+	struct v4l2_subdev *source_sd =
+	    media_entity_to_v4l2_subdev(link->source->entity);
+	struct ipu_isys_pipeline *ip = container_of(sd->entity.pipe,
+						    struct ipu_isys_pipeline,
+						    pipe);
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+
+	if (!source_sd)
+		return -ENODEV;
+	if (strncmp(source_sd->name, IPU_ISYS_ENTITY_PREFIX,
+		    strlen(IPU_ISYS_ENTITY_PREFIX)) != 0) {
+		/*
+		 * source_sd isn't ours --- sd must be the external
+		 * sub-device.
+		 */
+		ip->external = link->source;
+		ip->source = to_ipu_isys_subdev(sd)->source;
+		dev_dbg(&asd->isys->adev->dev, "%s: using source %d\n",
+			sd->entity.name, ip->source);
+	} else if (source_sd->entity.num_pads == 1) {
+		/* All internal sources have a single pad. */
+		ip->external = link->source;
+		ip->source = to_ipu_isys_subdev(source_sd)->source;
+
+		dev_dbg(&asd->isys->adev->dev, "%s: using source %d\n",
+			sd->entity.name, ip->source);
+	}
+
+	if (asd->isl_mode != IPU_ISL_OFF)
+		ip->isl_mode = asd->isl_mode;
+
+	return v4l2_subdev_link_validate_default(sd, link, source_fmt,
+						 sink_fmt);
+}
+
+int ipu_isys_subdev_open(struct v4l2_subdev *sd, struct v4l2_subdev_fh *fh)
+{
+	struct ipu_isys_subdev *asd = to_ipu_isys_subdev(sd);
+	unsigned int i;
+
+	mutex_lock(&asd->mutex);
+
+	for (i = 0; i < asd->sd.entity.num_pads; i++) {
+		struct v4l2_mbus_framefmt *try_fmt =
+			v4l2_subdev_get_try_format(sd, fh->state, i);
+		struct v4l2_rect *try_crop =
+			v4l2_subdev_get_try_crop(sd, fh->state, i);
+		struct v4l2_rect *try_compose =
+			v4l2_subdev_get_try_compose(sd, fh->state, i);
+
+		*try_fmt = asd->ffmt[i];
+		*try_crop = asd->crop[i];
+		*try_compose = asd->compose[i];
+	}
+
+	mutex_unlock(&asd->mutex);
+
+	return 0;
+}
+
+int ipu_isys_subdev_close(struct v4l2_subdev *sd, struct v4l2_subdev_fh *fh)
+{
+	return 0;
+}
+
+int ipu_isys_subdev_init(struct ipu_isys_subdev *asd,
+			 struct v4l2_subdev_ops *ops,
+			 unsigned int nr_ctrls,
+			 unsigned int num_pads,
+			 unsigned int num_source,
+			 unsigned int num_sink,
+			 unsigned int sd_flags)
+{
+	int rval = -EINVAL;
+
+	mutex_init(&asd->mutex);
+
+	v4l2_subdev_init(&asd->sd, ops);
+
+	asd->sd.flags |= V4L2_SUBDEV_FL_HAS_DEVNODE | sd_flags;
+	asd->sd.owner = THIS_MODULE;
+	asd->sd.entity.function = MEDIA_ENT_F_VID_IF_BRIDGE;
+
+	asd->nsources = num_source;
+	asd->nsinks = num_sink;
+
+	asd->pad = devm_kcalloc(&asd->isys->adev->dev, num_pads,
+				sizeof(*asd->pad), GFP_KERNEL);
+
+	asd->ffmt = devm_kcalloc(&asd->isys->adev->dev, num_pads,
+				 sizeof(*asd->ffmt), GFP_KERNEL);
+
+	asd->crop = devm_kcalloc(&asd->isys->adev->dev, num_pads,
+				 sizeof(*asd->crop), GFP_KERNEL);
+
+	asd->compose = devm_kcalloc(&asd->isys->adev->dev, num_pads,
+				    sizeof(*asd->compose), GFP_KERNEL);
+
+	asd->valid_tgts = devm_kcalloc(&asd->isys->adev->dev, num_pads,
+				       sizeof(*asd->valid_tgts), GFP_KERNEL);
+	if (!asd->pad || !asd->ffmt || !asd->crop || !asd->compose ||
+	    !asd->valid_tgts)
+		return -ENOMEM;
+
+	rval = media_entity_pads_init(&asd->sd.entity, num_pads, asd->pad);
+	if (rval)
+		goto out_mutex_destroy;
+
+	if (asd->ctrl_init) {
+		rval = v4l2_ctrl_handler_init(&asd->ctrl_handler, nr_ctrls);
+		if (rval)
+			goto out_media_entity_cleanup;
+
+		asd->ctrl_init(&asd->sd);
+		if (asd->ctrl_handler.error) {
+			rval = asd->ctrl_handler.error;
+			goto out_v4l2_ctrl_handler_free;
+		}
+
+		asd->sd.ctrl_handler = &asd->ctrl_handler;
+	}
+
+	asd->source = -1;
+
+	return 0;
+
+out_v4l2_ctrl_handler_free:
+	v4l2_ctrl_handler_free(&asd->ctrl_handler);
+
+out_media_entity_cleanup:
+	media_entity_cleanup(&asd->sd.entity);
+
+out_mutex_destroy:
+	mutex_destroy(&asd->mutex);
+
+	return rval;
+}
+
+void ipu_isys_subdev_cleanup(struct ipu_isys_subdev *asd)
+{
+	media_entity_cleanup(&asd->sd.entity);
+	v4l2_ctrl_handler_free(&asd->ctrl_handler);
+	mutex_destroy(&asd->mutex);
+}
diff -ruN a/drivers/media/pci/intel/ipu-isys-subdev.h b/drivers/media/pci/intel/ipu-isys-subdev.h
--- a/drivers/media/pci/intel/ipu-isys-subdev.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-subdev.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,152 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_ISYS_SUBDEV_H
+#define IPU_ISYS_SUBDEV_H
+
+#include <linux/mutex.h>
+
+#include <media/media-entity.h>
+#include <media/v4l2-device.h>
+#include <media/v4l2-ctrls.h>
+
+#include "ipu-isys-queue.h"
+
+#define IPU_ISYS_MIPI_CSI2_TYPE_NULL	0x10
+#define IPU_ISYS_MIPI_CSI2_TYPE_BLANKING	0x11
+#define IPU_ISYS_MIPI_CSI2_TYPE_EMBEDDED8	0x12
+#define IPU_ISYS_MIPI_CSI2_TYPE_YUV422_8	0x1e
+#define IPU_ISYS_MIPI_CSI2_TYPE_YUV422_10	0x1f
+#define IPU_ISYS_MIPI_CSI2_TYPE_RGB565	0x22
+#define IPU_ISYS_MIPI_CSI2_TYPE_RGB888	0x24
+#define IPU_ISYS_MIPI_CSI2_TYPE_RAW6	0x28
+#define IPU_ISYS_MIPI_CSI2_TYPE_RAW7	0x29
+#define IPU_ISYS_MIPI_CSI2_TYPE_RAW8	0x2a
+#define IPU_ISYS_MIPI_CSI2_TYPE_RAW10	0x2b
+#define IPU_ISYS_MIPI_CSI2_TYPE_RAW12	0x2c
+#define IPU_ISYS_MIPI_CSI2_TYPE_RAW14	0x2d
+/* 1-8 */
+#define IPU_ISYS_MIPI_CSI2_TYPE_USER_DEF(i)	(0x30 + (i) - 1)
+
+#define FMT_ENTRY (struct ipu_isys_fmt_entry [])
+
+enum isys_subdev_prop_tgt {
+	IPU_ISYS_SUBDEV_PROP_TGT_SINK_FMT,
+	IPU_ISYS_SUBDEV_PROP_TGT_SINK_CROP,
+	IPU_ISYS_SUBDEV_PROP_TGT_SINK_COMPOSE,
+	IPU_ISYS_SUBDEV_PROP_TGT_SOURCE_COMPOSE,
+	IPU_ISYS_SUBDEV_PROP_TGT_SOURCE_CROP,
+};
+
+#define	IPU_ISYS_SUBDEV_PROP_TGT_NR_OF \
+	(IPU_ISYS_SUBDEV_PROP_TGT_SOURCE_CROP + 1)
+
+enum ipu_isl_mode {
+	IPU_ISL_OFF = 0,	/* SOC BE */
+	IPU_ISL_CSI2_BE,	/* RAW BE */
+};
+
+enum ipu_be_mode {
+	IPU_BE_RAW = 0,
+	IPU_BE_SOC
+};
+
+enum ipu_isys_subdev_pixelorder {
+	IPU_ISYS_SUBDEV_PIXELORDER_BGGR = 0,
+	IPU_ISYS_SUBDEV_PIXELORDER_GBRG,
+	IPU_ISYS_SUBDEV_PIXELORDER_GRBG,
+	IPU_ISYS_SUBDEV_PIXELORDER_RGGB,
+};
+
+struct ipu_isys;
+
+struct ipu_isys_subdev {
+	/* Serialise access to any other field in the struct */
+	struct mutex mutex;
+	struct v4l2_subdev sd;
+	struct ipu_isys *isys;
+	u32 const *const *supported_codes;
+	struct media_pad *pad;
+	struct v4l2_mbus_framefmt *ffmt;
+	struct v4l2_rect *crop;
+	struct v4l2_rect *compose;
+	unsigned int nsinks;
+	unsigned int nsources;
+	struct v4l2_ctrl_handler ctrl_handler;
+	void (*ctrl_init)(struct v4l2_subdev *sd);
+	void (*set_ffmt)(struct v4l2_subdev *sd,
+			 struct v4l2_subdev_state *state,
+			 struct v4l2_subdev_format *fmt);
+	struct {
+		bool crop;
+		bool compose;
+	} *valid_tgts;
+	enum ipu_isl_mode isl_mode;
+	enum ipu_be_mode be_mode;
+	int source;	/* SSI stream source; -1 if unset */
+};
+
+#define to_ipu_isys_subdev(__sd) \
+	container_of(__sd, struct ipu_isys_subdev, sd)
+
+struct v4l2_mbus_framefmt *__ipu_isys_get_ffmt(struct v4l2_subdev *sd,
+					       struct v4l2_subdev_state *state,
+					       unsigned int pad,
+					       unsigned int which);
+
+unsigned int ipu_isys_mbus_code_to_bpp(u32 code);
+unsigned int ipu_isys_mbus_code_to_mipi(u32 code);
+u32 ipu_isys_subdev_code_to_uncompressed(u32 sink_code);
+
+enum ipu_isys_subdev_pixelorder ipu_isys_subdev_get_pixelorder(u32 code);
+
+int ipu_isys_subdev_fmt_propagate(struct v4l2_subdev *sd,
+				  struct v4l2_subdev_state *state,
+				  struct v4l2_mbus_framefmt *ffmt,
+				  struct v4l2_rect *r,
+				  enum isys_subdev_prop_tgt tgt,
+				  unsigned int pad, unsigned int which);
+
+int ipu_isys_subdev_set_ffmt_default(struct v4l2_subdev *sd,
+				     struct v4l2_subdev_state *state,
+				     struct v4l2_subdev_format *fmt);
+int __ipu_isys_subdev_set_ffmt(struct v4l2_subdev *sd,
+			       struct v4l2_subdev_state *state,
+			       struct v4l2_subdev_format *fmt);
+struct v4l2_rect *__ipu_isys_get_selection(struct v4l2_subdev *sd,
+					   struct v4l2_subdev_state *state,
+					   unsigned int target,
+					   unsigned int pad,
+					   unsigned int which);
+int ipu_isys_subdev_set_ffmt(struct v4l2_subdev *sd,
+			     struct v4l2_subdev_state *state,
+			     struct v4l2_subdev_format *fmt);
+int ipu_isys_subdev_get_ffmt(struct v4l2_subdev *sd,
+			     struct v4l2_subdev_state *state,
+			     struct v4l2_subdev_format *fmt);
+int ipu_isys_subdev_get_sel(struct v4l2_subdev *sd,
+			    struct v4l2_subdev_state *state,
+			    struct v4l2_subdev_selection *sel);
+int ipu_isys_subdev_set_sel(struct v4l2_subdev *sd,
+			    struct v4l2_subdev_state *state,
+			    struct v4l2_subdev_selection *sel);
+int ipu_isys_subdev_enum_mbus_code(struct v4l2_subdev *sd,
+				   struct v4l2_subdev_state *state,
+				   struct v4l2_subdev_mbus_code_enum
+				   *code);
+int ipu_isys_subdev_link_validate(struct v4l2_subdev *sd,
+				  struct media_link *link,
+				  struct v4l2_subdev_format *source_fmt,
+				  struct v4l2_subdev_format *sink_fmt);
+
+int ipu_isys_subdev_open(struct v4l2_subdev *sd, struct v4l2_subdev_fh *fh);
+int ipu_isys_subdev_close(struct v4l2_subdev *sd, struct v4l2_subdev_fh *fh);
+int ipu_isys_subdev_init(struct ipu_isys_subdev *asd,
+			 struct v4l2_subdev_ops *ops,
+			 unsigned int nr_ctrls,
+			 unsigned int num_pads,
+			 unsigned int num_source,
+			 unsigned int num_sink,
+			 unsigned int sd_flags);
+void ipu_isys_subdev_cleanup(struct ipu_isys_subdev *asd);
+#endif /* IPU_ISYS_SUBDEV_H */
diff -ruN a/drivers/media/pci/intel/ipu-isys-tpg.c b/drivers/media/pci/intel/ipu-isys-tpg.c
--- a/drivers/media/pci/intel/ipu-isys-tpg.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-tpg.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,311 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2020 Intel Corporation
+
+#include <linux/device.h>
+#include <linux/module.h>
+
+#include <media/media-entity.h>
+#include <media/v4l2-device.h>
+#include <media/v4l2-event.h>
+
+#include "ipu.h"
+#include "ipu-bus.h"
+#include "ipu-isys.h"
+#include "ipu-isys-subdev.h"
+#include "ipu-isys-tpg.h"
+#include "ipu-isys-video.h"
+#include "ipu-platform-isys-csi2-reg.h"
+
+static const u32 tpg_supported_codes_pad[] = {
+	MEDIA_BUS_FMT_SBGGR8_1X8,
+	MEDIA_BUS_FMT_SGBRG8_1X8,
+	MEDIA_BUS_FMT_SGRBG8_1X8,
+	MEDIA_BUS_FMT_SRGGB8_1X8,
+	MEDIA_BUS_FMT_SBGGR10_1X10,
+	MEDIA_BUS_FMT_SGBRG10_1X10,
+	MEDIA_BUS_FMT_SGRBG10_1X10,
+	MEDIA_BUS_FMT_SRGGB10_1X10,
+	0,
+};
+
+static const u32 *tpg_supported_codes[] = {
+	tpg_supported_codes_pad,
+};
+
+static struct v4l2_subdev_internal_ops tpg_sd_internal_ops = {
+	.open = ipu_isys_subdev_open,
+	.close = ipu_isys_subdev_close,
+};
+
+static const struct v4l2_subdev_video_ops tpg_sd_video_ops = {
+	.s_stream = tpg_set_stream,
+};
+
+static int ipu_isys_tpg_s_ctrl(struct v4l2_ctrl *ctrl)
+{
+	struct ipu_isys_tpg *tpg = container_of(container_of(ctrl->handler,
+							     struct
+							     ipu_isys_subdev,
+							     ctrl_handler),
+						struct ipu_isys_tpg, asd);
+
+	switch (ctrl->id) {
+	case V4L2_CID_HBLANK:
+		writel(ctrl->val, tpg->base + MIPI_GEN_REG_SYNG_HBLANK_CYC);
+		break;
+	case V4L2_CID_VBLANK:
+		writel(ctrl->val, tpg->base + MIPI_GEN_REG_SYNG_VBLANK_CYC);
+		break;
+	case V4L2_CID_TEST_PATTERN:
+		writel(ctrl->val, tpg->base + MIPI_GEN_REG_TPG_MODE);
+		break;
+	}
+
+	return 0;
+}
+
+static const struct v4l2_ctrl_ops ipu_isys_tpg_ctrl_ops = {
+	.s_ctrl = ipu_isys_tpg_s_ctrl,
+};
+
+static s64 ipu_isys_tpg_rate(struct ipu_isys_tpg *tpg, unsigned int bpp)
+{
+	return MIPI_GEN_PPC * IPU_ISYS_FREQ / bpp;
+}
+
+static const char *const tpg_mode_items[] = {
+	"Ramp",
+	"Checkerboard",	/* Does not work, disabled. */
+	"Frame Based Colour",
+};
+
+static struct v4l2_ctrl_config tpg_mode = {
+	.ops = &ipu_isys_tpg_ctrl_ops,
+	.id = V4L2_CID_TEST_PATTERN,
+	.name = "Test Pattern",
+	.type = V4L2_CTRL_TYPE_MENU,
+	.min = 0,
+	.max = ARRAY_SIZE(tpg_mode_items) - 1,
+	.def = 0,
+	.menu_skip_mask = 0x2,
+	.qmenu = tpg_mode_items,
+};
+
+static const struct v4l2_ctrl_config csi2_header_cfg = {
+	.id = V4L2_CID_IPU_STORE_CSI2_HEADER,
+	.name = "Store CSI-2 Headers",
+	.type = V4L2_CTRL_TYPE_BOOLEAN,
+	.min = 0,
+	.max = 1,
+	.step = 1,
+	.def = 1,
+};
+
+static void ipu_isys_tpg_init_controls(struct v4l2_subdev *sd)
+{
+	struct ipu_isys_tpg *tpg = to_ipu_isys_tpg(sd);
+	int hblank;
+	u64 default_pixel_rate;
+
+	hblank = 1024;
+
+	tpg->hblank = v4l2_ctrl_new_std(&tpg->asd.ctrl_handler,
+					&ipu_isys_tpg_ctrl_ops,
+					V4L2_CID_HBLANK, 8, 65535, 1, hblank);
+
+	tpg->vblank = v4l2_ctrl_new_std(&tpg->asd.ctrl_handler,
+					&ipu_isys_tpg_ctrl_ops,
+					V4L2_CID_VBLANK, 8, 65535, 1, 1024);
+
+	default_pixel_rate = ipu_isys_tpg_rate(tpg, 8);
+	tpg->pixel_rate = v4l2_ctrl_new_std(&tpg->asd.ctrl_handler,
+					    &ipu_isys_tpg_ctrl_ops,
+					    V4L2_CID_PIXEL_RATE,
+					    default_pixel_rate,
+					    default_pixel_rate,
+					    1, default_pixel_rate);
+	if (tpg->pixel_rate) {
+		tpg->pixel_rate->cur.val = default_pixel_rate;
+		tpg->pixel_rate->flags |= V4L2_CTRL_FLAG_READ_ONLY;
+	}
+
+	v4l2_ctrl_new_custom(&tpg->asd.ctrl_handler, &tpg_mode, NULL);
+	tpg->store_csi2_header =
+		v4l2_ctrl_new_custom(&tpg->asd.ctrl_handler,
+				     &csi2_header_cfg, NULL);
+}
+
+static void tpg_set_ffmt(struct v4l2_subdev *sd,
+			 struct v4l2_subdev_state *state,
+			 struct v4l2_subdev_format *fmt)
+{
+	fmt->format.field = V4L2_FIELD_NONE;
+	*__ipu_isys_get_ffmt(sd, state, fmt->pad, fmt->which) = fmt->format;
+}
+
+static int ipu_isys_tpg_set_ffmt(struct v4l2_subdev *sd,
+				 struct v4l2_subdev_state *state,
+				 struct v4l2_subdev_format *fmt)
+{
+	struct ipu_isys_tpg *tpg = to_ipu_isys_tpg(sd);
+	__u32 code = tpg->asd.ffmt[TPG_PAD_SOURCE].code;
+	unsigned int bpp = ipu_isys_mbus_code_to_bpp(code);
+	s64 tpg_rate = ipu_isys_tpg_rate(tpg, bpp);
+	int rval;
+
+	mutex_lock(&tpg->asd.mutex);
+	rval = __ipu_isys_subdev_set_ffmt(sd, state, fmt);
+	mutex_unlock(&tpg->asd.mutex);
+
+	if (rval || fmt->which != V4L2_SUBDEV_FORMAT_ACTIVE)
+		return rval;
+
+	v4l2_ctrl_s_ctrl_int64(tpg->pixel_rate, tpg_rate);
+
+	return 0;
+}
+
+static const struct ipu_isys_pixelformat *
+ipu_isys_tpg_try_fmt(struct ipu_isys_video *av,
+		     struct v4l2_pix_format_mplane *mpix)
+{
+	struct media_link *link = list_first_entry(&av->vdev.entity.links,
+						   struct media_link, list);
+	struct v4l2_subdev *sd =
+		media_entity_to_v4l2_subdev(link->source->entity);
+	struct ipu_isys_tpg *tpg;
+
+	if (!sd)
+		return NULL;
+
+	tpg = to_ipu_isys_tpg(sd);
+
+	return ipu_isys_video_try_fmt_vid_mplane(av, mpix,
+		v4l2_ctrl_g_ctrl(tpg->store_csi2_header));
+}
+
+static const struct v4l2_subdev_pad_ops tpg_sd_pad_ops = {
+	.get_fmt = ipu_isys_subdev_get_ffmt,
+	.set_fmt = ipu_isys_tpg_set_ffmt,
+	.enum_mbus_code = ipu_isys_subdev_enum_mbus_code,
+};
+
+static int subscribe_event(struct v4l2_subdev *sd, struct v4l2_fh *fh,
+			   struct v4l2_event_subscription *sub)
+{
+	switch (sub->type) {
+#ifdef IPU_TPG_FRAME_SYNC
+	case V4L2_EVENT_FRAME_SYNC:
+		return v4l2_event_subscribe(fh, sub, 10, NULL);
+#endif
+	case V4L2_EVENT_CTRL:
+		return v4l2_ctrl_subscribe_event(fh, sub);
+	default:
+		return -EINVAL;
+	}
+};
+
+/* V4L2 subdev core operations */
+static const struct v4l2_subdev_core_ops tpg_sd_core_ops = {
+	.subscribe_event = subscribe_event,
+	.unsubscribe_event = v4l2_event_subdev_unsubscribe,
+};
+
+static struct v4l2_subdev_ops tpg_sd_ops = {
+	.core = &tpg_sd_core_ops,
+	.video = &tpg_sd_video_ops,
+	.pad = &tpg_sd_pad_ops,
+};
+
+static struct media_entity_operations tpg_entity_ops = {
+	.link_validate = v4l2_subdev_link_validate,
+};
+
+void ipu_isys_tpg_cleanup(struct ipu_isys_tpg *tpg)
+{
+	v4l2_device_unregister_subdev(&tpg->asd.sd);
+	ipu_isys_subdev_cleanup(&tpg->asd);
+	ipu_isys_video_cleanup(&tpg->av);
+}
+
+int ipu_isys_tpg_init(struct ipu_isys_tpg *tpg,
+		      struct ipu_isys *isys,
+		      void __iomem *base, void __iomem *sel,
+		      unsigned int index)
+{
+	struct v4l2_subdev_format fmt = {
+		.which = V4L2_SUBDEV_FORMAT_ACTIVE,
+		.pad = TPG_PAD_SOURCE,
+		.format = {
+			   .width = 4096,
+			   .height = 3072,
+			   },
+	};
+	int rval;
+
+	tpg->isys = isys;
+	tpg->base = base;
+	tpg->sel = sel;
+	tpg->index = index;
+
+	tpg->asd.sd.entity.ops = &tpg_entity_ops;
+	tpg->asd.ctrl_init = ipu_isys_tpg_init_controls;
+	tpg->asd.isys = isys;
+
+	rval = ipu_isys_subdev_init(&tpg->asd, &tpg_sd_ops, 5,
+				    NR_OF_TPG_PADS,
+				    NR_OF_TPG_SOURCE_PADS,
+				    NR_OF_TPG_SINK_PADS,
+				    V4L2_SUBDEV_FL_HAS_EVENTS);
+	if (rval)
+		return rval;
+
+	tpg->asd.sd.entity.function = MEDIA_ENT_F_CAM_SENSOR;
+	tpg->asd.pad[TPG_PAD_SOURCE].flags = MEDIA_PAD_FL_SOURCE;
+
+	tpg->asd.source = IPU_FW_ISYS_STREAM_SRC_MIPIGEN_PORT0 + index;
+	tpg->asd.supported_codes = tpg_supported_codes;
+	tpg->asd.set_ffmt = tpg_set_ffmt;
+	ipu_isys_subdev_set_ffmt(&tpg->asd.sd, NULL, &fmt);
+
+	tpg->asd.sd.internal_ops = &tpg_sd_internal_ops;
+	snprintf(tpg->asd.sd.name, sizeof(tpg->asd.sd.name),
+		 IPU_ISYS_ENTITY_PREFIX " TPG %u", index);
+	v4l2_set_subdevdata(&tpg->asd.sd, &tpg->asd);
+	rval = v4l2_device_register_subdev(&isys->v4l2_dev, &tpg->asd.sd);
+	if (rval) {
+		dev_info(&isys->adev->dev, "can't register v4l2 subdev\n");
+		goto fail;
+	}
+
+	snprintf(tpg->av.vdev.name, sizeof(tpg->av.vdev.name),
+		 IPU_ISYS_ENTITY_PREFIX " TPG %u capture", index);
+	tpg->av.isys = isys;
+	tpg->av.aq.css_pin_type = IPU_FW_ISYS_PIN_TYPE_MIPI;
+	tpg->av.pfmts = ipu_isys_pfmts_packed;
+	tpg->av.try_fmt_vid_mplane = ipu_isys_tpg_try_fmt;
+	tpg->av.prepare_fw_stream =
+	    ipu_isys_prepare_fw_cfg_default;
+	tpg->av.packed = true;
+	tpg->av.line_header_length = IPU_ISYS_CSI2_LONG_PACKET_HEADER_SIZE;
+	tpg->av.line_footer_length = IPU_ISYS_CSI2_LONG_PACKET_FOOTER_SIZE;
+	tpg->av.aq.buf_prepare = ipu_isys_buf_prepare;
+	tpg->av.aq.fill_frame_buff_set_pin =
+	    ipu_isys_buffer_to_fw_frame_buff_pin;
+	tpg->av.aq.link_fmt_validate = ipu_isys_link_fmt_validate;
+	tpg->av.aq.vbq.buf_struct_size = sizeof(struct ipu_isys_video_buffer);
+
+	rval = ipu_isys_video_init(&tpg->av, &tpg->asd.sd.entity,
+				   TPG_PAD_SOURCE, MEDIA_PAD_FL_SINK, 0);
+	if (rval) {
+		dev_info(&isys->adev->dev, "can't init video node\n");
+		goto fail;
+	}
+
+	return 0;
+
+fail:
+	ipu_isys_tpg_cleanup(tpg);
+
+	return rval;
+}
diff -ruN a/drivers/media/pci/intel/ipu-isys-tpg.h b/drivers/media/pci/intel/ipu-isys-tpg.h
--- a/drivers/media/pci/intel/ipu-isys-tpg.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-tpg.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,99 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_ISYS_TPG_H
+#define IPU_ISYS_TPG_H
+
+#include <media/media-entity.h>
+#include <media/v4l2-ctrls.h>
+#include <media/v4l2-device.h>
+
+#include "ipu-isys-subdev.h"
+#include "ipu-isys-video.h"
+#include "ipu-isys-queue.h"
+
+struct ipu_isys_tpg_pdata;
+struct ipu_isys;
+
+#define TPG_PAD_SOURCE			0
+#define NR_OF_TPG_PADS			1
+#define NR_OF_TPG_SOURCE_PADS		1
+#define NR_OF_TPG_SINK_PADS		0
+#define NR_OF_TPG_STREAMS		1
+
+/*
+ * PPC is 4 pixels for clock for RAW8, RAW10 and RAW12.
+ * Source: FW validation test code.
+ */
+#define MIPI_GEN_PPC		4
+
+#define MIPI_GEN_REG_COM_ENABLE				0x0
+#define MIPI_GEN_REG_COM_DTYPE				0x4
+/* RAW8, RAW10 or RAW12 */
+#define MIPI_GEN_COM_DTYPE_RAW(n)			(((n) - 8) / 2)
+#define MIPI_GEN_REG_COM_VTYPE				0x8
+#define MIPI_GEN_REG_COM_VCHAN				0xc
+#define MIPI_GEN_REG_COM_WCOUNT				0x10
+#define MIPI_GEN_REG_PRBS_RSTVAL0			0x14
+#define MIPI_GEN_REG_PRBS_RSTVAL1			0x18
+#define MIPI_GEN_REG_SYNG_FREE_RUN			0x1c
+#define MIPI_GEN_REG_SYNG_PAUSE				0x20
+#define MIPI_GEN_REG_SYNG_NOF_FRAMES			0x24
+#define MIPI_GEN_REG_SYNG_NOF_PIXELS			0x28
+#define MIPI_GEN_REG_SYNG_NOF_LINES			0x2c
+#define MIPI_GEN_REG_SYNG_HBLANK_CYC			0x30
+#define MIPI_GEN_REG_SYNG_VBLANK_CYC			0x34
+#define MIPI_GEN_REG_SYNG_STAT_HCNT			0x38
+#define MIPI_GEN_REG_SYNG_STAT_VCNT			0x3c
+#define MIPI_GEN_REG_SYNG_STAT_FCNT			0x40
+#define MIPI_GEN_REG_SYNG_STAT_DONE			0x44
+#define MIPI_GEN_REG_TPG_MODE				0x48
+#define MIPI_GEN_REG_TPG_HCNT_MASK			0x4c
+#define MIPI_GEN_REG_TPG_VCNT_MASK			0x50
+#define MIPI_GEN_REG_TPG_XYCNT_MASK			0x54
+#define MIPI_GEN_REG_TPG_HCNT_DELTA			0x58
+#define MIPI_GEN_REG_TPG_VCNT_DELTA			0x5c
+#define MIPI_GEN_REG_TPG_R1				0x60
+#define MIPI_GEN_REG_TPG_G1				0x64
+#define MIPI_GEN_REG_TPG_B1				0x68
+#define MIPI_GEN_REG_TPG_R2				0x6c
+#define MIPI_GEN_REG_TPG_G2				0x70
+#define MIPI_GEN_REG_TPG_B2				0x74
+
+/*
+ * struct ipu_isys_tpg
+ *
+ * @nlanes: number of lanes in the receiver
+ */
+struct ipu_isys_tpg {
+	struct ipu_isys_tpg_pdata *pdata;
+	struct ipu_isys *isys;
+	struct ipu_isys_subdev asd;
+	struct ipu_isys_video av;
+
+	void __iomem *base;
+	void __iomem *sel;
+	unsigned int index;
+	int streaming;
+
+	struct v4l2_ctrl *hblank;
+	struct v4l2_ctrl *vblank;
+	struct v4l2_ctrl *pixel_rate;
+	struct v4l2_ctrl *store_csi2_header;
+};
+
+#define to_ipu_isys_tpg(sd)		\
+	container_of(to_ipu_isys_subdev(sd), \
+	struct ipu_isys_tpg, asd)
+#ifdef IPU_TPG_FRAME_SYNC
+void ipu_isys_tpg_sof_event(struct ipu_isys_tpg *tpg);
+void ipu_isys_tpg_eof_event(struct ipu_isys_tpg *tpg);
+#endif
+int ipu_isys_tpg_init(struct ipu_isys_tpg *tpg,
+		      struct ipu_isys *isys,
+		      void __iomem *base, void __iomem *sel,
+		      unsigned int index);
+void ipu_isys_tpg_cleanup(struct ipu_isys_tpg *tpg);
+int tpg_set_stream(struct v4l2_subdev *sd, int enable);
+
+#endif /* IPU_ISYS_TPG_H */
diff -ruN a/drivers/media/pci/intel/ipu-isys-video.c b/drivers/media/pci/intel/ipu-isys-video.c
--- a/drivers/media/pci/intel/ipu-isys-video.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-video.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,1766 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2021 Intel Corporation
+
+#include <linux/delay.h>
+#include <linux/firmware.h>
+#include <linux/init_task.h>
+#include <linux/kthread.h>
+#include <linux/pm_runtime.h>
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/compat.h>
+
+#include <uapi/linux/sched/types.h>
+
+#include <media/media-entity.h>
+#include <media/v4l2-device.h>
+#include <media/v4l2-ioctl.h>
+#include <media/v4l2-mc.h>
+
+#include "ipu.h"
+#include "ipu-bus.h"
+#include "ipu-cpd.h"
+#include "ipu-isys.h"
+#include "ipu-isys-video.h"
+#include "ipu-platform.h"
+#include "ipu-platform-regs.h"
+#include "ipu-platform-buttress-regs.h"
+#include "ipu-trace.h"
+#include "ipu-fw-isys.h"
+#include "ipu-fw-com.h"
+
+/* use max resolution pixel rate by default */
+#define DEFAULT_PIXEL_RATE	(360000000ULL * 2 * 4 / 10)
+
+const struct ipu_isys_pixelformat ipu_isys_pfmts_be_soc[] = {
+	{V4L2_PIX_FMT_Y10, 16, 10, 0, MEDIA_BUS_FMT_Y10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_UYVY, 16, 16, 0, MEDIA_BUS_FMT_UYVY8_1X16,
+	 IPU_FW_ISYS_FRAME_FORMAT_UYVY},
+	{V4L2_PIX_FMT_YUYV, 16, 16, 0, MEDIA_BUS_FMT_YUYV8_1X16,
+	 IPU_FW_ISYS_FRAME_FORMAT_YUYV},
+	{V4L2_PIX_FMT_NV16, 16, 16, 8, MEDIA_BUS_FMT_UYVY8_1X16,
+	 IPU_FW_ISYS_FRAME_FORMAT_NV16},
+	{V4L2_PIX_FMT_XRGB32, 32, 32, 0, MEDIA_BUS_FMT_RGB565_1X16,
+	 IPU_FW_ISYS_FRAME_FORMAT_RGBA888},
+	{V4L2_PIX_FMT_XBGR32, 32, 32, 0, MEDIA_BUS_FMT_RGB888_1X24,
+	 IPU_FW_ISYS_FRAME_FORMAT_RGBA888},
+	/* Raw bayer formats. */
+	{V4L2_PIX_FMT_SBGGR12, 16, 12, 0, MEDIA_BUS_FMT_SBGGR12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SGBRG12, 16, 12, 0, MEDIA_BUS_FMT_SGBRG12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SGRBG12, 16, 12, 0, MEDIA_BUS_FMT_SGRBG12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SRGGB12, 16, 12, 0, MEDIA_BUS_FMT_SRGGB12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SBGGR10, 16, 10, 0, MEDIA_BUS_FMT_SBGGR10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SGBRG10, 16, 10, 0, MEDIA_BUS_FMT_SGBRG10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SGRBG10, 16, 10, 0, MEDIA_BUS_FMT_SGRBG10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SRGGB10, 16, 10, 0, MEDIA_BUS_FMT_SRGGB10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW16},
+	{V4L2_PIX_FMT_SBGGR8, 8, 8, 0, MEDIA_BUS_FMT_SBGGR8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{V4L2_PIX_FMT_SGBRG8, 8, 8, 0, MEDIA_BUS_FMT_SGBRG8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{V4L2_PIX_FMT_SGRBG8, 8, 8, 0, MEDIA_BUS_FMT_SGRBG8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{V4L2_PIX_FMT_SRGGB8, 8, 8, 0, MEDIA_BUS_FMT_SRGGB8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{}
+};
+
+const struct ipu_isys_pixelformat ipu_isys_pfmts_packed[] = {
+	{V4L2_PIX_FMT_Y10, 10, 10, 0, MEDIA_BUS_FMT_Y10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW10},
+#ifdef V4L2_PIX_FMT_Y210
+	{V4L2_PIX_FMT_Y210, 20, 20, 0, MEDIA_BUS_FMT_YUYV10_1X20,
+	 IPU_FW_ISYS_FRAME_FORMAT_YUYV},
+#endif
+	{V4L2_PIX_FMT_UYVY, 16, 16, 0, MEDIA_BUS_FMT_UYVY8_1X16,
+	 IPU_FW_ISYS_FRAME_FORMAT_UYVY},
+	{V4L2_PIX_FMT_YUYV, 16, 16, 0, MEDIA_BUS_FMT_YUYV8_1X16,
+	 IPU_FW_ISYS_FRAME_FORMAT_YUYV},
+	{V4L2_PIX_FMT_RGB565, 16, 16, 0, MEDIA_BUS_FMT_RGB565_1X16,
+	 IPU_FW_ISYS_FRAME_FORMAT_RGB565},
+	{V4L2_PIX_FMT_BGR24, 24, 24, 0, MEDIA_BUS_FMT_RGB888_1X24,
+	 IPU_FW_ISYS_FRAME_FORMAT_RGBA888},
+#ifndef V4L2_PIX_FMT_SBGGR12P
+	{V4L2_PIX_FMT_SBGGR12, 12, 12, 0, MEDIA_BUS_FMT_SBGGR12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW12},
+	{V4L2_PIX_FMT_SGBRG12, 12, 12, 0, MEDIA_BUS_FMT_SGBRG12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW12},
+	{V4L2_PIX_FMT_SGRBG12, 12, 12, 0, MEDIA_BUS_FMT_SGRBG12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW12},
+	{V4L2_PIX_FMT_SRGGB12, 12, 12, 0, MEDIA_BUS_FMT_SRGGB12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW12},
+#else /* V4L2_PIX_FMT_SBGGR12P */
+	{V4L2_PIX_FMT_SBGGR12P, 12, 12, 0, MEDIA_BUS_FMT_SBGGR12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW12},
+	{V4L2_PIX_FMT_SGBRG12P, 12, 12, 0, MEDIA_BUS_FMT_SGBRG12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW12},
+	{V4L2_PIX_FMT_SGRBG12P, 12, 12, 0, MEDIA_BUS_FMT_SGRBG12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW12},
+	{V4L2_PIX_FMT_SRGGB12P, 12, 12, 0, MEDIA_BUS_FMT_SRGGB12_1X12,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW12},
+#endif /* V4L2_PIX_FMT_SBGGR12P */
+	{V4L2_PIX_FMT_SBGGR10P, 10, 10, 0, MEDIA_BUS_FMT_SBGGR10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW10},
+	{V4L2_PIX_FMT_SGBRG10P, 10, 10, 0, MEDIA_BUS_FMT_SGBRG10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW10},
+	{V4L2_PIX_FMT_SGRBG10P, 10, 10, 0, MEDIA_BUS_FMT_SGRBG10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW10},
+	{V4L2_PIX_FMT_SRGGB10P, 10, 10, 0, MEDIA_BUS_FMT_SRGGB10_1X10,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW10},
+	{V4L2_PIX_FMT_SBGGR8, 8, 8, 0, MEDIA_BUS_FMT_SBGGR8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{V4L2_PIX_FMT_SGBRG8, 8, 8, 0, MEDIA_BUS_FMT_SGBRG8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{V4L2_PIX_FMT_SGRBG8, 8, 8, 0, MEDIA_BUS_FMT_SGRBG8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{V4L2_PIX_FMT_SRGGB8, 8, 8, 0, MEDIA_BUS_FMT_SRGGB8_1X8,
+	 IPU_FW_ISYS_FRAME_FORMAT_RAW8},
+	{}
+};
+
+static int video_open(struct file *file)
+{
+	struct ipu_isys_video *av = video_drvdata(file);
+	struct ipu_isys *isys = av->isys;
+	struct ipu_bus_device *adev = to_ipu_bus_device(&isys->adev->dev);
+	struct ipu_device *isp = adev->isp;
+	int rval;
+	const struct ipu_isys_internal_pdata *ipdata;
+
+	mutex_lock(&isys->mutex);
+
+	if (isys->reset_needed || isp->flr_done) {
+		mutex_unlock(&isys->mutex);
+		dev_warn(&isys->adev->dev, "isys power cycle required\n");
+		return -EIO;
+	}
+	mutex_unlock(&isys->mutex);
+
+	rval = pm_runtime_get_sync(&isys->adev->dev);
+	if (rval < 0) {
+		pm_runtime_put_noidle(&isys->adev->dev);
+		return rval;
+	}
+
+	rval = v4l2_fh_open(file);
+	if (rval)
+		goto out_power_down;
+
+	rval = v4l2_pipeline_pm_get(&av->vdev.entity);
+	if (rval)
+		goto out_v4l2_fh_release;
+
+	mutex_lock(&isys->mutex);
+
+	if (isys->video_opened++) {
+		/* Already open */
+		mutex_unlock(&isys->mutex);
+		return 0;
+	}
+
+	ipdata = isys->pdata->ipdata;
+	ipu_configure_spc(adev->isp,
+			  &ipdata->hw_variant,
+			  IPU_CPD_PKG_DIR_ISYS_SERVER_IDX,
+			  isys->pdata->base, isys->pkg_dir,
+			  isys->pkg_dir_dma_addr);
+
+	/*
+	 * Buffers could have been left to wrong queue at last closure.
+	 * Move them now back to empty buffer queue.
+	 */
+	ipu_cleanup_fw_msg_bufs(isys);
+
+	if (isys->fwcom) {
+		/*
+		 * Something went wrong in previous shutdown. As we are now
+		 * restarting isys we can safely delete old context.
+		 */
+		dev_err(&isys->adev->dev, "Clearing old context\n");
+		ipu_fw_isys_cleanup(isys);
+	}
+
+	rval = ipu_fw_isys_init(av->isys, ipdata->num_parallel_streams);
+	if (rval < 0)
+		goto out_lib_init;
+
+	mutex_unlock(&isys->mutex);
+
+	return 0;
+
+out_lib_init:
+	isys->video_opened--;
+	mutex_unlock(&isys->mutex);
+	v4l2_pipeline_pm_put(&av->vdev.entity);
+
+out_v4l2_fh_release:
+	v4l2_fh_release(file);
+out_power_down:
+	pm_runtime_put(&isys->adev->dev);
+
+	return rval;
+}
+
+static int video_release(struct file *file)
+{
+	struct ipu_isys_video *av = video_drvdata(file);
+	int ret = 0;
+
+	vb2_fop_release(file);
+
+	mutex_lock(&av->isys->mutex);
+
+	if (!--av->isys->video_opened) {
+		ipu_fw_isys_close(av->isys);
+		if (av->isys->fwcom) {
+			av->isys->reset_needed = true;
+			ret = -EIO;
+		}
+	}
+
+	mutex_unlock(&av->isys->mutex);
+
+	v4l2_pipeline_pm_put(&av->vdev.entity);
+
+	if (av->isys->reset_needed)
+		pm_runtime_put_sync(&av->isys->adev->dev);
+	else
+		pm_runtime_put(&av->isys->adev->dev);
+
+	return ret;
+}
+
+static struct media_pad *other_pad(struct media_pad *pad)
+{
+	struct media_link *link;
+
+	list_for_each_entry(link, &pad->entity->links, list) {
+		if ((link->flags & MEDIA_LNK_FL_LINK_TYPE)
+		    != MEDIA_LNK_FL_DATA_LINK)
+			continue;
+
+		return link->source == pad ? link->sink : link->source;
+	}
+
+	WARN_ON(1);
+	return NULL;
+}
+
+const struct ipu_isys_pixelformat *
+ipu_isys_get_pixelformat(struct ipu_isys_video *av, u32 pixelformat)
+{
+	struct media_pad *pad = other_pad(&av->vdev.entity.pads[0]);
+	struct v4l2_subdev *sd;
+	const u32 *supported_codes;
+	const struct ipu_isys_pixelformat *pfmt;
+
+	if (!pad || !pad->entity) {
+		WARN_ON(1);
+		return NULL;
+	}
+
+	sd = media_entity_to_v4l2_subdev(pad->entity);
+	supported_codes = to_ipu_isys_subdev(sd)->supported_codes[pad->index];
+
+	for (pfmt = av->pfmts; pfmt->bpp; pfmt++) {
+		unsigned int i;
+
+		if (pfmt->pixelformat != pixelformat)
+			continue;
+
+		for (i = 0; supported_codes[i]; i++) {
+			if (pfmt->code == supported_codes[i])
+				return pfmt;
+		}
+	}
+
+	/* Not found. Get the default, i.e. the first defined one. */
+	for (pfmt = av->pfmts; pfmt->bpp; pfmt++) {
+		if (pfmt->code == *supported_codes)
+			return pfmt;
+	}
+
+	WARN_ON(1);
+	return NULL;
+}
+
+int ipu_isys_vidioc_querycap(struct file *file, void *fh,
+			     struct v4l2_capability *cap)
+{
+	struct ipu_isys_video *av = video_drvdata(file);
+
+	strlcpy(cap->driver, IPU_ISYS_NAME, sizeof(cap->driver));
+	strlcpy(cap->card, av->isys->media_dev.model, sizeof(cap->card));
+	snprintf(cap->bus_info, sizeof(cap->bus_info), "PCI:%s",
+		 av->isys->media_dev.bus_info);
+	return 0;
+}
+
+int ipu_isys_vidioc_enum_fmt(struct file *file, void *fh,
+			     struct v4l2_fmtdesc *f)
+{
+	struct ipu_isys_video *av = video_drvdata(file);
+	struct media_pad *pad = other_pad(&av->vdev.entity.pads[0]);
+	struct v4l2_subdev *sd;
+	const u32 *supported_codes;
+	const struct ipu_isys_pixelformat *pfmt;
+	u32 index;
+
+	if (!pad || !pad->entity)
+		return -EINVAL;
+	sd = media_entity_to_v4l2_subdev(pad->entity);
+	supported_codes = to_ipu_isys_subdev(sd)->supported_codes[pad->index];
+
+	/* Walk the 0-terminated array for the f->index-th code. */
+	for (index = f->index; *supported_codes && index;
+	     index--, supported_codes++) {
+	};
+
+	if (!*supported_codes)
+		return -EINVAL;
+
+	f->flags = 0;
+
+	/* Code found */
+	for (pfmt = av->pfmts; pfmt->bpp; pfmt++)
+		if (pfmt->code == *supported_codes)
+			break;
+
+	if (!pfmt->bpp) {
+		dev_warn(&av->isys->adev->dev,
+			 "Format not found in mapping table.");
+		return -EINVAL;
+	}
+
+	f->pixelformat = pfmt->pixelformat;
+
+	return 0;
+}
+
+static int vidioc_g_fmt_vid_cap_mplane(struct file *file, void *fh,
+				       struct v4l2_format *fmt)
+{
+	struct ipu_isys_video *av = video_drvdata(file);
+
+	fmt->fmt.pix_mp = av->mpix;
+
+	return 0;
+}
+
+const struct ipu_isys_pixelformat *
+ipu_isys_video_try_fmt_vid_mplane_default(struct ipu_isys_video *av,
+					  struct v4l2_pix_format_mplane *mpix)
+{
+	return ipu_isys_video_try_fmt_vid_mplane(av, mpix, 0);
+}
+
+const struct ipu_isys_pixelformat *
+ipu_isys_video_try_fmt_vid_mplane(struct ipu_isys_video *av,
+				  struct v4l2_pix_format_mplane *mpix,
+				  int store_csi2_header)
+{
+	const struct ipu_isys_pixelformat *pfmt =
+	    ipu_isys_get_pixelformat(av, mpix->pixelformat);
+
+	if (!pfmt)
+		return NULL;
+	mpix->pixelformat = pfmt->pixelformat;
+	mpix->num_planes = 1;
+
+	mpix->width = clamp(mpix->width, IPU_ISYS_MIN_WIDTH,
+			    IPU_ISYS_MAX_WIDTH);
+	mpix->height = clamp(mpix->height, IPU_ISYS_MIN_HEIGHT,
+			     IPU_ISYS_MAX_HEIGHT);
+
+	if (!av->packed)
+		mpix->plane_fmt[0].bytesperline =
+		    mpix->width * DIV_ROUND_UP(pfmt->bpp_planar ?
+					       pfmt->bpp_planar : pfmt->bpp,
+					       BITS_PER_BYTE);
+	else if (store_csi2_header)
+		mpix->plane_fmt[0].bytesperline =
+		    DIV_ROUND_UP(av->line_header_length +
+				 av->line_footer_length +
+				 (unsigned int)mpix->width * pfmt->bpp,
+				 BITS_PER_BYTE);
+	else
+		mpix->plane_fmt[0].bytesperline =
+		    DIV_ROUND_UP((unsigned int)mpix->width * pfmt->bpp,
+				 BITS_PER_BYTE);
+
+	mpix->plane_fmt[0].bytesperline = ALIGN(mpix->plane_fmt[0].bytesperline,
+						av->isys->line_align);
+
+	if (pfmt->bpp_planar)
+		mpix->plane_fmt[0].bytesperline =
+		    mpix->plane_fmt[0].bytesperline *
+		    pfmt->bpp / pfmt->bpp_planar;
+	/*
+	 * (height + 1) * bytesperline due to a hardware issue: the DMA unit
+	 * is a power of two, and a line should be transferred as few units
+	 * as possible. The result is that up to line length more data than
+	 * the image size may be transferred to memory after the image.
+	 * Another limition is the GDA allocation unit size. For low
+	 * resolution it gives a bigger number. Use larger one to avoid
+	 * memory corruption.
+	 */
+	mpix->plane_fmt[0].sizeimage =
+	    max(max(mpix->plane_fmt[0].sizeimage,
+		    mpix->plane_fmt[0].bytesperline * mpix->height +
+		    max(mpix->plane_fmt[0].bytesperline,
+			av->isys->pdata->ipdata->isys_dma_overshoot)), 1U);
+
+	if (av->compression_ctrl)
+		av->compression = v4l2_ctrl_g_ctrl(av->compression_ctrl);
+
+	/* overwrite bpl/height with compression alignment */
+	if (av->compression) {
+		u32 planar_tile_status_size, tile_status_size;
+
+		mpix->plane_fmt[0].bytesperline =
+		    ALIGN(mpix->plane_fmt[0].bytesperline,
+			  IPU_ISYS_COMPRESSION_LINE_ALIGN);
+		mpix->height = ALIGN(mpix->height,
+				     IPU_ISYS_COMPRESSION_HEIGHT_ALIGN);
+
+		mpix->plane_fmt[0].sizeimage =
+		    ALIGN(mpix->plane_fmt[0].bytesperline * mpix->height,
+			  IPU_ISYS_COMPRESSION_PAGE_ALIGN);
+
+		/* ISYS compression only for RAW and single plannar */
+		planar_tile_status_size =
+		    DIV_ROUND_UP_ULL((mpix->plane_fmt[0].bytesperline *
+				      mpix->height /
+				      IPU_ISYS_COMPRESSION_TILE_SIZE_BYTES) *
+				     IPU_ISYS_COMPRESSION_TILE_STATUS_BITS,
+				     BITS_PER_BYTE);
+		tile_status_size = ALIGN(planar_tile_status_size,
+					 IPU_ISYS_COMPRESSION_PAGE_ALIGN);
+
+		/* tile status buffer offsets relative to buffer base address */
+		av->ts_offsets[0] = mpix->plane_fmt[0].sizeimage;
+		mpix->plane_fmt[0].sizeimage += tile_status_size;
+
+		dev_dbg(&av->isys->adev->dev,
+			"cmprs: bpl:%d, height:%d img size:%d, ts_sz:%d\n",
+			mpix->plane_fmt[0].bytesperline, mpix->height,
+			av->ts_offsets[0], tile_status_size);
+	}
+
+	memset(mpix->plane_fmt[0].reserved, 0,
+	       sizeof(mpix->plane_fmt[0].reserved));
+
+	if (mpix->field == V4L2_FIELD_ANY)
+		mpix->field = V4L2_FIELD_NONE;
+	/* Use defaults */
+	mpix->colorspace = V4L2_COLORSPACE_RAW;
+	mpix->ycbcr_enc = V4L2_YCBCR_ENC_DEFAULT;
+	mpix->quantization = V4L2_QUANTIZATION_DEFAULT;
+	mpix->xfer_func = V4L2_XFER_FUNC_DEFAULT;
+
+	return pfmt;
+}
+
+static int vidioc_s_fmt_vid_cap_mplane(struct file *file, void *fh,
+				       struct v4l2_format *f)
+{
+	struct ipu_isys_video *av = video_drvdata(file);
+
+	if (av->aq.vbq.streaming)
+		return -EBUSY;
+
+	av->pfmt = av->try_fmt_vid_mplane(av, &f->fmt.pix_mp);
+	av->mpix = f->fmt.pix_mp;
+
+	return 0;
+}
+
+static int vidioc_try_fmt_vid_cap_mplane(struct file *file, void *fh,
+					 struct v4l2_format *f)
+{
+	struct ipu_isys_video *av = video_drvdata(file);
+
+	av->try_fmt_vid_mplane(av, &f->fmt.pix_mp);
+
+	return 0;
+}
+
+static long ipu_isys_vidioc_private(struct file *file, void *fh,
+				    bool valid_prio, unsigned int cmd,
+				    void *arg)
+{
+	struct ipu_isys_video *av = video_drvdata(file);
+	int ret = 0;
+
+	switch (cmd) {
+	case VIDIOC_IPU_GET_DRIVER_VERSION:
+		*(u32 *)arg = IPU_DRIVER_VERSION;
+		break;
+
+	default:
+		dev_dbg(&av->isys->adev->dev, "unsupported private ioctl %x\n",
+			cmd);
+	}
+
+	return ret;
+}
+
+static int vidioc_enum_input(struct file *file, void *fh,
+			     struct v4l2_input *input)
+{
+	if (input->index > 0)
+		return -EINVAL;
+	strlcpy(input->name, "camera", sizeof(input->name));
+	input->type = V4L2_INPUT_TYPE_CAMERA;
+
+	return 0;
+}
+
+static int vidioc_g_input(struct file *file, void *fh, unsigned int *input)
+{
+	*input = 0;
+
+	return 0;
+}
+
+static int vidioc_s_input(struct file *file, void *fh, unsigned int input)
+{
+	return input == 0 ? 0 : -EINVAL;
+}
+
+/*
+ * Return true if an entity directly connected to an Iunit entity is
+ * an image source for the ISP. This can be any external directly
+ * connected entity or any of the test pattern generators in the
+ * Iunit.
+ */
+static bool is_external(struct ipu_isys_video *av, struct media_entity *entity)
+{
+	struct v4l2_subdev *sd;
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	unsigned int i;
+#endif
+
+	/* All video nodes are ours. */
+	if (!is_media_entity_v4l2_subdev(entity))
+		return false;
+
+	sd = media_entity_to_v4l2_subdev(entity);
+	if (strncmp(sd->name, IPU_ISYS_ENTITY_PREFIX,
+		    strlen(IPU_ISYS_ENTITY_PREFIX)) != 0)
+		return true;
+
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	for (i = 0; i < av->isys->pdata->ipdata->tpg.ntpgs &&
+	     av->isys->tpg[i].isys; i++)
+		if (entity == &av->isys->tpg[i].asd.sd.entity)
+			return true;
+#endif
+
+	return false;
+}
+
+static int link_validate(struct media_link *link)
+{
+	struct ipu_isys_video *av =
+	    container_of(link->sink, struct ipu_isys_video, pad);
+	/* All sub-devices connected to a video node are ours. */
+	struct ipu_isys_pipeline *ip =
+		to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	struct v4l2_subdev *sd;
+
+	if (!link->source->entity)
+		return -EINVAL;
+	sd = media_entity_to_v4l2_subdev(link->source->entity);
+	if (is_external(av, link->source->entity)) {
+		ip->external = media_entity_remote_pad(av->vdev.entity.pads);
+		ip->source = to_ipu_isys_subdev(sd)->source;
+	}
+
+	ip->nr_queues++;
+
+	return 0;
+}
+
+static void get_stream_opened(struct ipu_isys_video *av)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&av->isys->lock, flags);
+	av->isys->stream_opened++;
+	spin_unlock_irqrestore(&av->isys->lock, flags);
+}
+
+static void put_stream_opened(struct ipu_isys_video *av)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&av->isys->lock, flags);
+	av->isys->stream_opened--;
+	spin_unlock_irqrestore(&av->isys->lock, flags);
+}
+
+static int get_stream_handle(struct ipu_isys_video *av)
+{
+	struct ipu_isys_pipeline *ip =
+	    to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	unsigned int stream_handle;
+	unsigned long flags;
+
+	spin_lock_irqsave(&av->isys->lock, flags);
+	for (stream_handle = 0;
+	     stream_handle < IPU_ISYS_MAX_STREAMS; stream_handle++)
+		if (!av->isys->pipes[stream_handle])
+			break;
+	if (stream_handle == IPU_ISYS_MAX_STREAMS) {
+		spin_unlock_irqrestore(&av->isys->lock, flags);
+		return -EBUSY;
+	}
+	av->isys->pipes[stream_handle] = ip;
+	ip->stream_handle = stream_handle;
+	spin_unlock_irqrestore(&av->isys->lock, flags);
+	return 0;
+}
+
+static void put_stream_handle(struct ipu_isys_video *av)
+{
+	struct ipu_isys_pipeline *ip =
+	    to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	unsigned long flags;
+
+	spin_lock_irqsave(&av->isys->lock, flags);
+	av->isys->pipes[ip->stream_handle] = NULL;
+	ip->stream_handle = -1;
+	spin_unlock_irqrestore(&av->isys->lock, flags);
+}
+
+static int get_external_facing_format(struct ipu_isys_pipeline *ip,
+				      struct v4l2_subdev_format *format)
+{
+	struct ipu_isys_video *av = container_of(ip, struct ipu_isys_video, ip);
+	struct v4l2_subdev *sd;
+	struct media_pad *external_facing;
+
+	if (!ip->external->entity) {
+		WARN_ON(1);
+		return -ENODEV;
+	}
+	sd = media_entity_to_v4l2_subdev(ip->external->entity);
+	external_facing = (strncmp(sd->name, IPU_ISYS_ENTITY_PREFIX,
+			   strlen(IPU_ISYS_ENTITY_PREFIX)) == 0) ?
+			   ip->external :
+			   media_entity_remote_pad(ip->external);
+	if (WARN_ON(!external_facing)) {
+		dev_warn(&av->isys->adev->dev,
+			 "no external facing pad --- driver bug?\n");
+		return -EINVAL;
+	}
+
+	format->which = V4L2_SUBDEV_FORMAT_ACTIVE;
+	format->pad = 0;
+	sd = media_entity_to_v4l2_subdev(external_facing->entity);
+
+	return v4l2_subdev_call(sd, pad, get_fmt, NULL, format);
+}
+
+static void short_packet_queue_destroy(struct ipu_isys_pipeline *ip)
+{
+	struct ipu_isys_video *av = container_of(ip, struct ipu_isys_video, ip);
+	unsigned int i;
+
+	if (!ip->short_packet_bufs)
+		return;
+	for (i = 0; i < IPU_ISYS_SHORT_PACKET_BUFFER_NUM; i++) {
+		if (ip->short_packet_bufs[i].buffer)
+			dma_free_coherent(&av->isys->adev->dev,
+					  ip->short_packet_buffer_size,
+					  ip->short_packet_bufs[i].buffer,
+					  ip->short_packet_bufs[i].dma_addr);
+	}
+	kfree(ip->short_packet_bufs);
+	ip->short_packet_bufs = NULL;
+}
+
+static int short_packet_queue_setup(struct ipu_isys_pipeline *ip)
+{
+	struct ipu_isys_video *av = container_of(ip, struct ipu_isys_video, ip);
+	struct v4l2_subdev_format source_fmt = { 0 };
+	unsigned int i;
+	int rval;
+	size_t buf_size;
+
+	INIT_LIST_HEAD(&ip->pending_interlaced_bufs);
+	ip->cur_field = V4L2_FIELD_TOP;
+
+	if (ip->isys->short_packet_source == IPU_ISYS_SHORT_PACKET_FROM_TUNIT) {
+		ip->short_packet_trace_index = 0;
+		return 0;
+	}
+
+	rval = get_external_facing_format(ip, &source_fmt);
+	if (rval)
+		return rval;
+	buf_size = IPU_ISYS_SHORT_PACKET_BUF_SIZE(source_fmt.format.height);
+	ip->short_packet_buffer_size = buf_size;
+	ip->num_short_packet_lines =
+	    IPU_ISYS_SHORT_PACKET_PKT_LINES(source_fmt.format.height);
+
+	/* Initialize short packet queue. */
+	INIT_LIST_HEAD(&ip->short_packet_incoming);
+	INIT_LIST_HEAD(&ip->short_packet_active);
+
+	ip->short_packet_bufs =
+	    kzalloc(sizeof(struct ipu_isys_private_buffer) *
+		    IPU_ISYS_SHORT_PACKET_BUFFER_NUM, GFP_KERNEL);
+	if (!ip->short_packet_bufs)
+		return -ENOMEM;
+
+	for (i = 0; i < IPU_ISYS_SHORT_PACKET_BUFFER_NUM; i++) {
+		struct ipu_isys_private_buffer *buf = &ip->short_packet_bufs[i];
+
+		buf->index = (unsigned int)i;
+		buf->ip = ip;
+		buf->ib.type = IPU_ISYS_SHORT_PACKET_BUFFER;
+		buf->bytesused = buf_size;
+		buf->buffer = dma_alloc_coherent(&av->isys->adev->dev, buf_size,
+						 &buf->dma_addr, GFP_KERNEL);
+		if (!buf->buffer) {
+			short_packet_queue_destroy(ip);
+			return -ENOMEM;
+		}
+		list_add(&buf->ib.head, &ip->short_packet_incoming);
+	}
+
+	return 0;
+}
+
+static void
+csi_short_packet_prepare_fw_cfg(struct ipu_isys_pipeline *ip,
+				struct ipu_fw_isys_stream_cfg_data_abi *cfg)
+{
+	int input_pin = cfg->nof_input_pins++;
+	int output_pin = cfg->nof_output_pins++;
+	struct ipu_fw_isys_input_pin_info_abi *input_info =
+	    &cfg->input_pins[input_pin];
+	struct ipu_fw_isys_output_pin_info_abi *output_info =
+	    &cfg->output_pins[output_pin];
+	struct ipu_isys *isys = ip->isys;
+
+	/*
+	 * Setting dt as IPU_ISYS_SHORT_PACKET_GENERAL_DT will cause
+	 * MIPI receiver to receive all MIPI short packets.
+	 */
+	input_info->dt = IPU_ISYS_SHORT_PACKET_GENERAL_DT;
+	input_info->input_res.width = IPU_ISYS_SHORT_PACKET_WIDTH;
+	input_info->input_res.height = ip->num_short_packet_lines;
+
+	ip->output_pins[output_pin].pin_ready =
+	    ipu_isys_queue_short_packet_ready;
+	ip->output_pins[output_pin].aq = NULL;
+	ip->short_packet_output_pin = output_pin;
+
+	output_info->input_pin_id = input_pin;
+	output_info->output_res.width = IPU_ISYS_SHORT_PACKET_WIDTH;
+	output_info->output_res.height = ip->num_short_packet_lines;
+	output_info->stride = IPU_ISYS_SHORT_PACKET_WIDTH *
+	    IPU_ISYS_SHORT_PACKET_UNITSIZE;
+	output_info->pt = IPU_ISYS_SHORT_PACKET_PT;
+	output_info->ft = IPU_ISYS_SHORT_PACKET_FT;
+	output_info->send_irq = 1;
+	memset(output_info->ts_offsets, 0, sizeof(output_info->ts_offsets));
+	output_info->s2m_pixel_soc_pixel_remapping =
+	    S2M_PIXEL_SOC_PIXEL_REMAPPING_FLAG_NO_REMAPPING;
+	output_info->csi_be_soc_pixel_remapping =
+	    CSI_BE_SOC_PIXEL_REMAPPING_FLAG_NO_REMAPPING;
+	output_info->sensor_type = isys->sensor_info.sensor_metadata;
+	output_info->snoopable = true;
+	output_info->error_handling_enable = false;
+}
+
+void
+ipu_isys_prepare_fw_cfg_default(struct ipu_isys_video *av,
+				struct ipu_fw_isys_stream_cfg_data_abi *cfg)
+{
+	struct ipu_isys_pipeline *ip =
+	    to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	struct ipu_isys_queue *aq = &av->aq;
+	struct ipu_fw_isys_output_pin_info_abi *pin_info;
+	struct ipu_isys *isys = av->isys;
+	unsigned int type_index, type;
+	int pin = cfg->nof_output_pins++;
+
+	aq->fw_output = pin;
+	ip->output_pins[pin].pin_ready = ipu_isys_queue_buf_ready;
+	ip->output_pins[pin].aq = aq;
+
+	pin_info = &cfg->output_pins[pin];
+	pin_info->input_pin_id = 0;
+	pin_info->output_res.width = av->mpix.width;
+	pin_info->output_res.height = av->mpix.height;
+
+	if (!av->pfmt->bpp_planar)
+		pin_info->stride = av->mpix.plane_fmt[0].bytesperline;
+	else
+		pin_info->stride = ALIGN(DIV_ROUND_UP(av->mpix.width *
+						      av->pfmt->bpp_planar,
+						      BITS_PER_BYTE),
+					 av->isys->line_align);
+
+	pin_info->pt = aq->css_pin_type;
+	pin_info->ft = av->pfmt->css_pixelformat;
+	pin_info->send_irq = 1;
+	memset(pin_info->ts_offsets, 0, sizeof(pin_info->ts_offsets));
+	pin_info->s2m_pixel_soc_pixel_remapping =
+	    S2M_PIXEL_SOC_PIXEL_REMAPPING_FLAG_NO_REMAPPING;
+	pin_info->csi_be_soc_pixel_remapping =
+	    CSI_BE_SOC_PIXEL_REMAPPING_FLAG_NO_REMAPPING;
+	cfg->vc = 0;
+
+	switch (pin_info->pt) {
+	/* non-snoopable sensor data to PSYS */
+	case IPU_FW_ISYS_PIN_TYPE_RAW_NS:
+		type_index = IPU_FW_ISYS_VC1_SENSOR_DATA;
+		pin_info->sensor_type = isys->sensor_types[type_index]++;
+		pin_info->snoopable = false;
+		pin_info->error_handling_enable = false;
+		type = isys->sensor_types[type_index];
+		if (type > isys->sensor_info.vc1_data_end)
+			isys->sensor_types[type_index] =
+				isys->sensor_info.vc1_data_start;
+
+		break;
+	/* snoopable META/Stats data to CPU */
+	case IPU_FW_ISYS_PIN_TYPE_METADATA_0:
+	case IPU_FW_ISYS_PIN_TYPE_METADATA_1:
+		pin_info->sensor_type = isys->sensor_info.sensor_metadata;
+		pin_info->snoopable = true;
+		pin_info->error_handling_enable = false;
+		break;
+	case IPU_FW_ISYS_PIN_TYPE_RAW_SOC:
+		if (av->compression) {
+			type_index = IPU_FW_ISYS_VC1_SENSOR_DATA;
+			pin_info->sensor_type
+				= isys->sensor_types[type_index]++;
+			pin_info->snoopable = false;
+			pin_info->error_handling_enable = false;
+			type = isys->sensor_types[type_index];
+			if (type > isys->sensor_info.vc1_data_end)
+				isys->sensor_types[type_index] =
+					isys->sensor_info.vc1_data_start;
+		} else {
+			type_index = IPU_FW_ISYS_VC0_SENSOR_DATA;
+			pin_info->sensor_type
+				= isys->sensor_types[type_index]++;
+			pin_info->snoopable = true;
+			pin_info->error_handling_enable = false;
+			type = isys->sensor_types[type_index];
+			if (type > isys->sensor_info.vc0_data_end)
+				isys->sensor_types[type_index] =
+					isys->sensor_info.vc0_data_start;
+		}
+		break;
+	case IPU_FW_ISYS_PIN_TYPE_MIPI:
+		type_index = IPU_FW_ISYS_VC0_SENSOR_DATA;
+		pin_info->sensor_type = isys->sensor_types[type_index]++;
+		pin_info->snoopable = true;
+		pin_info->error_handling_enable = false;
+		type = isys->sensor_types[type_index];
+		if (type > isys->sensor_info.vc0_data_end)
+			isys->sensor_types[type_index] =
+				isys->sensor_info.vc0_data_start;
+
+		break;
+
+	default:
+		dev_err(&av->isys->adev->dev,
+			"Unknown pin type, use metadata type as default\n");
+
+		pin_info->sensor_type = isys->sensor_info.sensor_metadata;
+		pin_info->snoopable = true;
+		pin_info->error_handling_enable = false;
+	}
+	if (av->compression) {
+		pin_info->payload_buf_size = av->mpix.plane_fmt[0].sizeimage;
+		pin_info->reserve_compression = av->compression;
+		pin_info->ts_offsets[0] = av->ts_offsets[0];
+	}
+}
+
+static unsigned int ipu_isys_get_compression_scheme(u32 code)
+{
+	switch (code) {
+	case MEDIA_BUS_FMT_SBGGR10_DPCM8_1X8:
+	case MEDIA_BUS_FMT_SGBRG10_DPCM8_1X8:
+	case MEDIA_BUS_FMT_SGRBG10_DPCM8_1X8:
+	case MEDIA_BUS_FMT_SRGGB10_DPCM8_1X8:
+		return 3;
+	default:
+		return 0;
+	}
+}
+
+static unsigned int get_comp_format(u32 code)
+{
+	unsigned int predictor = 0;	/* currently hard coded */
+	unsigned int udt = ipu_isys_mbus_code_to_mipi(code);
+	unsigned int scheme = ipu_isys_get_compression_scheme(code);
+
+	/* if data type is not user defined return here */
+	if (udt < IPU_ISYS_MIPI_CSI2_TYPE_USER_DEF(1) ||
+	    udt > IPU_ISYS_MIPI_CSI2_TYPE_USER_DEF(8))
+		return 0;
+
+	/*
+	 * For each user defined type (1..8) there is configuration bitfield for
+	 * decompression.
+	 *
+	 * | bit 3     | bits 2:0 |
+	 * | predictor | scheme   |
+	 * compression schemes:
+	 * 000 = no compression
+	 * 001 = 10 - 6 - 10
+	 * 010 = 10 - 7 - 10
+	 * 011 = 10 - 8 - 10
+	 * 100 = 12 - 6 - 12
+	 * 101 = 12 - 7 - 12
+	 * 110 = 12 - 8 - 12
+	 */
+
+	return ((predictor << 3) | scheme) <<
+	    ((udt - IPU_ISYS_MIPI_CSI2_TYPE_USER_DEF(1)) * 4);
+}
+
+/* Create stream and start it using the CSS FW ABI. */
+static int start_stream_firmware(struct ipu_isys_video *av,
+				 struct ipu_isys_buffer_list *bl)
+{
+	struct ipu_isys_pipeline *ip =
+	    to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	struct device *dev = &av->isys->adev->dev;
+	struct v4l2_subdev_selection sel_fmt = {
+		.which = V4L2_SUBDEV_FORMAT_ACTIVE,
+		.target = V4L2_SEL_TGT_CROP,
+		.pad = CSI2_BE_PAD_SOURCE,
+	};
+	struct ipu_fw_isys_stream_cfg_data_abi *stream_cfg;
+	struct isys_fw_msgs *msg = NULL;
+	struct ipu_fw_isys_frame_buff_set_abi *buf = NULL;
+	struct ipu_isys_queue *aq;
+	struct ipu_isys_video *isl_av = NULL;
+	struct v4l2_subdev_format source_fmt = { 0 };
+	struct v4l2_subdev *be_sd = NULL;
+	struct media_pad *source_pad = media_entity_remote_pad(&av->pad);
+	struct ipu_fw_isys_cropping_abi *crop;
+	enum ipu_fw_isys_send_type send_type;
+	int rval, rvalout, tout;
+
+	rval = get_external_facing_format(ip, &source_fmt);
+	if (rval)
+		return rval;
+
+	msg = ipu_get_fw_msg_buf(ip);
+	if (!msg)
+		return -ENOMEM;
+
+	stream_cfg = to_stream_cfg_msg_buf(msg);
+	stream_cfg->compfmt = get_comp_format(source_fmt.format.code);
+	stream_cfg->input_pins[0].input_res.width = source_fmt.format.width;
+	stream_cfg->input_pins[0].input_res.height = source_fmt.format.height;
+	stream_cfg->input_pins[0].dt =
+	    ipu_isys_mbus_code_to_mipi(source_fmt.format.code);
+	stream_cfg->input_pins[0].mapped_dt = N_IPU_FW_ISYS_MIPI_DATA_TYPE;
+	stream_cfg->input_pins[0].mipi_decompression =
+	    IPU_FW_ISYS_MIPI_COMPRESSION_TYPE_NO_COMPRESSION;
+	stream_cfg->input_pins[0].capture_mode =
+		IPU_FW_ISYS_CAPTURE_MODE_REGULAR;
+	if (ip->csi2 && !v4l2_ctrl_g_ctrl(ip->csi2->store_csi2_header))
+		stream_cfg->input_pins[0].mipi_store_mode =
+		    IPU_FW_ISYS_MIPI_STORE_MODE_DISCARD_LONG_HEADER;
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	else if (ip->tpg && !v4l2_ctrl_g_ctrl(ip->tpg->store_csi2_header))
+		stream_cfg->input_pins[0].mipi_store_mode =
+		    IPU_FW_ISYS_MIPI_STORE_MODE_DISCARD_LONG_HEADER;
+#endif
+
+	stream_cfg->src = ip->source;
+	stream_cfg->vc = 0;
+	stream_cfg->isl_use = ip->isl_mode;
+	stream_cfg->nof_input_pins = 1;
+	stream_cfg->sensor_type = IPU_FW_ISYS_SENSOR_MODE_NORMAL;
+
+	/*
+	 * Only CSI2-BE and SOC BE has the capability to do crop,
+	 * so get the crop info from csi2-be or csi2-be-soc.
+	 */
+	if (ip->csi2_be) {
+		be_sd = &ip->csi2_be->asd.sd;
+	} else if (ip->csi2_be_soc) {
+		be_sd = &ip->csi2_be_soc->asd.sd;
+		if (source_pad)
+			sel_fmt.pad = source_pad->index;
+	}
+	crop = &stream_cfg->crop;
+	if (be_sd &&
+	    !v4l2_subdev_call(be_sd, pad, get_selection, NULL, &sel_fmt)) {
+		crop->left_offset = sel_fmt.r.left;
+		crop->top_offset = sel_fmt.r.top;
+		crop->right_offset = sel_fmt.r.left + sel_fmt.r.width;
+		crop->bottom_offset = sel_fmt.r.top + sel_fmt.r.height;
+
+	} else {
+		crop->right_offset = source_fmt.format.width;
+		crop->bottom_offset = source_fmt.format.height;
+	}
+
+	/*
+	 * If the CSI-2 backend's video node is part of the pipeline
+	 * it must be arranged first in the output pin list. This is
+	 * the most probably a firmware requirement.
+	 */
+	if (ip->isl_mode == IPU_ISL_CSI2_BE)
+		isl_av = &ip->csi2_be->av;
+
+	if (isl_av) {
+		struct ipu_isys_queue *safe;
+
+		list_for_each_entry_safe(aq, safe, &ip->queues, node) {
+			struct ipu_isys_video *av = ipu_isys_queue_to_video(aq);
+
+			if (av != isl_av)
+				continue;
+
+			list_del(&aq->node);
+			list_add(&aq->node, &ip->queues);
+			break;
+		}
+	}
+
+	list_for_each_entry(aq, &ip->queues, node) {
+		struct ipu_isys_video *__av = ipu_isys_queue_to_video(aq);
+
+		__av->prepare_fw_stream(__av, stream_cfg);
+	}
+
+	if (ip->interlaced && ip->isys->short_packet_source ==
+	    IPU_ISYS_SHORT_PACKET_FROM_RECEIVER)
+		csi_short_packet_prepare_fw_cfg(ip, stream_cfg);
+
+	ipu_fw_isys_dump_stream_cfg(dev, stream_cfg);
+
+	ip->nr_output_pins = stream_cfg->nof_output_pins;
+
+	rval = get_stream_handle(av);
+	if (rval) {
+		dev_dbg(dev, "Can't get stream_handle\n");
+		return rval;
+	}
+
+	reinit_completion(&ip->stream_open_completion);
+
+	ipu_fw_isys_set_params(stream_cfg);
+
+	rval = ipu_fw_isys_complex_cmd(av->isys,
+				       ip->stream_handle,
+				       stream_cfg,
+				       to_dma_addr(msg),
+				       sizeof(*stream_cfg),
+				       IPU_FW_ISYS_SEND_TYPE_STREAM_OPEN);
+	ipu_put_fw_mgs_buf(av->isys, (uintptr_t)stream_cfg);
+
+	if (rval < 0) {
+		dev_err(dev, "can't open stream (%d)\n", rval);
+		goto out_put_stream_handle;
+	}
+
+	get_stream_opened(av);
+
+	tout = wait_for_completion_timeout(&ip->stream_open_completion,
+					   IPU_LIB_CALL_TIMEOUT_JIFFIES);
+	if (!tout) {
+		dev_err(dev, "stream open time out\n");
+		rval = -ETIMEDOUT;
+		goto out_put_stream_opened;
+	}
+	if (ip->error) {
+		dev_err(dev, "stream open error: %d\n", ip->error);
+		rval = -EIO;
+		goto out_put_stream_opened;
+	}
+	dev_dbg(dev, "start stream: open complete\n");
+
+	if (bl) {
+		msg = ipu_get_fw_msg_buf(ip);
+		if (!msg) {
+			rval = -ENOMEM;
+			goto out_put_stream_opened;
+		}
+		buf = to_frame_msg_buf(msg);
+	}
+
+	if (bl) {
+		ipu_isys_buffer_to_fw_frame_buff(buf, ip, bl);
+		ipu_isys_buffer_list_queue(bl,
+					   IPU_ISYS_BUFFER_LIST_FL_ACTIVE, 0);
+	}
+
+	reinit_completion(&ip->stream_start_completion);
+
+	if (bl) {
+		send_type = IPU_FW_ISYS_SEND_TYPE_STREAM_START_AND_CAPTURE;
+		ipu_fw_isys_dump_frame_buff_set(dev, buf,
+						stream_cfg->nof_output_pins);
+		rval = ipu_fw_isys_complex_cmd(av->isys,
+					       ip->stream_handle,
+					       buf, to_dma_addr(msg),
+					       sizeof(*buf),
+					       send_type);
+		ipu_put_fw_mgs_buf(av->isys, (uintptr_t)buf);
+	} else {
+		send_type = IPU_FW_ISYS_SEND_TYPE_STREAM_START;
+		rval = ipu_fw_isys_simple_cmd(av->isys,
+					      ip->stream_handle,
+					      send_type);
+	}
+
+	if (rval < 0) {
+		dev_err(dev, "can't start streaming (%d)\n", rval);
+		goto out_stream_close;
+	}
+
+	tout = wait_for_completion_timeout(&ip->stream_start_completion,
+					   IPU_LIB_CALL_TIMEOUT_JIFFIES);
+	if (!tout) {
+		dev_err(dev, "stream start time out\n");
+		rval = -ETIMEDOUT;
+		goto out_stream_close;
+	}
+	if (ip->error) {
+		dev_err(dev, "stream start error: %d\n", ip->error);
+		rval = -EIO;
+		goto out_stream_close;
+	}
+	dev_dbg(dev, "start stream: complete\n");
+
+	return 0;
+
+out_stream_close:
+	reinit_completion(&ip->stream_close_completion);
+
+	rvalout = ipu_fw_isys_simple_cmd(av->isys,
+					 ip->stream_handle,
+					 IPU_FW_ISYS_SEND_TYPE_STREAM_CLOSE);
+	if (rvalout < 0) {
+		dev_dbg(dev, "can't close stream (%d)\n", rvalout);
+		goto out_put_stream_opened;
+	}
+
+	tout = wait_for_completion_timeout(&ip->stream_close_completion,
+					   IPU_LIB_CALL_TIMEOUT_JIFFIES);
+	if (!tout)
+		dev_err(dev, "stream close time out\n");
+	else if (ip->error)
+		dev_err(dev, "stream close error: %d\n", ip->error);
+	else
+		dev_dbg(dev, "stream close complete\n");
+
+out_put_stream_opened:
+	put_stream_opened(av);
+
+out_put_stream_handle:
+	put_stream_handle(av);
+	return rval;
+}
+
+static void stop_streaming_firmware(struct ipu_isys_video *av)
+{
+	struct ipu_isys_pipeline *ip =
+	    to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	struct device *dev = &av->isys->adev->dev;
+	int rval, tout;
+	enum ipu_fw_isys_send_type send_type =
+		IPU_FW_ISYS_SEND_TYPE_STREAM_FLUSH;
+
+	reinit_completion(&ip->stream_stop_completion);
+
+	rval = ipu_fw_isys_simple_cmd(av->isys, ip->stream_handle,
+				      send_type);
+
+	if (rval < 0) {
+		dev_err(dev, "can't stop stream (%d)\n", rval);
+		return;
+	}
+
+	tout = wait_for_completion_timeout(&ip->stream_stop_completion,
+					   IPU_LIB_CALL_TIMEOUT_JIFFIES);
+	if (!tout)
+		dev_err(dev, "stream stop time out\n");
+	else if (ip->error)
+		dev_err(dev, "stream stop error: %d\n", ip->error);
+	else
+		dev_dbg(dev, "stop stream: complete\n");
+}
+
+static void close_streaming_firmware(struct ipu_isys_video *av)
+{
+	struct ipu_isys_pipeline *ip =
+	    to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	struct device *dev = &av->isys->adev->dev;
+	int rval, tout;
+
+	reinit_completion(&ip->stream_close_completion);
+
+	rval = ipu_fw_isys_simple_cmd(av->isys, ip->stream_handle,
+				      IPU_FW_ISYS_SEND_TYPE_STREAM_CLOSE);
+	if (rval < 0) {
+		dev_err(dev, "can't close stream (%d)\n", rval);
+		return;
+	}
+
+	tout = wait_for_completion_timeout(&ip->stream_close_completion,
+					   IPU_LIB_CALL_TIMEOUT_JIFFIES);
+	if (!tout)
+		dev_err(dev, "stream close time out\n");
+	else if (ip->error)
+		dev_err(dev, "stream close error: %d\n", ip->error);
+	else
+		dev_dbg(dev, "close stream: complete\n");
+
+	put_stream_opened(av);
+	put_stream_handle(av);
+}
+
+void
+ipu_isys_video_add_capture_done(struct ipu_isys_pipeline *ip,
+				void (*capture_done)
+				 (struct ipu_isys_pipeline *ip,
+				  struct ipu_fw_isys_resp_info_abi *resp))
+{
+	unsigned int i;
+
+	/* Different instances may register same function. Add only once */
+	for (i = 0; i < IPU_NUM_CAPTURE_DONE; i++)
+		if (ip->capture_done[i] == capture_done)
+			return;
+
+	for (i = 0; i < IPU_NUM_CAPTURE_DONE; i++) {
+		if (!ip->capture_done[i]) {
+			ip->capture_done[i] = capture_done;
+			return;
+		}
+	}
+	/*
+	 * Too many call backs registered. Change to IPU_NUM_CAPTURE_DONE
+	 * constant probably required.
+	 */
+	WARN_ON(1);
+}
+
+int ipu_isys_video_prepare_streaming(struct ipu_isys_video *av,
+				     unsigned int state)
+{
+	struct ipu_isys *isys = av->isys;
+	struct device *dev = &isys->adev->dev;
+	struct ipu_isys_pipeline *ip;
+	struct media_graph graph;
+	struct media_entity *entity;
+	struct media_device *mdev = &av->isys->media_dev;
+	int rval;
+	unsigned int i;
+
+	dev_dbg(dev, "prepare stream: %d\n", state);
+
+	if (!state) {
+		ip = to_ipu_isys_pipeline(av->vdev.entity.pipe);
+
+		if (ip->interlaced && isys->short_packet_source ==
+		    IPU_ISYS_SHORT_PACKET_FROM_RECEIVER)
+			short_packet_queue_destroy(ip);
+		media_pipeline_stop(&av->vdev.entity);
+		media_entity_enum_cleanup(&ip->entity_enum);
+		return 0;
+	}
+
+	ip = &av->ip;
+
+	WARN_ON(ip->nr_streaming);
+	ip->has_sof = false;
+	ip->nr_queues = 0;
+	ip->external = NULL;
+	atomic_set(&ip->sequence, 0);
+	ip->isl_mode = IPU_ISL_OFF;
+
+	for (i = 0; i < IPU_NUM_CAPTURE_DONE; i++)
+		ip->capture_done[i] = NULL;
+	ip->csi2_be = NULL;
+	ip->csi2_be_soc = NULL;
+	ip->csi2 = NULL;
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	ip->tpg = NULL;
+#endif
+	ip->seq_index = 0;
+	memset(ip->seq, 0, sizeof(ip->seq));
+
+	WARN_ON(!list_empty(&ip->queues));
+	ip->interlaced = false;
+
+	rval = media_entity_enum_init(&ip->entity_enum, mdev);
+	if (rval)
+		return rval;
+
+	rval = media_pipeline_start(&av->vdev.entity, &ip->pipe);
+	if (rval < 0) {
+		dev_dbg(dev, "pipeline start failed\n");
+		goto out_enum_cleanup;
+	}
+
+	if (!ip->external) {
+		dev_err(dev, "no external entity set! Driver bug?\n");
+		rval = -EINVAL;
+		goto out_pipeline_stop;
+	}
+
+	rval = media_graph_walk_init(&graph, mdev);
+	if (rval)
+		goto out_pipeline_stop;
+
+	/* Gather all entities in the graph. */
+	mutex_lock(&mdev->graph_mutex);
+	media_graph_walk_start(&graph, &av->vdev.entity);
+	while ((entity = media_graph_walk_next(&graph)))
+		media_entity_enum_set(&ip->entity_enum, entity);
+
+	mutex_unlock(&mdev->graph_mutex);
+
+	media_graph_walk_cleanup(&graph);
+
+	if (ip->interlaced) {
+		rval = short_packet_queue_setup(ip);
+		if (rval) {
+			dev_err(&isys->adev->dev,
+				"Failed to setup short packet buffer.\n");
+			goto out_pipeline_stop;
+		}
+	}
+
+	dev_dbg(dev, "prepare stream: external entity %s\n",
+		ip->external->entity->name);
+
+	return 0;
+
+out_pipeline_stop:
+	media_pipeline_stop(&av->vdev.entity);
+
+out_enum_cleanup:
+	media_entity_enum_cleanup(&ip->entity_enum);
+
+	return rval;
+}
+
+static void configure_stream_watermark(struct ipu_isys_video *av)
+{
+	u32 vblank, hblank;
+	u64 pixel_rate;
+	int ret = 0;
+	struct v4l2_subdev *esd;
+	struct v4l2_ctrl *ctrl;
+	struct ipu_isys_pipeline *ip;
+	struct isys_iwake_watermark *iwake_watermark;
+	struct v4l2_control vb = { .id = V4L2_CID_VBLANK, .value = 0 };
+	struct v4l2_control hb = { .id = V4L2_CID_HBLANK, .value = 0 };
+
+	ip = to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	if (!ip->external->entity) {
+		WARN_ON(1);
+		return;
+	}
+	esd = media_entity_to_v4l2_subdev(ip->external->entity);
+
+	av->watermark->width = av->mpix.width;
+	av->watermark->height = av->mpix.height;
+
+	ret = v4l2_g_ctrl(esd->ctrl_handler, &vb);
+	if (!ret && vb.value >= 0)
+		vblank = vb.value;
+	else
+		vblank = 0;
+
+	ret = v4l2_g_ctrl(esd->ctrl_handler, &hb);
+	if (!ret && hb.value >= 0)
+		hblank = hb.value;
+	else
+		hblank = 0;
+
+	ctrl = v4l2_ctrl_find(esd->ctrl_handler, V4L2_CID_PIXEL_RATE);
+
+	if (!ctrl)
+		pixel_rate = DEFAULT_PIXEL_RATE;
+	else
+		pixel_rate = v4l2_ctrl_g_ctrl_int64(ctrl);
+
+	av->watermark->vblank = vblank;
+	av->watermark->hblank = hblank;
+	av->watermark->pixel_rate = pixel_rate;
+	if (!pixel_rate) {
+		iwake_watermark = av->isys->iwake_watermark;
+		mutex_lock(&iwake_watermark->mutex);
+		iwake_watermark->force_iwake_disable = true;
+		mutex_unlock(&iwake_watermark->mutex);
+		WARN(1, "%s Invalid pixel_rate, disable iwake.\n", __func__);
+		return;
+	}
+}
+
+static void calculate_stream_datarate(struct video_stream_watermark *watermark)
+{
+	u64 pixels_per_line, bytes_per_line, line_time_ns;
+	u64 pages_per_line, pb_bytes_per_line, stream_data_rate;
+	u16 sram_granulrity_shift =
+		(ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) ?
+		IPU6_SRAM_GRANULRITY_SHIFT : IPU6SE_SRAM_GRANULRITY_SHIFT;
+	u16 sram_granulrity_size =
+		(ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP) ?
+		IPU6_SRAM_GRANULRITY_SIZE : IPU6SE_SRAM_GRANULRITY_SIZE;
+
+	pixels_per_line = watermark->width + watermark->hblank;
+	line_time_ns =
+		pixels_per_line * 1000 / (watermark->pixel_rate / 1000000);
+	/* 2 bytes per Bayer pixel */
+	bytes_per_line = watermark->width << 1;
+	/* bytes to IS pixel buffer pages */
+	pages_per_line = bytes_per_line >> sram_granulrity_shift;
+
+	/* pages for each line */
+	pages_per_line = DIV_ROUND_UP(bytes_per_line,
+				      sram_granulrity_size);
+	pb_bytes_per_line = pages_per_line << sram_granulrity_shift;
+
+	/* data rate MB/s */
+	stream_data_rate = (pb_bytes_per_line * 1000) / line_time_ns;
+	watermark->stream_data_rate = stream_data_rate;
+}
+
+static void update_stream_watermark(struct ipu_isys_video *av, bool state)
+{
+	struct isys_iwake_watermark *iwake_watermark;
+
+	iwake_watermark = av->isys->iwake_watermark;
+	if (state) {
+		calculate_stream_datarate(av->watermark);
+		mutex_lock(&iwake_watermark->mutex);
+		list_add(&av->watermark->stream_node,
+			 &iwake_watermark->video_list);
+		mutex_unlock(&iwake_watermark->mutex);
+	} else {
+		av->watermark->stream_data_rate = 0;
+		mutex_lock(&iwake_watermark->mutex);
+		list_del(&av->watermark->stream_node);
+		mutex_unlock(&iwake_watermark->mutex);
+	}
+	update_watermark_setting(av->isys);
+}
+
+int ipu_isys_video_set_streaming(struct ipu_isys_video *av,
+				 unsigned int state,
+				 struct ipu_isys_buffer_list *bl)
+{
+	struct device *dev = &av->isys->adev->dev;
+	struct media_device *mdev = av->vdev.entity.graph_obj.mdev;
+	struct media_entity_enum entities;
+
+	struct media_entity *entity, *entity2;
+	struct ipu_isys_pipeline *ip =
+	    to_ipu_isys_pipeline(av->vdev.entity.pipe);
+	struct v4l2_subdev *sd, *esd;
+	int rval = 0;
+
+	dev_dbg(dev, "set stream: %d\n", state);
+
+	if (!ip->external->entity) {
+		WARN_ON(1);
+		return -ENODEV;
+	}
+	esd = media_entity_to_v4l2_subdev(ip->external->entity);
+
+	if (state) {
+		rval = media_graph_walk_init(&ip->graph, mdev);
+		if (rval)
+			return rval;
+		rval = media_entity_enum_init(&entities, mdev);
+		if (rval)
+			goto out_media_entity_graph_init;
+	}
+
+	if (!state) {
+		stop_streaming_firmware(av);
+
+		/* stop external sub-device now. */
+		dev_info(dev, "stream off %s\n", ip->external->entity->name);
+
+		v4l2_subdev_call(esd, video, s_stream, state);
+	}
+
+	mutex_lock(&mdev->graph_mutex);
+
+	media_graph_walk_start(&ip->graph,
+			       &av->vdev.entity);
+
+	while ((entity = media_graph_walk_next(&ip->graph))) {
+		sd = media_entity_to_v4l2_subdev(entity);
+
+		/* Non-subdev nodes can be safely ignored here. */
+		if (!is_media_entity_v4l2_subdev(entity))
+			continue;
+
+		/* Don't start truly external devices quite yet. */
+		if (strncmp(sd->name, IPU_ISYS_ENTITY_PREFIX,
+			    strlen(IPU_ISYS_ENTITY_PREFIX)) != 0 ||
+		    ip->external->entity == entity)
+			continue;
+
+		dev_dbg(dev, "s_stream %s entity %s\n", state ? "on" : "off",
+			entity->name);
+		rval = v4l2_subdev_call(sd, video, s_stream, state);
+		if (!state)
+			continue;
+		if (rval && rval != -ENOIOCTLCMD) {
+			mutex_unlock(&mdev->graph_mutex);
+			goto out_media_entity_stop_streaming;
+		}
+
+		media_entity_enum_set(&entities, entity);
+	}
+
+	mutex_unlock(&mdev->graph_mutex);
+
+	if (av->aq.css_pin_type == IPU_FW_ISYS_PIN_TYPE_RAW_SOC) {
+		if (state)
+			configure_stream_watermark(av);
+		update_stream_watermark(av, state);
+	}
+
+	/* Oh crap */
+	if (state) {
+		rval = start_stream_firmware(av, bl);
+		if (rval)
+			goto out_media_entity_stop_streaming;
+
+		dev_dbg(dev, "set stream: source %d, stream_handle %d\n",
+			ip->source, ip->stream_handle);
+
+		/* Start external sub-device now. */
+		dev_info(dev, "stream on %s\n", ip->external->entity->name);
+
+		rval = v4l2_subdev_call(esd, video, s_stream, state);
+		if (rval)
+			goto out_media_entity_stop_streaming_firmware;
+	} else {
+		close_streaming_firmware(av);
+	}
+
+	if (state)
+		media_entity_enum_cleanup(&entities);
+	else
+		media_graph_walk_cleanup(&ip->graph);
+	av->streaming = state;
+
+	return 0;
+
+out_media_entity_stop_streaming_firmware:
+	stop_streaming_firmware(av);
+
+out_media_entity_stop_streaming:
+	mutex_lock(&mdev->graph_mutex);
+
+	media_graph_walk_start(&ip->graph,
+			       &av->vdev.entity);
+
+	while (state && (entity2 = media_graph_walk_next(&ip->graph)) &&
+	       entity2 != entity) {
+		sd = media_entity_to_v4l2_subdev(entity2);
+
+		if (!media_entity_enum_test(&entities, entity2))
+			continue;
+
+		v4l2_subdev_call(sd, video, s_stream, 0);
+	}
+
+	mutex_unlock(&mdev->graph_mutex);
+
+	media_entity_enum_cleanup(&entities);
+
+out_media_entity_graph_init:
+	media_graph_walk_cleanup(&ip->graph);
+
+	return rval;
+}
+
+#ifdef CONFIG_COMPAT
+static long ipu_isys_compat_ioctl(struct file *file, unsigned int cmd,
+				  unsigned long arg)
+{
+	long ret = -ENOIOCTLCMD;
+	void __user *up = compat_ptr(arg);
+
+	/*
+	 * at present, there is not any private IOCTL need to compat handle
+	 */
+	if (file->f_op->unlocked_ioctl)
+		ret = file->f_op->unlocked_ioctl(file, cmd, (unsigned long)up);
+
+	return ret;
+}
+#endif
+
+static const struct v4l2_ioctl_ops ioctl_ops_mplane = {
+	.vidioc_querycap = ipu_isys_vidioc_querycap,
+	.vidioc_enum_fmt_vid_cap = ipu_isys_vidioc_enum_fmt,
+	.vidioc_g_fmt_vid_cap_mplane = vidioc_g_fmt_vid_cap_mplane,
+	.vidioc_s_fmt_vid_cap_mplane = vidioc_s_fmt_vid_cap_mplane,
+	.vidioc_try_fmt_vid_cap_mplane = vidioc_try_fmt_vid_cap_mplane,
+	.vidioc_reqbufs = vb2_ioctl_reqbufs,
+	.vidioc_create_bufs = vb2_ioctl_create_bufs,
+	.vidioc_prepare_buf = vb2_ioctl_prepare_buf,
+	.vidioc_querybuf = vb2_ioctl_querybuf,
+	.vidioc_qbuf = vb2_ioctl_qbuf,
+	.vidioc_dqbuf = vb2_ioctl_dqbuf,
+	.vidioc_streamon = vb2_ioctl_streamon,
+	.vidioc_streamoff = vb2_ioctl_streamoff,
+	.vidioc_expbuf = vb2_ioctl_expbuf,
+	.vidioc_default = ipu_isys_vidioc_private,
+	.vidioc_enum_input = vidioc_enum_input,
+	.vidioc_g_input = vidioc_g_input,
+	.vidioc_s_input = vidioc_s_input,
+};
+
+static const struct media_entity_operations entity_ops = {
+	.link_validate = link_validate,
+};
+
+static const struct v4l2_file_operations isys_fops = {
+	.owner = THIS_MODULE,
+	.poll = vb2_fop_poll,
+	.unlocked_ioctl = video_ioctl2,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl32 = ipu_isys_compat_ioctl,
+#endif
+	.mmap = vb2_fop_mmap,
+	.open = video_open,
+	.release = video_release,
+};
+
+/*
+ * Do everything that's needed to initialise things related to video
+ * buffer queue, video node, and the related media entity. The caller
+ * is expected to assign isys field and set the name of the video
+ * device.
+ */
+int ipu_isys_video_init(struct ipu_isys_video *av,
+			struct media_entity *entity,
+			unsigned int pad, unsigned long pad_flags,
+			unsigned int flags)
+{
+	const struct v4l2_ioctl_ops *ioctl_ops = NULL;
+	int rval;
+
+	mutex_init(&av->mutex);
+	init_completion(&av->ip.stream_open_completion);
+	init_completion(&av->ip.stream_close_completion);
+	init_completion(&av->ip.stream_start_completion);
+	init_completion(&av->ip.stream_stop_completion);
+	INIT_LIST_HEAD(&av->ip.queues);
+	spin_lock_init(&av->ip.short_packet_queue_lock);
+	av->ip.isys = av->isys;
+
+	if (!av->watermark) {
+		av->watermark = kzalloc(sizeof(*av->watermark), GFP_KERNEL);
+		if (!av->watermark) {
+			rval = -ENOMEM;
+			goto out_mutex_destroy;
+		}
+	}
+
+	av->vdev.device_caps = V4L2_CAP_STREAMING;
+	if (pad_flags & MEDIA_PAD_FL_SINK) {
+		av->aq.vbq.type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
+		ioctl_ops = &ioctl_ops_mplane;
+		av->vdev.device_caps |= V4L2_CAP_VIDEO_CAPTURE_MPLANE;
+		av->vdev.vfl_dir = VFL_DIR_RX;
+	} else {
+		av->aq.vbq.type = V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE;
+		av->vdev.vfl_dir = VFL_DIR_TX;
+		av->vdev.device_caps |= V4L2_CAP_VIDEO_OUTPUT_MPLANE;
+	}
+	rval = ipu_isys_queue_init(&av->aq);
+	if (rval)
+		goto out_mutex_destroy;
+
+	av->pad.flags = pad_flags | MEDIA_PAD_FL_MUST_CONNECT;
+	rval = media_entity_pads_init(&av->vdev.entity, 1, &av->pad);
+	if (rval)
+		goto out_ipu_isys_queue_cleanup;
+
+	av->vdev.entity.ops = &entity_ops;
+	av->vdev.release = video_device_release_empty;
+	av->vdev.fops = &isys_fops;
+	av->vdev.v4l2_dev = &av->isys->v4l2_dev;
+	if (!av->vdev.ioctl_ops)
+		av->vdev.ioctl_ops = ioctl_ops;
+	av->vdev.queue = &av->aq.vbq;
+	av->vdev.lock = &av->mutex;
+	set_bit(V4L2_FL_USES_V4L2_FH, &av->vdev.flags);
+	video_set_drvdata(&av->vdev, av);
+
+	mutex_lock(&av->mutex);
+
+	rval = video_register_device(&av->vdev, VFL_TYPE_VIDEO, -1);
+	if (rval)
+		goto out_media_entity_cleanup;
+
+	if (pad_flags & MEDIA_PAD_FL_SINK)
+		rval = media_create_pad_link(entity, pad,
+					     &av->vdev.entity, 0, flags);
+	else
+		rval = media_create_pad_link(&av->vdev.entity, 0, entity,
+					     pad, flags);
+	if (rval) {
+		dev_info(&av->isys->adev->dev, "can't create link\n");
+		goto out_media_entity_cleanup;
+	}
+
+	av->pfmt = av->try_fmt_vid_mplane(av, &av->mpix);
+
+	mutex_unlock(&av->mutex);
+
+	return rval;
+
+out_media_entity_cleanup:
+	video_unregister_device(&av->vdev);
+	mutex_unlock(&av->mutex);
+	media_entity_cleanup(&av->vdev.entity);
+
+out_ipu_isys_queue_cleanup:
+	ipu_isys_queue_cleanup(&av->aq);
+
+out_mutex_destroy:
+	kfree(av->watermark);
+	mutex_destroy(&av->mutex);
+
+	return rval;
+}
+
+void ipu_isys_video_cleanup(struct ipu_isys_video *av)
+{
+	kfree(av->watermark);
+	video_unregister_device(&av->vdev);
+	media_entity_cleanup(&av->vdev.entity);
+	mutex_destroy(&av->mutex);
+	ipu_isys_queue_cleanup(&av->aq);
+}
diff -ruN a/drivers/media/pci/intel/ipu-isys-video.h b/drivers/media/pci/intel/ipu-isys-video.h
--- a/drivers/media/pci/intel/ipu-isys-video.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-isys-video.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,181 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_ISYS_VIDEO_H
+#define IPU_ISYS_VIDEO_H
+
+#include <linux/mutex.h>
+#include <linux/list.h>
+#include <linux/videodev2.h>
+#include <media/media-entity.h>
+#include <media/v4l2-device.h>
+#include <media/v4l2-subdev.h>
+
+#include "ipu-isys-queue.h"
+
+#define IPU_ISYS_OUTPUT_PINS 11
+#define IPU_NUM_CAPTURE_DONE 2
+#define IPU_ISYS_MAX_PARALLEL_SOF 2
+
+struct ipu_isys;
+struct ipu_isys_csi2_be_soc;
+struct ipu_fw_isys_stream_cfg_data_abi;
+
+struct ipu_isys_pixelformat {
+	u32 pixelformat;
+	u32 bpp;
+	u32 bpp_packed;
+	u32 bpp_planar;
+	u32 code;
+	u32 css_pixelformat;
+};
+
+struct sequence_info {
+	unsigned int sequence;
+	u64 timestamp;
+};
+
+struct output_pin_data {
+	void (*pin_ready)(struct ipu_isys_pipeline *ip,
+			  struct ipu_fw_isys_resp_info_abi *info);
+	struct ipu_isys_queue *aq;
+};
+
+struct ipu_isys_pipeline {
+	struct media_pipeline pipe;
+	struct media_pad *external;
+	atomic_t sequence;
+	unsigned int seq_index;
+	struct sequence_info seq[IPU_ISYS_MAX_PARALLEL_SOF];
+	int source;	/* SSI stream source */
+	int stream_handle;	/* stream handle for CSS API */
+	unsigned int nr_output_pins;	/* How many firmware pins? */
+	enum ipu_isl_mode isl_mode;
+	struct ipu_isys_csi2_be *csi2_be;
+	struct ipu_isys_csi2_be_soc *csi2_be_soc;
+	struct ipu_isys_csi2 *csi2;
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	struct ipu_isys_tpg *tpg;
+#endif
+
+	/*
+	 * Number of capture queues, write access serialised using struct
+	 * ipu_isys.stream_mutex
+	 */
+	int nr_queues;
+	int nr_streaming;	/* Number of capture queues streaming */
+	int streaming;	/* Has streaming been really started? */
+	struct list_head queues;
+	struct completion stream_open_completion;
+	struct completion stream_close_completion;
+	struct completion stream_start_completion;
+	struct completion stream_stop_completion;
+	struct ipu_isys *isys;
+
+	void (*capture_done[IPU_NUM_CAPTURE_DONE])
+	 (struct ipu_isys_pipeline *ip,
+	  struct ipu_fw_isys_resp_info_abi *resp);
+	struct output_pin_data output_pins[IPU_ISYS_OUTPUT_PINS];
+	bool has_sof;
+	bool interlaced;
+	int error;
+	struct ipu_isys_private_buffer *short_packet_bufs;
+	size_t short_packet_buffer_size;
+	unsigned int num_short_packet_lines;
+	unsigned int short_packet_output_pin;
+	unsigned int cur_field;
+	struct list_head short_packet_incoming;
+	struct list_head short_packet_active;
+	/* Serialize access to short packet active and incoming lists */
+	spinlock_t short_packet_queue_lock;
+	struct list_head pending_interlaced_bufs;
+	unsigned int short_packet_trace_index;
+	struct media_graph graph;
+	struct media_entity_enum entity_enum;
+};
+
+#define to_ipu_isys_pipeline(__pipe)				\
+	container_of((__pipe), struct ipu_isys_pipeline, pipe)
+
+struct video_stream_watermark {
+	u32 width;
+	u32 height;
+	u32 vblank;
+	u32 hblank;
+	u32 frame_rate;
+	u64 pixel_rate;
+	u64 stream_data_rate;
+	struct list_head stream_node;
+};
+
+struct ipu_isys_video {
+	/* Serialise access to other fields in the struct. */
+	struct mutex mutex;
+	struct media_pad pad;
+	struct video_device vdev;
+	struct v4l2_pix_format_mplane mpix;
+	const struct ipu_isys_pixelformat *pfmts;
+	const struct ipu_isys_pixelformat *pfmt;
+	struct ipu_isys_queue aq;
+	struct ipu_isys *isys;
+	struct ipu_isys_pipeline ip;
+	unsigned int streaming;
+	bool packed;
+	bool compression;
+	struct v4l2_ctrl_handler ctrl_handler;
+	struct v4l2_ctrl *compression_ctrl;
+	unsigned int ts_offsets[VIDEO_MAX_PLANES];
+	unsigned int line_header_length;	/* bits */
+	unsigned int line_footer_length;	/* bits */
+
+	struct video_stream_watermark *watermark;
+
+	const struct ipu_isys_pixelformat *
+		(*try_fmt_vid_mplane)(struct ipu_isys_video *av,
+				      struct v4l2_pix_format_mplane *mpix);
+	void (*prepare_fw_stream)(struct ipu_isys_video *av,
+				  struct ipu_fw_isys_stream_cfg_data_abi *cfg);
+};
+
+#define ipu_isys_queue_to_video(__aq) \
+	container_of(__aq, struct ipu_isys_video, aq)
+
+extern const struct ipu_isys_pixelformat ipu_isys_pfmts[];
+extern const struct ipu_isys_pixelformat ipu_isys_pfmts_be_soc[];
+extern const struct ipu_isys_pixelformat ipu_isys_pfmts_packed[];
+
+const struct ipu_isys_pixelformat *
+ipu_isys_get_pixelformat(struct ipu_isys_video *av, u32 pixelformat);
+
+int ipu_isys_vidioc_querycap(struct file *file, void *fh,
+			     struct v4l2_capability *cap);
+
+int ipu_isys_vidioc_enum_fmt(struct file *file, void *fh,
+			     struct v4l2_fmtdesc *f);
+
+const struct ipu_isys_pixelformat *
+ipu_isys_video_try_fmt_vid_mplane_default(struct ipu_isys_video *av,
+					  struct v4l2_pix_format_mplane *mpix);
+
+const struct ipu_isys_pixelformat *
+ipu_isys_video_try_fmt_vid_mplane(struct ipu_isys_video *av,
+				  struct v4l2_pix_format_mplane *mpix,
+				  int store_csi2_header);
+
+void
+ipu_isys_prepare_fw_cfg_default(struct ipu_isys_video *av,
+				struct ipu_fw_isys_stream_cfg_data_abi *cfg);
+int ipu_isys_video_prepare_streaming(struct ipu_isys_video *av,
+				     unsigned int state);
+int ipu_isys_video_set_streaming(struct ipu_isys_video *av, unsigned int state,
+				 struct ipu_isys_buffer_list *bl);
+int ipu_isys_video_init(struct ipu_isys_video *av, struct media_entity *source,
+			unsigned int source_pad, unsigned long pad_flags,
+			unsigned int flags);
+void ipu_isys_video_cleanup(struct ipu_isys_video *av);
+void ipu_isys_video_add_capture_done(struct ipu_isys_pipeline *ip,
+				     void (*capture_done)
+				      (struct ipu_isys_pipeline *ip,
+				       struct ipu_fw_isys_resp_info_abi *resp));
+
+#endif /* IPU_ISYS_VIDEO_H */
diff -ruN a/drivers/media/pci/intel/ipu-mmu.c b/drivers/media/pci/intel/ipu-mmu.c
--- a/drivers/media/pci/intel/ipu-mmu.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-mmu.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,858 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2021 Intel Corporation
+
+#include <asm/cacheflush.h>
+
+#include <linux/device.h>
+#include <linux/iova.h>
+#include <linux/module.h>
+#include <linux/sizes.h>
+
+#include "ipu.h"
+#include "ipu-platform.h"
+#include "ipu-dma.h"
+#include "ipu-mmu.h"
+#include "ipu-platform-regs.h"
+
+#define ISP_PAGE_SHIFT		12
+#define ISP_PAGE_SIZE		BIT(ISP_PAGE_SHIFT)
+#define ISP_PAGE_MASK		(~(ISP_PAGE_SIZE - 1))
+
+#define ISP_L1PT_SHIFT		22
+#define ISP_L1PT_MASK		(~((1U << ISP_L1PT_SHIFT) - 1))
+
+#define ISP_L2PT_SHIFT		12
+#define ISP_L2PT_MASK		(~(ISP_L1PT_MASK | (~(ISP_PAGE_MASK))))
+
+#define ISP_L1PT_PTES           1024
+#define ISP_L2PT_PTES           1024
+
+#define ISP_PADDR_SHIFT		12
+
+#define REG_TLB_INVALIDATE	0x0000
+
+#define REG_L1_PHYS		0x0004	/* 27-bit pfn */
+#define REG_INFO		0x0008
+
+/* The range of stream ID i in L1 cache is from 0 to 15 */
+#define MMUV2_REG_L1_STREAMID(i)	(0x0c + ((i) * 4))
+
+/* The range of stream ID i in L2 cache is from 0 to 15 */
+#define MMUV2_REG_L2_STREAMID(i)	(0x4c + ((i) * 4))
+
+#define TBL_PHYS_ADDR(a)	((phys_addr_t)(a) << ISP_PADDR_SHIFT)
+
+static void tlb_invalidate(struct ipu_mmu *mmu)
+{
+	unsigned int i;
+	unsigned long flags;
+
+	spin_lock_irqsave(&mmu->ready_lock, flags);
+	if (!mmu->ready) {
+		spin_unlock_irqrestore(&mmu->ready_lock, flags);
+		return;
+	}
+
+	for (i = 0; i < mmu->nr_mmus; i++) {
+		/*
+		 * To avoid the HW bug induced dead lock in some of the IPU
+		 * MMUs on successive invalidate calls, we need to first do a
+		 * read to the page table base before writing the invalidate
+		 * register. MMUs which need to implement this WA, will have
+		 * the insert_read_before_invalidate flags set as true.
+		 * Disregard the return value of the read.
+		 */
+		if (mmu->mmu_hw[i].insert_read_before_invalidate)
+			readl(mmu->mmu_hw[i].base + REG_L1_PHYS);
+
+		writel(0xffffffff, mmu->mmu_hw[i].base +
+		       REG_TLB_INVALIDATE);
+		/*
+		 * The TLB invalidation is a "single cycle" (IOMMU clock cycles)
+		 * When the actual MMIO write reaches the IPU TLB Invalidate
+		 * register, wmb() will force the TLB invalidate out if the CPU
+		 * attempts to update the IOMMU page table (or sooner).
+		 */
+		wmb();
+	}
+	spin_unlock_irqrestore(&mmu->ready_lock, flags);
+}
+
+#ifdef DEBUG
+static void page_table_dump(struct ipu_mmu_info *mmu_info)
+{
+	u32 l1_idx;
+
+	dev_dbg(mmu_info->dev, "begin IOMMU page table dump\n");
+
+	for (l1_idx = 0; l1_idx < ISP_L1PT_PTES; l1_idx++) {
+		u32 l2_idx;
+		u32 iova = (phys_addr_t)l1_idx << ISP_L1PT_SHIFT;
+
+		if (mmu_info->l1_pt[l1_idx] == mmu_info->dummy_l2_pteval)
+			continue;
+		dev_dbg(mmu_info->dev,
+			"l1 entry %u; iovas 0x%8.8x-0x%8.8x, at %p\n",
+			l1_idx, iova, iova + ISP_PAGE_SIZE,
+			(void *)TBL_PHYS_ADDR(mmu_info->l1_pt[l1_idx]));
+
+		for (l2_idx = 0; l2_idx < ISP_L2PT_PTES; l2_idx++) {
+			u32 *l2_pt = mmu_info->l2_pts[l1_idx];
+			u32 iova2 = iova + (l2_idx << ISP_L2PT_SHIFT);
+
+			if (l2_pt[l2_idx] == mmu_info->dummy_page_pteval)
+				continue;
+
+			dev_dbg(mmu_info->dev,
+				"\tl2 entry %u; iova 0x%8.8x, phys %p\n",
+				l2_idx, iova2,
+				(void *)TBL_PHYS_ADDR(l2_pt[l2_idx]));
+		}
+	}
+
+	dev_dbg(mmu_info->dev, "end IOMMU page table dump\n");
+}
+#endif /* DEBUG */
+
+static dma_addr_t map_single(struct ipu_mmu_info *mmu_info, void *ptr)
+{
+	dma_addr_t dma;
+
+	dma = dma_map_single(mmu_info->dev, ptr, PAGE_SIZE, DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(mmu_info->dev, dma))
+		return 0;
+
+	return dma;
+}
+
+static int get_dummy_page(struct ipu_mmu_info *mmu_info)
+{
+	dma_addr_t dma;
+	void *pt = (void *)get_zeroed_page(GFP_ATOMIC | GFP_DMA32);
+
+	if (!pt)
+		return -ENOMEM;
+
+	dev_dbg(mmu_info->dev, "%s get_zeroed_page() == %p\n", __func__, pt);
+
+	dma = map_single(mmu_info, pt);
+	if (!dma) {
+		dev_err(mmu_info->dev, "Failed to map dummy page\n");
+		goto err_free_page;
+	}
+
+	mmu_info->dummy_page = pt;
+	mmu_info->dummy_page_pteval = dma >> ISP_PAGE_SHIFT;
+
+	return 0;
+
+err_free_page:
+	free_page((unsigned long)pt);
+	return -ENOMEM;
+}
+
+static void free_dummy_page(struct ipu_mmu_info *mmu_info)
+{
+	dma_unmap_single(mmu_info->dev,
+			 TBL_PHYS_ADDR(mmu_info->dummy_page_pteval),
+			 PAGE_SIZE, DMA_BIDIRECTIONAL);
+	free_page((unsigned long)mmu_info->dummy_page);
+}
+
+static int alloc_dummy_l2_pt(struct ipu_mmu_info *mmu_info)
+{
+	dma_addr_t dma;
+	u32 *pt = (u32 *)get_zeroed_page(GFP_ATOMIC | GFP_DMA32);
+	int i;
+
+	if (!pt)
+		return -ENOMEM;
+
+	dev_dbg(mmu_info->dev, "%s get_zeroed_page() == %p\n", __func__, pt);
+
+	dma = map_single(mmu_info, pt);
+	if (!dma) {
+		dev_err(mmu_info->dev, "Failed to map l2pt page\n");
+		goto err_free_page;
+	}
+
+	for (i = 0; i < ISP_L2PT_PTES; i++)
+		pt[i] = mmu_info->dummy_page_pteval;
+
+	mmu_info->dummy_l2_pt = pt;
+	mmu_info->dummy_l2_pteval = dma >> ISP_PAGE_SHIFT;
+
+	return 0;
+
+err_free_page:
+	free_page((unsigned long)pt);
+	return -ENOMEM;
+}
+
+static void free_dummy_l2_pt(struct ipu_mmu_info *mmu_info)
+{
+	dma_unmap_single(mmu_info->dev,
+			 TBL_PHYS_ADDR(mmu_info->dummy_l2_pteval),
+			 PAGE_SIZE, DMA_BIDIRECTIONAL);
+	free_page((unsigned long)mmu_info->dummy_l2_pt);
+}
+
+static u32 *alloc_l1_pt(struct ipu_mmu_info *mmu_info)
+{
+	dma_addr_t dma;
+	u32 *pt = (u32 *)get_zeroed_page(GFP_ATOMIC | GFP_DMA32);
+	int i;
+
+	if (!pt)
+		return NULL;
+
+	dev_dbg(mmu_info->dev, "%s get_zeroed_page() == %p\n", __func__, pt);
+
+	for (i = 0; i < ISP_L1PT_PTES; i++)
+		pt[i] = mmu_info->dummy_l2_pteval;
+
+	dma = map_single(mmu_info, pt);
+	if (!dma) {
+		dev_err(mmu_info->dev, "Failed to map l1pt page\n");
+		goto err_free_page;
+	}
+
+	mmu_info->l1_pt_dma = dma >> ISP_PADDR_SHIFT;
+	dev_dbg(mmu_info->dev, "l1 pt %p mapped at %llx\n", pt, dma);
+
+	return pt;
+
+err_free_page:
+	free_page((unsigned long)pt);
+	return NULL;
+}
+
+static u32 *alloc_l2_pt(struct ipu_mmu_info *mmu_info)
+{
+	u32 *pt = (u32 *)get_zeroed_page(GFP_ATOMIC | GFP_DMA32);
+	int i;
+
+	if (!pt)
+		return NULL;
+
+	dev_dbg(mmu_info->dev, "%s get_zeroed_page() == %p\n", __func__, pt);
+
+	for (i = 0; i < ISP_L1PT_PTES; i++)
+		pt[i] = mmu_info->dummy_page_pteval;
+
+	return pt;
+}
+
+static int l2_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
+		  phys_addr_t paddr, size_t size)
+{
+	u32 l1_idx = iova >> ISP_L1PT_SHIFT;
+	u32 l1_entry;
+	u32 *l2_pt, *l2_virt;
+	u32 iova_start = iova;
+	unsigned int l2_idx;
+	unsigned long flags;
+	dma_addr_t dma;
+
+	dev_dbg(mmu_info->dev,
+		"mapping l2 page table for l1 index %u (iova %8.8x)\n",
+		l1_idx, (u32)iova);
+
+	spin_lock_irqsave(&mmu_info->lock, flags);
+	l1_entry = mmu_info->l1_pt[l1_idx];
+	if (l1_entry == mmu_info->dummy_l2_pteval) {
+		l2_virt = mmu_info->l2_pts[l1_idx];
+		if (likely(!l2_virt)) {
+			l2_virt = alloc_l2_pt(mmu_info);
+			if (!l2_virt) {
+				spin_unlock_irqrestore(&mmu_info->lock, flags);
+				return -ENOMEM;
+			}
+		}
+
+		dma = map_single(mmu_info, l2_virt);
+		if (!dma) {
+			dev_err(mmu_info->dev, "Failed to map l2pt page\n");
+			free_page((unsigned long)l2_virt);
+			spin_unlock_irqrestore(&mmu_info->lock, flags);
+			return -EINVAL;
+		}
+
+		l1_entry = dma >> ISP_PADDR_SHIFT;
+
+		dev_dbg(mmu_info->dev, "page for l1_idx %u %p allocated\n",
+			l1_idx, l2_virt);
+		mmu_info->l1_pt[l1_idx] = l1_entry;
+		mmu_info->l2_pts[l1_idx] = l2_virt;
+		clflush_cache_range(&mmu_info->l1_pt[l1_idx],
+				    sizeof(mmu_info->l1_pt[l1_idx]));
+	}
+
+	l2_pt = mmu_info->l2_pts[l1_idx];
+
+	dev_dbg(mmu_info->dev, "l2_pt at %p with dma 0x%x\n", l2_pt, l1_entry);
+
+	paddr = ALIGN(paddr, ISP_PAGE_SIZE);
+
+	l2_idx = (iova_start & ISP_L2PT_MASK) >> ISP_L2PT_SHIFT;
+
+	dev_dbg(mmu_info->dev, "l2_idx %u, phys 0x%8.8x\n", l2_idx,
+		l2_pt[l2_idx]);
+	if (l2_pt[l2_idx] != mmu_info->dummy_page_pteval) {
+		spin_unlock_irqrestore(&mmu_info->lock, flags);
+		return -EINVAL;
+	}
+
+	l2_pt[l2_idx] = paddr >> ISP_PADDR_SHIFT;
+
+	clflush_cache_range(&l2_pt[l2_idx], sizeof(l2_pt[l2_idx]));
+	spin_unlock_irqrestore(&mmu_info->lock, flags);
+
+	dev_dbg(mmu_info->dev, "l2 index %u mapped as 0x%8.8x\n", l2_idx,
+		l2_pt[l2_idx]);
+
+	return 0;
+}
+
+static int __ipu_mmu_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
+			 phys_addr_t paddr, size_t size)
+{
+	u32 iova_start = round_down(iova, ISP_PAGE_SIZE);
+	u32 iova_end = ALIGN(iova + size, ISP_PAGE_SIZE);
+
+	dev_dbg(mmu_info->dev,
+		"mapping iova 0x%8.8x--0x%8.8x, size %zu at paddr 0x%10.10llx\n",
+		iova_start, iova_end, size, paddr);
+
+	return l2_map(mmu_info, iova_start, paddr, size);
+}
+
+static size_t l2_unmap(struct ipu_mmu_info *mmu_info, unsigned long iova,
+		       phys_addr_t dummy, size_t size)
+{
+	u32 l1_idx = iova >> ISP_L1PT_SHIFT;
+	u32 *l2_pt;
+	u32 iova_start = iova;
+	unsigned int l2_idx;
+	size_t unmapped = 0;
+	unsigned long flags;
+
+	dev_dbg(mmu_info->dev, "unmapping l2 page table for l1 index %u (iova 0x%8.8lx)\n",
+		l1_idx, iova);
+
+	spin_lock_irqsave(&mmu_info->lock, flags);
+	if (mmu_info->l1_pt[l1_idx] == mmu_info->dummy_l2_pteval) {
+		spin_unlock_irqrestore(&mmu_info->lock, flags);
+		dev_err(mmu_info->dev,
+			"unmap iova 0x%8.8lx l1 idx %u which was not mapped\n",
+			iova, l1_idx);
+		return 0;
+	}
+
+	for (l2_idx = (iova_start & ISP_L2PT_MASK) >> ISP_L2PT_SHIFT;
+	     (iova_start & ISP_L1PT_MASK) + (l2_idx << ISP_PAGE_SHIFT)
+	     < iova_start + size && l2_idx < ISP_L2PT_PTES; l2_idx++) {
+		l2_pt = mmu_info->l2_pts[l1_idx];
+		dev_dbg(mmu_info->dev,
+			"unmap l2 index %u with pteval 0x%10.10llx\n",
+			l2_idx, TBL_PHYS_ADDR(l2_pt[l2_idx]));
+		l2_pt[l2_idx] = mmu_info->dummy_page_pteval;
+
+		clflush_cache_range(&l2_pt[l2_idx], sizeof(l2_pt[l2_idx]));
+		unmapped++;
+	}
+	spin_unlock_irqrestore(&mmu_info->lock, flags);
+
+	return unmapped << ISP_PAGE_SHIFT;
+}
+
+static size_t __ipu_mmu_unmap(struct ipu_mmu_info *mmu_info,
+			      unsigned long iova, size_t size)
+{
+	return l2_unmap(mmu_info, iova, 0, size);
+}
+
+static int allocate_trash_buffer(struct ipu_mmu *mmu)
+{
+	unsigned int n_pages = PAGE_ALIGN(IPU_MMUV2_TRASH_RANGE) >> PAGE_SHIFT;
+	struct iova *iova;
+	u32 iova_addr;
+	unsigned int i;
+	dma_addr_t dma;
+	int ret;
+
+	/* Allocate 8MB in iova range */
+	iova = alloc_iova(&mmu->dmap->iovad, n_pages,
+			  mmu->dmap->mmu_info->aperture_end >> PAGE_SHIFT, 0);
+	if (!iova) {
+		dev_err(mmu->dev, "cannot allocate iova range for trash\n");
+		return -ENOMEM;
+	}
+
+	dma = dma_map_page(mmu->dmap->mmu_info->dev, mmu->trash_page, 0,
+			   PAGE_SIZE, DMA_BIDIRECTIONAL);
+	if (dma_mapping_error(mmu->dmap->mmu_info->dev, dma)) {
+		dev_err(mmu->dmap->mmu_info->dev, "Failed to map trash page\n");
+		ret = -ENOMEM;
+		goto out_free_iova;
+	}
+
+	mmu->pci_trash_page = dma;
+
+	/*
+	 * Map the 8MB iova address range to the same physical trash page
+	 * mmu->trash_page which is already reserved at the probe
+	 */
+	iova_addr = iova->pfn_lo;
+	for (i = 0; i < n_pages; i++) {
+		ret = ipu_mmu_map(mmu->dmap->mmu_info, iova_addr << PAGE_SHIFT,
+				  mmu->pci_trash_page, PAGE_SIZE);
+		if (ret) {
+			dev_err(mmu->dev,
+				"mapping trash buffer range failed\n");
+			goto out_unmap;
+		}
+
+		iova_addr++;
+	}
+
+	mmu->iova_trash_page = iova->pfn_lo << PAGE_SHIFT;
+	dev_dbg(mmu->dev, "iova trash buffer for MMUID: %d is %u\n",
+		mmu->mmid, (unsigned int)mmu->iova_trash_page);
+	return 0;
+
+out_unmap:
+	ipu_mmu_unmap(mmu->dmap->mmu_info, iova->pfn_lo << PAGE_SHIFT,
+		      (iova->pfn_hi - iova->pfn_lo + 1) << PAGE_SHIFT);
+	dma_unmap_page(mmu->dmap->mmu_info->dev, mmu->pci_trash_page,
+		       PAGE_SIZE, DMA_BIDIRECTIONAL);
+out_free_iova:
+	__free_iova(&mmu->dmap->iovad, iova);
+	return ret;
+}
+
+int ipu_mmu_hw_init(struct ipu_mmu *mmu)
+{
+	unsigned int i;
+	unsigned long flags;
+	struct ipu_mmu_info *mmu_info;
+
+	dev_dbg(mmu->dev, "mmu hw init\n");
+
+	mmu_info = mmu->dmap->mmu_info;
+
+	/* Initialise the each MMU HW block */
+	for (i = 0; i < mmu->nr_mmus; i++) {
+		struct ipu_mmu_hw *mmu_hw = &mmu->mmu_hw[i];
+		unsigned int j;
+		u16 block_addr;
+
+		/* Write page table address per MMU */
+		writel((phys_addr_t)mmu_info->l1_pt_dma,
+		       mmu->mmu_hw[i].base + REG_L1_PHYS);
+
+		/* Set info bits per MMU */
+		writel(mmu->mmu_hw[i].info_bits,
+		       mmu->mmu_hw[i].base + REG_INFO);
+
+		/* Configure MMU TLB stream configuration for L1 */
+		for (j = 0, block_addr = 0; j < mmu_hw->nr_l1streams;
+		     block_addr += mmu->mmu_hw[i].l1_block_sz[j], j++) {
+			if (block_addr > IPU_MAX_LI_BLOCK_ADDR) {
+				dev_err(mmu->dev, "invalid L1 configuration\n");
+				return -EINVAL;
+			}
+
+			/* Write block start address for each streams */
+			writel(block_addr, mmu_hw->base +
+				   mmu_hw->l1_stream_id_reg_offset + 4 * j);
+		}
+
+		/* Configure MMU TLB stream configuration for L2 */
+		for (j = 0, block_addr = 0; j < mmu_hw->nr_l2streams;
+		     block_addr += mmu->mmu_hw[i].l2_block_sz[j], j++) {
+			if (block_addr > IPU_MAX_L2_BLOCK_ADDR) {
+				dev_err(mmu->dev, "invalid L2 configuration\n");
+				return -EINVAL;
+			}
+
+			writel(block_addr, mmu_hw->base +
+				   mmu_hw->l2_stream_id_reg_offset + 4 * j);
+		}
+	}
+
+	if (!mmu->trash_page) {
+		int ret;
+
+		mmu->trash_page = alloc_page(GFP_KERNEL);
+		if (!mmu->trash_page) {
+			dev_err(mmu->dev, "insufficient memory for trash buffer\n");
+			return -ENOMEM;
+		}
+
+		ret = allocate_trash_buffer(mmu);
+		if (ret) {
+			__free_page(mmu->trash_page);
+			mmu->trash_page = NULL;
+			dev_err(mmu->dev, "trash buffer allocation failed\n");
+			return ret;
+		}
+	}
+
+	spin_lock_irqsave(&mmu->ready_lock, flags);
+	mmu->ready = true;
+	spin_unlock_irqrestore(&mmu->ready_lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(ipu_mmu_hw_init);
+
+static struct ipu_mmu_info *ipu_mmu_alloc(struct ipu_device *isp)
+{
+	struct ipu_mmu_info *mmu_info;
+	int ret;
+
+	mmu_info = kzalloc(sizeof(*mmu_info), GFP_KERNEL);
+	if (!mmu_info)
+		return NULL;
+
+	mmu_info->aperture_start = 0;
+	mmu_info->aperture_end = DMA_BIT_MASK(isp->secure_mode ?
+					      IPU_MMU_ADDRESS_BITS :
+					      IPU_MMU_ADDRESS_BITS_NON_SECURE);
+	mmu_info->pgsize_bitmap = SZ_4K;
+	mmu_info->dev = &isp->pdev->dev;
+
+	ret = get_dummy_page(mmu_info);
+	if (ret)
+		goto err_free_info;
+
+	ret = alloc_dummy_l2_pt(mmu_info);
+	if (ret)
+		goto err_free_dummy_page;
+
+	mmu_info->l2_pts = vzalloc(ISP_L2PT_PTES * sizeof(*mmu_info->l2_pts));
+	if (!mmu_info->l2_pts)
+		goto err_free_dummy_l2_pt;
+
+	/*
+	 * We always map the L1 page table (a single page as well as
+	 * the L2 page tables).
+	 */
+	mmu_info->l1_pt = alloc_l1_pt(mmu_info);
+	if (!mmu_info->l1_pt)
+		goto err_free_l2_pts;
+
+	spin_lock_init(&mmu_info->lock);
+
+	dev_dbg(mmu_info->dev, "domain initialised\n");
+
+	return mmu_info;
+
+err_free_l2_pts:
+	vfree(mmu_info->l2_pts);
+err_free_dummy_l2_pt:
+	free_dummy_l2_pt(mmu_info);
+err_free_dummy_page:
+	free_dummy_page(mmu_info);
+err_free_info:
+	kfree(mmu_info);
+
+	return NULL;
+}
+
+int ipu_mmu_hw_cleanup(struct ipu_mmu *mmu)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&mmu->ready_lock, flags);
+	mmu->ready = false;
+	spin_unlock_irqrestore(&mmu->ready_lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(ipu_mmu_hw_cleanup);
+
+static struct ipu_dma_mapping *alloc_dma_mapping(struct ipu_device *isp)
+{
+	struct ipu_dma_mapping *dmap;
+
+	dmap = kzalloc(sizeof(*dmap), GFP_KERNEL);
+	if (!dmap)
+		return NULL;
+
+	dmap->mmu_info = ipu_mmu_alloc(isp);
+	if (!dmap->mmu_info) {
+		kfree(dmap);
+		return NULL;
+	}
+	init_iova_domain(&dmap->iovad, SZ_4K, 1);
+	dmap->mmu_info->dmap = dmap;
+
+	kref_init(&dmap->ref);
+
+	dev_dbg(&isp->pdev->dev, "alloc mapping\n");
+
+	iova_cache_get();
+
+	return dmap;
+}
+
+phys_addr_t ipu_mmu_iova_to_phys(struct ipu_mmu_info *mmu_info,
+				 dma_addr_t iova)
+{
+	unsigned long flags;
+	u32 *l2_pt;
+	phys_addr_t phy_addr;
+
+	spin_lock_irqsave(&mmu_info->lock, flags);
+	l2_pt = mmu_info->l2_pts[iova >> ISP_L1PT_SHIFT];
+	phy_addr = (phys_addr_t)l2_pt[(iova & ISP_L2PT_MASK) >> ISP_L2PT_SHIFT];
+	phy_addr <<= ISP_PAGE_SHIFT;
+	spin_unlock_irqrestore(&mmu_info->lock, flags);
+
+	return phy_addr;
+}
+
+/*
+ * The following four functions are implemented based on iommu.c
+ * drivers/iommu/iommu.c:iommu_pgsize().
+ */
+static size_t ipu_mmu_pgsize(unsigned long pgsize_bitmap,
+			     unsigned long addr_merge, size_t size)
+{
+	unsigned int pgsize_idx;
+	size_t pgsize;
+
+	/* Max page size that still fits into 'size' */
+	pgsize_idx = __fls(size);
+
+	/* need to consider alignment requirements ? */
+	if (likely(addr_merge)) {
+		/* Max page size allowed by address */
+		unsigned int align_pgsize_idx = __ffs(addr_merge);
+
+		pgsize_idx = min(pgsize_idx, align_pgsize_idx);
+	}
+
+	/* build a mask of acceptable page sizes */
+	pgsize = (1UL << (pgsize_idx + 1)) - 1;
+
+	/* throw away page sizes not supported by the hardware */
+	pgsize &= pgsize_bitmap;
+
+	/* make sure we're still sane */
+	WARN_ON(!pgsize);
+
+	/* pick the biggest page */
+	pgsize_idx = __fls(pgsize);
+	pgsize = 1UL << pgsize_idx;
+
+	return pgsize;
+}
+
+/* drivers/iommu/iommu.c:iommu_unmap() */
+size_t ipu_mmu_unmap(struct ipu_mmu_info *mmu_info, unsigned long iova,
+		     size_t size)
+{
+	size_t unmapped_page, unmapped = 0;
+	unsigned int min_pagesz;
+
+	/* find out the minimum page size supported */
+	min_pagesz = 1 << __ffs(mmu_info->pgsize_bitmap);
+
+	/*
+	 * The virtual address, as well as the size of the mapping, must be
+	 * aligned (at least) to the size of the smallest page supported
+	 * by the hardware
+	 */
+	if (!IS_ALIGNED(iova | size, min_pagesz)) {
+		dev_err(NULL, "unaligned: iova 0x%lx size 0x%zx min_pagesz 0x%x\n",
+			iova, size, min_pagesz);
+		return -EINVAL;
+	}
+
+	/*
+	 * Keep iterating until we either unmap 'size' bytes (or more)
+	 * or we hit an area that isn't mapped.
+	 */
+	while (unmapped < size) {
+		size_t pgsize = ipu_mmu_pgsize(mmu_info->pgsize_bitmap,
+						iova, size - unmapped);
+
+		unmapped_page = __ipu_mmu_unmap(mmu_info, iova, pgsize);
+		if (!unmapped_page)
+			break;
+
+		dev_dbg(mmu_info->dev, "unmapped: iova 0x%lx size 0x%zx\n",
+			iova, unmapped_page);
+
+		iova += unmapped_page;
+		unmapped += unmapped_page;
+	}
+
+	return unmapped;
+}
+
+/* drivers/iommu/iommu.c:iommu_map() */
+int ipu_mmu_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
+		phys_addr_t paddr, size_t size)
+{
+	unsigned long orig_iova = iova;
+	unsigned int min_pagesz;
+	size_t orig_size = size;
+	int ret = 0;
+
+	if (mmu_info->pgsize_bitmap == 0UL)
+		return -ENODEV;
+
+	/* find out the minimum page size supported */
+	min_pagesz = 1 << __ffs(mmu_info->pgsize_bitmap);
+
+	/*
+	 * both the virtual address and the physical one, as well as
+	 * the size of the mapping, must be aligned (at least) to the
+	 * size of the smallest page supported by the hardware
+	 */
+	if (!IS_ALIGNED(iova | paddr | size, min_pagesz)) {
+		dev_err(mmu_info->dev,
+			"unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%x\n",
+			iova, &paddr, size, min_pagesz);
+		return -EINVAL;
+	}
+
+	dev_dbg(mmu_info->dev, "map: iova 0x%lx pa %pa size 0x%zx\n",
+		iova, &paddr, size);
+
+	while (size) {
+		size_t pgsize = ipu_mmu_pgsize(mmu_info->pgsize_bitmap,
+					       iova | paddr, size);
+
+		dev_dbg(mmu_info->dev,
+			"mapping: iova 0x%lx pa %pa pgsize 0x%zx\n",
+			iova, &paddr, pgsize);
+
+		ret = __ipu_mmu_map(mmu_info, iova, paddr, pgsize);
+		if (ret)
+			break;
+
+		iova += pgsize;
+		paddr += pgsize;
+		size -= pgsize;
+	}
+
+	/* unroll mapping in case something went wrong */
+	if (ret)
+		ipu_mmu_unmap(mmu_info, orig_iova, orig_size - size);
+
+	return ret;
+}
+
+static void ipu_mmu_destroy(struct ipu_mmu *mmu)
+{
+	struct ipu_dma_mapping *dmap = mmu->dmap;
+	struct ipu_mmu_info *mmu_info = dmap->mmu_info;
+	struct iova *iova;
+	u32 l1_idx;
+
+	if (mmu->iova_trash_page) {
+		iova = find_iova(&dmap->iovad,
+				 mmu->iova_trash_page >> PAGE_SHIFT);
+		if (iova) {
+			/* unmap and free the trash buffer iova */
+			ipu_mmu_unmap(mmu_info, iova->pfn_lo << PAGE_SHIFT,
+				      (iova->pfn_hi - iova->pfn_lo + 1) <<
+				      PAGE_SHIFT);
+			__free_iova(&dmap->iovad, iova);
+		} else {
+			dev_err(mmu->dev, "trash buffer iova not found.\n");
+		}
+
+		mmu->iova_trash_page = 0;
+		dma_unmap_page(mmu_info->dev, mmu->pci_trash_page,
+			       PAGE_SIZE, DMA_BIDIRECTIONAL);
+		mmu->pci_trash_page = 0;
+		__free_page(mmu->trash_page);
+	}
+
+	for (l1_idx = 0; l1_idx < ISP_L1PT_PTES; l1_idx++) {
+		if (mmu_info->l1_pt[l1_idx] != mmu_info->dummy_l2_pteval) {
+			dma_unmap_single(mmu_info->dev,
+					 TBL_PHYS_ADDR(mmu_info->l1_pt[l1_idx]),
+					 PAGE_SIZE, DMA_BIDIRECTIONAL);
+			free_page((unsigned long)mmu_info->l2_pts[l1_idx]);
+		}
+	}
+
+	free_dummy_page(mmu_info);
+	dma_unmap_single(mmu_info->dev, mmu_info->l1_pt_dma << ISP_PADDR_SHIFT,
+			 PAGE_SIZE, DMA_BIDIRECTIONAL);
+	free_page((unsigned long)mmu_info->dummy_l2_pt);
+	free_page((unsigned long)mmu_info->l1_pt);
+	kfree(mmu_info);
+}
+
+struct ipu_mmu *ipu_mmu_init(struct device *dev,
+			     void __iomem *base, int mmid,
+			     const struct ipu_hw_variants *hw)
+{
+	struct ipu_mmu *mmu;
+	struct ipu_mmu_pdata *pdata;
+	struct ipu_device *isp = pci_get_drvdata(to_pci_dev(dev));
+	unsigned int i;
+
+	if (hw->nr_mmus > IPU_MMU_MAX_DEVICES)
+		return ERR_PTR(-EINVAL);
+
+	pdata = devm_kzalloc(dev, sizeof(*pdata), GFP_KERNEL);
+	if (!pdata)
+		return ERR_PTR(-ENOMEM);
+
+	for (i = 0; i < hw->nr_mmus; i++) {
+		struct ipu_mmu_hw *pdata_mmu = &pdata->mmu_hw[i];
+		const struct ipu_mmu_hw *src_mmu = &hw->mmu_hw[i];
+
+		if (src_mmu->nr_l1streams > IPU_MMU_MAX_TLB_L1_STREAMS ||
+		    src_mmu->nr_l2streams > IPU_MMU_MAX_TLB_L2_STREAMS)
+			return ERR_PTR(-EINVAL);
+
+		*pdata_mmu = *src_mmu;
+		pdata_mmu->base = base + src_mmu->offset;
+	}
+
+	mmu = devm_kzalloc(dev, sizeof(*mmu), GFP_KERNEL);
+	if (!mmu)
+		return ERR_PTR(-ENOMEM);
+
+	mmu->mmid = mmid;
+	mmu->mmu_hw = pdata->mmu_hw;
+	mmu->nr_mmus = hw->nr_mmus;
+	mmu->tlb_invalidate = tlb_invalidate;
+	mmu->ready = false;
+	INIT_LIST_HEAD(&mmu->vma_list);
+	spin_lock_init(&mmu->ready_lock);
+
+	mmu->dmap = alloc_dma_mapping(isp);
+	if (!mmu->dmap) {
+		dev_err(dev, "can't alloc dma mapping\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	return mmu;
+}
+
+void ipu_mmu_cleanup(struct ipu_mmu *mmu)
+{
+	struct ipu_dma_mapping *dmap = mmu->dmap;
+
+	ipu_mmu_destroy(mmu);
+	mmu->dmap = NULL;
+	iova_cache_put();
+	put_iova_domain(&dmap->iovad);
+	kfree(dmap);
+}
+
+MODULE_AUTHOR("Sakari Ailus <sakari.ailus@linux.intel.com>");
+MODULE_AUTHOR("Samu Onkalo <samu.onkalo@intel.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Intel ipu mmu driver");
diff -ruN a/drivers/media/pci/intel/ipu-mmu.h b/drivers/media/pci/intel/ipu-mmu.h
--- a/drivers/media/pci/intel/ipu-mmu.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-mmu.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,76 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2021 Intel Corporation */
+
+#ifndef IPU_MMU_H
+#define IPU_MMU_H
+
+#include <linux/dma-mapping.h>
+
+#include "ipu.h"
+#include "ipu-pdata.h"
+
+#define ISYS_MMID 1
+#define PSYS_MMID 0
+
+/*
+ * @pgtbl: virtual address of the l1 page table (one page)
+ */
+struct ipu_mmu_info {
+	struct device *dev;
+
+	u32 __iomem *l1_pt;
+	u32 l1_pt_dma;
+	u32 **l2_pts;
+
+	u32 *dummy_l2_pt;
+	u32 dummy_l2_pteval;
+	void *dummy_page;
+	u32 dummy_page_pteval;
+
+	dma_addr_t aperture_start;
+	dma_addr_t aperture_end;
+	unsigned long pgsize_bitmap;
+
+	spinlock_t lock;	/* Serialize access to users */
+	struct ipu_dma_mapping *dmap;
+};
+
+/*
+ * @pgtbl: physical address of the l1 page table
+ */
+struct ipu_mmu {
+	struct list_head node;
+
+	struct ipu_mmu_hw *mmu_hw;
+	unsigned int nr_mmus;
+	int mmid;
+
+	phys_addr_t pgtbl;
+	struct device *dev;
+
+	struct ipu_dma_mapping *dmap;
+	struct list_head vma_list;
+
+	struct page *trash_page;
+	dma_addr_t pci_trash_page; /* IOVA from PCI DMA services (parent) */
+	dma_addr_t iova_trash_page; /* IOVA for IPU child nodes to use */
+
+	bool ready;
+	spinlock_t ready_lock;	/* Serialize access to bool ready */
+
+	void (*tlb_invalidate)(struct ipu_mmu *mmu);
+};
+
+struct ipu_mmu *ipu_mmu_init(struct device *dev,
+			     void __iomem *base, int mmid,
+			     const struct ipu_hw_variants *hw);
+void ipu_mmu_cleanup(struct ipu_mmu *mmu);
+int ipu_mmu_hw_init(struct ipu_mmu *mmu);
+int ipu_mmu_hw_cleanup(struct ipu_mmu *mmu);
+int ipu_mmu_map(struct ipu_mmu_info *mmu_info, unsigned long iova,
+		phys_addr_t paddr, size_t size);
+size_t ipu_mmu_unmap(struct ipu_mmu_info *mmu_info, unsigned long iova,
+		     size_t size);
+phys_addr_t ipu_mmu_iova_to_phys(struct ipu_mmu_info *mmu_info,
+				 dma_addr_t iova);
+#endif
diff -ruN a/drivers/media/pci/intel/ipu-pdata.h b/drivers/media/pci/intel/ipu-pdata.h
--- a/drivers/media/pci/intel/ipu-pdata.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-pdata.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,253 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2021 Intel Corporation */
+
+#ifndef IPU_PDATA_H
+#define IPU_PDATA_H
+
+#define IPU_MMU_NAME IPU_NAME "-mmu"
+#define IPU_ISYS_CSI2_NAME IPU_NAME "-csi2"
+#define IPU_ISYS_NAME IPU_NAME "-isys"
+#define IPU_PSYS_NAME IPU_NAME "-psys"
+#define IPU_BUTTRESS_NAME IPU_NAME "-buttress"
+
+#define IPU_MMU_MAX_DEVICES		4
+#define IPU_MMU_ADDRESS_BITS		32
+/* The firmware is accessible within the first 2 GiB only in non-secure mode. */
+#define IPU_MMU_ADDRESS_BITS_NON_SECURE	31
+
+#define IPU_MMU_MAX_TLB_L1_STREAMS	32
+#define IPU_MMU_MAX_TLB_L2_STREAMS	32
+#define IPU_MAX_LI_BLOCK_ADDR		128
+#define IPU_MAX_L2_BLOCK_ADDR		64
+
+#define IPU_ISYS_MAX_CSI2_LEGACY_PORTS	4
+#define IPU_ISYS_MAX_CSI2_COMBO_PORTS	2
+
+#define IPU_MAX_FRAME_COUNTER	0xff
+
+/*
+ * To maximize the IOSF utlization, IPU need to send requests in bursts.
+ * At the DMA interface with the buttress, there are CDC FIFOs with burst
+ * collection capability. CDC FIFO burst collectors have a configurable
+ * threshold and is configured based on the outcome of performance measurements.
+ *
+ * isys has 3 ports with IOSF interface for VC0, VC1 and VC2
+ * psys has 4 ports with IOSF interface for VC0, VC1w, VC1r and VC2
+ *
+ * Threshold values are pre-defined and are arrived at after performance
+ * evaluations on a type of IPU
+ */
+#define IPU_MAX_VC_IOSF_PORTS		4
+
+/*
+ * IPU must configure correct arbitration mechanism related to the IOSF VC
+ * requests. There are two options per VC0 and VC1 - > 0 means rearbitrate on
+ * stall and 1 means stall until the request is completed.
+ */
+#define IPU_BTRS_ARB_MODE_TYPE_REARB	0
+#define IPU_BTRS_ARB_MODE_TYPE_STALL	1
+
+/* Currently chosen arbitration mechanism for VC0 */
+#define IPU_BTRS_ARB_STALL_MODE_VC0	\
+			IPU_BTRS_ARB_MODE_TYPE_REARB
+
+/* Currently chosen arbitration mechanism for VC1 */
+#define IPU_BTRS_ARB_STALL_MODE_VC1	\
+			IPU_BTRS_ARB_MODE_TYPE_REARB
+
+struct ipu_isys_subdev_pdata;
+
+/*
+ * MMU Invalidation HW bug workaround by ZLW mechanism
+ *
+ * Old IPU MMUV2 has a bug in the invalidation mechanism which might result in
+ * wrong translation or replication of the translation. This will cause data
+ * corruption. So we cannot directly use the MMU V2 invalidation registers
+ * to invalidate the MMU. Instead, whenever an invalidate is called, we need to
+ * clear the TLB by evicting all the valid translations by filling it with trash
+ * buffer (which is guaranteed not to be used by any other processes). ZLW is
+ * used to fill the L1 and L2 caches with the trash buffer translations. ZLW
+ * or Zero length write, is pre-fetch mechanism to pre-fetch the pages in
+ * advance to the L1 and L2 caches without triggering any memory operations.
+ *
+ * In MMU V2, L1 -> 16 streams and 64 blocks, maximum 16 blocks per stream
+ * One L1 block has 16 entries, hence points to 16 * 4K pages
+ * L2 -> 16 streams and 32 blocks. 2 blocks per streams
+ * One L2 block maps to 1024 L1 entries, hence points to 4MB address range
+ * 2 blocks per L2 stream means, 1 stream points to 8MB range
+ *
+ * As we need to clear the caches and 8MB being the biggest cache size, we need
+ * to have trash buffer which points to 8MB address range. As these trash
+ * buffers are not used for any memory transactions, we need only the least
+ * amount of physical memory. So we reserve 8MB IOVA address range but only
+ * one page is reserved from physical memory. Each of this 8MB IOVA address
+ * range is then mapped to the same physical memory page.
+ */
+/* One L2 entry maps 1024 L1 entries and one L1 entry per page */
+#define IPU_MMUV2_L2_RANGE		(1024 * PAGE_SIZE)
+/* Max L2 blocks per stream */
+#define IPU_MMUV2_MAX_L2_BLOCKS		2
+/* Max L1 blocks per stream */
+#define IPU_MMUV2_MAX_L1_BLOCKS		16
+#define IPU_MMUV2_TRASH_RANGE		(IPU_MMUV2_L2_RANGE * \
+						 IPU_MMUV2_MAX_L2_BLOCKS)
+/* Entries per L1 block */
+#define MMUV2_ENTRIES_PER_L1_BLOCK		16
+#define MMUV2_TRASH_L1_BLOCK_OFFSET		(MMUV2_ENTRIES_PER_L1_BLOCK * \
+						 PAGE_SIZE)
+#define MMUV2_TRASH_L2_BLOCK_OFFSET		IPU_MMUV2_L2_RANGE
+
+/*
+ * In some of the IPU MMUs, there is provision to configure L1 and L2 page
+ * table caches. Both these L1 and L2 caches are divided into multiple sections
+ * called streams. There is maximum 16 streams for both caches. Each of these
+ * sections are subdivided into multiple blocks. When nr_l1streams = 0 and
+ * nr_l2streams = 0, means the MMU is of type MMU_V1 and do not support
+ * L1/L2 page table caches.
+ *
+ * L1 stream per block sizes are configurable and varies per usecase.
+ * L2 has constant block sizes - 2 blocks per stream.
+ *
+ * MMU1 support pre-fetching of the pages to have less cache lookup misses. To
+ * enable the pre-fetching, MMU1 AT (Address Translator) device registers
+ * need to be configured.
+ *
+ * There are four types of memory accesses which requires ZLW configuration.
+ * ZLW(Zero Length Write) is a mechanism to enable VT-d pre-fetching on IOMMU.
+ *
+ * 1. Sequential Access or 1D mode
+ *	Set ZLW_EN -> 1
+ *	set ZLW_PAGE_CROSS_1D -> 1
+ *	Set ZLW_N to "N" pages so that ZLW will be inserte N pages ahead where
+ *		  N is pre-defined and hardcoded in the platform data
+ *	Set ZLW_2D -> 0
+ *
+ * 2. ZLW 2D mode
+ *	Set ZLW_EN -> 1
+ *	set ZLW_PAGE_CROSS_1D -> 1,
+ *	Set ZLW_N -> 0
+ *	Set ZLW_2D -> 1
+ *
+ * 3. ZLW Enable (no 1D or 2D mode)
+ *	Set ZLW_EN -> 1
+ *	set ZLW_PAGE_CROSS_1D -> 0,
+ *	Set ZLW_N -> 0
+ *	Set ZLW_2D -> 0
+ *
+ * 4. ZLW disable
+ *	Set ZLW_EN -> 0
+ *	set ZLW_PAGE_CROSS_1D -> 0,
+ *	Set ZLW_N -> 0
+ *	Set ZLW_2D -> 0
+ *
+ * To configure the ZLW for the above memory access, four registers are
+ * available. Hence to track these four settings, we have the following entries
+ * in the struct ipu_mmu_hw. Each of these entries are per stream and
+ * available only for the L1 streams.
+ *
+ * a. l1_zlw_en -> To track zlw enabled per stream (ZLW_EN)
+ * b. l1_zlw_1d_mode -> Track 1D mode per stream. ZLW inserted at page boundary
+ * c. l1_ins_zlw_ahead_pages -> to track how advance the ZLW need to be inserted
+ *			Insert ZLW request N pages ahead address.
+ * d. l1_zlw_2d_mode -> To track 2D mode per stream (ZLW_2D)
+ *
+ *
+ * Currently L1/L2 streams, blocks, AT ZLW configurations etc. are pre-defined
+ * as per the usecase specific calculations. Any change to this pre-defined
+ * table has to happen in sync with IPU FW.
+ */
+struct ipu_mmu_hw {
+	union {
+		unsigned long offset;
+		void __iomem *base;
+	};
+	unsigned int info_bits;
+	u8 nr_l1streams;
+	/*
+	 * L1 has variable blocks per stream - total of 64 blocks and maximum of
+	 * 16 blocks per stream. Configurable by using the block start address
+	 * per stream. Block start address is calculated from the block size
+	 */
+	u8 l1_block_sz[IPU_MMU_MAX_TLB_L1_STREAMS];
+	/* Is ZLW is enabled in each stream */
+	bool l1_zlw_en[IPU_MMU_MAX_TLB_L1_STREAMS];
+	bool l1_zlw_1d_mode[IPU_MMU_MAX_TLB_L1_STREAMS];
+	u8 l1_ins_zlw_ahead_pages[IPU_MMU_MAX_TLB_L1_STREAMS];
+	bool l1_zlw_2d_mode[IPU_MMU_MAX_TLB_L1_STREAMS];
+
+	u32 l1_stream_id_reg_offset;
+	u32 l2_stream_id_reg_offset;
+
+	u8 nr_l2streams;
+	/*
+	 * L2 has fixed 2 blocks per stream. Block address is calculated
+	 * from the block size
+	 */
+	u8 l2_block_sz[IPU_MMU_MAX_TLB_L2_STREAMS];
+	/* flag to track if WA is needed for successive invalidate HW bug */
+	bool insert_read_before_invalidate;
+};
+
+struct ipu_mmu_pdata {
+	unsigned int nr_mmus;
+	struct ipu_mmu_hw mmu_hw[IPU_MMU_MAX_DEVICES];
+	int mmid;
+};
+
+struct ipu_isys_csi2_pdata {
+	void __iomem *base;
+};
+
+#define IPU_EV_AUTO 0xff
+
+struct ipu_isys_internal_csi2_pdata {
+	unsigned int nports;
+	unsigned int *offsets;
+};
+
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+struct ipu_isys_internal_tpg_pdata {
+	unsigned int ntpgs;
+	unsigned int *offsets;
+	unsigned int *sels;
+};
+#endif
+
+/*
+ * One place to handle all the IPU HW variations
+ */
+struct ipu_hw_variants {
+	unsigned long offset;
+	unsigned int nr_mmus;
+	struct ipu_mmu_hw mmu_hw[IPU_MMU_MAX_DEVICES];
+	u8 cdc_fifos;
+	u8 cdc_fifo_threshold[IPU_MAX_VC_IOSF_PORTS];
+	u32 dmem_offset;
+	u32 spc_offset;	/* SPC offset from psys base */
+};
+
+struct ipu_isys_internal_pdata {
+	struct ipu_isys_internal_csi2_pdata csi2;
+#ifdef CONFIG_VIDEO_INTEL_IPU_TPG
+	struct ipu_isys_internal_tpg_pdata tpg;
+#endif
+	struct ipu_hw_variants hw_variant;
+	u32 num_parallel_streams;
+	u32 isys_dma_overshoot;
+};
+
+struct ipu_isys_pdata {
+	void __iomem *base;
+	const struct ipu_isys_internal_pdata *ipdata;
+};
+
+struct ipu_psys_internal_pdata {
+	struct ipu_hw_variants hw_variant;
+};
+
+struct ipu_psys_pdata {
+	void __iomem *base;
+	const struct ipu_psys_internal_pdata *ipdata;
+};
+
+#endif
diff -ruN a/drivers/media/pci/intel/ipu-psys.c b/drivers/media/pci/intel/ipu-psys.c
--- a/drivers/media/pci/intel/ipu-psys.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-psys.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,1617 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2020 Intel Corporation
+
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-buf.h>
+#include <linux/firmware.h>
+#include <linux/fs.h>
+#include <linux/highmem.h>
+#include <linux/init_task.h>
+#include <linux/kthread.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/pm_runtime.h>
+#include <linux/version.h>
+#include <linux/poll.h>
+#include <uapi/linux/sched/types.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/dma-mapping.h>
+
+#include <uapi/linux/ipu-psys.h>
+
+#include "ipu.h"
+#include "ipu-mmu.h"
+#include "ipu-bus.h"
+#include "ipu-platform.h"
+#include "ipu-buttress.h"
+#include "ipu-cpd.h"
+#include "ipu-fw-psys.h"
+#include "ipu-psys.h"
+#include "ipu-platform-psys.h"
+#include "ipu-platform-regs.h"
+#include "ipu-fw-com.h"
+
+static bool async_fw_init;
+module_param(async_fw_init, bool, 0664);
+MODULE_PARM_DESC(async_fw_init, "Enable asynchronous firmware initialization");
+
+#define IPU_PSYS_NUM_DEVICES		4
+#define IPU_PSYS_AUTOSUSPEND_DELAY	2000
+
+#ifdef CONFIG_PM
+static int psys_runtime_pm_resume(struct device *dev);
+static int psys_runtime_pm_suspend(struct device *dev);
+#else
+#define pm_runtime_dont_use_autosuspend(d)
+#define pm_runtime_use_autosuspend(d)
+#define pm_runtime_set_autosuspend_delay(d, f)	0
+#define pm_runtime_get_sync(d)			0
+#define pm_runtime_put(d)			0
+#define pm_runtime_put_sync(d)			0
+#define pm_runtime_put_noidle(d)		0
+#define pm_runtime_put_autosuspend(d)		0
+#endif
+
+static dev_t ipu_psys_dev_t;
+static DECLARE_BITMAP(ipu_psys_devices, IPU_PSYS_NUM_DEVICES);
+static DEFINE_MUTEX(ipu_psys_mutex);
+
+static struct fw_init_task {
+	struct delayed_work work;
+	struct ipu_psys *psys;
+} fw_init_task;
+
+static void ipu_psys_remove(struct ipu_bus_device *adev);
+
+static struct bus_type ipu_psys_bus = {
+	.name = IPU_PSYS_NAME,
+};
+
+struct ipu_psys_pg *__get_pg_buf(struct ipu_psys *psys, size_t pg_size)
+{
+	struct ipu_psys_pg *kpg;
+	unsigned long flags;
+
+	spin_lock_irqsave(&psys->pgs_lock, flags);
+	list_for_each_entry(kpg, &psys->pgs, list) {
+		if (!kpg->pg_size && kpg->size >= pg_size) {
+			kpg->pg_size = pg_size;
+			spin_unlock_irqrestore(&psys->pgs_lock, flags);
+			return kpg;
+		}
+	}
+	spin_unlock_irqrestore(&psys->pgs_lock, flags);
+	/* no big enough buffer available, allocate new one */
+	kpg = kzalloc(sizeof(*kpg), GFP_KERNEL);
+	if (!kpg)
+		return NULL;
+
+	kpg->pg = dma_alloc_attrs(&psys->adev->dev, pg_size,
+				  &kpg->pg_dma_addr, GFP_KERNEL, 0);
+	if (!kpg->pg) {
+		kfree(kpg);
+		return NULL;
+	}
+
+	kpg->pg_size = pg_size;
+	kpg->size = pg_size;
+	spin_lock_irqsave(&psys->pgs_lock, flags);
+	list_add(&kpg->list, &psys->pgs);
+	spin_unlock_irqrestore(&psys->pgs_lock, flags);
+
+	return kpg;
+}
+
+static int ipu_psys_unmapbuf_locked(int fd, struct ipu_psys_fh *fh,
+				    struct ipu_psys_kbuffer *kbuf);
+struct ipu_psys_kbuffer *ipu_psys_lookup_kbuffer(struct ipu_psys_fh *fh, int fd)
+{
+	struct ipu_psys_kbuffer *kbuf;
+
+	list_for_each_entry(kbuf, &fh->bufmap, list) {
+		if (kbuf->fd == fd)
+			return kbuf;
+	}
+
+	return NULL;
+}
+
+struct ipu_psys_kbuffer *
+ipu_psys_lookup_kbuffer_by_kaddr(struct ipu_psys_fh *fh, void *kaddr)
+{
+	struct ipu_psys_kbuffer *kbuffer;
+
+	list_for_each_entry(kbuffer, &fh->bufmap, list) {
+		if (kbuffer->kaddr == kaddr)
+			return kbuffer;
+	}
+
+	return NULL;
+}
+
+static int ipu_psys_get_userpages(struct ipu_dma_buf_attach *attach)
+{
+	struct vm_area_struct *vma;
+	unsigned long start, end;
+	int npages, array_size;
+	struct page **pages;
+	struct sg_table *sgt;
+	int nr = 0;
+	int ret = -ENOMEM;
+
+	start = (unsigned long)attach->userptr;
+	end = PAGE_ALIGN(start + attach->len);
+	npages = (end - (start & PAGE_MASK)) >> PAGE_SHIFT;
+	array_size = npages * sizeof(struct page *);
+
+	sgt = kzalloc(sizeof(*sgt), GFP_KERNEL);
+	if (!sgt)
+		return -ENOMEM;
+
+	if (attach->npages != 0) {
+		pages = attach->pages;
+		npages = attach->npages;
+		attach->vma_is_io = 1;
+		goto skip_pages;
+	}
+
+	pages = kvzalloc(array_size, GFP_KERNEL);
+	if (!pages)
+		goto free_sgt;
+
+	mmap_read_lock(current->mm);
+	vma = find_vma(current->mm, start);
+	if (!vma) {
+		ret = -EFAULT;
+		goto error_up_read;
+	}
+
+	/*
+	 * For buffers from Gralloc, VM_PFNMAP is expected,
+	 * but VM_IO is set. Possibly bug in Gralloc.
+	 */
+	attach->vma_is_io = vma->vm_flags & (VM_IO | VM_PFNMAP);
+
+	if (attach->vma_is_io) {
+		unsigned long io_start = start;
+
+		if (vma->vm_end < start + attach->len) {
+			dev_err(attach->dev,
+				"vma at %lu is too small for %llu bytes\n",
+				start, attach->len);
+			ret = -EFAULT;
+			goto error_up_read;
+		}
+
+		for (nr = 0; nr < npages; nr++, io_start += PAGE_SIZE) {
+			unsigned long pfn;
+
+			ret = follow_pfn(vma, io_start, &pfn);
+			if (ret)
+				goto error_up_read;
+			pages[nr] = pfn_to_page(pfn);
+		}
+	} else {
+		nr = get_user_pages(start & PAGE_MASK, npages,
+				    FOLL_WRITE,
+				    pages, NULL);
+		if (nr < npages)
+			goto error_up_read;
+	}
+	mmap_read_unlock(current->mm);
+
+	attach->pages = pages;
+	attach->npages = npages;
+
+skip_pages:
+	ret = sg_alloc_table_from_pages(sgt, pages, npages,
+					start & ~PAGE_MASK, attach->len,
+					GFP_KERNEL);
+	if (ret < 0)
+		goto error;
+
+	attach->sgt = sgt;
+
+	return 0;
+
+error_up_read:
+	mmap_read_unlock(current->mm);
+error:
+	if (!attach->vma_is_io)
+		while (nr > 0)
+			put_page(pages[--nr]);
+
+	if (array_size <= PAGE_SIZE)
+		kfree(pages);
+	else
+		vfree(pages);
+free_sgt:
+	kfree(sgt);
+
+	dev_err(attach->dev, "failed to get userpages:%d\n", ret);
+
+	return ret;
+}
+
+static void ipu_psys_put_userpages(struct ipu_dma_buf_attach *attach)
+{
+	if (!attach || !attach->userptr || !attach->sgt)
+		return;
+
+	if (!attach->vma_is_io) {
+		int i = attach->npages;
+
+		while (--i >= 0) {
+			set_page_dirty_lock(attach->pages[i]);
+			put_page(attach->pages[i]);
+		}
+	}
+
+	kvfree(attach->pages);
+
+	sg_free_table(attach->sgt);
+	kfree(attach->sgt);
+	attach->sgt = NULL;
+}
+
+static int ipu_dma_buf_attach(struct dma_buf *dbuf,
+			      struct dma_buf_attachment *attach)
+{
+	struct ipu_psys_kbuffer *kbuf = dbuf->priv;
+	struct ipu_dma_buf_attach *ipu_attach;
+
+	ipu_attach = kzalloc(sizeof(*ipu_attach), GFP_KERNEL);
+	if (!ipu_attach)
+		return -ENOMEM;
+
+	ipu_attach->len = kbuf->len;
+	ipu_attach->userptr = kbuf->userptr;
+
+	attach->priv = ipu_attach;
+	return 0;
+}
+
+static void ipu_dma_buf_detach(struct dma_buf *dbuf,
+			       struct dma_buf_attachment *attach)
+{
+	struct ipu_dma_buf_attach *ipu_attach = attach->priv;
+
+	kfree(ipu_attach);
+	attach->priv = NULL;
+}
+
+static struct sg_table *ipu_dma_buf_map(struct dma_buf_attachment *attach,
+					enum dma_data_direction dir)
+{
+	struct ipu_dma_buf_attach *ipu_attach = attach->priv;
+	unsigned long attrs;
+	int ret;
+
+	ret = ipu_psys_get_userpages(ipu_attach);
+	if (ret)
+		return NULL;
+
+	attrs = DMA_ATTR_SKIP_CPU_SYNC;
+	ret = dma_map_sg_attrs(attach->dev, ipu_attach->sgt->sgl,
+			       ipu_attach->sgt->orig_nents, dir, attrs);
+	if (ret < ipu_attach->sgt->orig_nents) {
+		ipu_psys_put_userpages(ipu_attach);
+		dev_dbg(attach->dev, "buf map failed\n");
+
+		return ERR_PTR(-EIO);
+	}
+
+	/*
+	 * Initial cache flush to avoid writing dirty pages for buffers which
+	 * are later marked as IPU_BUFFER_FLAG_NO_FLUSH.
+	 */
+	dma_sync_sg_for_device(attach->dev, ipu_attach->sgt->sgl,
+			       ipu_attach->sgt->orig_nents, DMA_BIDIRECTIONAL);
+
+	return ipu_attach->sgt;
+}
+
+static void ipu_dma_buf_unmap(struct dma_buf_attachment *attach,
+			      struct sg_table *sg, enum dma_data_direction dir)
+{
+	struct ipu_dma_buf_attach *ipu_attach = attach->priv;
+
+	dma_unmap_sg(attach->dev, sg->sgl, sg->orig_nents, dir);
+	ipu_psys_put_userpages(ipu_attach);
+}
+
+static int ipu_dma_buf_mmap(struct dma_buf *dbuf, struct vm_area_struct *vma)
+{
+	return -ENOTTY;
+}
+
+static void ipu_dma_buf_release(struct dma_buf *buf)
+{
+	struct ipu_psys_kbuffer *kbuf = buf->priv;
+
+	if (!kbuf)
+		return;
+
+	if (kbuf->db_attach) {
+		dev_dbg(kbuf->db_attach->dev,
+			"releasing buffer %d\n", kbuf->fd);
+		ipu_psys_put_userpages(kbuf->db_attach->priv);
+	}
+	kfree(kbuf);
+}
+
+static int ipu_dma_buf_begin_cpu_access(struct dma_buf *dma_buf,
+					enum dma_data_direction dir)
+{
+	return -ENOTTY;
+}
+
+static int ipu_dma_buf_vmap(struct dma_buf *dmabuf, struct dma_buf_map *map)
+{
+	struct dma_buf_attachment *attach;
+	struct ipu_dma_buf_attach *ipu_attach;
+
+	if (list_empty(&dmabuf->attachments))
+		return -EINVAL;
+
+	attach = list_last_entry(&dmabuf->attachments,
+				 struct dma_buf_attachment, node);
+	ipu_attach = attach->priv;
+
+	if (!ipu_attach || !ipu_attach->pages || !ipu_attach->npages)
+		return -EINVAL;
+
+	map->vaddr = vm_map_ram(ipu_attach->pages, ipu_attach->npages, 0);
+	map->is_iomem = false;
+	if (!map->vaddr)
+		return -EINVAL;
+
+	return 0;
+}
+
+static void ipu_dma_buf_vunmap(struct dma_buf *dmabuf, struct dma_buf_map *map)
+{
+	struct dma_buf_attachment *attach;
+	struct ipu_dma_buf_attach *ipu_attach;
+
+	if (WARN_ON(list_empty(&dmabuf->attachments)))
+		return;
+
+	attach = list_last_entry(&dmabuf->attachments,
+				 struct dma_buf_attachment, node);
+	ipu_attach = attach->priv;
+
+	if (WARN_ON(!ipu_attach || !ipu_attach->pages || !ipu_attach->npages))
+		return;
+
+	vm_unmap_ram(map->vaddr, ipu_attach->npages);
+}
+
+struct dma_buf_ops ipu_dma_buf_ops = {
+	.attach = ipu_dma_buf_attach,
+	.detach = ipu_dma_buf_detach,
+	.map_dma_buf = ipu_dma_buf_map,
+	.unmap_dma_buf = ipu_dma_buf_unmap,
+	.release = ipu_dma_buf_release,
+	.begin_cpu_access = ipu_dma_buf_begin_cpu_access,
+	.mmap = ipu_dma_buf_mmap,
+	.vmap = ipu_dma_buf_vmap,
+	.vunmap = ipu_dma_buf_vunmap,
+};
+
+static int ipu_psys_open(struct inode *inode, struct file *file)
+{
+	struct ipu_psys *psys = inode_to_ipu_psys(inode);
+	struct ipu_device *isp = psys->adev->isp;
+	struct ipu_psys_fh *fh;
+	int rval;
+
+	if (isp->flr_done)
+		return -EIO;
+
+	fh = kzalloc(sizeof(*fh), GFP_KERNEL);
+	if (!fh)
+		return -ENOMEM;
+
+	fh->psys = psys;
+
+	file->private_data = fh;
+
+	mutex_init(&fh->mutex);
+	INIT_LIST_HEAD(&fh->bufmap);
+	init_waitqueue_head(&fh->wait);
+
+	rval = ipu_psys_fh_init(fh);
+	if (rval)
+		goto open_failed;
+
+	mutex_lock(&psys->mutex);
+	list_add_tail(&fh->list, &psys->fhs);
+	mutex_unlock(&psys->mutex);
+
+	return 0;
+
+open_failed:
+	mutex_destroy(&fh->mutex);
+	kfree(fh);
+	return rval;
+}
+
+static inline void ipu_psys_kbuf_unmap(struct ipu_psys_kbuffer *kbuf)
+{
+	if (!kbuf)
+		return;
+
+	kbuf->valid = false;
+	if (kbuf->kaddr) {
+		struct dma_buf_map dmap;
+
+		dma_buf_map_set_vaddr(&dmap, kbuf->kaddr);
+		dma_buf_vunmap(kbuf->dbuf, &dmap);
+	}
+	if (kbuf->sgt)
+		dma_buf_unmap_attachment(kbuf->db_attach,
+					 kbuf->sgt,
+					 DMA_BIDIRECTIONAL);
+	if (kbuf->db_attach)
+		dma_buf_detach(kbuf->dbuf, kbuf->db_attach);
+	dma_buf_put(kbuf->dbuf);
+
+	kbuf->db_attach = NULL;
+	kbuf->dbuf = NULL;
+	kbuf->sgt = NULL;
+}
+
+static int ipu_psys_release(struct inode *inode, struct file *file)
+{
+	struct ipu_psys *psys = inode_to_ipu_psys(inode);
+	struct ipu_psys_fh *fh = file->private_data;
+	struct ipu_psys_kbuffer *kbuf, *kbuf0;
+	struct dma_buf_attachment *db_attach;
+
+	mutex_lock(&fh->mutex);
+	/* clean up buffers */
+	if (!list_empty(&fh->bufmap)) {
+		list_for_each_entry_safe(kbuf, kbuf0, &fh->bufmap, list) {
+			list_del(&kbuf->list);
+			db_attach = kbuf->db_attach;
+
+			/* Unmap and release buffers */
+			if (kbuf->dbuf && db_attach) {
+
+				ipu_psys_kbuf_unmap(kbuf);
+			} else {
+				if (db_attach)
+					ipu_psys_put_userpages(db_attach->priv);
+				kfree(kbuf);
+			}
+		}
+	}
+	mutex_unlock(&fh->mutex);
+
+	mutex_lock(&psys->mutex);
+	list_del(&fh->list);
+
+	mutex_unlock(&psys->mutex);
+	ipu_psys_fh_deinit(fh);
+
+	mutex_lock(&psys->mutex);
+	if (list_empty(&psys->fhs))
+		psys->power_gating = 0;
+	mutex_unlock(&psys->mutex);
+	mutex_destroy(&fh->mutex);
+	kfree(fh);
+
+	return 0;
+}
+
+static int ipu_psys_getbuf(struct ipu_psys_buffer *buf, struct ipu_psys_fh *fh)
+{
+	struct ipu_psys_kbuffer *kbuf;
+	struct ipu_psys *psys = fh->psys;
+
+	DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
+	struct dma_buf *dbuf;
+	int ret;
+
+	if (!buf->base.userptr) {
+		dev_err(&psys->adev->dev, "Buffer allocation not supported\n");
+		return -EINVAL;
+	}
+
+	kbuf = kzalloc(sizeof(*kbuf), GFP_KERNEL);
+	if (!kbuf)
+		return -ENOMEM;
+
+	kbuf->len = buf->len;
+	kbuf->userptr = buf->base.userptr;
+	kbuf->flags = buf->flags;
+
+	exp_info.ops = &ipu_dma_buf_ops;
+	exp_info.size = kbuf->len;
+	exp_info.flags = O_RDWR;
+	exp_info.priv = kbuf;
+
+	dbuf = dma_buf_export(&exp_info);
+	if (IS_ERR(dbuf)) {
+		kfree(kbuf);
+		return PTR_ERR(dbuf);
+	}
+
+	ret = dma_buf_fd(dbuf, 0);
+	if (ret < 0) {
+		kfree(kbuf);
+		dma_buf_put(dbuf);
+		return ret;
+	}
+
+	kbuf->fd = ret;
+	buf->base.fd = ret;
+	kbuf->flags = buf->flags &= ~IPU_BUFFER_FLAG_USERPTR;
+	kbuf->flags = buf->flags |= IPU_BUFFER_FLAG_DMA_HANDLE;
+
+	mutex_lock(&fh->mutex);
+	list_add(&kbuf->list, &fh->bufmap);
+	mutex_unlock(&fh->mutex);
+
+	dev_dbg(&psys->adev->dev, "IOC_GETBUF: userptr %p size %llu to fd %d",
+		buf->base.userptr, buf->len, buf->base.fd);
+
+	return 0;
+}
+
+static int ipu_psys_putbuf(struct ipu_psys_buffer *buf, struct ipu_psys_fh *fh)
+{
+	return 0;
+}
+
+int ipu_psys_mapbuf_locked(int fd, struct ipu_psys_fh *fh,
+			   struct ipu_psys_kbuffer *kbuf)
+{
+	struct ipu_psys *psys = fh->psys;
+	struct dma_buf *dbuf;
+	struct dma_buf_map dmap;
+	int ret;
+
+	dbuf = dma_buf_get(fd);
+	if (IS_ERR(dbuf))
+		return -EINVAL;
+
+	if (!kbuf) {
+		/* This fd isn't generated by ipu_psys_getbuf, it
+		 * is a new fd. Create a new kbuf item for this fd, and
+		 * add this kbuf to bufmap list.
+		 */
+		kbuf = kzalloc(sizeof(*kbuf), GFP_KERNEL);
+		if (!kbuf) {
+			ret = -ENOMEM;
+			goto mapbuf_fail;
+		}
+
+		list_add(&kbuf->list, &fh->bufmap);
+	}
+
+	/* fd valid and found, need remap */
+	if (kbuf->dbuf && (kbuf->dbuf != dbuf || kbuf->len != dbuf->size)) {
+		dev_dbg(&psys->adev->dev,
+			"dmabuf fd %d with kbuf %p changed, need remap.\n",
+			fd, kbuf);
+		ret = ipu_psys_unmapbuf_locked(fd, fh, kbuf);
+		if (ret)
+			goto mapbuf_fail;
+
+		kbuf = ipu_psys_lookup_kbuffer(fh, fd);
+		/* changed external dmabuf */
+		if (!kbuf) {
+			kbuf = kzalloc(sizeof(*kbuf), GFP_KERNEL);
+			if (!kbuf) {
+				ret = -ENOMEM;
+				goto mapbuf_fail;
+			}
+			list_add(&kbuf->list, &fh->bufmap);
+		}
+	}
+
+	if (kbuf->sgt) {
+		dev_dbg(&psys->adev->dev, "fd %d has been mapped!\n", fd);
+		dma_buf_put(dbuf);
+		goto mapbuf_end;
+	}
+
+	kbuf->dbuf = dbuf;
+
+	if (kbuf->len == 0)
+		kbuf->len = kbuf->dbuf->size;
+
+	kbuf->fd = fd;
+
+	kbuf->db_attach = dma_buf_attach(kbuf->dbuf, &psys->adev->dev);
+	if (IS_ERR(kbuf->db_attach)) {
+		ret = PTR_ERR(kbuf->db_attach);
+		dev_dbg(&psys->adev->dev, "dma buf attach failed\n");
+		goto kbuf_map_fail;
+	}
+
+	kbuf->sgt = dma_buf_map_attachment(kbuf->db_attach, DMA_BIDIRECTIONAL);
+	if (IS_ERR_OR_NULL(kbuf->sgt)) {
+		ret = -EINVAL;
+		kbuf->sgt = NULL;
+		dev_dbg(&psys->adev->dev, "dma buf map attachment failed\n");
+		goto kbuf_map_fail;
+	}
+
+	kbuf->dma_addr = sg_dma_address(kbuf->sgt->sgl);
+
+	ret = dma_buf_vmap(kbuf->dbuf, &dmap);
+	if (ret) {
+		dev_dbg(&psys->adev->dev, "dma buf vmap failed\n");
+		goto kbuf_map_fail;
+	}
+	kbuf->kaddr = dmap.vaddr;
+
+	dev_dbg(&psys->adev->dev, "%s kbuf %p fd %d with len %llu mapped\n",
+		__func__, kbuf, fd, kbuf->len);
+mapbuf_end:
+
+	kbuf->valid = true;
+
+	return 0;
+
+kbuf_map_fail:
+	ipu_psys_kbuf_unmap(kbuf);
+
+	list_del(&kbuf->list);
+	if (!kbuf->userptr)
+		kfree(kbuf);
+	return ret;
+
+mapbuf_fail:
+	dma_buf_put(dbuf);
+
+	dev_err(&psys->adev->dev, "%s failed for fd %d\n", __func__, fd);
+	return ret;
+}
+
+static long ipu_psys_mapbuf(int fd, struct ipu_psys_fh *fh)
+{
+	long ret;
+	struct ipu_psys_kbuffer *kbuf;
+
+	mutex_lock(&fh->mutex);
+	kbuf = ipu_psys_lookup_kbuffer(fh, fd);
+	ret = ipu_psys_mapbuf_locked(fd, fh, kbuf);
+	mutex_unlock(&fh->mutex);
+
+	dev_dbg(&fh->psys->adev->dev, "IOC_MAPBUF ret %ld\n", ret);
+
+	return ret;
+}
+
+static int ipu_psys_unmapbuf_locked(int fd, struct ipu_psys_fh *fh,
+				    struct ipu_psys_kbuffer *kbuf)
+{
+	struct ipu_psys *psys = fh->psys;
+
+	if (!kbuf || fd != kbuf->fd) {
+		dev_err(&psys->adev->dev, "invalid kbuffer\n");
+		return -EINVAL;
+	}
+
+	/* From now on it is not safe to use this kbuffer */
+	ipu_psys_kbuf_unmap(kbuf);
+
+	list_del(&kbuf->list);
+
+	if (!kbuf->userptr)
+		kfree(kbuf);
+
+	dev_dbg(&psys->adev->dev, "%s fd %d unmapped\n", __func__, fd);
+
+	return 0;
+}
+
+static long ipu_psys_unmapbuf(int fd, struct ipu_psys_fh *fh)
+{
+	struct ipu_psys_kbuffer *kbuf;
+	long ret;
+
+	mutex_lock(&fh->mutex);
+	kbuf = ipu_psys_lookup_kbuffer(fh, fd);
+	if (!kbuf) {
+		dev_err(&fh->psys->adev->dev,
+			"buffer with fd %d not found\n", fd);
+		mutex_unlock(&fh->mutex);
+		return -EINVAL;
+	}
+	ret = ipu_psys_unmapbuf_locked(fd, fh, kbuf);
+	mutex_unlock(&fh->mutex);
+
+	dev_dbg(&fh->psys->adev->dev, "IOC_UNMAPBUF\n");
+
+	return ret;
+}
+
+static unsigned int ipu_psys_poll(struct file *file,
+				  struct poll_table_struct *wait)
+{
+	struct ipu_psys_fh *fh = file->private_data;
+	struct ipu_psys *psys = fh->psys;
+	unsigned int res = 0;
+
+	dev_dbg(&psys->adev->dev, "ipu psys poll\n");
+
+	poll_wait(file, &fh->wait, wait);
+
+	if (ipu_get_completed_kcmd(fh))
+		res = POLLIN;
+
+	dev_dbg(&psys->adev->dev, "ipu psys poll res %u\n", res);
+
+	return res;
+}
+
+static long ipu_get_manifest(struct ipu_psys_manifest *manifest,
+			     struct ipu_psys_fh *fh)
+{
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_device *isp = psys->adev->isp;
+	struct ipu_cpd_client_pkg_hdr *client_pkg;
+	u32 entries;
+	void *host_fw_data;
+	dma_addr_t dma_fw_data;
+	u32 client_pkg_offset;
+
+	host_fw_data = (void *)isp->cpd_fw->data;
+	dma_fw_data = sg_dma_address(psys->fw_sgt.sgl);
+
+	entries = ipu_cpd_pkg_dir_get_num_entries(psys->pkg_dir);
+	if (!manifest || manifest->index > entries - 1) {
+		dev_err(&psys->adev->dev, "invalid argument\n");
+		return -EINVAL;
+	}
+
+	if (!ipu_cpd_pkg_dir_get_size(psys->pkg_dir, manifest->index) ||
+	    ipu_cpd_pkg_dir_get_type(psys->pkg_dir, manifest->index) <
+	    IPU_CPD_PKG_DIR_CLIENT_PG_TYPE) {
+		dev_dbg(&psys->adev->dev, "invalid pkg dir entry\n");
+		return -ENOENT;
+	}
+
+	client_pkg_offset = ipu_cpd_pkg_dir_get_address(psys->pkg_dir,
+							manifest->index);
+	client_pkg_offset -= dma_fw_data;
+
+	client_pkg = host_fw_data + client_pkg_offset;
+	manifest->size = client_pkg->pg_manifest_size;
+
+	if (!manifest->manifest)
+		return 0;
+
+	if (copy_to_user(manifest->manifest,
+			 (uint8_t *)client_pkg + client_pkg->pg_manifest_offs,
+			 manifest->size)) {
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static long ipu_psys_ioctl(struct file *file, unsigned int cmd,
+			   unsigned long arg)
+{
+	union {
+		struct ipu_psys_buffer buf;
+		struct ipu_psys_command cmd;
+		struct ipu_psys_event ev;
+		struct ipu_psys_capability caps;
+		struct ipu_psys_manifest m;
+	} karg;
+	struct ipu_psys_fh *fh = file->private_data;
+	long err = 0;
+	void __user *up = (void __user *)arg;
+	bool copy = (cmd != IPU_IOC_MAPBUF && cmd != IPU_IOC_UNMAPBUF);
+
+	if (copy) {
+		if (_IOC_SIZE(cmd) > sizeof(karg))
+			return -ENOTTY;
+
+		if (_IOC_DIR(cmd) & _IOC_WRITE) {
+			err = copy_from_user(&karg, up, _IOC_SIZE(cmd));
+			if (err)
+				return -EFAULT;
+		}
+	}
+
+	switch (cmd) {
+	case IPU_IOC_MAPBUF:
+		err = ipu_psys_mapbuf(arg, fh);
+		break;
+	case IPU_IOC_UNMAPBUF:
+		err = ipu_psys_unmapbuf(arg, fh);
+		break;
+	case IPU_IOC_QUERYCAP:
+		karg.caps = fh->psys->caps;
+		break;
+	case IPU_IOC_GETBUF:
+		err = ipu_psys_getbuf(&karg.buf, fh);
+		break;
+	case IPU_IOC_PUTBUF:
+		err = ipu_psys_putbuf(&karg.buf, fh);
+		break;
+	case IPU_IOC_QCMD:
+		err = ipu_psys_kcmd_new(&karg.cmd, fh);
+		break;
+	case IPU_IOC_DQEVENT:
+		err = ipu_ioctl_dqevent(&karg.ev, fh, file->f_flags);
+		break;
+	case IPU_IOC_GET_MANIFEST:
+		err = ipu_get_manifest(&karg.m, fh);
+		break;
+	default:
+		err = -ENOTTY;
+		break;
+	}
+
+	if (err)
+		return err;
+
+	if (copy && _IOC_DIR(cmd) & _IOC_READ)
+		if (copy_to_user(up, &karg, _IOC_SIZE(cmd)))
+			return -EFAULT;
+
+	return 0;
+}
+
+static const struct file_operations ipu_psys_fops = {
+	.open = ipu_psys_open,
+	.release = ipu_psys_release,
+	.unlocked_ioctl = ipu_psys_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl = ipu_psys_compat_ioctl32,
+#endif
+	.poll = ipu_psys_poll,
+	.owner = THIS_MODULE,
+};
+
+static void ipu_psys_dev_release(struct device *dev)
+{
+}
+
+#ifdef CONFIG_PM
+static int psys_runtime_pm_resume(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	struct ipu_psys *psys = ipu_bus_get_drvdata(adev);
+	unsigned long flags;
+	int retval;
+
+	if (!psys)
+		return 0;
+
+	/*
+	 * In runtime autosuspend mode, if the psys is in power on state, no
+	 * need to resume again.
+	 */
+	spin_lock_irqsave(&psys->ready_lock, flags);
+	if (psys->ready) {
+		spin_unlock_irqrestore(&psys->ready_lock, flags);
+		return 0;
+	}
+	spin_unlock_irqrestore(&psys->ready_lock, flags);
+
+	retval = ipu_mmu_hw_init(adev->mmu);
+	if (retval)
+		return retval;
+
+	if (async_fw_init && !psys->fwcom) {
+		dev_err(dev,
+			"%s: asynchronous firmware init not finished, skipping\n",
+			__func__);
+		return 0;
+	}
+
+	if (!ipu_buttress_auth_done(adev->isp)) {
+		dev_dbg(dev, "%s: not yet authenticated, skipping\n", __func__);
+		return 0;
+	}
+
+	ipu_psys_setup_hw(psys);
+
+	ipu_psys_subdomains_power(psys, 1);
+	ipu_trace_restore(&psys->adev->dev);
+
+	ipu_configure_spc(adev->isp,
+			  &psys->pdata->ipdata->hw_variant,
+			  IPU_CPD_PKG_DIR_PSYS_SERVER_IDX,
+			  psys->pdata->base, psys->pkg_dir,
+			  psys->pkg_dir_dma_addr);
+
+	retval = ipu_fw_psys_open(psys);
+	if (retval) {
+		dev_err(&psys->adev->dev, "Failed to open abi.\n");
+		return retval;
+	}
+
+	spin_lock_irqsave(&psys->ready_lock, flags);
+	psys->ready = 1;
+	spin_unlock_irqrestore(&psys->ready_lock, flags);
+
+	return 0;
+}
+
+static int psys_runtime_pm_suspend(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	struct ipu_psys *psys = ipu_bus_get_drvdata(adev);
+	unsigned long flags;
+	int rval;
+
+	if (!psys)
+		return 0;
+
+	if (!psys->ready)
+		return 0;
+
+	spin_lock_irqsave(&psys->ready_lock, flags);
+	psys->ready = 0;
+	spin_unlock_irqrestore(&psys->ready_lock, flags);
+
+	/*
+	 * We can trace failure but better to not return an error.
+	 * At suspend we are progressing towards psys power gated state.
+	 * Any hang / failure inside psys will be forgotten soon.
+	 */
+	rval = ipu_fw_psys_close(psys);
+	if (rval)
+		dev_err(dev, "Device close failure: %d\n", rval);
+
+	ipu_psys_subdomains_power(psys, 0);
+
+	ipu_mmu_hw_cleanup(adev->mmu);
+
+	return 0;
+}
+
+/* The following PM callbacks are needed to enable runtime PM in IPU PCI
+ * device resume, otherwise, runtime PM can't work in PCI resume from
+ * S3 state.
+ */
+static int psys_resume(struct device *dev)
+{
+	return 0;
+}
+
+static int psys_suspend(struct device *dev)
+{
+	return 0;
+}
+
+static const struct dev_pm_ops psys_pm_ops = {
+	.runtime_suspend = psys_runtime_pm_suspend,
+	.runtime_resume = psys_runtime_pm_resume,
+	.suspend = psys_suspend,
+	.resume = psys_resume,
+};
+
+#define PSYS_PM_OPS (&psys_pm_ops)
+#else
+#define PSYS_PM_OPS NULL
+#endif
+
+static int cpd_fw_reload(struct ipu_device *isp)
+{
+	struct ipu_psys *psys = ipu_bus_get_drvdata(isp->psys);
+	int rval;
+
+	if (!isp->secure_mode) {
+		dev_warn(&isp->pdev->dev,
+			 "CPD firmware reload was only supported for secure mode.\n");
+		return -EINVAL;
+	}
+
+	if (isp->cpd_fw) {
+		ipu_cpd_free_pkg_dir(isp->psys, psys->pkg_dir,
+				     psys->pkg_dir_dma_addr,
+				     psys->pkg_dir_size);
+
+		ipu_buttress_unmap_fw_image(isp->psys, &psys->fw_sgt);
+		release_firmware(isp->cpd_fw);
+		isp->cpd_fw = NULL;
+		dev_info(&isp->pdev->dev, "Old FW removed\n");
+	}
+
+	rval = request_cpd_fw(&isp->cpd_fw, isp->cpd_fw_name,
+			      &isp->pdev->dev);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Requesting firmware(%s) failed\n",
+			isp->cpd_fw_name);
+		return rval;
+	}
+
+	rval = ipu_cpd_validate_cpd_file(isp, isp->cpd_fw->data,
+					 isp->cpd_fw->size);
+	if (rval) {
+		dev_err(&isp->pdev->dev, "Failed to validate cpd file\n");
+		goto out_release_firmware;
+	}
+
+	rval = ipu_buttress_map_fw_image(isp->psys, isp->cpd_fw, &psys->fw_sgt);
+	if (rval)
+		goto out_release_firmware;
+
+	psys->pkg_dir = ipu_cpd_create_pkg_dir(isp->psys,
+					       isp->cpd_fw->data,
+					       sg_dma_address(psys->fw_sgt.sgl),
+					       &psys->pkg_dir_dma_addr,
+					       &psys->pkg_dir_size);
+
+	if (!psys->pkg_dir) {
+		rval = -EINVAL;
+		goto out_unmap_fw_image;
+	}
+
+	isp->pkg_dir = psys->pkg_dir;
+	isp->pkg_dir_dma_addr = psys->pkg_dir_dma_addr;
+	isp->pkg_dir_size = psys->pkg_dir_size;
+
+	if (!isp->secure_mode)
+		return 0;
+
+	rval = ipu_fw_authenticate(isp, 1);
+	if (rval)
+		goto out_free_pkg_dir;
+
+	return 0;
+
+out_free_pkg_dir:
+	ipu_cpd_free_pkg_dir(isp->psys, psys->pkg_dir,
+			     psys->pkg_dir_dma_addr, psys->pkg_dir_size);
+out_unmap_fw_image:
+	ipu_buttress_unmap_fw_image(isp->psys, &psys->fw_sgt);
+out_release_firmware:
+	release_firmware(isp->cpd_fw);
+	isp->cpd_fw = NULL;
+
+	return rval;
+}
+
+#ifdef CONFIG_DEBUG_FS
+static int ipu_psys_icache_prefetch_sp_get(void *data, u64 *val)
+{
+	struct ipu_psys *psys = data;
+
+	*val = psys->icache_prefetch_sp;
+	return 0;
+}
+
+static int ipu_psys_icache_prefetch_sp_set(void *data, u64 val)
+{
+	struct ipu_psys *psys = data;
+
+	if (val != !!val)
+		return -EINVAL;
+
+	psys->icache_prefetch_sp = val;
+
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(psys_icache_prefetch_sp_fops,
+			ipu_psys_icache_prefetch_sp_get,
+			ipu_psys_icache_prefetch_sp_set, "%llu\n");
+
+static int ipu_psys_icache_prefetch_isp_get(void *data, u64 *val)
+{
+	struct ipu_psys *psys = data;
+
+	*val = psys->icache_prefetch_isp;
+	return 0;
+}
+
+static int ipu_psys_icache_prefetch_isp_set(void *data, u64 val)
+{
+	struct ipu_psys *psys = data;
+
+	if (val != !!val)
+		return -EINVAL;
+
+	psys->icache_prefetch_isp = val;
+
+	return 0;
+}
+
+DEFINE_SIMPLE_ATTRIBUTE(psys_icache_prefetch_isp_fops,
+			ipu_psys_icache_prefetch_isp_get,
+			ipu_psys_icache_prefetch_isp_set, "%llu\n");
+
+static int ipu_psys_init_debugfs(struct ipu_psys *psys)
+{
+	struct dentry *file;
+	struct dentry *dir;
+
+	dir = debugfs_create_dir("psys", psys->adev->isp->ipu_dir);
+	if (IS_ERR(dir))
+		return -ENOMEM;
+
+	file = debugfs_create_file("icache_prefetch_sp", 0600,
+				   dir, psys, &psys_icache_prefetch_sp_fops);
+	if (IS_ERR(file))
+		goto err;
+
+	file = debugfs_create_file("icache_prefetch_isp", 0600,
+				   dir, psys, &psys_icache_prefetch_isp_fops);
+	if (IS_ERR(file))
+		goto err;
+
+	psys->debugfsdir = dir;
+
+#ifdef IPU_PSYS_GPC
+	if (ipu_psys_gpc_init_debugfs(psys))
+		return -ENOMEM;
+#endif
+
+	return 0;
+err:
+	debugfs_remove_recursive(dir);
+	return -ENOMEM;
+}
+#endif
+
+static int ipu_psys_sched_cmd(void *ptr)
+{
+	struct ipu_psys *psys = ptr;
+	size_t pending = 0;
+
+	while (1) {
+		wait_event_interruptible(psys->sched_cmd_wq,
+					 (kthread_should_stop() ||
+					  (pending =
+					   atomic_read(&psys->wakeup_count))));
+
+		if (kthread_should_stop())
+			break;
+
+		if (pending == 0)
+			continue;
+
+		mutex_lock(&psys->mutex);
+		atomic_set(&psys->wakeup_count, 0);
+		ipu_psys_run_next(psys);
+		mutex_unlock(&psys->mutex);
+	}
+
+	return 0;
+}
+
+static void start_sp(struct ipu_bus_device *adev)
+{
+	struct ipu_psys *psys = ipu_bus_get_drvdata(adev);
+	void __iomem *spc_regs_base = psys->pdata->base +
+	    psys->pdata->ipdata->hw_variant.spc_offset;
+	u32 val = 0;
+
+	val |= IPU_PSYS_SPC_STATUS_START |
+	    IPU_PSYS_SPC_STATUS_RUN |
+	    IPU_PSYS_SPC_STATUS_CTRL_ICACHE_INVALIDATE;
+	val |= psys->icache_prefetch_sp ?
+	    IPU_PSYS_SPC_STATUS_ICACHE_PREFETCH : 0;
+	writel(val, spc_regs_base + IPU_PSYS_REG_SPC_STATUS_CTRL);
+}
+
+static int query_sp(struct ipu_bus_device *adev)
+{
+	struct ipu_psys *psys = ipu_bus_get_drvdata(adev);
+	void __iomem *spc_regs_base = psys->pdata->base +
+	    psys->pdata->ipdata->hw_variant.spc_offset;
+	u32 val = readl(spc_regs_base + IPU_PSYS_REG_SPC_STATUS_CTRL);
+
+	/* return true when READY == 1, START == 0 */
+	val &= IPU_PSYS_SPC_STATUS_READY | IPU_PSYS_SPC_STATUS_START;
+
+	return val == IPU_PSYS_SPC_STATUS_READY;
+}
+
+static int ipu_psys_fw_init(struct ipu_psys *psys)
+{
+	unsigned int size;
+	struct ipu_fw_syscom_queue_config *queue_cfg;
+	struct ipu_fw_syscom_queue_config fw_psys_event_queue_cfg[] = {
+		{
+			IPU_FW_PSYS_EVENT_QUEUE_SIZE,
+			sizeof(struct ipu_fw_psys_event)
+		}
+	};
+	struct ipu_fw_psys_srv_init server_init = {
+		.ddr_pkg_dir_address = 0,
+		.host_ddr_pkg_dir = NULL,
+		.pkg_dir_size = 0,
+		.icache_prefetch_sp = psys->icache_prefetch_sp,
+		.icache_prefetch_isp = psys->icache_prefetch_isp,
+	};
+	struct ipu_fw_com_cfg fwcom = {
+		.num_output_queues = IPU_FW_PSYS_N_PSYS_EVENT_QUEUE_ID,
+		.output = fw_psys_event_queue_cfg,
+		.specific_addr = &server_init,
+		.specific_size = sizeof(server_init),
+		.cell_start = start_sp,
+		.cell_ready = query_sp,
+		.buttress_boot_offset = SYSCOM_BUTTRESS_FW_PARAMS_PSYS_OFFSET,
+	};
+	int i;
+
+	size = IPU6SE_FW_PSYS_N_PSYS_CMD_QUEUE_ID;
+	if (ipu_ver == IPU_VER_6 || ipu_ver == IPU_VER_6EP)
+		size = IPU6_FW_PSYS_N_PSYS_CMD_QUEUE_ID;
+
+	queue_cfg = devm_kzalloc(&psys->adev->dev, sizeof(*queue_cfg) * size,
+				 GFP_KERNEL);
+	if (!queue_cfg)
+		return -ENOMEM;
+
+	for (i = 0; i < size; i++) {
+		queue_cfg[i].queue_size = IPU_FW_PSYS_CMD_QUEUE_SIZE;
+		queue_cfg[i].token_size = sizeof(struct ipu_fw_psys_cmd);
+	}
+
+	fwcom.input = queue_cfg;
+	fwcom.num_input_queues = size;
+	fwcom.dmem_addr = psys->pdata->ipdata->hw_variant.dmem_offset;
+
+	psys->fwcom = ipu_fw_com_prepare(&fwcom, psys->adev, psys->pdata->base);
+	if (!psys->fwcom) {
+		dev_err(&psys->adev->dev, "psys fw com prepare failed\n");
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static void run_fw_init_work(struct work_struct *work)
+{
+	struct fw_init_task *task = (struct fw_init_task *)work;
+	struct ipu_psys *psys = task->psys;
+	int rval;
+
+	rval = ipu_psys_fw_init(psys);
+
+	if (rval) {
+		dev_err(&psys->adev->dev, "FW init failed(%d)\n", rval);
+		ipu_psys_remove(psys->adev);
+	} else {
+		dev_info(&psys->adev->dev, "FW init done\n");
+	}
+}
+
+static int ipu_psys_probe(struct ipu_bus_device *adev)
+{
+	struct ipu_device *isp = adev->isp;
+	struct ipu_psys_pg *kpg, *kpg0;
+	struct ipu_psys *psys;
+	unsigned int minor;
+	int i, rval = -E2BIG;
+
+	rval = ipu_mmu_hw_init(adev->mmu);
+	if (rval)
+		return rval;
+
+	mutex_lock(&ipu_psys_mutex);
+
+	minor = find_next_zero_bit(ipu_psys_devices, IPU_PSYS_NUM_DEVICES, 0);
+	if (minor == IPU_PSYS_NUM_DEVICES) {
+		dev_err(&adev->dev, "too many devices\n");
+		goto out_unlock;
+	}
+
+	psys = devm_kzalloc(&adev->dev, sizeof(*psys), GFP_KERNEL);
+	if (!psys) {
+		rval = -ENOMEM;
+		goto out_unlock;
+	}
+
+	psys->adev = adev;
+	psys->pdata = adev->pdata;
+	psys->icache_prefetch_sp = 0;
+
+	psys->power_gating = 0;
+
+	ipu_trace_init(adev->isp, psys->pdata->base, &adev->dev,
+		       psys_trace_blocks);
+
+	cdev_init(&psys->cdev, &ipu_psys_fops);
+	psys->cdev.owner = ipu_psys_fops.owner;
+
+	rval = cdev_add(&psys->cdev, MKDEV(MAJOR(ipu_psys_dev_t), minor), 1);
+	if (rval) {
+		dev_err(&adev->dev, "cdev_add failed (%d)\n", rval);
+		goto out_unlock;
+	}
+
+	set_bit(minor, ipu_psys_devices);
+
+	spin_lock_init(&psys->ready_lock);
+	spin_lock_init(&psys->pgs_lock);
+	psys->ready = 0;
+	psys->timeout = IPU_PSYS_CMD_TIMEOUT_MS;
+
+	mutex_init(&psys->mutex);
+	INIT_LIST_HEAD(&psys->fhs);
+	INIT_LIST_HEAD(&psys->pgs);
+	INIT_LIST_HEAD(&psys->started_kcmds_list);
+
+	init_waitqueue_head(&psys->sched_cmd_wq);
+	atomic_set(&psys->wakeup_count, 0);
+	/*
+	 * Create a thread to schedule commands sent to IPU firmware.
+	 * The thread reduces the coupling between the command scheduler
+	 * and queueing commands from the user to driver.
+	 */
+	psys->sched_cmd_thread = kthread_run(ipu_psys_sched_cmd, psys,
+					     "psys_sched_cmd");
+
+	if (IS_ERR(psys->sched_cmd_thread)) {
+		psys->sched_cmd_thread = NULL;
+		mutex_destroy(&psys->mutex);
+		goto out_unlock;
+	}
+
+	ipu_bus_set_drvdata(adev, psys);
+
+	rval = ipu_psys_resource_pool_init(&psys->resource_pool_running);
+	if (rval < 0) {
+		dev_err(&psys->dev,
+			"unable to alloc process group resources\n");
+		goto out_mutex_destroy;
+	}
+
+	ipu6_psys_hw_res_variant_init();
+	psys->pkg_dir = isp->pkg_dir;
+	psys->pkg_dir_dma_addr = isp->pkg_dir_dma_addr;
+	psys->pkg_dir_size = isp->pkg_dir_size;
+	psys->fw_sgt = isp->fw_sgt;
+
+	/* allocate and map memory for process groups */
+	for (i = 0; i < IPU_PSYS_PG_POOL_SIZE; i++) {
+		kpg = kzalloc(sizeof(*kpg), GFP_KERNEL);
+		if (!kpg)
+			goto out_free_pgs;
+		kpg->pg = dma_alloc_attrs(&adev->dev,
+					  IPU_PSYS_PG_MAX_SIZE,
+					  &kpg->pg_dma_addr,
+					  GFP_KERNEL, 0);
+		if (!kpg->pg) {
+			kfree(kpg);
+			goto out_free_pgs;
+		}
+		kpg->size = IPU_PSYS_PG_MAX_SIZE;
+		list_add(&kpg->list, &psys->pgs);
+	}
+
+	psys->caps.pg_count = ipu_cpd_pkg_dir_get_num_entries(psys->pkg_dir);
+
+	dev_info(&adev->dev, "pkg_dir entry count:%d\n", psys->caps.pg_count);
+	if (async_fw_init) {
+		INIT_DELAYED_WORK((struct delayed_work *)&fw_init_task,
+				  run_fw_init_work);
+		fw_init_task.psys = psys;
+		schedule_delayed_work((struct delayed_work *)&fw_init_task, 0);
+	} else {
+		rval = ipu_psys_fw_init(psys);
+	}
+
+	if (rval) {
+		dev_err(&adev->dev, "FW init failed(%d)\n", rval);
+		goto out_free_pgs;
+	}
+
+	psys->dev.parent = &adev->dev;
+	psys->dev.bus = &ipu_psys_bus;
+	psys->dev.devt = MKDEV(MAJOR(ipu_psys_dev_t), minor);
+	psys->dev.release = ipu_psys_dev_release;
+	dev_set_name(&psys->dev, "ipu-psys%d", minor);
+	rval = device_register(&psys->dev);
+	if (rval < 0) {
+		dev_err(&psys->dev, "psys device_register failed\n");
+		goto out_release_fw_com;
+	}
+
+	/* Add the hw stepping information to caps */
+	strlcpy(psys->caps.dev_model, IPU_MEDIA_DEV_MODEL_NAME,
+		sizeof(psys->caps.dev_model));
+
+	pm_runtime_set_autosuspend_delay(&psys->adev->dev,
+					 IPU_PSYS_AUTOSUSPEND_DELAY);
+	pm_runtime_use_autosuspend(&psys->adev->dev);
+	pm_runtime_mark_last_busy(&psys->adev->dev);
+
+	mutex_unlock(&ipu_psys_mutex);
+
+#ifdef CONFIG_DEBUG_FS
+	/* Debug fs failure is not fatal. */
+	ipu_psys_init_debugfs(psys);
+#endif
+
+	adev->isp->cpd_fw_reload = &cpd_fw_reload;
+
+	dev_info(&adev->dev, "psys probe minor: %d\n", minor);
+
+	ipu_mmu_hw_cleanup(adev->mmu);
+
+	return 0;
+
+out_release_fw_com:
+	ipu_fw_com_release(psys->fwcom, 1);
+out_free_pgs:
+	list_for_each_entry_safe(kpg, kpg0, &psys->pgs, list) {
+		dma_free_attrs(&adev->dev, kpg->size, kpg->pg,
+			       kpg->pg_dma_addr, 0);
+		kfree(kpg);
+	}
+
+	ipu_psys_resource_pool_cleanup(&psys->resource_pool_running);
+out_mutex_destroy:
+	mutex_destroy(&psys->mutex);
+	cdev_del(&psys->cdev);
+	if (psys->sched_cmd_thread) {
+		kthread_stop(psys->sched_cmd_thread);
+		psys->sched_cmd_thread = NULL;
+	}
+out_unlock:
+	/* Safe to call even if the init is not called */
+	ipu_trace_uninit(&adev->dev);
+	mutex_unlock(&ipu_psys_mutex);
+
+	ipu_mmu_hw_cleanup(adev->mmu);
+
+	return rval;
+}
+
+static void ipu_psys_remove(struct ipu_bus_device *adev)
+{
+	struct ipu_device *isp = adev->isp;
+	struct ipu_psys *psys = ipu_bus_get_drvdata(adev);
+	struct ipu_psys_pg *kpg, *kpg0;
+
+#ifdef CONFIG_DEBUG_FS
+	if (isp->ipu_dir)
+		debugfs_remove_recursive(psys->debugfsdir);
+#endif
+
+	flush_workqueue(IPU_PSYS_WORK_QUEUE);
+
+	if (psys->sched_cmd_thread) {
+		kthread_stop(psys->sched_cmd_thread);
+		psys->sched_cmd_thread = NULL;
+	}
+
+	pm_runtime_dont_use_autosuspend(&psys->adev->dev);
+
+	mutex_lock(&ipu_psys_mutex);
+
+	list_for_each_entry_safe(kpg, kpg0, &psys->pgs, list) {
+		dma_free_attrs(&adev->dev, kpg->size, kpg->pg,
+			       kpg->pg_dma_addr, 0);
+		kfree(kpg);
+	}
+
+	if (psys->fwcom && ipu_fw_com_release(psys->fwcom, 1))
+		dev_err(&adev->dev, "fw com release failed.\n");
+
+	kfree(psys->server_init);
+	kfree(psys->syscom_config);
+
+	ipu_trace_uninit(&adev->dev);
+
+	ipu_psys_resource_pool_cleanup(&psys->resource_pool_running);
+
+	device_unregister(&psys->dev);
+
+	clear_bit(MINOR(psys->cdev.dev), ipu_psys_devices);
+	cdev_del(&psys->cdev);
+
+	mutex_unlock(&ipu_psys_mutex);
+
+	mutex_destroy(&psys->mutex);
+
+	dev_info(&adev->dev, "removed\n");
+}
+
+static irqreturn_t psys_isr_threaded(struct ipu_bus_device *adev)
+{
+	struct ipu_psys *psys = ipu_bus_get_drvdata(adev);
+	void __iomem *base = psys->pdata->base;
+	u32 status;
+	int r;
+
+	mutex_lock(&psys->mutex);
+#ifdef CONFIG_PM
+	r = pm_runtime_get_if_in_use(&psys->adev->dev);
+	if (!r || WARN_ON_ONCE(r < 0)) {
+		mutex_unlock(&psys->mutex);
+		return IRQ_NONE;
+	}
+#endif
+
+	status = readl(base + IPU_REG_PSYS_GPDEV_IRQ_STATUS);
+	writel(status, base + IPU_REG_PSYS_GPDEV_IRQ_CLEAR);
+
+	if (status & IPU_PSYS_GPDEV_IRQ_FWIRQ(IPU_PSYS_GPDEV_FWIRQ0)) {
+		writel(0, base + IPU_REG_PSYS_GPDEV_FWIRQ(0));
+		ipu_psys_handle_events(psys);
+	}
+
+	pm_runtime_mark_last_busy(&psys->adev->dev);
+	pm_runtime_put_autosuspend(&psys->adev->dev);
+	mutex_unlock(&psys->mutex);
+
+	return status ? IRQ_HANDLED : IRQ_NONE;
+}
+
+static struct ipu_bus_driver ipu_psys_driver = {
+	.probe = ipu_psys_probe,
+	.remove = ipu_psys_remove,
+	.isr_threaded = psys_isr_threaded,
+	.wanted = IPU_PSYS_NAME,
+	.drv = {
+		.name = IPU_PSYS_NAME,
+		.owner = THIS_MODULE,
+		.pm = PSYS_PM_OPS,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+};
+
+static int __init ipu_psys_init(void)
+{
+	int rval = alloc_chrdev_region(&ipu_psys_dev_t, 0,
+				       IPU_PSYS_NUM_DEVICES, IPU_PSYS_NAME);
+	if (rval) {
+		pr_err("can't alloc psys chrdev region (%d)\n", rval);
+		return rval;
+	}
+
+	rval = bus_register(&ipu_psys_bus);
+	if (rval) {
+		pr_warn("can't register psys bus (%d)\n", rval);
+		goto out_bus_register;
+	}
+
+	ipu_bus_register_driver(&ipu_psys_driver);
+
+	return rval;
+
+out_bus_register:
+	unregister_chrdev_region(ipu_psys_dev_t, IPU_PSYS_NUM_DEVICES);
+
+	return rval;
+}
+
+static void __exit ipu_psys_exit(void)
+{
+	ipu_bus_unregister_driver(&ipu_psys_driver);
+	bus_unregister(&ipu_psys_bus);
+	unregister_chrdev_region(ipu_psys_dev_t, IPU_PSYS_NUM_DEVICES);
+}
+
+static const struct pci_device_id ipu_pci_tbl[] = {
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, IPU6_PCI_ID)},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, IPU6SE_PCI_ID)},
+	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, IPU6EP_PCI_ID)},
+	{0,}
+};
+MODULE_DEVICE_TABLE(pci, ipu_pci_tbl);
+
+module_init(ipu_psys_init);
+module_exit(ipu_psys_exit);
+
+MODULE_AUTHOR("Antti Laakso <antti.laakso@intel.com>");
+MODULE_AUTHOR("Bin Han <bin.b.han@intel.com>");
+MODULE_AUTHOR("Renwei Wu <renwei.wu@intel.com>");
+MODULE_AUTHOR("Jianxu Zheng <jian.xu.zheng@intel.com>");
+MODULE_AUTHOR("Xia Wu <xia.wu@intel.com>");
+MODULE_AUTHOR("Bingbu Cao <bingbu.cao@intel.com>");
+MODULE_AUTHOR("Zaikuo Wang <zaikuo.wang@intel.com>");
+MODULE_AUTHOR("Yunliang Ding <yunliang.ding@intel.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Intel ipu processing system driver");
diff -ruN a/drivers/media/pci/intel/ipu-psys-compat32.c b/drivers/media/pci/intel/ipu-psys-compat32.c
--- a/drivers/media/pci/intel/ipu-psys-compat32.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-psys-compat32.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,225 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2013 - 2020 Intel Corporation
+
+#include <linux/compat.h>
+#include <linux/errno.h>
+#include <linux/uaccess.h>
+
+#include <uapi/linux/ipu-psys.h>
+
+#include "ipu-psys.h"
+
+static long native_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	long ret = -ENOTTY;
+
+	if (file->f_op->unlocked_ioctl)
+		ret = file->f_op->unlocked_ioctl(file, cmd, arg);
+
+	return ret;
+}
+
+struct ipu_psys_buffer32 {
+	u64 len;
+	union {
+		int fd;
+		compat_uptr_t userptr;
+		u64 reserved;
+	} base;
+	u32 data_offset;
+	u32 bytes_used;
+	u32 flags;
+	u32 reserved[2];
+} __packed;
+
+struct ipu_psys_command32 {
+	u64 issue_id;
+	u64 user_token;
+	u32 priority;
+	compat_uptr_t pg_manifest;
+	compat_uptr_t buffers;
+	int pg;
+	u32 pg_manifest_size;
+	u32 bufcount;
+	u32 min_psys_freq;
+	u32 frame_counter;
+	u32 reserved[2];
+} __packed;
+
+struct ipu_psys_manifest32 {
+	u32 index;
+	u32 size;
+	compat_uptr_t manifest;
+	u32 reserved[5];
+} __packed;
+
+static int
+get_ipu_psys_command32(struct ipu_psys_command *kp,
+		       struct ipu_psys_command32 __user *up)
+{
+	compat_uptr_t pgm, bufs;
+	bool access_ok;
+
+	access_ok = access_ok(up, sizeof(struct ipu_psys_command32));
+	if (!access_ok || get_user(kp->issue_id, &up->issue_id) ||
+	    get_user(kp->user_token, &up->user_token) ||
+	    get_user(kp->priority, &up->priority) ||
+	    get_user(pgm, &up->pg_manifest) ||
+	    get_user(bufs, &up->buffers) ||
+	    get_user(kp->pg, &up->pg) ||
+	    get_user(kp->pg_manifest_size, &up->pg_manifest_size) ||
+	    get_user(kp->bufcount, &up->bufcount) ||
+	    get_user(kp->min_psys_freq, &up->min_psys_freq) ||
+	    get_user(kp->frame_counter, &up->frame_counter)
+	    )
+		return -EFAULT;
+
+	kp->pg_manifest = compat_ptr(pgm);
+	kp->buffers = compat_ptr(bufs);
+
+	return 0;
+}
+
+static int
+get_ipu_psys_buffer32(struct ipu_psys_buffer *kp,
+		      struct ipu_psys_buffer32 __user *up)
+{
+	compat_uptr_t ptr;
+	bool access_ok;
+
+	access_ok = access_ok(up, sizeof(struct ipu_psys_buffer32));
+	if (!access_ok || get_user(kp->len, &up->len) ||
+	    get_user(ptr, &up->base.userptr) ||
+	    get_user(kp->data_offset, &up->data_offset) ||
+	    get_user(kp->bytes_used, &up->bytes_used) ||
+	    get_user(kp->flags, &up->flags))
+		return -EFAULT;
+
+	kp->base.userptr = compat_ptr(ptr);
+
+	return 0;
+}
+
+static int
+put_ipu_psys_buffer32(struct ipu_psys_buffer *kp,
+		      struct ipu_psys_buffer32 __user *up)
+{
+	bool access_ok;
+
+	access_ok = access_ok(up, sizeof(struct ipu_psys_buffer32));
+	if (!access_ok || put_user(kp->len, &up->len) ||
+	    put_user(kp->base.fd, &up->base.fd) ||
+	    put_user(kp->data_offset, &up->data_offset) ||
+	    put_user(kp->bytes_used, &up->bytes_used) ||
+	    put_user(kp->flags, &up->flags))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int
+get_ipu_psys_manifest32(struct ipu_psys_manifest *kp,
+			struct ipu_psys_manifest32 __user *up)
+{
+	compat_uptr_t ptr;
+	bool access_ok;
+
+	access_ok = access_ok(up, sizeof(struct ipu_psys_manifest32));
+	if (!access_ok || get_user(kp->index, &up->index) ||
+	    get_user(kp->size, &up->size) || get_user(ptr, &up->manifest))
+		return -EFAULT;
+
+	kp->manifest = compat_ptr(ptr);
+
+	return 0;
+}
+
+static int
+put_ipu_psys_manifest32(struct ipu_psys_manifest *kp,
+			struct ipu_psys_manifest32 __user *up)
+{
+	compat_uptr_t ptr = (u32)((unsigned long)kp->manifest);
+	bool access_ok;
+
+	access_ok = access_ok(up, sizeof(struct ipu_psys_manifest32));
+	if (!access_ok || put_user(kp->index, &up->index) ||
+	    put_user(kp->size, &up->size) || put_user(ptr, &up->manifest))
+		return -EFAULT;
+
+	return 0;
+}
+
+#define IPU_IOC_GETBUF32 _IOWR('A', 4, struct ipu_psys_buffer32)
+#define IPU_IOC_PUTBUF32 _IOWR('A', 5, struct ipu_psys_buffer32)
+#define IPU_IOC_QCMD32 _IOWR('A', 6, struct ipu_psys_command32)
+#define IPU_IOC_CMD_CANCEL32 _IOWR('A', 8, struct ipu_psys_command32)
+#define IPU_IOC_GET_MANIFEST32 _IOWR('A', 9, struct ipu_psys_manifest32)
+
+long ipu_psys_compat_ioctl32(struct file *file, unsigned int cmd,
+			     unsigned long arg)
+{
+	union {
+		struct ipu_psys_buffer buf;
+		struct ipu_psys_command cmd;
+		struct ipu_psys_event ev;
+		struct ipu_psys_manifest m;
+	} karg;
+	int compatible_arg = 1;
+	int err = 0;
+	void __user *up = compat_ptr(arg);
+
+	switch (cmd) {
+	case IPU_IOC_GETBUF32:
+		cmd = IPU_IOC_GETBUF;
+		break;
+	case IPU_IOC_PUTBUF32:
+		cmd = IPU_IOC_PUTBUF;
+		break;
+	case IPU_IOC_QCMD32:
+		cmd = IPU_IOC_QCMD;
+		break;
+	case IPU_IOC_GET_MANIFEST32:
+		cmd = IPU_IOC_GET_MANIFEST;
+		break;
+	}
+
+	switch (cmd) {
+	case IPU_IOC_GETBUF:
+	case IPU_IOC_PUTBUF:
+		err = get_ipu_psys_buffer32(&karg.buf, up);
+		compatible_arg = 0;
+		break;
+	case IPU_IOC_QCMD:
+		err = get_ipu_psys_command32(&karg.cmd, up);
+		compatible_arg = 0;
+		break;
+	case IPU_IOC_GET_MANIFEST:
+		err = get_ipu_psys_manifest32(&karg.m, up);
+		compatible_arg = 0;
+		break;
+	}
+	if (err)
+		return err;
+
+	if (compatible_arg) {
+		err = native_ioctl(file, cmd, (unsigned long)up);
+	} else {
+		mm_segment_t old_fs = force_uaccess_begin();
+
+		err = native_ioctl(file, cmd, (unsigned long)&karg);
+		force_uaccess_end(old_fs);
+	}
+
+	if (err)
+		return err;
+
+	switch (cmd) {
+	case IPU_IOC_GETBUF:
+		err = put_ipu_psys_buffer32(&karg.buf, up);
+		break;
+	case IPU_IOC_GET_MANIFEST:
+		err = put_ipu_psys_manifest32(&karg.m, up);
+		break;
+	}
+	return err;
+}
diff -ruN a/drivers/media/pci/intel/ipu-psys.h b/drivers/media/pci/intel/ipu-psys.h
--- a/drivers/media/pci/intel/ipu-psys.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-psys.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,217 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef IPU_PSYS_H
+#define IPU_PSYS_H
+
+#include <linux/cdev.h>
+#include <linux/workqueue.h>
+
+#include "ipu.h"
+#include "ipu-pdata.h"
+#include "ipu-fw-psys.h"
+#include "ipu-platform-psys.h"
+
+#define IPU_PSYS_PG_POOL_SIZE 16
+#define IPU_PSYS_PG_MAX_SIZE 8192
+#define IPU_MAX_PSYS_CMD_BUFFERS 32
+#define IPU_PSYS_EVENT_CMD_COMPLETE IPU_FW_PSYS_EVENT_TYPE_SUCCESS
+#define IPU_PSYS_EVENT_FRAGMENT_COMPLETE IPU_FW_PSYS_EVENT_TYPE_SUCCESS
+#define IPU_PSYS_CLOSE_TIMEOUT_US   50
+#define IPU_PSYS_CLOSE_TIMEOUT (100000 / IPU_PSYS_CLOSE_TIMEOUT_US)
+#define IPU_PSYS_WORK_QUEUE		system_power_efficient_wq
+#define IPU_MAX_RESOURCES 128
+
+/* Opaque structure. Do not access fields. */
+struct ipu_resource {
+	u32 id;
+	int elements;	/* Number of elements available to allocation */
+	unsigned long *bitmap;	/* Allocation bitmap, a bit for each element */
+};
+
+enum ipu_resource_type {
+	IPU_RESOURCE_DEV_CHN = 0,
+	IPU_RESOURCE_EXT_MEM,
+	IPU_RESOURCE_DFM
+};
+
+/* Allocation of resource(s) */
+/* Opaque structure. Do not access fields. */
+struct ipu_resource_alloc {
+	enum ipu_resource_type type;
+	struct ipu_resource *resource;
+	int elements;
+	int pos;
+};
+
+/*
+ * This struct represents all of the currently allocated
+ * resources from IPU model. It is used also for allocating
+ * resources for the next set of PGs to be run on IPU
+ * (ie. those PGs which are not yet being run and which don't
+ * yet reserve real IPU resources).
+ * Use larger array to cover existing resource quantity
+ */
+
+/* resource size may need expand for new resource model */
+struct ipu_psys_resource_pool {
+	u32 cells;	/* Bitmask of cells allocated */
+	struct ipu_resource dev_channels[16];
+	struct ipu_resource ext_memory[32];
+	struct ipu_resource dfms[16];
+	DECLARE_BITMAP(cmd_queues, 32);
+	/* Protects cmd_queues bitmap */
+	spinlock_t queues_lock;
+};
+
+/*
+ * This struct keeps book of the resources allocated for a specific PG.
+ * It is used for freeing up resources from struct ipu_psys_resources
+ * when the PG is released from IPU (or model of IPU).
+ */
+struct ipu_psys_resource_alloc {
+	u32 cells;	/* Bitmask of cells needed */
+	struct ipu_resource_alloc
+	 resource_alloc[IPU_MAX_RESOURCES];
+	int resources;
+};
+
+struct task_struct;
+struct ipu_psys {
+	struct ipu_psys_capability caps;
+	struct cdev cdev;
+	struct device dev;
+
+	struct mutex mutex;	/* Psys various */
+	int ready; /* psys fw status */
+	bool icache_prefetch_sp;
+	bool icache_prefetch_isp;
+	spinlock_t ready_lock;	/* protect psys firmware state */
+	spinlock_t pgs_lock;	/* Protect pgs list access */
+	struct list_head fhs;
+	struct list_head pgs;
+	struct list_head started_kcmds_list;
+	struct ipu_psys_pdata *pdata;
+	struct ipu_bus_device *adev;
+	struct ia_css_syscom_context *dev_ctx;
+	struct ia_css_syscom_config *syscom_config;
+	struct ia_css_psys_server_init *server_init;
+	struct task_struct *sched_cmd_thread;
+	wait_queue_head_t sched_cmd_wq;
+	atomic_t wakeup_count;  /* Psys schedule thread wakeup count */
+#ifdef CONFIG_DEBUG_FS
+	struct dentry *debugfsdir;
+#endif
+
+	/* Resources needed to be managed for process groups */
+	struct ipu_psys_resource_pool resource_pool_running;
+
+	const struct firmware *fw;
+	struct sg_table fw_sgt;
+	u64 *pkg_dir;
+	dma_addr_t pkg_dir_dma_addr;
+	unsigned int pkg_dir_size;
+	unsigned long timeout;
+
+	int active_kcmds, started_kcmds;
+	void *fwcom;
+
+	int power_gating;
+};
+
+struct ipu_psys_fh {
+	struct ipu_psys *psys;
+	struct mutex mutex;	/* Protects bufmap & kcmds fields */
+	struct list_head list;
+	struct list_head bufmap;
+	wait_queue_head_t wait;
+	struct ipu_psys_scheduler sched;
+};
+
+struct ipu_psys_pg {
+	struct ipu_fw_psys_process_group *pg;
+	size_t size;
+	size_t pg_size;
+	dma_addr_t pg_dma_addr;
+	struct list_head list;
+	struct ipu_psys_resource_alloc resource_alloc;
+};
+
+struct ipu_psys_kcmd {
+	struct ipu_psys_fh *fh;
+	struct list_head list;
+	struct ipu_psys_buffer_set *kbuf_set;
+	enum ipu_psys_cmd_state state;
+	void *pg_manifest;
+	size_t pg_manifest_size;
+	struct ipu_psys_kbuffer **kbufs;
+	struct ipu_psys_buffer *buffers;
+	size_t nbuffers;
+	struct ipu_fw_psys_process_group *pg_user;
+	struct ipu_psys_pg *kpg;
+	u64 user_token;
+	u64 issue_id;
+	u32 priority;
+	u32 kernel_enable_bitmap[4];
+	u32 terminal_enable_bitmap[4];
+	u32 routing_enable_bitmap[4];
+	u32 rbm[5];
+	struct ipu_buttress_constraint constraint;
+	struct ipu_psys_event ev;
+	struct timer_list watchdog;
+};
+
+struct ipu_dma_buf_attach {
+	struct device *dev;
+	u64 len;
+	void *userptr;
+	struct sg_table *sgt;
+	bool vma_is_io;
+	struct page **pages;
+	size_t npages;
+};
+
+struct ipu_psys_kbuffer {
+	u64 len;
+	void *userptr;
+	u32 flags;
+	int fd;
+	void *kaddr;
+	struct list_head list;
+	dma_addr_t dma_addr;
+	struct sg_table *sgt;
+	struct dma_buf_attachment *db_attach;
+	struct dma_buf *dbuf;
+	bool valid;	/* True when buffer is usable */
+};
+
+#define inode_to_ipu_psys(inode) \
+	container_of((inode)->i_cdev, struct ipu_psys, cdev)
+
+#ifdef CONFIG_COMPAT
+long ipu_psys_compat_ioctl32(struct file *file, unsigned int cmd,
+			     unsigned long arg);
+#endif
+
+void ipu_psys_setup_hw(struct ipu_psys *psys);
+void ipu_psys_subdomains_power(struct ipu_psys *psys, bool on);
+void ipu_psys_handle_events(struct ipu_psys *psys);
+int ipu_psys_kcmd_new(struct ipu_psys_command *cmd, struct ipu_psys_fh *fh);
+void ipu_psys_run_next(struct ipu_psys *psys);
+struct ipu_psys_pg *__get_pg_buf(struct ipu_psys *psys, size_t pg_size);
+struct ipu_psys_kbuffer *
+ipu_psys_lookup_kbuffer(struct ipu_psys_fh *fh, int fd);
+int ipu_psys_mapbuf_locked(int fd, struct ipu_psys_fh *fh,
+			   struct ipu_psys_kbuffer *kbuf);
+struct ipu_psys_kbuffer *
+ipu_psys_lookup_kbuffer_by_kaddr(struct ipu_psys_fh *fh, void *kaddr);
+#ifdef IPU_PSYS_GPC
+int ipu_psys_gpc_init_debugfs(struct ipu_psys *psys);
+#endif
+int ipu_psys_resource_pool_init(struct ipu_psys_resource_pool *pool);
+void ipu_psys_resource_pool_cleanup(struct ipu_psys_resource_pool *pool);
+struct ipu_psys_kcmd *ipu_get_completed_kcmd(struct ipu_psys_fh *fh);
+long ipu_ioctl_dqevent(struct ipu_psys_event *event,
+		       struct ipu_psys_fh *fh, unsigned int f_flags);
+
+#endif /* IPU_PSYS_H */
diff -ruN a/drivers/media/pci/intel/ipu-trace.c b/drivers/media/pci/intel/ipu-trace.c
--- a/drivers/media/pci/intel/ipu-trace.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-trace.c	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,869 @@
+// SPDX-License-Identifier: GPL-2.0
+// Copyright (C) 2014 - 2021 Intel Corporation
+
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/module.h>
+#include <linux/pm_runtime.h>
+#include <linux/sizes.h>
+#include <linux/uaccess.h>
+#include <linux/vmalloc.h>
+
+#include "ipu.h"
+#include "ipu-platform-regs.h"
+#include "ipu-trace.h"
+
+struct trace_register_range {
+	u32 start;
+	u32 end;
+};
+
+#define MEMORY_RING_BUFFER_SIZE		(SZ_1M * 32)
+#define TRACE_MESSAGE_SIZE		16
+/*
+ * It looks that the trace unit sometimes writes outside the given buffer.
+ * To avoid memory corruption one extra page is reserved at the end
+ * of the buffer. Read also the extra area since it may contain valid data.
+ */
+#define MEMORY_RING_BUFFER_GUARD	PAGE_SIZE
+#define MEMORY_RING_BUFFER_OVERREAD	MEMORY_RING_BUFFER_GUARD
+#define MAX_TRACE_REGISTERS		200
+#define TRACE_CONF_DUMP_BUFFER_SIZE	(MAX_TRACE_REGISTERS * 2 * 32)
+#define TRACE_CONF_DATA_MAX_LEN		(1024 * 4)
+#define WPT_TRACE_CONF_DATA_MAX_LEN	(1024 * 64)
+
+struct config_value {
+	u32 reg;
+	u32 value;
+};
+
+struct ipu_trace_buffer {
+	dma_addr_t dma_handle;
+	void *memory_buffer;
+};
+
+struct ipu_subsystem_wptrace_config {
+	bool open;
+	char *conf_dump_buffer;
+	int size_conf_dump;
+	unsigned int fill_level;
+	struct config_value config[MAX_TRACE_REGISTERS];
+};
+
+struct ipu_subsystem_trace_config {
+	u32 offset;
+	void __iomem *base;
+	struct ipu_trace_buffer memory;	/* ring buffer */
+	struct device *dev;
+	struct ipu_trace_block *blocks;
+	unsigned int fill_level;	/* Nbr of regs in config table below */
+	bool running;
+	/* Cached register values  */
+	struct config_value config[MAX_TRACE_REGISTERS];
+	/* watchpoint trace info */
+	struct ipu_subsystem_wptrace_config wpt;
+};
+
+struct ipu_trace {
+	struct mutex lock; /* Protect ipu trace operations */
+	bool open;
+	char *conf_dump_buffer;
+	int size_conf_dump;
+
+	struct ipu_subsystem_trace_config isys;
+	struct ipu_subsystem_trace_config psys;
+};
+
+static void __ipu_trace_restore(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	struct ipu_device *isp = adev->isp;
+	struct ipu_trace *trace = isp->trace;
+	struct config_value *config;
+	struct ipu_subsystem_trace_config *sys = adev->trace_cfg;
+	struct ipu_trace_block *blocks;
+	u32 mapped_trace_buffer;
+	void __iomem *addr = NULL;
+	int i;
+
+	if (trace->open) {
+		dev_info(dev, "Trace control file open. Skipping update\n");
+		return;
+	}
+
+	if (!sys)
+		return;
+
+	/* leave if no trace configuration for this subsystem */
+	if (sys->fill_level == 0)
+		return;
+
+	/* Find trace unit base address */
+	blocks = sys->blocks;
+	while (blocks->type != IPU_TRACE_BLOCK_END) {
+		if (blocks->type == IPU_TRACE_BLOCK_TUN) {
+			addr = sys->base + blocks->offset;
+			break;
+		}
+		blocks++;
+	}
+	if (!addr)
+		return;
+
+	if (!sys->memory.memory_buffer) {
+		sys->memory.memory_buffer =
+		    dma_alloc_coherent(dev, MEMORY_RING_BUFFER_SIZE +
+				       MEMORY_RING_BUFFER_GUARD,
+				       &sys->memory.dma_handle,
+				       GFP_KERNEL);
+	}
+
+	if (!sys->memory.memory_buffer) {
+		dev_err(dev, "No memory for tracing. Trace unit disabled\n");
+		return;
+	}
+
+	config = sys->config;
+	mapped_trace_buffer = sys->memory.dma_handle;
+
+	/* ring buffer base */
+	writel(mapped_trace_buffer, addr + TRACE_REG_TUN_DRAM_BASE_ADDR);
+
+	/* ring buffer end */
+	writel(mapped_trace_buffer + MEMORY_RING_BUFFER_SIZE -
+		   TRACE_MESSAGE_SIZE, addr + TRACE_REG_TUN_DRAM_END_ADDR);
+
+	/* Infobits for ddr trace */
+	writel(IPU_INFO_REQUEST_DESTINATION_PRIMARY,
+	       addr + TRACE_REG_TUN_DDR_INFO_VAL);
+
+	/* Find trace timer reset address */
+	addr = NULL;
+	blocks = sys->blocks;
+	while (blocks->type != IPU_TRACE_BLOCK_END) {
+		if (blocks->type == IPU_TRACE_TIMER_RST) {
+			addr = sys->base + blocks->offset;
+			break;
+		}
+		blocks++;
+	}
+	if (!addr) {
+		dev_err(dev, "No trace reset addr\n");
+		return;
+	}
+
+	/* Remove reset from trace timers */
+	writel(TRACE_REG_GPREG_TRACE_TIMER_RST_OFF, addr);
+
+	/* Register config received from userspace */
+	for (i = 0; i < sys->fill_level; i++) {
+		dev_dbg(dev,
+			"Trace restore: reg 0x%08x, value 0x%08x\n",
+			config[i].reg, config[i].value);
+		writel(config[i].value, isp->base + config[i].reg);
+	}
+
+	/* Register wpt config received from userspace, and only psys has wpt */
+	config = sys->wpt.config;
+	for (i = 0; i < sys->wpt.fill_level; i++) {
+		dev_dbg(dev, "Trace restore: reg 0x%08x, value 0x%08x\n",
+			config[i].reg, config[i].value);
+		writel(config[i].value, isp->base + config[i].reg);
+	}
+	sys->running = true;
+}
+
+void ipu_trace_restore(struct device *dev)
+{
+	struct ipu_trace *trace = to_ipu_bus_device(dev)->isp->trace;
+
+	if (!trace)
+		return;
+
+	mutex_lock(&trace->lock);
+	__ipu_trace_restore(dev);
+	mutex_unlock(&trace->lock);
+}
+EXPORT_SYMBOL_GPL(ipu_trace_restore);
+
+static void __ipu_trace_stop(struct device *dev)
+{
+	struct ipu_subsystem_trace_config *sys =
+	    to_ipu_bus_device(dev)->trace_cfg;
+	struct ipu_trace_block *blocks;
+
+	if (!sys)
+		return;
+
+	if (!sys->running)
+		return;
+	sys->running = false;
+
+	/* Turn off all the gpc blocks */
+	blocks = sys->blocks;
+	while (blocks->type != IPU_TRACE_BLOCK_END) {
+		if (blocks->type == IPU_TRACE_BLOCK_GPC) {
+			writel(0, sys->base + blocks->offset +
+				   TRACE_REG_GPC_OVERALL_ENABLE);
+		}
+		blocks++;
+	}
+
+	/* Turn off all the trace monitors */
+	blocks = sys->blocks;
+	while (blocks->type != IPU_TRACE_BLOCK_END) {
+		if (blocks->type == IPU_TRACE_BLOCK_TM) {
+			writel(0, sys->base + blocks->offset +
+				   TRACE_REG_TM_TRACE_ENABLE_NPK);
+
+			writel(0, sys->base + blocks->offset +
+				   TRACE_REG_TM_TRACE_ENABLE_DDR);
+		}
+		blocks++;
+	}
+
+	/* Turn off trace units */
+	blocks = sys->blocks;
+	while (blocks->type != IPU_TRACE_BLOCK_END) {
+		if (blocks->type == IPU_TRACE_BLOCK_TUN) {
+			writel(0, sys->base + blocks->offset +
+				   TRACE_REG_TUN_DDR_ENABLE);
+			writel(0, sys->base + blocks->offset +
+				   TRACE_REG_TUN_NPK_ENABLE);
+		}
+		blocks++;
+	}
+}
+
+void ipu_trace_stop(struct device *dev)
+{
+	struct ipu_trace *trace = to_ipu_bus_device(dev)->isp->trace;
+
+	if (!trace)
+		return;
+
+	mutex_lock(&trace->lock);
+	__ipu_trace_stop(dev);
+	mutex_unlock(&trace->lock);
+}
+EXPORT_SYMBOL_GPL(ipu_trace_stop);
+
+static int update_register_cache(struct ipu_device *isp, u32 reg, u32 value)
+{
+	struct ipu_trace *dctrl = isp->trace;
+	struct ipu_subsystem_trace_config *sys;
+	int rval = -EINVAL;
+
+	if (dctrl->isys.offset == dctrl->psys.offset) {
+		/* For the IPU with uniform address space */
+		if (reg >= IPU_ISYS_OFFSET &&
+		    reg < IPU_ISYS_OFFSET + TRACE_REG_MAX_ISYS_OFFSET)
+			sys = &dctrl->isys;
+		else if (reg >= IPU_PSYS_OFFSET &&
+			 reg < IPU_PSYS_OFFSET + TRACE_REG_MAX_PSYS_OFFSET)
+			sys = &dctrl->psys;
+		else
+			goto error;
+	} else {
+		if (dctrl->isys.offset &&
+		    reg >= dctrl->isys.offset &&
+		    reg < dctrl->isys.offset + TRACE_REG_MAX_ISYS_OFFSET)
+			sys = &dctrl->isys;
+		else if (dctrl->psys.offset &&
+			 reg >= dctrl->psys.offset &&
+			 reg < dctrl->psys.offset + TRACE_REG_MAX_PSYS_OFFSET)
+			sys = &dctrl->psys;
+		else
+			goto error;
+	}
+
+	if (sys->fill_level < MAX_TRACE_REGISTERS) {
+		dev_dbg(sys->dev,
+			"Trace reg addr 0x%08x value 0x%08x\n", reg, value);
+		sys->config[sys->fill_level].reg = reg;
+		sys->config[sys->fill_level].value = value;
+		sys->fill_level++;
+	} else {
+		rval = -ENOMEM;
+		goto error;
+	}
+	return 0;
+error:
+	dev_info(&isp->pdev->dev,
+		 "Trace register address 0x%08x ignored as invalid register\n",
+		 reg);
+	return rval;
+}
+
+static void traceconf_dump(struct ipu_device *isp)
+{
+	struct ipu_subsystem_trace_config *sys[2] = {
+		&isp->trace->isys,
+		&isp->trace->psys
+	};
+	int i, j, rem_size;
+	char *out;
+
+	isp->trace->size_conf_dump = 0;
+	out = isp->trace->conf_dump_buffer;
+	rem_size = TRACE_CONF_DUMP_BUFFER_SIZE;
+
+	for (j = 0; j < ARRAY_SIZE(sys); j++) {
+		for (i = 0; i < sys[j]->fill_level && rem_size > 0; i++) {
+			int bytes_print;
+			int n = snprintf(out, rem_size, "0x%08x = 0x%08x\n",
+					 sys[j]->config[i].reg,
+					 sys[j]->config[i].value);
+
+			bytes_print = min(n, rem_size - 1);
+			rem_size -= bytes_print;
+			out += bytes_print;
+		}
+	}
+	isp->trace->size_conf_dump = out - isp->trace->conf_dump_buffer;
+}
+
+static void clear_trace_buffer(struct ipu_subsystem_trace_config *sys)
+{
+	if (!sys->memory.memory_buffer)
+		return;
+
+	memset(sys->memory.memory_buffer, 0, MEMORY_RING_BUFFER_SIZE +
+	       MEMORY_RING_BUFFER_OVERREAD);
+
+	dma_sync_single_for_device(sys->dev,
+				   sys->memory.dma_handle,
+				   MEMORY_RING_BUFFER_SIZE +
+				   MEMORY_RING_BUFFER_GUARD, DMA_FROM_DEVICE);
+}
+
+static int traceconf_open(struct inode *inode, struct file *file)
+{
+	int ret;
+	struct ipu_device *isp;
+
+	if (!inode->i_private)
+		return -EACCES;
+
+	isp = inode->i_private;
+
+	ret = mutex_trylock(&isp->trace->lock);
+	if (!ret)
+		return -EBUSY;
+
+	if (isp->trace->open) {
+		mutex_unlock(&isp->trace->lock);
+		return -EBUSY;
+	}
+
+	file->private_data = isp;
+	isp->trace->open = 1;
+	if (file->f_mode & FMODE_WRITE) {
+		/* TBD: Allocate temp buffer for processing.
+		 * Push validated buffer to active config
+		 */
+
+		/* Forget old config if opened for write */
+		isp->trace->isys.fill_level = 0;
+		isp->trace->psys.fill_level = 0;
+		isp->trace->psys.wpt.fill_level = 0;
+	}
+
+	if (file->f_mode & FMODE_READ) {
+		isp->trace->conf_dump_buffer =
+		    vzalloc(TRACE_CONF_DUMP_BUFFER_SIZE);
+		if (!isp->trace->conf_dump_buffer) {
+			isp->trace->open = 0;
+			mutex_unlock(&isp->trace->lock);
+			return -ENOMEM;
+		}
+		traceconf_dump(isp);
+	}
+	mutex_unlock(&isp->trace->lock);
+	return 0;
+}
+
+static ssize_t traceconf_read(struct file *file, char __user *buf,
+			      size_t len, loff_t *ppos)
+{
+	struct ipu_device *isp = file->private_data;
+
+	return simple_read_from_buffer(buf, len, ppos,
+				       isp->trace->conf_dump_buffer,
+				       isp->trace->size_conf_dump);
+}
+
+static ssize_t traceconf_write(struct file *file, const char __user *buf,
+			       size_t len, loff_t *ppos)
+{
+	int i;
+	struct ipu_device *isp = file->private_data;
+	ssize_t bytes = 0;
+	char *ipu_trace_buffer = NULL;
+	size_t buffer_size = 0;
+	u32 ipu_trace_number = 0;
+	struct config_value *cfg_buffer = NULL;
+
+	if ((*ppos < 0) || (len > TRACE_CONF_DATA_MAX_LEN) ||
+	    (len < sizeof(ipu_trace_number))) {
+		dev_info(&isp->pdev->dev,
+			"length is error, len:%ld, loff:%lld\n",
+			len, *ppos);
+		return -EINVAL;
+	}
+
+	ipu_trace_buffer = vzalloc(len);
+	if (!ipu_trace_buffer)
+		return -ENOMEM;
+
+	bytes = copy_from_user(ipu_trace_buffer, buf, len);
+	if (bytes != 0) {
+		vfree(ipu_trace_buffer);
+		return -EFAULT;
+	}
+
+	memcpy(&ipu_trace_number, ipu_trace_buffer, sizeof(u32));
+	buffer_size = ipu_trace_number * sizeof(struct config_value);
+	if ((buffer_size + sizeof(ipu_trace_number)) != len) {
+		dev_info(&isp->pdev->dev,
+			"File size is not right, len:%ld, buffer_size:%zu\n",
+			len, buffer_size);
+		vfree(ipu_trace_buffer);
+		return -EFAULT;
+	}
+
+	mutex_lock(&isp->trace->lock);
+	cfg_buffer = (struct config_value *)(ipu_trace_buffer + sizeof(u32));
+	for (i = 0; i < ipu_trace_number; i++) {
+		update_register_cache(isp, cfg_buffer[i].reg,
+			cfg_buffer[i].value);
+	}
+	mutex_unlock(&isp->trace->lock);
+	vfree(ipu_trace_buffer);
+
+	return len;
+}
+
+static int traceconf_release(struct inode *inode, struct file *file)
+{
+	struct ipu_device *isp = file->private_data;
+	struct device *psys_dev = isp->psys ? &isp->psys->dev : NULL;
+	struct device *isys_dev = isp->isys ? &isp->isys->dev : NULL;
+	int pm_rval = -EINVAL;
+
+	/*
+	 * Turn devices on outside trace->lock mutex. PM transition may
+	 * cause call to function which tries to take the same lock.
+	 * Also do this before trace->open is set back to 0 to avoid
+	 * double restore (one here and one in pm transition). We can't
+	 * rely purely on the restore done by pm call backs since trace
+	 * configuration can occur in any phase compared to other activity.
+	 */
+
+	if (file->f_mode & FMODE_WRITE) {
+		if (isys_dev)
+			pm_rval = pm_runtime_get_sync(isys_dev);
+
+		if (pm_rval >= 0) {
+			/* ISYS ok or missing */
+			if (psys_dev)
+				pm_rval = pm_runtime_get_sync(psys_dev);
+
+			if (pm_rval < 0) {
+				pm_runtime_put_noidle(psys_dev);
+				if (isys_dev)
+					pm_runtime_put(isys_dev);
+			}
+		} else {
+			pm_runtime_put_noidle(&isp->isys->dev);
+		}
+	}
+
+	mutex_lock(&isp->trace->lock);
+	isp->trace->open = 0;
+	vfree(isp->trace->conf_dump_buffer);
+	isp->trace->conf_dump_buffer = NULL;
+
+	if (pm_rval >= 0) {
+		/* Update new cfg to HW */
+		if (isys_dev) {
+			__ipu_trace_stop(isys_dev);
+			clear_trace_buffer(isp->isys->trace_cfg);
+			__ipu_trace_restore(isys_dev);
+		}
+
+		if (psys_dev) {
+			__ipu_trace_stop(psys_dev);
+			clear_trace_buffer(isp->psys->trace_cfg);
+			__ipu_trace_restore(psys_dev);
+		}
+	}
+
+	mutex_unlock(&isp->trace->lock);
+
+	if (pm_rval >= 0) {
+		/* Again - this must be done with trace->lock not taken */
+		if (psys_dev)
+			pm_runtime_put(psys_dev);
+		if (isys_dev)
+			pm_runtime_put(isys_dev);
+	}
+	return 0;
+}
+
+static const struct file_operations ipu_traceconf_fops = {
+	.owner = THIS_MODULE,
+	.open = traceconf_open,
+	.release = traceconf_release,
+	.read = traceconf_read,
+	.write = traceconf_write,
+	.llseek = no_llseek,
+};
+
+static void wptraceconf_dump(struct ipu_device *isp)
+{
+	struct ipu_subsystem_wptrace_config *sys = &isp->trace->psys.wpt;
+	int i, rem_size;
+	char *out;
+
+	sys->size_conf_dump = 0;
+	out = sys->conf_dump_buffer;
+	rem_size = TRACE_CONF_DUMP_BUFFER_SIZE;
+
+	for (i = 0; i < sys->fill_level && rem_size > 0; i++) {
+		int bytes_print;
+		int n = snprintf(out, rem_size, "0x%08x = 0x%08x\n",
+				 sys->config[i].reg,
+				 sys->config[i].value);
+
+		bytes_print = min(n, rem_size - 1);
+		rem_size -= bytes_print;
+		out += bytes_print;
+	}
+	sys->size_conf_dump = out - sys->conf_dump_buffer;
+}
+
+static int wptraceconf_open(struct inode *inode, struct file *file)
+{
+	int ret;
+	struct ipu_device *isp;
+
+	if (!inode->i_private)
+		return -EACCES;
+
+	isp = inode->i_private;
+	ret = mutex_trylock(&isp->trace->lock);
+	if (!ret)
+		return -EBUSY;
+
+	if (isp->trace->psys.wpt.open) {
+		mutex_unlock(&isp->trace->lock);
+		return -EBUSY;
+	}
+
+	file->private_data = isp;
+	if (file->f_mode & FMODE_WRITE) {
+		/* TBD: Allocate temp buffer for processing.
+		 * Push validated buffer to active config
+		 */
+		/* Forget old config if opened for write */
+		isp->trace->psys.wpt.fill_level = 0;
+	}
+
+	if (file->f_mode & FMODE_READ) {
+		isp->trace->psys.wpt.conf_dump_buffer =
+		    vzalloc(TRACE_CONF_DUMP_BUFFER_SIZE);
+		if (!isp->trace->psys.wpt.conf_dump_buffer) {
+			mutex_unlock(&isp->trace->lock);
+			return -ENOMEM;
+		}
+		wptraceconf_dump(isp);
+	}
+	mutex_unlock(&isp->trace->lock);
+	return 0;
+}
+
+static ssize_t wptraceconf_read(struct file *file, char __user *buf,
+			      size_t len, loff_t *ppos)
+{
+	struct ipu_device *isp = file->private_data;
+
+	return simple_read_from_buffer(buf, len, ppos,
+				       isp->trace->psys.wpt.conf_dump_buffer,
+				       isp->trace->psys.wpt.size_conf_dump);
+}
+
+static ssize_t wptraceconf_write(struct file *file, const char __user *buf,
+			       size_t len, loff_t *ppos)
+{
+	int i;
+	struct ipu_device *isp = file->private_data;
+	ssize_t bytes = 0;
+	char *wpt_info_buffer = NULL;
+	size_t buffer_size = 0;
+	u32 wp_node_number = 0;
+	struct config_value *wpt_buffer = NULL;
+	struct ipu_subsystem_wptrace_config *wpt = &isp->trace->psys.wpt;
+
+	if ((*ppos < 0) || (len > WPT_TRACE_CONF_DATA_MAX_LEN) ||
+	    (len < sizeof(wp_node_number))) {
+		dev_info(&isp->pdev->dev,
+			"length is error, len:%ld, loff:%lld\n",
+			len, *ppos);
+		return -EINVAL;
+	}
+
+	wpt_info_buffer = vzalloc(len);
+	if (!wpt_info_buffer)
+		return -ENOMEM;
+
+	bytes = copy_from_user(wpt_info_buffer, buf, len);
+	if (bytes != 0) {
+		vfree(wpt_info_buffer);
+		return -EFAULT;
+	}
+
+	memcpy(&wp_node_number, wpt_info_buffer, sizeof(u32));
+	buffer_size = wp_node_number * sizeof(struct config_value);
+	if ((buffer_size + sizeof(wp_node_number)) != len) {
+		dev_info(&isp->pdev->dev,
+			"File size is not right, len:%ld, buffer_size:%zu\n",
+			len, buffer_size);
+		vfree(wpt_info_buffer);
+		return -EFAULT;
+	}
+
+	mutex_lock(&isp->trace->lock);
+	wpt_buffer = (struct config_value *)(wpt_info_buffer + sizeof(u32));
+	for (i = 0; i < wp_node_number; i++) {
+		if (wpt->fill_level < MAX_TRACE_REGISTERS) {
+			wpt->config[wpt->fill_level].reg = wpt_buffer[i].reg;
+			wpt->config[wpt->fill_level].value =
+				wpt_buffer[i].value;
+			wpt->fill_level++;
+		} else {
+			dev_info(&isp->pdev->dev,
+				 "Address 0x%08x ignored as invalid register\n",
+				 wpt_buffer[i].reg);
+			break;
+		}
+	}
+	mutex_unlock(&isp->trace->lock);
+	vfree(wpt_info_buffer);
+
+	return len;
+}
+
+static int wptraceconf_release(struct inode *inode, struct file *file)
+{
+	struct ipu_device *isp = file->private_data;
+
+	mutex_lock(&isp->trace->lock);
+	isp->trace->open = 0;
+	vfree(isp->trace->psys.wpt.conf_dump_buffer);
+	isp->trace->psys.wpt.conf_dump_buffer = NULL;
+	mutex_unlock(&isp->trace->lock);
+
+	return 0;
+}
+
+static const struct file_operations ipu_wptraceconf_fops = {
+	.owner = THIS_MODULE,
+	.open = wptraceconf_open,
+	.release = wptraceconf_release,
+	.read = wptraceconf_read,
+	.write = wptraceconf_write,
+	.llseek = no_llseek,
+};
+
+static int gettrace_open(struct inode *inode, struct file *file)
+{
+	struct ipu_subsystem_trace_config *sys = inode->i_private;
+
+	if (!sys)
+		return -EACCES;
+
+	if (!sys->memory.memory_buffer)
+		return -EACCES;
+
+	dma_sync_single_for_cpu(sys->dev,
+				sys->memory.dma_handle,
+				MEMORY_RING_BUFFER_SIZE +
+				MEMORY_RING_BUFFER_GUARD, DMA_FROM_DEVICE);
+
+	file->private_data = sys;
+	return 0;
+};
+
+static ssize_t gettrace_read(struct file *file, char __user *buf,
+			     size_t len, loff_t *ppos)
+{
+	struct ipu_subsystem_trace_config *sys = file->private_data;
+
+	return simple_read_from_buffer(buf, len, ppos,
+				       sys->memory.memory_buffer,
+				       MEMORY_RING_BUFFER_SIZE +
+				       MEMORY_RING_BUFFER_OVERREAD);
+}
+
+static ssize_t gettrace_write(struct file *file, const char __user *buf,
+			      size_t len, loff_t *ppos)
+{
+	struct ipu_subsystem_trace_config *sys = file->private_data;
+	static const char str[] = "clear";
+	char buffer[sizeof(str)] = { 0 };
+	ssize_t ret;
+
+	ret = simple_write_to_buffer(buffer, sizeof(buffer), ppos, buf, len);
+	if (ret < 0)
+		return ret;
+
+	if (ret < sizeof(str) - 1)
+		return -EINVAL;
+
+	if (!strncmp(str, buffer, sizeof(str) - 1)) {
+		clear_trace_buffer(sys);
+		return len;
+	}
+
+	return -EINVAL;
+}
+
+static int gettrace_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static const struct file_operations ipu_gettrace_fops = {
+	.owner = THIS_MODULE,
+	.open = gettrace_open,
+	.release = gettrace_release,
+	.read = gettrace_read,
+	.write = gettrace_write,
+	.llseek = no_llseek,
+};
+
+int ipu_trace_init(struct ipu_device *isp, void __iomem *base,
+		   struct device *dev, struct ipu_trace_block *blocks)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	struct ipu_trace *trace = isp->trace;
+	struct ipu_subsystem_trace_config *sys;
+	int ret = 0;
+
+	if (!isp->trace)
+		return 0;
+
+	mutex_lock(&isp->trace->lock);
+
+	if (dev == &isp->isys->dev) {
+		sys = &trace->isys;
+	} else if (dev == &isp->psys->dev) {
+		sys = &trace->psys;
+	} else {
+		ret = -EINVAL;
+		goto leave;
+	}
+
+	adev->trace_cfg = sys;
+	sys->dev = dev;
+	sys->offset = base - isp->base;	/* sub system offset */
+	sys->base = base;
+	sys->blocks = blocks;
+
+leave:
+	mutex_unlock(&isp->trace->lock);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(ipu_trace_init);
+
+void ipu_trace_uninit(struct device *dev)
+{
+	struct ipu_bus_device *adev = to_ipu_bus_device(dev);
+	struct ipu_device *isp = adev->isp;
+	struct ipu_trace *trace = isp->trace;
+	struct ipu_subsystem_trace_config *sys = adev->trace_cfg;
+
+	if (!trace || !sys)
+		return;
+
+	mutex_lock(&trace->lock);
+
+	if (sys->memory.memory_buffer)
+		dma_free_coherent(sys->dev,
+				  MEMORY_RING_BUFFER_SIZE +
+				  MEMORY_RING_BUFFER_GUARD,
+				  sys->memory.memory_buffer,
+				  sys->memory.dma_handle);
+
+	sys->dev = NULL;
+	sys->memory.memory_buffer = NULL;
+
+	mutex_unlock(&trace->lock);
+}
+EXPORT_SYMBOL_GPL(ipu_trace_uninit);
+
+int ipu_trace_debugfs_add(struct ipu_device *isp, struct dentry *dir)
+{
+	struct dentry *files[4];
+	int i = 0;
+
+	files[i] = debugfs_create_file("traceconf", 0644,
+				       dir, isp, &ipu_traceconf_fops);
+	if (!files[i])
+		return -ENOMEM;
+	i++;
+
+	files[i] = debugfs_create_file("wptraceconf", 0644,
+				       dir, isp, &ipu_wptraceconf_fops);
+	if (!files[i])
+		goto error;
+	i++;
+
+	files[i] = debugfs_create_file("getisystrace", 0444,
+				       dir,
+				       &isp->trace->isys, &ipu_gettrace_fops);
+
+	if (!files[i])
+		goto error;
+	i++;
+
+	files[i] = debugfs_create_file("getpsystrace", 0444,
+				       dir,
+				       &isp->trace->psys, &ipu_gettrace_fops);
+	if (!files[i])
+		goto error;
+
+	return 0;
+
+error:
+	for (; i > 0; i--)
+		debugfs_remove(files[i - 1]);
+	return -ENOMEM;
+}
+
+int ipu_trace_add(struct ipu_device *isp)
+{
+	isp->trace = devm_kzalloc(&isp->pdev->dev,
+				  sizeof(struct ipu_trace), GFP_KERNEL);
+	if (!isp->trace)
+		return -ENOMEM;
+
+	mutex_init(&isp->trace->lock);
+
+	return 0;
+}
+
+void ipu_trace_release(struct ipu_device *isp)
+{
+	if (!isp->trace)
+		return;
+	mutex_destroy(&isp->trace->lock);
+}
+
+MODULE_AUTHOR("Samu Onkalo <samu.onkalo@intel.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Intel ipu trace support");
diff -ruN a/drivers/media/pci/intel/ipu-trace.h b/drivers/media/pci/intel/ipu-trace.h
--- a/drivers/media/pci/intel/ipu-trace.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/ipu-trace.h	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,146 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2014 - 2021 Intel Corporation */
+
+#ifndef IPU_TRACE_H
+#define IPU_TRACE_H
+#include <linux/debugfs.h>
+
+/* Trace unit register offsets */
+#define TRACE_REG_TUN_DDR_ENABLE        0x000
+#define TRACE_REG_TUN_NPK_ENABLE	0x004
+#define TRACE_REG_TUN_DDR_INFO_VAL	0x008
+#define TRACE_REG_TUN_NPK_ADDR		0x00C
+#define TRACE_REG_TUN_DRAM_BASE_ADDR	0x010
+#define TRACE_REG_TUN_DRAM_END_ADDR	0x014
+#define TRACE_REG_TUN_LOCAL_TIMER0	0x018
+#define TRACE_REG_TUN_LOCAL_TIMER1	0x01C
+#define TRACE_REG_TUN_WR_PTR		0x020
+#define TRACE_REG_TUN_RD_PTR		0x024
+
+/*
+ * Following registers are left out on purpose:
+ * TUN_LOCAL_TIMER0, TUN_LOCAL_TIMER1, TUN_DRAM_BASE_ADDR
+ * TUN_DRAM_END_ADDR, TUN_WR_PTR, TUN_RD_PTR
+ */
+
+/* Trace monitor register offsets */
+#define TRACE_REG_TM_TRACE_ADDR_A		0x0900
+#define TRACE_REG_TM_TRACE_ADDR_B		0x0904
+#define TRACE_REG_TM_TRACE_ADDR_C		0x0908
+#define TRACE_REG_TM_TRACE_ADDR_D		0x090c
+#define TRACE_REG_TM_TRACE_ENABLE_NPK		0x0910
+#define TRACE_REG_TM_TRACE_ENABLE_DDR		0x0914
+#define TRACE_REG_TM_TRACE_PER_PC		0x0918
+#define TRACE_REG_TM_TRACE_PER_BRANCH		0x091c
+#define TRACE_REG_TM_TRACE_HEADER		0x0920
+#define TRACE_REG_TM_TRACE_CFG			0x0924
+#define TRACE_REG_TM_TRACE_LOST_PACKETS		0x0928
+#define TRACE_REG_TM_TRACE_LP_CLEAR		0x092c
+#define TRACE_REG_TM_TRACE_LMRUN_MASK		0x0930
+#define TRACE_REG_TM_TRACE_LMRUN_PC_LOW		0x0934
+#define TRACE_REG_TM_TRACE_LMRUN_PC_HIGH	0x0938
+#define TRACE_REG_TM_TRACE_MMIO_SEL		0x093c
+#define TRACE_REG_TM_TRACE_MMIO_WP0_LOW		0x0940
+#define TRACE_REG_TM_TRACE_MMIO_WP1_LOW		0x0944
+#define TRACE_REG_TM_TRACE_MMIO_WP2_LOW		0x0948
+#define TRACE_REG_TM_TRACE_MMIO_WP3_LOW		0x094c
+#define TRACE_REG_TM_TRACE_MMIO_WP0_HIGH	0x0950
+#define TRACE_REG_TM_TRACE_MMIO_WP1_HIGH	0x0954
+#define TRACE_REG_TM_TRACE_MMIO_WP2_HIGH	0x0958
+#define TRACE_REG_TM_TRACE_MMIO_WP3_HIGH	0x095c
+#define TRACE_REG_TM_FWTRACE_FIRST		0x0A00
+#define TRACE_REG_TM_FWTRACE_MIDDLE		0x0A04
+#define TRACE_REG_TM_FWTRACE_LAST		0x0A08
+
+/*
+ * Following exists only in (I)SP address space:
+ * TM_FWTRACE_FIRST, TM_FWTRACE_MIDDLE, TM_FWTRACE_LAST
+ */
+
+#define TRACE_REG_GPC_RESET			0x000
+#define TRACE_REG_GPC_OVERALL_ENABLE		0x004
+#define TRACE_REG_GPC_TRACE_HEADER		0x008
+#define TRACE_REG_GPC_TRACE_ADDRESS		0x00C
+#define TRACE_REG_GPC_TRACE_NPK_EN		0x010
+#define TRACE_REG_GPC_TRACE_DDR_EN		0x014
+#define TRACE_REG_GPC_TRACE_LPKT_CLEAR		0x018
+#define TRACE_REG_GPC_TRACE_LPKT		0x01C
+
+#define TRACE_REG_GPC_ENABLE_ID0		0x020
+#define TRACE_REG_GPC_ENABLE_ID1		0x024
+#define TRACE_REG_GPC_ENABLE_ID2		0x028
+#define TRACE_REG_GPC_ENABLE_ID3		0x02c
+
+#define TRACE_REG_GPC_VALUE_ID0			0x030
+#define TRACE_REG_GPC_VALUE_ID1			0x034
+#define TRACE_REG_GPC_VALUE_ID2			0x038
+#define TRACE_REG_GPC_VALUE_ID3			0x03c
+
+#define TRACE_REG_GPC_CNT_INPUT_SELECT_ID0	0x040
+#define TRACE_REG_GPC_CNT_INPUT_SELECT_ID1	0x044
+#define TRACE_REG_GPC_CNT_INPUT_SELECT_ID2	0x048
+#define TRACE_REG_GPC_CNT_INPUT_SELECT_ID3	0x04c
+
+#define TRACE_REG_GPC_CNT_START_SELECT_ID0	0x050
+#define TRACE_REG_GPC_CNT_START_SELECT_ID1	0x054
+#define TRACE_REG_GPC_CNT_START_SELECT_ID2	0x058
+#define TRACE_REG_GPC_CNT_START_SELECT_ID3	0x05c
+
+#define TRACE_REG_GPC_CNT_STOP_SELECT_ID0	0x060
+#define TRACE_REG_GPC_CNT_STOP_SELECT_ID1	0x064
+#define TRACE_REG_GPC_CNT_STOP_SELECT_ID2	0x068
+#define TRACE_REG_GPC_CNT_STOP_SELECT_ID3	0x06c
+
+#define TRACE_REG_GPC_CNT_MSG_SELECT_ID0	0x070
+#define TRACE_REG_GPC_CNT_MSG_SELECT_ID1	0x074
+#define TRACE_REG_GPC_CNT_MSG_SELECT_ID2	0x078
+#define TRACE_REG_GPC_CNT_MSG_SELECT_ID3	0x07c
+
+#define TRACE_REG_GPC_CNT_MSG_PLOAD_SELECT_ID0	0x080
+#define TRACE_REG_GPC_CNT_MSG_PLOAD_SELECT_ID1	0x084
+#define TRACE_REG_GPC_CNT_MSG_PLOAD_SELECT_ID2	0x088
+#define TRACE_REG_GPC_CNT_MSG_PLOAD_SELECT_ID3	0x08c
+
+#define TRACE_REG_GPC_IRQ_TRIGGER_VALUE_ID0	0x090
+#define TRACE_REG_GPC_IRQ_TRIGGER_VALUE_ID1	0x094
+#define TRACE_REG_GPC_IRQ_TRIGGER_VALUE_ID2	0x098
+#define TRACE_REG_GPC_IRQ_TRIGGER_VALUE_ID3	0x09c
+
+#define TRACE_REG_GPC_IRQ_TIMER_SELECT_ID0	0x0a0
+#define TRACE_REG_GPC_IRQ_TIMER_SELECT_ID1	0x0a4
+#define TRACE_REG_GPC_IRQ_TIMER_SELECT_ID2	0x0a8
+#define TRACE_REG_GPC_IRQ_TIMER_SELECT_ID3	0x0ac
+
+#define TRACE_REG_GPC_IRQ_ENABLE_ID0		0x0b0
+#define TRACE_REG_GPC_IRQ_ENABLE_ID1		0x0b4
+#define TRACE_REG_GPC_IRQ_ENABLE_ID2		0x0b8
+#define TRACE_REG_GPC_IRQ_ENABLE_ID3		0x0bc
+
+struct ipu_trace;
+struct ipu_subsystem_trace_config;
+
+enum ipu_trace_block_type {
+	IPU_TRACE_BLOCK_TUN = 0,	/* Trace unit */
+	IPU_TRACE_BLOCK_TM,	/* Trace monitor */
+	IPU_TRACE_BLOCK_GPC,	/* General purpose control */
+	IPU_TRACE_CSI2,	/* CSI2 legacy receiver */
+	IPU_TRACE_CSI2_3PH,	/* CSI2 combo receiver */
+	IPU_TRACE_SIG2CIOS,
+	IPU_TRACE_TIMER_RST,	/* Trace reset control timer */
+	IPU_TRACE_BLOCK_END	/* End of list */
+};
+
+struct ipu_trace_block {
+	u32 offset;	/* Offset to block inside subsystem */
+	enum ipu_trace_block_type type;
+};
+
+int ipu_trace_add(struct ipu_device *isp);
+int ipu_trace_debugfs_add(struct ipu_device *isp, struct dentry *dir);
+void ipu_trace_release(struct ipu_device *isp);
+int ipu_trace_init(struct ipu_device *isp, void __iomem *base,
+		   struct device *dev, struct ipu_trace_block *blocks);
+void ipu_trace_restore(struct device *dev);
+void ipu_trace_uninit(struct device *dev);
+void ipu_trace_stop(struct device *dev);
+#endif
diff -ruN a/drivers/media/pci/intel/Kconfig b/drivers/media/pci/intel/Kconfig
--- a/drivers/media/pci/intel/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/media/pci/intel/Kconfig	2021-12-23 08:35:33.000000000 +0100
@@ -0,0 +1,32 @@
+config VIDEO_INTEL_IPU6
+	tristate "Intel IPU driver"
+	depends on ACPI
+	depends on MEDIA_SUPPORT
+	depends on MEDIA_PCI_SUPPORT
+	depends on X86_64
+	select IOMMU_API
+	select IOMMU_IOVA
+	select X86_DEV_DMA_OPS
+	select VIDEOBUF2_DMA_CONTIG
+	select V4L2_FWNODE
+	select PHYS_ADDR_T_64BIT
+	select COMMON_CLK
+	help
+	  This is the Intel imaging processing unit, found in Intel SoCs and
+	  used for capturing images and video from a camera sensor.
+
+	  To compile this driver, say Y here! It contains 3 modules -
+	  intel_ipu6, intel_ipu6_isys and intel_ipu6_psys.
+
+config VIDEO_INTEL_IPU_TPG
+	bool "Compile for TPG driver"
+	depends on VIDEO_INTEL_IPU6
+	help
+	  If selected, TPG device nodes would be created.
+
+	  Recommended for driver developers only.
+
+	  If you want to the TPG devices exposed to user as media entity,
+	  you must select this option, otherwise no.
+
+source "drivers/media/pci/intel/ipu3/Kconfig"
diff -ruN a/drivers/media/pci/intel/Makefile b/drivers/media/pci/intel/Makefile
--- a/drivers/media/pci/intel/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/media/pci/intel/Makefile	2021-12-23 08:35:33.000000000 +0100
@@ -4,3 +4,4 @@
 #
 
 obj-y	+= ipu3/
+obj-y	+= ipu6/
diff -ruN a/drivers/media/pci/Kconfig b/drivers/media/pci/Kconfig
--- a/drivers/media/pci/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/media/pci/Kconfig	2021-12-23 08:35:33.000000000 +0100
@@ -55,7 +55,7 @@
 source "drivers/media/pci/netup_unidvb/Kconfig"
 endif
 
-source "drivers/media/pci/intel/ipu3/Kconfig"
+source "drivers/media/pci/intel/Kconfig"
 
 config VIDEO_PCI_SKELETON
 	tristate "Skeleton PCI V4L2 driver"
diff -ruN a/drivers/mfd/cros_ec_dev.c b/drivers/mfd/cros_ec_dev.c
--- a/drivers/mfd/cros_ec_dev.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/mfd/cros_ec_dev.c	2021-12-23 08:35:34.000000000 +0100
@@ -113,8 +113,10 @@
 static const struct mfd_cell cros_ec_platform_cells[] = {
 	{ .name = "cros-ec-chardev", },
 	{ .name = "cros-ec-debugfs", },
+	{ .name = "cros-ec-pd-sysfs" },
 	{ .name = "cros-ec-sysfs", },
 	{ .name = "cros-ec-pchg", },
+	{ .name = "cros-ec-pd-update", },
 };
 
 static const struct mfd_cell cros_ec_lightbar_cells[] = {
diff -ruN a/drivers/misc/Kconfig b/drivers/misc/Kconfig
--- a/drivers/misc/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/misc/Kconfig	2021-12-23 08:35:34.000000000 +0100
@@ -460,6 +460,21 @@
 	tristate
 	default MISC_RTSX_PCI || MISC_RTSX_USB
 
+config UID_SYS_STATS
+	bool "Per-UID statistics"
+	depends on PROFILING && TASK_XACCT && TASK_IO_ACCOUNTING
+	help
+	  Per UID based cpu time statistics exported to /proc/uid_cputime
+	  Per UID based io statistics exported to /proc/uid_io
+	  Per UID based procstat control in /proc/uid_procstat
+
+config UID_SYS_STATS_DEBUG
+	bool "Per-TASK statistics"
+	depends on UID_SYS_STATS
+	default n
+	help
+	  Per TASK based io statistics exported to /proc/uid_io
+
 config HISI_HIKEY_USB
 	tristate "USB GPIO Hub on HiSilicon Hikey 960/970 Platform"
 	depends on (OF && GPIOLIB) || COMPILE_TEST
diff -ruN a/drivers/misc/Makefile b/drivers/misc/Makefile
--- a/drivers/misc/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/misc/Makefile	2021-12-23 08:35:34.000000000 +0100
@@ -59,3 +59,4 @@
 obj-$(CONFIG_XILINX_SDFEC)	+= xilinx_sdfec.o
 obj-$(CONFIG_HISI_HIKEY_USB)	+= hisi_hikey_usb.o
 obj-$(CONFIG_HI6421V600_IRQ)	+= hi6421v600-irq.o
+obj-$(CONFIG_UID_SYS_STATS)	+= uid_sys_stats.o
diff -ruN a/drivers/misc/uid_sys_stats.c b/drivers/misc/uid_sys_stats.c
--- a/drivers/misc/uid_sys_stats.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/misc/uid_sys_stats.c	2021-12-23 08:35:35.000000000 +0100
@@ -0,0 +1,706 @@
+/* drivers/misc/uid_sys_stats.c
+ *
+ * Copyright (C) 2014 - 2015 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/atomic.h>
+#include <linux/err.h>
+#include <linux/hashtable.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/proc_fs.h>
+#include <linux/profile.h>
+#include <linux/rtmutex.h>
+#include <linux/sched/cputime.h>
+#include <linux/seq_file.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+
+
+#define UID_HASH_BITS	10
+DECLARE_HASHTABLE(hash_table, UID_HASH_BITS);
+
+static DEFINE_RT_MUTEX(uid_lock);
+static struct proc_dir_entry *cpu_parent;
+static struct proc_dir_entry *io_parent;
+static struct proc_dir_entry *proc_parent;
+
+struct io_stats {
+	u64 read_bytes;
+	u64 write_bytes;
+	u64 rchar;
+	u64 wchar;
+	u64 fsync;
+};
+
+#define UID_STATE_FOREGROUND	0
+#define UID_STATE_BACKGROUND	1
+#define UID_STATE_BUCKET_SIZE	2
+
+#define UID_STATE_TOTAL_CURR	2
+#define UID_STATE_TOTAL_LAST	3
+#define UID_STATE_DEAD_TASKS	4
+#define UID_STATE_SIZE		5
+
+#define MAX_TASK_COMM_LEN 256
+
+struct task_entry {
+	char comm[MAX_TASK_COMM_LEN];
+	pid_t pid;
+	struct io_stats io[UID_STATE_SIZE];
+	struct hlist_node hash;
+};
+
+struct uid_entry {
+	uid_t uid;
+	u64 utime;
+	u64 stime;
+	u64 active_utime;
+	u64 active_stime;
+	int state;
+	struct io_stats io[UID_STATE_SIZE];
+	struct hlist_node hash;
+#ifdef CONFIG_UID_SYS_STATS_DEBUG
+	DECLARE_HASHTABLE(task_entries, UID_HASH_BITS);
+#endif
+};
+
+static u64 compute_write_bytes(struct task_struct *task)
+{
+	if (task->ioac.write_bytes <= task->ioac.cancelled_write_bytes)
+		return 0;
+
+	return task->ioac.write_bytes - task->ioac.cancelled_write_bytes;
+}
+
+static void compute_io_bucket_stats(struct io_stats *io_bucket,
+					struct io_stats *io_curr,
+					struct io_stats *io_last,
+					struct io_stats *io_dead)
+{
+	/* tasks could switch to another uid group, but its io_last in the
+	 * previous uid group could still be positive.
+	 * therefore before each update, do an overflow check first
+	 */
+	int64_t delta;
+
+	delta = io_curr->read_bytes + io_dead->read_bytes -
+		io_last->read_bytes;
+	io_bucket->read_bytes += delta > 0 ? delta : 0;
+	delta = io_curr->write_bytes + io_dead->write_bytes -
+		io_last->write_bytes;
+	io_bucket->write_bytes += delta > 0 ? delta : 0;
+	delta = io_curr->rchar + io_dead->rchar - io_last->rchar;
+	io_bucket->rchar += delta > 0 ? delta : 0;
+	delta = io_curr->wchar + io_dead->wchar - io_last->wchar;
+	io_bucket->wchar += delta > 0 ? delta : 0;
+	delta = io_curr->fsync + io_dead->fsync - io_last->fsync;
+	io_bucket->fsync += delta > 0 ? delta : 0;
+
+	io_last->read_bytes = io_curr->read_bytes;
+	io_last->write_bytes = io_curr->write_bytes;
+	io_last->rchar = io_curr->rchar;
+	io_last->wchar = io_curr->wchar;
+	io_last->fsync = io_curr->fsync;
+
+	memset(io_dead, 0, sizeof(struct io_stats));
+}
+
+#ifdef CONFIG_UID_SYS_STATS_DEBUG
+static void get_full_task_comm(struct task_entry *task_entry,
+		struct task_struct *task)
+{
+	int i = 0, offset = 0, len = 0;
+	/* save one byte for terminating null character */
+	int unused_len = MAX_TASK_COMM_LEN - TASK_COMM_LEN - 1;
+	char buf[MAX_TASK_COMM_LEN - TASK_COMM_LEN - 1];
+	struct mm_struct *mm = task->mm;
+
+	/* fill the first TASK_COMM_LEN bytes with thread name */
+	__get_task_comm(task_entry->comm, TASK_COMM_LEN, task);
+	i = strlen(task_entry->comm);
+	while (i < TASK_COMM_LEN)
+		task_entry->comm[i++] = ' ';
+
+	/* next the executable file name */
+	if (mm) {
+		mmap_write_lock(mm);
+		if (mm->exe_file) {
+			char *pathname = d_path(&mm->exe_file->f_path, buf,
+					unused_len);
+
+			if (!IS_ERR(pathname)) {
+				len = strlcpy(task_entry->comm + i, pathname,
+						unused_len);
+				i += len;
+				task_entry->comm[i++] = ' ';
+				unused_len--;
+			}
+		}
+		mmap_write_unlock(mm);
+	}
+	unused_len -= len;
+
+	/* fill the rest with command line argument
+	 * replace each null or new line character
+	 * between args in argv with whitespace */
+	len = get_cmdline(task, buf, unused_len);
+	while (offset < len) {
+		if (buf[offset] != '\0' && buf[offset] != '\n')
+			task_entry->comm[i++] = buf[offset];
+		else
+			task_entry->comm[i++] = ' ';
+		offset++;
+	}
+
+	/* get rid of trailing whitespaces in case when arg is memset to
+	 * zero before being reset in userspace
+	 */
+	while (task_entry->comm[i-1] == ' ')
+		i--;
+	task_entry->comm[i] = '\0';
+}
+
+static struct task_entry *find_task_entry(struct uid_entry *uid_entry,
+		struct task_struct *task)
+{
+	struct task_entry *task_entry;
+
+	hash_for_each_possible(uid_entry->task_entries, task_entry, hash,
+			task->pid) {
+		if (task->pid == task_entry->pid) {
+			/* if thread name changed, update the entire command */
+			int len = strnchr(task_entry->comm, ' ', TASK_COMM_LEN)
+				- task_entry->comm;
+
+			if (strncmp(task_entry->comm, task->comm, len))
+				get_full_task_comm(task_entry, task);
+			return task_entry;
+		}
+	}
+	return NULL;
+}
+
+static struct task_entry *find_or_register_task(struct uid_entry *uid_entry,
+		struct task_struct *task)
+{
+	struct task_entry *task_entry;
+	pid_t pid = task->pid;
+
+	task_entry = find_task_entry(uid_entry, task);
+	if (task_entry)
+		return task_entry;
+
+	task_entry = kzalloc(sizeof(struct task_entry), GFP_ATOMIC);
+	if (!task_entry)
+		return NULL;
+
+	get_full_task_comm(task_entry, task);
+
+	task_entry->pid = pid;
+	hash_add(uid_entry->task_entries, &task_entry->hash, (unsigned int)pid);
+
+	return task_entry;
+}
+
+static void remove_uid_tasks(struct uid_entry *uid_entry)
+{
+	struct task_entry *task_entry;
+	unsigned long bkt_task;
+	struct hlist_node *tmp_task;
+
+	hash_for_each_safe(uid_entry->task_entries, bkt_task,
+			tmp_task, task_entry, hash) {
+		hash_del(&task_entry->hash);
+		kfree(task_entry);
+	}
+}
+
+static void set_io_uid_tasks_zero(struct uid_entry *uid_entry)
+{
+	struct task_entry *task_entry;
+	unsigned long bkt_task;
+
+	hash_for_each(uid_entry->task_entries, bkt_task, task_entry, hash) {
+		memset(&task_entry->io[UID_STATE_TOTAL_CURR], 0,
+			sizeof(struct io_stats));
+	}
+}
+
+static void add_uid_tasks_io_stats(struct uid_entry *uid_entry,
+		struct task_struct *task, int slot)
+{
+	struct task_entry *task_entry = find_or_register_task(uid_entry, task);
+	struct io_stats *task_io_slot = &task_entry->io[slot];
+
+	task_io_slot->read_bytes += task->ioac.read_bytes;
+	task_io_slot->write_bytes += compute_write_bytes(task);
+	task_io_slot->rchar += task->ioac.rchar;
+	task_io_slot->wchar += task->ioac.wchar;
+	task_io_slot->fsync += task->ioac.syscfs;
+}
+
+static void compute_io_uid_tasks(struct uid_entry *uid_entry)
+{
+	struct task_entry *task_entry;
+	unsigned long bkt_task;
+
+	hash_for_each(uid_entry->task_entries, bkt_task, task_entry, hash) {
+		compute_io_bucket_stats(&task_entry->io[uid_entry->state],
+					&task_entry->io[UID_STATE_TOTAL_CURR],
+					&task_entry->io[UID_STATE_TOTAL_LAST],
+					&task_entry->io[UID_STATE_DEAD_TASKS]);
+	}
+}
+
+static void show_io_uid_tasks(struct seq_file *m, struct uid_entry *uid_entry)
+{
+	struct task_entry *task_entry;
+	unsigned long bkt_task;
+
+	hash_for_each(uid_entry->task_entries, bkt_task, task_entry, hash) {
+		/* Separated by comma because space exists in task comm */
+		seq_printf(m, "task,%s,%lu,%llu,%llu,%llu,%llu,%llu,%llu,%llu,%llu,%llu,%llu\n",
+				task_entry->comm,
+				(unsigned long)task_entry->pid,
+				task_entry->io[UID_STATE_FOREGROUND].rchar,
+				task_entry->io[UID_STATE_FOREGROUND].wchar,
+				task_entry->io[UID_STATE_FOREGROUND].read_bytes,
+				task_entry->io[UID_STATE_FOREGROUND].write_bytes,
+				task_entry->io[UID_STATE_BACKGROUND].rchar,
+				task_entry->io[UID_STATE_BACKGROUND].wchar,
+				task_entry->io[UID_STATE_BACKGROUND].read_bytes,
+				task_entry->io[UID_STATE_BACKGROUND].write_bytes,
+				task_entry->io[UID_STATE_FOREGROUND].fsync,
+				task_entry->io[UID_STATE_BACKGROUND].fsync);
+	}
+}
+#else
+static void remove_uid_tasks(struct uid_entry *uid_entry) {};
+static void set_io_uid_tasks_zero(struct uid_entry *uid_entry) {};
+static void add_uid_tasks_io_stats(struct uid_entry *uid_entry,
+		struct task_struct *task, int slot) {};
+static void compute_io_uid_tasks(struct uid_entry *uid_entry) {};
+static void show_io_uid_tasks(struct seq_file *m,
+		struct uid_entry *uid_entry) {}
+#endif
+
+static struct uid_entry *find_uid_entry(uid_t uid)
+{
+	struct uid_entry *uid_entry;
+	hash_for_each_possible(hash_table, uid_entry, hash, uid) {
+		if (uid_entry->uid == uid)
+			return uid_entry;
+	}
+	return NULL;
+}
+
+static struct uid_entry *find_or_register_uid(uid_t uid)
+{
+	struct uid_entry *uid_entry;
+
+	uid_entry = find_uid_entry(uid);
+	if (uid_entry)
+		return uid_entry;
+
+	uid_entry = kzalloc(sizeof(struct uid_entry), GFP_ATOMIC);
+	if (!uid_entry)
+		return NULL;
+
+	uid_entry->uid = uid;
+#ifdef CONFIG_UID_SYS_STATS_DEBUG
+	hash_init(uid_entry->task_entries);
+#endif
+	hash_add(hash_table, &uid_entry->hash, uid);
+
+	return uid_entry;
+}
+
+static int uid_cputime_show(struct seq_file *m, void *v)
+{
+	struct uid_entry *uid_entry = NULL;
+	struct task_struct *task, *temp;
+	struct user_namespace *user_ns = current_user_ns();
+	u64 utime;
+	u64 stime;
+	unsigned long bkt;
+	uid_t uid;
+
+	rt_mutex_lock(&uid_lock);
+
+	hash_for_each(hash_table, bkt, uid_entry, hash) {
+		uid_entry->active_stime = 0;
+		uid_entry->active_utime = 0;
+	}
+
+	rcu_read_lock();
+	do_each_thread(temp, task) {
+		uid = from_kuid_munged(user_ns, task_uid(task));
+		if (!uid_entry || uid_entry->uid != uid)
+			uid_entry = find_or_register_uid(uid);
+		if (!uid_entry) {
+			rcu_read_unlock();
+			rt_mutex_unlock(&uid_lock);
+			pr_err("%s: failed to find the uid_entry for uid %d\n",
+				__func__, uid);
+			return -ENOMEM;
+		}
+		/* avoid double accounting of dying threads */
+		if (!(task->flags & PF_EXITING)) {
+			task_cputime_adjusted(task, &utime, &stime);
+			uid_entry->active_utime += utime;
+			uid_entry->active_stime += stime;
+		}
+	} while_each_thread(temp, task);
+	rcu_read_unlock();
+
+	hash_for_each(hash_table, bkt, uid_entry, hash) {
+		u64 total_utime = uid_entry->utime +
+							uid_entry->active_utime;
+		u64 total_stime = uid_entry->stime +
+							uid_entry->active_stime;
+		seq_printf(m, "%d: %llu %llu\n", uid_entry->uid,
+			ktime_to_us(total_utime), ktime_to_us(total_stime));
+	}
+
+	rt_mutex_unlock(&uid_lock);
+	return 0;
+}
+
+static int uid_cputime_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, uid_cputime_show, PDE_DATA(inode));
+}
+
+static const struct proc_ops uid_cputime_fops = {
+	.proc_open	= uid_cputime_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= single_release,
+};
+
+static int uid_remove_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, NULL, NULL);
+}
+
+static ssize_t uid_remove_write(struct file *file,
+			const char __user *buffer, size_t count, loff_t *ppos)
+{
+	struct uid_entry *uid_entry;
+	struct hlist_node *tmp;
+	char uids[128];
+	char *start_uid, *end_uid = NULL;
+	long int uid_start = 0, uid_end = 0;
+
+	if (count >= sizeof(uids))
+		count = sizeof(uids) - 1;
+
+	if (copy_from_user(uids, buffer, count))
+		return -EFAULT;
+
+	uids[count] = '\0';
+	end_uid = uids;
+	start_uid = strsep(&end_uid, "-");
+
+	if (!start_uid || !end_uid)
+		return -EINVAL;
+
+	if (kstrtol(start_uid, 10, &uid_start) != 0 ||
+		kstrtol(end_uid, 10, &uid_end) != 0) {
+		return -EINVAL;
+	}
+
+	rt_mutex_lock(&uid_lock);
+
+	for (; uid_start <= uid_end; uid_start++) {
+		hash_for_each_possible_safe(hash_table, uid_entry, tmp,
+							hash, (uid_t)uid_start) {
+			if (uid_start == uid_entry->uid) {
+				remove_uid_tasks(uid_entry);
+				hash_del(&uid_entry->hash);
+				kfree(uid_entry);
+			}
+		}
+	}
+
+	rt_mutex_unlock(&uid_lock);
+	return count;
+}
+
+static const struct proc_ops uid_remove_fops = {
+	.proc_open	= uid_remove_open,
+	.proc_release	= single_release,
+	.proc_write	= uid_remove_write,
+};
+
+
+static void add_uid_io_stats(struct uid_entry *uid_entry,
+			struct task_struct *task, int slot)
+{
+	struct io_stats *io_slot = &uid_entry->io[slot];
+
+	/* avoid double accounting of dying threads */
+	if (slot != UID_STATE_DEAD_TASKS && (task->flags & PF_EXITING))
+		return;
+
+	io_slot->read_bytes += task->ioac.read_bytes;
+	io_slot->write_bytes += compute_write_bytes(task);
+	io_slot->rchar += task->ioac.rchar;
+	io_slot->wchar += task->ioac.wchar;
+	io_slot->fsync += task->ioac.syscfs;
+
+	add_uid_tasks_io_stats(uid_entry, task, slot);
+}
+
+static void update_io_stats_all_locked(void)
+{
+	struct uid_entry *uid_entry = NULL;
+	struct task_struct *task, *temp;
+	struct user_namespace *user_ns = current_user_ns();
+	unsigned long bkt;
+	uid_t uid;
+
+	hash_for_each(hash_table, bkt, uid_entry, hash) {
+		memset(&uid_entry->io[UID_STATE_TOTAL_CURR], 0,
+			sizeof(struct io_stats));
+		set_io_uid_tasks_zero(uid_entry);
+	}
+
+	rcu_read_lock();
+	do_each_thread(temp, task) {
+		uid = from_kuid_munged(user_ns, task_uid(task));
+		if (!uid_entry || uid_entry->uid != uid)
+			uid_entry = find_or_register_uid(uid);
+		if (!uid_entry)
+			continue;
+		add_uid_io_stats(uid_entry, task, UID_STATE_TOTAL_CURR);
+	} while_each_thread(temp, task);
+	rcu_read_unlock();
+
+	hash_for_each(hash_table, bkt, uid_entry, hash) {
+		compute_io_bucket_stats(&uid_entry->io[uid_entry->state],
+					&uid_entry->io[UID_STATE_TOTAL_CURR],
+					&uid_entry->io[UID_STATE_TOTAL_LAST],
+					&uid_entry->io[UID_STATE_DEAD_TASKS]);
+		compute_io_uid_tasks(uid_entry);
+	}
+}
+
+static void update_io_stats_uid_locked(struct uid_entry *uid_entry)
+{
+	struct task_struct *task, *temp;
+	struct user_namespace *user_ns = current_user_ns();
+
+	memset(&uid_entry->io[UID_STATE_TOTAL_CURR], 0,
+		sizeof(struct io_stats));
+	set_io_uid_tasks_zero(uid_entry);
+
+	rcu_read_lock();
+	do_each_thread(temp, task) {
+		if (from_kuid_munged(user_ns, task_uid(task)) != uid_entry->uid)
+			continue;
+		add_uid_io_stats(uid_entry, task, UID_STATE_TOTAL_CURR);
+	} while_each_thread(temp, task);
+	rcu_read_unlock();
+
+	compute_io_bucket_stats(&uid_entry->io[uid_entry->state],
+				&uid_entry->io[UID_STATE_TOTAL_CURR],
+				&uid_entry->io[UID_STATE_TOTAL_LAST],
+				&uid_entry->io[UID_STATE_DEAD_TASKS]);
+	compute_io_uid_tasks(uid_entry);
+}
+
+
+static int uid_io_show(struct seq_file *m, void *v)
+{
+	struct uid_entry *uid_entry;
+	unsigned long bkt;
+
+	rt_mutex_lock(&uid_lock);
+
+	update_io_stats_all_locked();
+
+	hash_for_each(hash_table, bkt, uid_entry, hash) {
+		seq_printf(m, "%d %llu %llu %llu %llu %llu %llu %llu %llu %llu %llu\n",
+				uid_entry->uid,
+				uid_entry->io[UID_STATE_FOREGROUND].rchar,
+				uid_entry->io[UID_STATE_FOREGROUND].wchar,
+				uid_entry->io[UID_STATE_FOREGROUND].read_bytes,
+				uid_entry->io[UID_STATE_FOREGROUND].write_bytes,
+				uid_entry->io[UID_STATE_BACKGROUND].rchar,
+				uid_entry->io[UID_STATE_BACKGROUND].wchar,
+				uid_entry->io[UID_STATE_BACKGROUND].read_bytes,
+				uid_entry->io[UID_STATE_BACKGROUND].write_bytes,
+				uid_entry->io[UID_STATE_FOREGROUND].fsync,
+				uid_entry->io[UID_STATE_BACKGROUND].fsync);
+
+		show_io_uid_tasks(m, uid_entry);
+	}
+
+	rt_mutex_unlock(&uid_lock);
+	return 0;
+}
+
+static int uid_io_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, uid_io_show, PDE_DATA(inode));
+}
+
+static const struct proc_ops uid_io_fops = {
+	.proc_open	= uid_io_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= single_release,
+};
+
+static int uid_procstat_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, NULL, NULL);
+}
+
+static ssize_t uid_procstat_write(struct file *file,
+			const char __user *buffer, size_t count, loff_t *ppos)
+{
+	struct uid_entry *uid_entry;
+	uid_t uid;
+	int argc, state;
+	char input[128];
+
+	if (count >= sizeof(input))
+		return -EINVAL;
+
+	if (copy_from_user(input, buffer, count))
+		return -EFAULT;
+
+	input[count] = '\0';
+
+	argc = sscanf(input, "%u %d", &uid, &state);
+	if (argc != 2)
+		return -EINVAL;
+
+	if (state != UID_STATE_BACKGROUND && state != UID_STATE_FOREGROUND)
+		return -EINVAL;
+
+	rt_mutex_lock(&uid_lock);
+
+	uid_entry = find_or_register_uid(uid);
+	if (!uid_entry) {
+		rt_mutex_unlock(&uid_lock);
+		return -EINVAL;
+	}
+
+	if (uid_entry->state == state) {
+		rt_mutex_unlock(&uid_lock);
+		return count;
+	}
+
+	update_io_stats_uid_locked(uid_entry);
+
+	uid_entry->state = state;
+
+	rt_mutex_unlock(&uid_lock);
+
+	return count;
+}
+
+static const struct proc_ops uid_procstat_fops = {
+	.proc_open	= uid_procstat_open,
+	.proc_release	= single_release,
+	.proc_write	= uid_procstat_write,
+};
+
+static int process_notifier(struct notifier_block *self,
+			unsigned long cmd, void *v)
+{
+	struct task_struct *task = v;
+	struct uid_entry *uid_entry;
+	u64 utime, stime;
+	uid_t uid;
+
+	if (!task)
+		return NOTIFY_OK;
+
+	rt_mutex_lock(&uid_lock);
+	uid = from_kuid_munged(current_user_ns(), task_uid(task));
+	uid_entry = find_or_register_uid(uid);
+	if (!uid_entry) {
+		pr_err("%s: failed to find uid %d\n", __func__, uid);
+		goto exit;
+	}
+
+	task_cputime_adjusted(task, &utime, &stime);
+	uid_entry->utime += utime;
+	uid_entry->stime += stime;
+
+	add_uid_io_stats(uid_entry, task, UID_STATE_DEAD_TASKS);
+
+exit:
+	rt_mutex_unlock(&uid_lock);
+	return NOTIFY_OK;
+}
+
+static struct notifier_block process_notifier_block = {
+	.notifier_call	= process_notifier,
+};
+
+static int __init proc_uid_sys_stats_init(void)
+{
+	hash_init(hash_table);
+
+	cpu_parent = proc_mkdir("uid_cputime", NULL);
+	if (!cpu_parent) {
+		pr_err("%s: failed to create uid_cputime proc entry\n",
+			__func__);
+		goto err;
+	}
+
+	proc_create_data("remove_uid_range", 0222, cpu_parent,
+		&uid_remove_fops, NULL);
+	proc_create_data("show_uid_stat", 0444, cpu_parent,
+		&uid_cputime_fops, NULL);
+
+	io_parent = proc_mkdir("uid_io", NULL);
+	if (!io_parent) {
+		pr_err("%s: failed to create uid_io proc entry\n",
+			__func__);
+		goto err;
+	}
+
+	proc_create_data("stats", 0444, io_parent,
+		&uid_io_fops, NULL);
+
+	proc_parent = proc_mkdir("uid_procstat", NULL);
+	if (!proc_parent) {
+		pr_err("%s: failed to create uid_procstat proc entry\n",
+			__func__);
+		goto err;
+	}
+
+	proc_create_data("set", 0222, proc_parent,
+		&uid_procstat_fops, NULL);
+
+	profile_event_register(PROFILE_TASK_EXIT, &process_notifier_block);
+
+	return 0;
+
+err:
+	remove_proc_subtree("uid_cputime", NULL);
+	remove_proc_subtree("uid_io", NULL);
+	remove_proc_subtree("uid_procstat", NULL);
+	return -ENOMEM;
+}
+
+early_initcall(proc_uid_sys_stats_init);
diff -ruN a/drivers/net/ppp/ppp_generic.c b/drivers/net/ppp/ppp_generic.c
--- a/drivers/net/ppp/ppp_generic.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/net/ppp/ppp_generic.c	2021-12-23 08:35:40.000000000 +0100
@@ -51,6 +51,7 @@
 
 #include <linux/nsproxy.h>
 #include <net/net_namespace.h>
+#include <net/sock.h>
 #include <net/netns/generic.h>
 
 #define PPP_VERSION	"2.4.2"
@@ -385,7 +386,7 @@
 	/*
 	 * This could (should?) be enforced by the permissions on /dev/ppp.
 	 */
-	if (!ns_capable(file->f_cred->user_ns, CAP_NET_ADMIN))
+	if (!android_ns_capable(current->nsproxy->net_ns, CAP_NET_ADMIN))
 		return -EPERM;
 	return 0;
 }
diff -ruN a/drivers/net/tun.c b/drivers/net/tun.c
--- a/drivers/net/tun.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/net/tun.c	2021-12-23 08:35:40.000000000 +0100
@@ -2991,6 +2991,12 @@
 	int ret;
 	bool do_notify = false;
 
+	if (current->nsproxy->net_ns->core.sysctl_android_paranoid &&
+	    cmd != TUNGETIFF &&
+	    !ns_capable(sock_net(&tfile->sk)->user_ns, CAP_NET_ADMIN)) {
+		return -EPERM;
+	}
+
 	if (cmd == TUNSETIFF || cmd == TUNSETQUEUE ||
 	    (_IOC_TYPE(cmd) == SOCK_IOC_TYPE && cmd != SIOCGSKNS)) {
 		if (copy_from_user(&ifr, argp, ifreq_len))
diff -ruN a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
--- a/drivers/nvme/host/pci.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/nvme/host/pci.c	2021-12-23 08:35:42.000000000 +0100
@@ -3363,6 +3363,13 @@
 		.driver_data = NVME_QUIRK_DMA_ADDRESS_BITS_48, },
 	{ PCI_DEVICE(PCI_VENDOR_ID_AMAZON, 0xcd02),
 		.driver_data = NVME_QUIRK_DMA_ADDRESS_BITS_48, },
+	{ PCI_DEVICE(0x144d, 0xa809),   /* Samsung 128HBHQ and 256HAJD */
+		.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },
+	{ PCI_DEVICE(0x1987, 0x5013),   /* Phison PS5013 E13 */
+		.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },
+	{ PCI_DEVICE(0x2646, 0x500d),   /* Kingston OM3PDP3256B-AH 256G */
+		.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },
+	{ PCI_DEVICE_CLASS(PCI_CLASS_STORAGE_EXPRESS, 0xffffff) },
 	{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2001),
 		.driver_data = NVME_QUIRK_SINGLE_VECTOR },
 	{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2003) },
diff -ruN a/drivers/of/base.c b/drivers/of/base.c
--- a/drivers/of/base.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/of/base.c	2021-12-23 08:35:43.000000000 +0100
@@ -1365,6 +1365,7 @@
 
 	return count;
 }
+EXPORT_SYMBOL_GPL(of_phandle_iterator_args);
 
 static int __of_parse_phandle_with_args(const struct device_node *np,
 					const char *list_name,
diff -ruN a/drivers/opp/core.c b/drivers/opp/core.c
--- a/drivers/opp/core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/opp/core.c	2021-12-23 08:35:43.000000000 +0100
@@ -114,6 +114,34 @@
 EXPORT_SYMBOL_GPL(dev_pm_opp_get_voltage);
 
 /**
+ * dev_pm_opp_get_voltage_supply() - Gets the voltage corresponding to an opp
+ * with index
+ * @opp:        opp for which voltage has to be returned for
+ * @index:      index to specify the returned supplies
+ *
+ * Return: voltage in micro volt corresponding to the opp with index, else
+ * return 0
+ *
+ * This is useful for devices with multiple power supplies.
+ */
+unsigned long dev_pm_opp_get_voltage_supply(struct dev_pm_opp *opp,
+					    unsigned int index)
+{
+	if (IS_ERR_OR_NULL(opp)) {
+		pr_err("%s: Invalid parameters\n", __func__);
+		return 0;
+	}
+
+	if (index >= opp->opp_table->regulator_count) {
+		pr_err("%s: Invalid supply index: %u\n", __func__, index);
+		return 0;
+	}
+
+	return opp->supplies[index].u_volt;
+}
+EXPORT_SYMBOL_GPL(dev_pm_opp_get_voltage_supply);
+
+/**
  * dev_pm_opp_get_freq() - Gets the frequency corresponding to an available opp
  * @opp:	opp for which frequency has to be returned for
  *
diff -ruN a/drivers/pci/controller/dwc/pcie-qcom.c b/drivers/pci/controller/dwc/pcie-qcom.c
--- a/drivers/pci/controller/dwc/pcie-qcom.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pci/controller/dwc/pcie-qcom.c	2021-12-23 08:35:43.000000000 +0100
@@ -166,6 +166,9 @@
 	struct regulator_bulk_data supplies[2];
 	struct reset_control *pci_reset;
 	struct clk *pipe_clk;
+	struct clk *pipe_clk_src;
+	struct clk *phy_pipe_clk;
+	struct clk *ref_clk_src;
 };
 
 union qcom_pcie_resources {
@@ -189,6 +192,11 @@
 	int (*config_sid)(struct qcom_pcie *pcie);
 };
 
+struct qcom_pcie_cfg {
+	const struct qcom_pcie_ops *ops;
+	unsigned int pipe_clk_need_muxing:1;
+};
+
 struct qcom_pcie {
 	struct dw_pcie *pci;
 	void __iomem *parf;			/* DT parf */
@@ -197,6 +205,7 @@
 	struct phy *phy;
 	struct gpio_desc *reset;
 	const struct qcom_pcie_ops *ops;
+	unsigned int pipe_clk_need_muxing:1;
 };
 
 #define to_qcom_pcie(x)		dev_get_drvdata((x)->dev)
@@ -1167,6 +1176,20 @@
 	if (ret < 0)
 		return ret;
 
+	if (pcie->pipe_clk_need_muxing) {
+		res->pipe_clk_src = devm_clk_get(dev, "pipe_mux");
+		if (IS_ERR(res->pipe_clk_src))
+			return PTR_ERR(res->pipe_clk_src);
+
+		res->phy_pipe_clk = devm_clk_get(dev, "phy_pipe");
+		if (IS_ERR(res->phy_pipe_clk))
+			return PTR_ERR(res->phy_pipe_clk);
+
+		res->ref_clk_src = devm_clk_get(dev, "ref");
+		if (IS_ERR(res->ref_clk_src))
+			return PTR_ERR(res->ref_clk_src);
+	}
+
 	res->pipe_clk = devm_clk_get(dev, "pipe");
 	return PTR_ERR_OR_ZERO(res->pipe_clk);
 }
@@ -1185,6 +1208,10 @@
 		return ret;
 	}
 
+	/* Set TCXO as clock source for pcie_pipe_clk_src */
+	if (pcie->pipe_clk_need_muxing)
+		clk_set_parent(res->pipe_clk_src, res->ref_clk_src);
+
 	ret = clk_bulk_prepare_enable(res->num_clks, res->clks);
 	if (ret < 0)
 		goto err_disable_regulators;
@@ -1256,6 +1283,10 @@
 {
 	struct qcom_pcie_resources_2_7_0 *res = &pcie->res.v2_7_0;
 
+	/* Set pipe clock as clock source for pcie_pipe_clk_src */
+	if (pcie->pipe_clk_need_muxing)
+		clk_set_parent(res->pipe_clk_src, res->phy_pipe_clk);
+
 	return clk_prepare_enable(res->pipe_clk);
 }
 
@@ -1456,6 +1487,39 @@
 	.config_sid = qcom_pcie_config_sid_sm8250,
 };
 
+static const struct qcom_pcie_cfg apq8084_cfg = {
+	.ops = &ops_1_0_0,
+};
+
+static const struct qcom_pcie_cfg ipq8064_cfg = {
+	.ops = &ops_2_1_0,
+};
+
+static const struct qcom_pcie_cfg msm8996_cfg = {
+	.ops = &ops_2_3_2,
+};
+
+static const struct qcom_pcie_cfg ipq8074_cfg = {
+	.ops = &ops_2_3_3,
+};
+
+static const struct qcom_pcie_cfg ipq4019_cfg = {
+	.ops = &ops_2_4_0,
+};
+
+static const struct qcom_pcie_cfg sdm845_cfg = {
+	.ops = &ops_2_7_0,
+};
+
+static const struct qcom_pcie_cfg sm8250_cfg = {
+	.ops = &ops_1_9_0,
+};
+
+static const struct qcom_pcie_cfg sc7280_cfg = {
+	.ops = &ops_1_9_0,
+	.pipe_clk_need_muxing = true,
+};
+
 static const struct dw_pcie_ops dw_pcie_ops = {
 	.link_up = qcom_pcie_link_up,
 	.start_link = qcom_pcie_start_link,
@@ -1467,6 +1531,7 @@
 	struct pcie_port *pp;
 	struct dw_pcie *pci;
 	struct qcom_pcie *pcie;
+	const struct qcom_pcie_cfg *pcie_cfg;
 	int ret;
 
 	pcie = devm_kzalloc(dev, sizeof(*pcie), GFP_KERNEL);
@@ -1488,7 +1553,14 @@
 
 	pcie->pci = pci;
 
-	pcie->ops = of_device_get_match_data(dev);
+	pcie_cfg = of_device_get_match_data(dev);
+	if (!pcie_cfg || !pcie_cfg->ops) {
+		dev_err(dev, "Invalid platform data\n");
+		return -EINVAL;
+	}
+
+	pcie->ops = pcie_cfg->ops;
+	pcie->pipe_clk_need_muxing = pcie_cfg->pipe_clk_need_muxing;
 
 	pcie->reset = devm_gpiod_get_optional(dev, "perst", GPIOD_OUT_HIGH);
 	if (IS_ERR(pcie->reset)) {
@@ -1545,16 +1617,17 @@
 }
 
 static const struct of_device_id qcom_pcie_match[] = {
-	{ .compatible = "qcom,pcie-apq8084", .data = &ops_1_0_0 },
-	{ .compatible = "qcom,pcie-ipq8064", .data = &ops_2_1_0 },
-	{ .compatible = "qcom,pcie-ipq8064-v2", .data = &ops_2_1_0 },
-	{ .compatible = "qcom,pcie-apq8064", .data = &ops_2_1_0 },
-	{ .compatible = "qcom,pcie-msm8996", .data = &ops_2_3_2 },
-	{ .compatible = "qcom,pcie-ipq8074", .data = &ops_2_3_3 },
-	{ .compatible = "qcom,pcie-ipq4019", .data = &ops_2_4_0 },
-	{ .compatible = "qcom,pcie-qcs404", .data = &ops_2_4_0 },
-	{ .compatible = "qcom,pcie-sdm845", .data = &ops_2_7_0 },
-	{ .compatible = "qcom,pcie-sm8250", .data = &ops_1_9_0 },
+	{ .compatible = "qcom,pcie-apq8084", .data = &apq8084_cfg },
+	{ .compatible = "qcom,pcie-ipq8064", .data = &ipq8064_cfg },
+	{ .compatible = "qcom,pcie-ipq8064-v2", .data = &ipq8064_cfg },
+	{ .compatible = "qcom,pcie-apq8064", .data = &ipq8064_cfg },
+	{ .compatible = "qcom,pcie-msm8996", .data = &msm8996_cfg },
+	{ .compatible = "qcom,pcie-ipq8074", .data = &ipq8074_cfg },
+	{ .compatible = "qcom,pcie-ipq4019", .data = &ipq4019_cfg },
+	{ .compatible = "qcom,pcie-qcs404", .data = &ipq4019_cfg },
+	{ .compatible = "qcom,pcie-sdm845", .data = &sdm845_cfg },
+	{ .compatible = "qcom,pcie-sm8250", .data = &sm8250_cfg },
+	{ .compatible = "qcom,pcie-sc7280", .data = &sc7280_cfg },
 	{ }
 };
 
diff -ruN a/drivers/pci/drvr-allowlist.c b/drivers/pci/drvr-allowlist.c
--- a/drivers/pci/drvr-allowlist.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/pci/drvr-allowlist.c	2021-12-23 08:35:43.000000000 +0100
@@ -0,0 +1,238 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Allowlist of PCI drivers that are allowed to bind to external devices
+ */
+
+#include <linux/ctype.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include "pci.h"
+
+/*
+ * Parameter to essentially disable allowlist code (thus allow all drivers to
+ * connect to any external PCI devices).
+ */
+static bool trust_external_pci_devices;
+core_param(trust_external_pci_devices, trust_external_pci_devices, bool, 0444);
+
+/* Driver allowlist */
+struct allowlist_entry {
+	const char *drvr_name;
+	struct list_head node;
+};
+
+static LIST_HEAD(allowlist);
+static DECLARE_RWSEM(allowlist_sem);
+
+#define TRUNCATED	"...<truncated>\n"
+
+/*
+ * Locks down the binding of drivers to untrusted devices
+ * (No PCI drivers to bind to any new untrusted PCI device)
+ */
+static bool drivers_allowlist_lockdown = true;
+static DECLARE_RWSEM(lockdown_sem);
+
+static ssize_t drivers_allowlist_show(struct bus_type *bus, char *buf)
+{
+	size_t count = 0;
+	struct allowlist_entry *entry;
+
+	down_read(&allowlist_sem);
+	list_for_each_entry(entry, &allowlist, node) {
+		if (count + strlen(entry->drvr_name) + sizeof(TRUNCATED) <
+		    PAGE_SIZE) {
+			count += snprintf(buf + count, PAGE_SIZE - count,
+					  "%s\n", entry->drvr_name);
+		} else {
+			count += snprintf(buf + count, PAGE_SIZE - count,
+					  TRUNCATED);
+			break;
+		}
+	}
+	up_read(&allowlist_sem);
+	return count;
+}
+
+static ssize_t drivers_allowlist_store(struct bus_type *bus, const char *buf,
+				       size_t count)
+{
+	struct allowlist_entry *entry;
+	ssize_t ret = count;
+	unsigned int i;
+	char *drv;
+
+	if (!count)
+		return -EINVAL;
+
+	drv = kstrndup(buf, count, GFP_KERNEL);
+	if (!drv)
+		return -ENOMEM;
+
+	/* Remove any trailing white spaces */
+	strim(drv);
+	if (!*drv) {
+		ret = -EINVAL;
+		goto out_kfree;
+	}
+
+	/* Driver names cannot have special characters */
+	for (i = 0; i < strlen(drv); i++)
+		if (!isalnum(drv[i]) && drv[i] != '_') {
+			ret = -EINVAL;
+			goto out_kfree;
+		}
+
+	down_write(&allowlist_sem);
+
+	/* Lookup in the allowlist */
+	list_for_each_entry(entry, &allowlist, node)
+		if (!strcmp(drv, entry->drvr_name)) {
+			ret = -EEXIST;
+			goto out_release_sem;
+		}
+
+	/* Add a driver to the allowlist */
+	entry = kmalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry) {
+		ret = -ENOMEM;
+		goto out_release_sem;
+	}
+	entry->drvr_name = drv;
+	list_add_tail(&entry->node, &allowlist);
+	up_write(&allowlist_sem);
+	return ret;
+
+out_release_sem:
+	up_write(&allowlist_sem);
+out_kfree:
+	kfree(drv);
+	return ret;
+}
+static BUS_ATTR_RW(drivers_allowlist);
+
+static ssize_t drivers_allowlist_lockdown_show(struct bus_type *bus, char *buf)
+{
+	int ret;
+
+	down_read(&lockdown_sem);
+	ret = sprintf(buf, "%u\n", drivers_allowlist_lockdown);
+	up_read(&lockdown_sem);
+
+	return ret;
+}
+
+static ssize_t
+drivers_allowlist_lockdown_store(struct bus_type *bus, const char *buf,
+				 size_t count)
+{
+	bool lockdown, state_changed = false;
+	struct pci_dev *dev = NULL;
+
+	if (strtobool(buf, &lockdown))
+		return -EINVAL;
+
+	down_write(&lockdown_sem);
+	if (drivers_allowlist_lockdown != lockdown) {
+		drivers_allowlist_lockdown = lockdown;
+		state_changed = true;
+	}
+	up_write(&lockdown_sem);
+
+	if (state_changed && !lockdown) {
+		/* Attach any devices blocked earlier, subject to allowlist */
+		for_each_pci_dev(dev) {
+			if (dev_is_removable(&dev->dev) &&
+			    !device_attach(&dev->dev))
+				pci_dbg(dev, "No driver\n");
+		}
+	}
+	return count;
+}
+static BUS_ATTR_RW(drivers_allowlist_lockdown);
+
+static int __init pci_drivers_allowlist_init(void)
+{
+	int ret;
+
+	if (trust_external_pci_devices)
+		return 0;
+
+	ret = bus_create_file(&pci_bus_type, &bus_attr_drivers_allowlist);
+	if (ret) {
+		pr_err("%s: failed to create allowlist in sysfs\n", __func__);
+		return ret;
+	}
+
+	ret = bus_create_file(&pci_bus_type,
+			      &bus_attr_drivers_allowlist_lockdown);
+	if (ret) {
+		pr_err("%s: failed to create allowlist_lockdown\n", __func__);
+		bus_remove_file(&pci_bus_type, &bus_attr_drivers_allowlist);
+	}
+	return ret;
+}
+late_initcall(pci_drivers_allowlist_init);
+
+static bool pci_driver_is_allowed(const char *name)
+{
+	struct allowlist_entry *entry;
+
+	down_read(&allowlist_sem);
+	list_for_each_entry(entry, &allowlist, node) {
+		if (!strcmp(name, entry->drvr_name)) {
+			up_read(&allowlist_sem);
+			return true;
+		}
+	}
+	up_read(&allowlist_sem);
+	return false;
+}
+
+bool pci_allowed_to_attach(struct pci_driver *drv, struct pci_dev *dev)
+{
+	char event[16], drvr[32], *reason;
+	char *udev_env[] = { event, drvr, NULL };
+
+	snprintf(drvr, sizeof(drvr), "DRVR=%s", drv->name);
+
+	/* Bypass Allowlist code, if platform wants so */
+	if (trust_external_pci_devices) {
+		reason = "trust_external_pci_devices";
+		goto allowed;
+	}
+
+	/* Allow internal devices */
+	if (!dev_is_removable(&dev->dev)) {
+		reason = "internal device";
+		goto allowed;
+	}
+
+	/* Don't allow any driver attaches, if locked down */
+	down_read(&lockdown_sem);
+	if (drivers_allowlist_lockdown) {
+		up_read(&lockdown_sem);
+		reason = "drivers_allowlist_lockdown enforced";
+		goto not_allowed;
+	}
+	up_read(&lockdown_sem);
+
+	/* Allow if driver is in allowlist */
+	if (pci_driver_is_allowed(drv->name)) {
+		reason = "drvr in allowlist";
+		goto allowed;
+	}
+	reason = "drvr not in allowlist";
+
+not_allowed:
+	pci_err(dev, "attach not allowed to drvr %s [%s]\n", drv->name, reason);
+	snprintf(event, sizeof(event), "EVENT=BLOCKED");
+	kobject_uevent_env(&dev->dev.kobj, KOBJ_CHANGE, udev_env);
+	return false;
+
+allowed:
+	pci_info(dev, "attach allowed to drvr %s [%s]\n", drv->name, reason);
+	snprintf(event, sizeof(event), "EVENT=ALLOWED");
+	kobject_uevent_env(&dev->dev.kobj, KOBJ_CHANGE, udev_env);
+	return true;
+}
diff -ruN a/drivers/pci/Makefile b/drivers/pci/Makefile
--- a/drivers/pci/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pci/Makefile	2021-12-23 08:35:43.000000000 +0100
@@ -5,7 +5,8 @@
 obj-$(CONFIG_PCI)		+= access.o bus.o probe.o host-bridge.o \
 				   remove.o pci.o pci-driver.o search.o \
 				   pci-sysfs.o rom.o setup-res.o irq.o vpd.o \
-				   setup-bus.o vc.o mmap.o setup-irq.o msi.o
+				   setup-bus.o vc.o mmap.o setup-irq.o msi.o \
+				   drvr-allowlist.o
 
 obj-$(CONFIG_PCI)		+= pcie/
 
diff -ruN a/drivers/pci/pci.c b/drivers/pci/pci.c
--- a/drivers/pci/pci.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pci/pci.c	2021-12-23 08:35:43.000000000 +0100
@@ -835,6 +835,7 @@
 {
 	pci_acs_enable = 1;
 }
+EXPORT_SYMBOL_GPL(pci_request_acs);
 
 static const char *disable_acs_redir_param;
 
@@ -1477,6 +1478,24 @@
 	return 0;
 }
 
+void pci_bridge_reconfigure_ltr(struct pci_dev *dev)
+{
+#ifdef CONFIG_PCIEASPM
+	struct pci_dev *bridge;
+	u32 ctl;
+
+	bridge = pci_upstream_bridge(dev);
+	if (bridge && bridge->ltr_path) {
+		pcie_capability_read_dword(bridge, PCI_EXP_DEVCTL2, &ctl);
+		if (!(ctl & PCI_EXP_DEVCTL2_LTR_EN)) {
+			pci_dbg(bridge, "re-enabling LTR\n");
+			pcie_capability_set_word(bridge, PCI_EXP_DEVCTL2,
+						 PCI_EXP_DEVCTL2_LTR_EN);
+		}
+	}
+#endif
+}
+
 static void pci_restore_pcie_state(struct pci_dev *dev)
 {
 	int i = 0;
@@ -1487,6 +1506,13 @@
 	if (!save_state)
 		return;
 
+	/*
+	 * Downstream ports reset the LTR enable bit when link goes down.
+	 * Check and re-configure the bit here before restoring device.
+	 * PCIe r5.0, sec 7.5.3.16.
+	 */
+	pci_bridge_reconfigure_ltr(dev);
+
 	cap = (u16 *)&save_state->cap.data[0];
 	pcie_capability_write_word(dev, PCI_EXP_DEVCTL, cap[i++]);
 	pcie_capability_write_word(dev, PCI_EXP_LNKCTL, cap[i++]);
diff -ruN a/drivers/pci/pci-driver.c b/drivers/pci/pci-driver.c
--- a/drivers/pci/pci-driver.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pci/pci-driver.c	2021-12-23 08:35:43.000000000 +0100
@@ -1470,7 +1470,7 @@
 
 	pci_drv = to_pci_driver(drv);
 	found_id = pci_match_device(pci_drv, pci_dev);
-	if (found_id)
+	if (found_id && pci_allowed_to_attach(pci_drv, pci_dev))
 		return 1;
 
 	return 0;
diff -ruN a/drivers/pci/pci.h b/drivers/pci/pci.h
--- a/drivers/pci/pci.h	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pci/pci.h	2021-12-23 08:35:43.000000000 +0100
@@ -125,6 +125,7 @@
 void pci_bridge_d3_update(struct pci_dev *dev);
 int pci_bridge_wait_for_secondary_bus(struct pci_dev *dev, char *reset_type,
 				      int timeout);
+void pci_bridge_reconfigure_ltr(struct pci_dev *dev);
 
 static inline void pci_wakeup_event(struct pci_dev *dev)
 {
@@ -743,5 +744,7 @@
 #endif
 
 extern const struct attribute_group pci_dev_reset_method_attr_group;
+bool pci_drv_allowed_for_untrusted_devs(struct device_driver *drvr);
+bool pci_allowed_to_attach(struct pci_driver *drv, struct pci_dev *dev);
 
 #endif /* DRIVERS_PCI_H */
diff -ruN a/drivers/pci/pci-sysfs.c b/drivers/pci/pci-sysfs.c
--- a/drivers/pci/pci-sysfs.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pci/pci-sysfs.c	2021-12-23 08:35:43.000000000 +0100
@@ -50,6 +50,7 @@
 pci_config_attr(revision, "0x%02x\n");
 pci_config_attr(class, "0x%06x\n");
 pci_config_attr(irq, "%u\n");
+pci_config_attr(untrusted, "%u\n");
 
 static ssize_t broken_parity_status_show(struct device *dev,
 					 struct device_attribute *attr,
@@ -618,6 +619,7 @@
 #endif
 	&dev_attr_driver_override.attr,
 	&dev_attr_ari_enabled.attr,
+	&dev_attr_untrusted.attr,
 	NULL,
 };
 
diff -ruN a/drivers/pci/probe.c b/drivers/pci/probe.c
--- a/drivers/pci/probe.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pci/probe.c	2021-12-23 08:35:43.000000000 +0100
@@ -2168,9 +2168,21 @@
 	 * Complex and all intermediate Switches indicate support for LTR.
 	 * PCIe r4.0, sec 6.18.
 	 */
-	if (pci_pcie_type(dev) == PCI_EXP_TYPE_ROOT_PORT ||
-	    ((bridge = pci_upstream_bridge(dev)) &&
-	      bridge->ltr_path)) {
+	if (pci_pcie_type(dev) == PCI_EXP_TYPE_ROOT_PORT) {
+		pcie_capability_set_word(dev, PCI_EXP_DEVCTL2,
+					 PCI_EXP_DEVCTL2_LTR_EN);
+		dev->ltr_path = 1;
+		return;
+	}
+
+	/*
+	 * If we're configuring a hot-added device, LTR was likely
+	 * disabled in the upstream bridge, so re-enable it before enabling
+	 * it in the new device.
+	 */
+	bridge = pci_upstream_bridge(dev);
+	if (bridge && bridge->ltr_path) {
+		pci_bridge_reconfigure_ltr(dev);
 		pcie_capability_set_word(dev, PCI_EXP_DEVCTL2,
 					 PCI_EXP_DEVCTL2_LTR_EN);
 		dev->ltr_path = 1;
diff -ruN a/drivers/pci/quirks.c b/drivers/pci/quirks.c
--- a/drivers/pci/quirks.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pci/quirks.c	2021-12-23 08:35:43.000000000 +0100
@@ -5797,6 +5797,17 @@
 DECLARE_PCI_FIXUP_CLASS_HEADER(0x1ac1, 0x089a,
 			       PCI_CLASS_NOT_DEFINED, 8, apex_pci_fixup_class);
 
+static void chromeos_internal_but_untrusted_device(struct pci_dev *pdev)
+{
+	if (dmi_match(DMI_SYS_VENDOR, "Google")) {
+		pci_info(pdev, "ChromeOS internal device marked untrusted\n");
+		pdev->untrusted = true;
+	}
+}
+/* 5G Modem on x86 systems (Brya onwards) */
+DECLARE_PCI_FIXUP_EARLY(PCI_VENDOR_ID_MEDIATEK, 0x4d75,
+			chromeos_internal_but_untrusted_device);
+
 static void nvidia_ion_ahci_fixup(struct pci_dev *pdev)
 {
 	pdev->dev_flags |= PCI_DEV_FLAGS_HAS_MSI_MASKING;
diff -ruN a/drivers/pci/search.c b/drivers/pci/search.c
--- a/drivers/pci/search.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pci/search.c	2021-12-23 08:35:43.000000000 +0100
@@ -112,6 +112,7 @@
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(pci_for_each_dma_alias);
 
 static struct pci_bus *pci_do_find_bus(struct pci_bus *bus, unsigned char busnr)
 {
diff -ruN a/drivers/pkglist/Kconfig b/drivers/pkglist/Kconfig
--- a/drivers/pkglist/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/pkglist/Kconfig	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,30 @@
+config PKGLIST
+	tristate "Package list for emulated 'SD card' file system for Android"
+	depends on CONFIGFS_FS || !CONFIGFS_FS
+	help
+	  Pkglist presents an interface for Android's emulated sdcard layer.
+	  It relates the names of packages to their package ids, so that they can be
+	  given access to their app specific folders.
+
+	  Additionally, pkglist allows configuring the gid assigned to the lower file
+	  outside of package specific directories for the purpose of tracking storage
+	  with quotas.
+
+choice
+	prompt "Configuration options"
+	depends on PKGLIST
+	help
+	  Configuration options. This controls how you provide the emulated
+	  SD card layer with configuration information from userspace.
+
+config PKGLIST_USE_CONFIGFS
+	bool "Use Configfs based pkglist"
+	depends on CONFIGFS_FS
+	help
+	  Use configfs based pkglist driver for configuration information.
+
+config PKGLIST_NO_CONFIG
+	bool "None"
+	help
+	  This does not allow configuration of sdcardfs.
+endchoice
diff -ruN a/drivers/pkglist/Makefile b/drivers/pkglist/Makefile
--- a/drivers/pkglist/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/pkglist/Makefile	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,3 @@
+obj-$(CONFIG_PKGLIST) += pkg.o
+pkg-$(CONFIG_PKGLIST_USE_CONFIGFS) += pkglist.o
+pkg-$(CONFIG_PKGLIST_NO_CONFIG) += pkglist_none.o
diff -ruN a/drivers/pkglist/pkglist.c b/drivers/pkglist/pkglist.c
--- a/drivers/pkglist/pkglist.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/pkglist/pkglist.c	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,966 @@
+/*
+ * Copyright (C) 2017 Google Inc., Author: Daniel Rosenberg <drosen@google.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/module.h>
+#include <linux/hashtable.h>
+#include <linux/atomic.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/configfs.h>
+#include <linux/dcache.h>
+#include <linux/ctype.h>
+#include <linux/cred.h>
+
+#include <linux/pkglist.h>
+
+/*
+ * This presents a configfs interface for Android's emulated sdcard layer.
+ * It relates the names of packages to their package ids, so that they can be
+ * given access to their app specific folders.
+ *
+ * To add a package, create a directory at the base level with the name of that
+ * package. Within these folders, write to appid to set its id.
+ * If an Android user should not know of an app's installation, write their
+ * Android user id to excluded_userids. Write to clear_userid to remove users
+ * from that list.
+ *
+ * remove_userid offers a way to remove all instances of a user from all exclude
+ * lists.
+ *
+ * Additionally, pkglist allows configuring the gid assigned to the lower file
+ * outside of package specific directories for the purpose of tracking storage
+ * with quotas.
+ *
+ * To track files with a particular extension, create a folder inside extensions
+ * for each class of thing you wish to track. Inside that directory, write the
+ * gid you want to associate to the group to ext_gid, and make a directory for
+ * extension you want to include. All are assumed to be case insensitive.
+ *
+ * ex: mkdir /config/[config_location]/extension/audio/
+ *     echo 1055 > /config/[config_location]/extension/audio/ext_gid
+ *     mkdir /config/[config_location]/extension/audio/
+ *
+ */
+
+static char *pkglist_config_location = "sdcardfs";
+module_param(pkglist_config_location, charp, 0);
+MODULE_PARM_DESC(pkglist_config_location, "Location of pkglist in configfs");
+
+static struct kmem_cache *hashtable_entry_cachep;
+
+static DEFINE_HASHTABLE(package_to_appid, 8);
+static DEFINE_HASHTABLE(package_to_userid, 8);
+static DEFINE_HASHTABLE(ext_to_groupid, 8);
+static DEFINE_MUTEX(pkg_list_lock);
+static LIST_HEAD(pkglist_listeners);
+
+struct extensions_value {
+	struct config_group group;
+	kgid_t gid;
+};
+
+struct extension_details {
+	struct config_item item;
+	struct hlist_node hlist;
+	struct qstr name;
+	struct extensions_value *value;
+};
+
+struct hashtable_entry {
+	struct hlist_node hlist;
+	struct hlist_node dlist; /* for deletion cleanup */
+	struct qstr key;
+	atomic_t value;
+};
+
+static unsigned int full_name_case_hash(const unsigned char *name,
+					unsigned int len)
+{
+	unsigned long hash = init_name_hash(0);
+
+	while (len--)
+		hash = partial_name_hash(tolower(*name++), hash);
+	return end_name_hash(hash);
+}
+
+static inline void qstr_init(struct qstr *q, const char *name)
+{
+	q->name = name;
+	q->len = strlen(q->name);
+	q->hash = full_name_case_hash(q->name, q->len);
+}
+
+static inline int qstr_copy(const struct qstr *src, struct qstr *dest)
+{
+	dest->name = kstrdup(src->name, GFP_KERNEL);
+	dest->hash_len = src->hash_len;
+	return !!dest->name;
+}
+
+static kuid_t __get_appid(const struct qstr *key)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = key->hash;
+	uid_t ret_id;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(package_to_appid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			ret_id = atomic_read(&hash_cur->value);
+			rcu_read_unlock();
+			return make_kuid(&init_user_ns, ret_id);
+		}
+	}
+	rcu_read_unlock();
+	return INVALID_UID;
+}
+
+kuid_t pkglist_get_appid(const char *key)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	return __get_appid(&q);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_appid);
+
+static kgid_t __get_ext_gid(const struct qstr *key)
+{
+	struct extension_details *hash_cur;
+	unsigned int hash = key->hash;
+	kgid_t ret_id;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(ext_to_groupid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->name)) {
+			ret_id = hash_cur->value->gid;
+			rcu_read_unlock();
+			return ret_id;
+		}
+	}
+	rcu_read_unlock();
+	return INVALID_GID;
+}
+
+kgid_t pkglist_get_ext_gid(const char *key)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	return __get_ext_gid(&q);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_ext_gid);
+
+static bool __is_excluded(const struct qstr *app_name, uint32_t user)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = app_name->hash;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (atomic_read(&hash_cur->value) == user &&
+				qstr_case_eq(app_name, &hash_cur->key)) {
+			rcu_read_unlock();
+			return true;
+		}
+	}
+	rcu_read_unlock();
+	return false;
+}
+
+bool pkglist_user_is_excluded(const char *key, uint32_t user)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	return __is_excluded(&q, user);
+}
+EXPORT_SYMBOL_GPL(pkglist_user_is_excluded);
+
+kuid_t pkglist_get_allowed_appid(const char *key, uint32_t user)
+{
+	struct qstr q;
+
+	qstr_init(&q, key);
+	if (!__is_excluded(&q, user))
+		return __get_appid(&q);
+	else
+		return INVALID_UID;
+}
+EXPORT_SYMBOL_GPL(pkglist_get_allowed_appid);
+
+static struct hashtable_entry *alloc_hashtable_entry(const struct qstr *key,
+		uid_t value)
+{
+	struct hashtable_entry *ret = kmem_cache_alloc(hashtable_entry_cachep,
+			GFP_KERNEL);
+	if (!ret)
+		return NULL;
+	INIT_HLIST_NODE(&ret->dlist);
+	INIT_HLIST_NODE(&ret->hlist);
+
+	if (!qstr_copy(key, &ret->key)) {
+		kmem_cache_free(hashtable_entry_cachep, ret);
+		return NULL;
+	}
+
+	atomic_set(&ret->value, value);
+	return ret;
+}
+
+static int insert_packagelist_appid_entry_locked(const struct qstr *key,
+						kuid_t value)
+{
+	struct hashtable_entry *hash_cur;
+	struct hashtable_entry *new_entry;
+	unsigned int hash = key->hash;
+
+	hash_for_each_possible_rcu(package_to_appid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			atomic_set(&hash_cur->value, value.val);
+			return 0;
+		}
+	}
+	new_entry = alloc_hashtable_entry(key, value.val);
+	if (!new_entry)
+		return -ENOMEM;
+	hash_add_rcu(package_to_appid, &new_entry->hlist, hash);
+	return 0;
+}
+
+static int insert_ext_gid_entry_locked(struct extension_details *ed)
+{
+	struct extension_details *hash_cur;
+	unsigned int hash = ed->name.hash;
+
+	/* An extension can only belong to one gid */
+	hash_for_each_possible_rcu(ext_to_groupid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(&ed->name, &hash_cur->name))
+			return -EINVAL;
+	}
+
+	hash_add_rcu(ext_to_groupid, &ed->hlist, hash);
+	return 0;
+}
+
+static int insert_userid_exclude_entry_locked(const struct qstr *key,
+						unsigned int value)
+{
+	struct hashtable_entry *hash_cur;
+	struct hashtable_entry *new_entry;
+	unsigned int hash = key->hash;
+
+	/* Only insert if not already present */
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (atomic_read(&hash_cur->value) == value &&
+				qstr_case_eq(key, &hash_cur->key))
+			return 0;
+	}
+	new_entry = alloc_hashtable_entry(key, value);
+	if (!new_entry)
+		return -ENOMEM;
+	hash_add_rcu(package_to_userid, &new_entry->hlist, hash);
+	return 0;
+}
+
+static int insert_packagelist_entry(const struct qstr *key, kuid_t value)
+{
+	struct pkg_list *pkg;
+	int err;
+
+	mutex_lock(&pkg_list_lock);
+	err = insert_packagelist_appid_entry_locked(key, value);
+	if (!err) {
+		list_for_each_entry(pkg, &pkglist_listeners, list) {
+			pkg->update(BY_NAME, key, 0);
+		}
+	}
+	mutex_unlock(&pkg_list_lock);
+
+	return err;
+}
+
+static int insert_ext_gid_entry(struct extension_details *ed)
+{
+	int err;
+
+	mutex_lock(&pkg_list_lock);
+	err = insert_ext_gid_entry_locked(ed);
+	mutex_unlock(&pkg_list_lock);
+
+	return err;
+}
+
+static int insert_userid_exclude_entry(const struct qstr *key, uint32_t value)
+{
+	int err;
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	err = insert_userid_exclude_entry_locked(key, value);
+	if (!err) {
+		list_for_each_entry(pkg, &pkglist_listeners, list) {
+			pkg->update(BY_NAME|BY_USERID, key, value);
+		}
+	}
+	mutex_unlock(&pkg_list_lock);
+
+	return err;
+}
+
+static void free_hashtable_entry(struct hashtable_entry *entry)
+{
+	kfree(entry->key.name);
+	kmem_cache_free(hashtable_entry_cachep, entry);
+}
+
+static void remove_packagelist_entry_locked(const struct qstr *key)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = key->hash;
+	struct hlist_node *h_t;
+	HLIST_HEAD(free_list);
+
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			hash_del_rcu(&hash_cur->hlist);
+			hlist_add_head(&hash_cur->dlist, &free_list);
+		}
+	}
+	hash_for_each_possible_rcu(package_to_appid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key)) {
+			hash_del_rcu(&hash_cur->hlist);
+			hlist_add_head(&hash_cur->dlist, &free_list);
+			break;
+		}
+	}
+	synchronize_rcu();
+	hlist_for_each_entry_safe(hash_cur, h_t, &free_list, dlist)
+		free_hashtable_entry(hash_cur);
+}
+
+static void remove_packagelist_entry(const struct qstr *key)
+{
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	remove_packagelist_entry_locked(key);
+	list_for_each_entry(pkg, &pkglist_listeners, list) {
+		pkg->update(BY_NAME, key, 0);
+	}
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void remove_ext_gid_entry_locked(struct extension_details *ed)
+{
+	struct extension_details *hash_cur;
+	struct qstr *key = &ed->name;
+	unsigned int hash = key->hash;
+
+	hash_for_each_possible_rcu(ext_to_groupid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->name)
+				&& hash_cur->value == ed->value) {
+			hash_del_rcu(&hash_cur->hlist);
+			synchronize_rcu();
+			break;
+		}
+	}
+}
+
+static void remove_ext_gid_entry(struct extension_details *ed)
+{
+	mutex_lock(&pkg_list_lock);
+	remove_ext_gid_entry_locked(ed);
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void remove_userid_all_entry_locked(uint32_t userid)
+{
+	struct hashtable_entry *hash_cur;
+	struct hlist_node *h_t;
+	HLIST_HEAD(free_list);
+	int i;
+
+	hash_for_each_rcu(package_to_userid, i, hash_cur, hlist) {
+		if (atomic_read(&hash_cur->value) == userid) {
+			hash_del_rcu(&hash_cur->hlist);
+			hlist_add_head(&hash_cur->dlist, &free_list);
+		}
+	}
+	synchronize_rcu();
+	hlist_for_each_entry_safe(hash_cur, h_t, &free_list, dlist) {
+		free_hashtable_entry(hash_cur);
+	}
+}
+
+static void remove_userid_all_entry(uint32_t userid)
+{
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	remove_userid_all_entry_locked(userid);
+
+	list_for_each_entry(pkg, &pkglist_listeners, list) {
+		pkg->update(BY_USERID, NULL, userid);
+	}
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void remove_userid_exclude_entry_locked(const struct qstr *key,
+						uint32_t userid)
+{
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = key->hash;
+
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(key, &hash_cur->key) &&
+				atomic_read(&hash_cur->value) == userid) {
+			hash_del_rcu(&hash_cur->hlist);
+			synchronize_rcu();
+			free_hashtable_entry(hash_cur);
+			break;
+		}
+	}
+}
+
+static void remove_userid_exclude_entry(const struct qstr *key, uint32_t userid)
+{
+	struct pkg_list *pkg;
+
+	mutex_lock(&pkg_list_lock);
+	remove_userid_exclude_entry_locked(key, userid);
+	list_for_each_entry(pkg, &pkglist_listeners, list) {
+		pkg->update(BY_NAME|BY_USERID, key, userid);
+	}
+	mutex_unlock(&pkg_list_lock);
+}
+
+static void packagelist_destroy(void)
+{
+	struct hashtable_entry *hash_cur;
+	struct hlist_node *h_t;
+	HLIST_HEAD(free_list);
+	int i;
+
+	mutex_lock(&pkg_list_lock);
+	hash_for_each_rcu(package_to_appid, i, hash_cur, hlist) {
+		hash_del_rcu(&hash_cur->hlist);
+		hlist_add_head(&hash_cur->dlist, &free_list);
+	}
+	hash_for_each_rcu(package_to_userid, i, hash_cur, hlist) {
+		hash_del_rcu(&hash_cur->hlist);
+		hlist_add_head(&hash_cur->dlist, &free_list);
+	}
+	synchronize_rcu();
+	hlist_for_each_entry_safe(hash_cur, h_t, &free_list, dlist)
+		free_hashtable_entry(hash_cur);
+	mutex_unlock(&pkg_list_lock);
+	pr_info("pkglist: destroyed pkglist\n");
+}
+
+#define PACKAGE_DETAILS_ATTR(_pfx, _name)			\
+static struct configfs_attribute _pfx##attr_##_name = {	\
+	.ca_name	= __stringify(_name),		\
+	.ca_mode	= S_IRUGO | S_IWUGO,		\
+	.ca_owner	= THIS_MODULE,			\
+	.show		= _pfx##_name##_show,		\
+	.store		= _pfx##_name##_store,		\
+}
+
+#define PACKAGE_DETAILS_ATTR_RO(_pfx, _name)			\
+static struct configfs_attribute _pfx##attr_##_name = {	\
+	.ca_name	= __stringify(_name),		\
+	.ca_mode	= S_IRUGO,			\
+	.ca_owner	= THIS_MODULE,			\
+	.show		= _pfx##_name##_show,		\
+}
+
+#define PACKAGE_DETAILS_ATTR_WO(_pfx, _name)			\
+static struct configfs_attribute _pfx##attr_##_name = {	\
+	.ca_name	= __stringify(_name),		\
+	.ca_mode	= S_IWUGO,			\
+	.ca_owner	= THIS_MODULE,			\
+	.store		= _pfx##_name##_store,		\
+}
+
+
+struct package_details {
+	struct config_item item;
+	struct qstr name;
+};
+
+static inline struct package_details *to_package_details(
+						struct config_item *item)
+{
+	return item ? container_of(item, struct package_details, item) : NULL;
+}
+
+#define PACKAGE_DETAILS_ATTRIBUTE(name) (&package_details_attr_##name)
+
+static ssize_t package_details_appid_show(struct config_item *item, char *page)
+{
+	return scnprintf(page, PAGE_SIZE, "%u\n", from_kuid(current_user_ns(),
+				__get_appid(&to_package_details(item)->name)));
+}
+
+static ssize_t package_details_appid_store(struct config_item *item,
+					   const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+	kuid_t uid;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+
+	uid = make_kuid(current_user_ns(), tmp);
+
+	ret = insert_packagelist_entry(&to_package_details(item)->name, uid);
+
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+static ssize_t package_details_excluded_userids_show(struct config_item *item,
+						     char *page)
+{
+	struct package_details *package_details = to_package_details(item);
+	struct hashtable_entry *hash_cur;
+	unsigned int hash = package_details->name.hash;
+	int count = 0;
+
+	rcu_read_lock();
+	hash_for_each_possible_rcu(package_to_userid, hash_cur, hlist, hash) {
+		if (qstr_case_eq(&package_details->name, &hash_cur->key))
+			count += scnprintf(page + count, PAGE_SIZE - count,
+					   "%d ", atomic_read(&hash_cur->value));
+	}
+	rcu_read_unlock();
+	if (count)
+		count--;
+	count += scnprintf(page + count, PAGE_SIZE - count, "\n");
+	return count;
+}
+
+static ssize_t package_details_excluded_userids_store(struct config_item *item,
+						      const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+
+	ret = insert_userid_exclude_entry(&to_package_details(item)->name, tmp);
+
+	if (ret)
+		return ret;
+
+	return count;
+}
+
+static ssize_t package_details_clear_userid_store(struct config_item *item,
+						  const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+	remove_userid_exclude_entry(&to_package_details(item)->name, tmp);
+	return count;
+}
+
+static void package_details_release(struct config_item *item)
+{
+	struct package_details *package_details = to_package_details(item);
+
+	pr_debug("pkglist: removing %s\n", package_details->name.name);
+	remove_packagelist_entry(&package_details->name);
+	kfree(package_details->name.name);
+	kfree(package_details);
+}
+
+PACKAGE_DETAILS_ATTR(package_details_, appid);
+PACKAGE_DETAILS_ATTR(package_details_, excluded_userids);
+PACKAGE_DETAILS_ATTR_WO(package_details_, clear_userid);
+
+static struct configfs_attribute *package_details_attrs[] = {
+	PACKAGE_DETAILS_ATTRIBUTE(appid),
+	PACKAGE_DETAILS_ATTRIBUTE(excluded_userids),
+	PACKAGE_DETAILS_ATTRIBUTE(clear_userid),
+	NULL,
+};
+
+static struct configfs_item_operations package_details_item_ops = {
+	.release = package_details_release,
+};
+
+static struct config_item_type package_appid_type = {
+	.ct_item_ops	= &package_details_item_ops,
+	.ct_attrs	= package_details_attrs,
+	.ct_owner	= THIS_MODULE,
+};
+
+static inline struct extensions_value *to_extensions_value(
+					struct config_item *item)
+{
+	return item ? container_of(to_config_group(item),
+				struct extensions_value, group)
+			: NULL;
+}
+
+static inline struct extension_details *to_extension_details(
+					struct config_item *item)
+{
+	return item ? container_of(item, struct extension_details, item)
+			: NULL;
+}
+
+#define EXTENSIONS_VALUE_ATTRIBUTE(name) (&extensions_value_attr_##name)
+
+static void extension_details_release(struct config_item *item)
+{
+	struct extension_details *ed = to_extension_details(item);
+
+	pr_debug("pkglist: No longer mapping %s files to gid %d\n",
+				ed->name.name,
+				from_kgid(current_user_ns(), ed->value->gid));
+	remove_ext_gid_entry(ed);
+	kfree(ed->name.name);
+	kfree(ed);
+}
+
+static struct configfs_item_operations extension_details_item_ops = {
+	.release = extension_details_release,
+};
+
+static ssize_t extensions_value_ext_gid_show(
+			struct config_item *item, char *page)
+{
+	return scnprintf(page, PAGE_SIZE, "%u\n",
+				from_kgid(current_user_ns(), to_extensions_value(item)->gid));
+}
+
+static ssize_t extensions_value_ext_gid_store(
+				struct config_item *item,
+				const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+
+	to_extensions_value(item)->gid = make_kgid(current_user_ns(), tmp);
+
+	return count;
+}
+
+PACKAGE_DETAILS_ATTR(extensions_value_, ext_gid);
+
+static struct configfs_attribute *extensions_value_attrs[] = {
+	EXTENSIONS_VALUE_ATTRIBUTE(ext_gid),
+	NULL,
+};
+
+static struct config_item_type extension_details_type = {
+	.ct_item_ops = &extension_details_item_ops,
+	.ct_owner = THIS_MODULE,
+};
+
+static struct config_item *extension_details_make_item(
+				struct config_group *group, const char *name)
+{
+	struct extensions_value *extensions_value =
+			to_extensions_value(&group->cg_item);
+	struct extension_details *extension_details =
+			kzalloc(sizeof(struct extension_details), GFP_KERNEL);
+	const char *tmp;
+	int ret;
+
+	if (!extension_details)
+		return ERR_PTR(-ENOMEM);
+
+	tmp = kstrdup(name, GFP_KERNEL);
+	if (!tmp) {
+		kfree(extension_details);
+		return ERR_PTR(-ENOMEM);
+	}
+	qstr_init(&extension_details->name, tmp);
+	extension_details->value = extensions_value;
+	ret = insert_ext_gid_entry(extension_details);
+
+	if (ret) {
+		kfree(extension_details->name.name);
+		kfree(extension_details);
+		return ERR_PTR(ret);
+	}
+	config_item_init_type_name(&extension_details->item, name,
+					&extension_details_type);
+
+	return &extension_details->item;
+}
+
+static struct configfs_group_operations extensions_value_group_ops = {
+	.make_item = extension_details_make_item,
+};
+
+static struct config_item_type extensions_name_type = {
+	.ct_attrs	= extensions_value_attrs,
+	.ct_group_ops	= &extensions_value_group_ops,
+	.ct_owner	= THIS_MODULE,
+};
+
+static struct config_group *extensions_make_group(struct config_group *group,
+							const char *name)
+{
+	struct extensions_value *extensions_value;
+	unsigned int tmp;
+	int ret;
+
+	extensions_value = kzalloc(sizeof(struct extensions_value), GFP_KERNEL);
+	if (!extensions_value)
+		return ERR_PTR(-ENOMEM);
+	/* For legacy reasons, if the name is a number, assume it's the gid*/
+	ret = kstrtouint(name, 10, &tmp);
+	if (!ret)
+		extensions_value->gid = make_kgid(current_user_ns(), tmp);
+
+	config_group_init_type_name(&extensions_value->group, name,
+						&extensions_name_type);
+	return &extensions_value->group;
+}
+
+static void extensions_drop_group(struct config_group *group,
+					struct config_item *item)
+{
+	struct extensions_value *value = to_extensions_value(item);
+
+	pr_debug("pkglist: No longer mapping any files to gid %d\n",
+			from_kgid(current_user_ns(), value->gid));
+	kfree(value);
+}
+
+static struct configfs_group_operations extensions_group_ops = {
+	.make_group	= extensions_make_group,
+	.drop_item	= extensions_drop_group,
+};
+
+static struct config_item_type extensions_type = {
+	.ct_group_ops	= &extensions_group_ops,
+	.ct_owner	= THIS_MODULE,
+};
+
+static struct config_group extension_group = {
+	.cg_item = {
+		.ci_namebuf = "extensions",
+		.ci_type = &extensions_type,
+	},
+};
+
+struct packages {
+	struct configfs_subsystem subsystem;
+};
+
+static inline struct packages *to_packages(struct config_item *item)
+{
+	return item ? container_of(
+			to_configfs_subsystem(to_config_group(item)),
+					struct packages, subsystem) : NULL;
+}
+
+static struct config_item *packages_make_item(struct config_group *group,
+							const char *name)
+{
+	struct package_details *package_details;
+	const char *tmp;
+
+	package_details = kzalloc(sizeof(struct package_details), GFP_KERNEL);
+	if (!package_details)
+		return ERR_PTR(-ENOMEM);
+	tmp = kstrdup(name, GFP_KERNEL);
+	if (!tmp) {
+		kfree(package_details);
+		return ERR_PTR(-ENOMEM);
+	}
+	qstr_init(&package_details->name, tmp);
+	config_item_init_type_name(&package_details->item, name,
+						&package_appid_type);
+
+	return &package_details->item;
+}
+
+static ssize_t packages_list_show(struct config_item *item, char *page)
+{
+	struct hashtable_entry *hash_cur_app;
+	struct hashtable_entry *hash_cur_user;
+	int i;
+	int count = 0, written = 0;
+	const char errormsg[] = "<truncated>\n";
+	unsigned int hash;
+
+	rcu_read_lock();
+	hash_for_each_rcu(package_to_appid, i, hash_cur_app, hlist) {
+		written = scnprintf(page + count,
+				    PAGE_SIZE - sizeof(errormsg) - count,
+				    "%s %d\n",
+				    hash_cur_app->key.name,
+				    atomic_read(&hash_cur_app->value));
+		hash = hash_cur_app->key.hash;
+		hash_for_each_possible_rcu(package_to_userid, hash_cur_user, hlist, hash) {
+			if (qstr_case_eq(&hash_cur_app->key, &hash_cur_user->key)) {
+				written += scnprintf(page + count + written - 1,
+					PAGE_SIZE - sizeof(errormsg) - count - written + 1,
+					" %d\n", atomic_read(&hash_cur_user->value)) - 1;
+			}
+		}
+		if (count + written == PAGE_SIZE - sizeof(errormsg) - 1) {
+			count += scnprintf(page + count, PAGE_SIZE - count, errormsg);
+			break;
+		}
+		count += written;
+	}
+	rcu_read_unlock();
+
+	return count;
+}
+
+static ssize_t packages_remove_userid_store(struct config_item *item,
+					    const char *page, size_t count)
+{
+	unsigned int tmp;
+	int ret;
+
+	ret = kstrtouint(page, 10, &tmp);
+	if (ret)
+		return ret;
+	remove_userid_all_entry(tmp);
+	return count;
+}
+
+static struct configfs_attribute packages_attr_packages_gid_list = {
+    .ca_name	= "packages_gid.list",
+    .ca_mode	= S_IRUGO,
+    .ca_owner	= THIS_MODULE,
+    .show	= packages_list_show,
+};
+PACKAGE_DETAILS_ATTR_WO(packages_, remove_userid);
+
+static struct configfs_attribute *packages_attrs[] = {
+	&packages_attr_packages_gid_list,
+	&packages_attr_remove_userid,
+	NULL,
+};
+
+/*
+ * Note that, since no extra work is required on ->drop_item(),
+ * no ->drop_item() is provided.
+ */
+static struct configfs_group_operations packages_group_ops = {
+	.make_item	= packages_make_item,
+};
+
+static struct config_item_type packages_type = {
+	.ct_group_ops	= &packages_group_ops,
+	.ct_attrs	= packages_attrs,
+	.ct_owner	= THIS_MODULE,
+};
+
+static struct config_group *sd_default_groups[] = {
+	&extension_group,
+	NULL,
+};
+
+static struct packages pkglist_packages = {
+	.subsystem = {
+		.su_group = {
+			.cg_item = {
+				.ci_type = &packages_type,
+			},
+		},
+	},
+};
+
+static int configfs_pkglist_init(void)
+{
+	int ret, i;
+	struct configfs_subsystem *subsys = &pkglist_packages.subsystem;
+	config_item_set_name(&pkglist_packages.subsystem.su_group.cg_item,
+						pkglist_config_location);
+	config_group_init(&subsys->su_group);
+
+	for (i = 0; sd_default_groups[i]; i++) {
+		config_group_init(sd_default_groups[i]);
+		configfs_add_default_group(sd_default_groups[i], &subsys->su_group);
+	}
+	mutex_init(&subsys->su_mutex);
+	ret = configfs_register_subsystem(subsys);
+	if (ret) {
+		pr_err("Error %d while registering subsystem %s\n", ret,
+				subsys->su_group.cg_item.ci_namebuf);
+	}
+	return ret;
+}
+
+static void configfs_pkglist_exit(void)
+{
+	configfs_unregister_subsystem(&pkglist_packages.subsystem);
+}
+
+void pkglist_register_update_listener(struct pkg_list *pkg)
+{
+	if (!pkg->update)
+		return;
+	mutex_lock(&pkg_list_lock);
+	list_add(&pkg->list, &pkglist_listeners);
+	mutex_unlock(&pkg_list_lock);
+}
+EXPORT_SYMBOL_GPL(pkglist_register_update_listener);
+
+void pkglist_unregister_update_listener(struct pkg_list *pkg)
+{
+	mutex_lock(&pkg_list_lock);
+	list_del(&pkg->list);
+	mutex_unlock(&pkg_list_lock);
+}
+EXPORT_SYMBOL_GPL(pkglist_unregister_update_listener);
+
+static int __init pkglist_init(void)
+{
+	hashtable_entry_cachep =
+		kmem_cache_create("packagelist_hashtable_entry",
+				sizeof(struct hashtable_entry), 0, 0, NULL);
+	if (!hashtable_entry_cachep) {
+		pr_err("pkglist: failed creating pkgl_hashtable entry slab cache\n");
+		return -ENOMEM;
+	}
+
+	return configfs_pkglist_init();
+}
+module_init(pkglist_init);
+
+static void __exit pkglist_exit(void)
+{
+	configfs_pkglist_exit();
+	packagelist_destroy();
+	kmem_cache_destroy(hashtable_entry_cachep);
+}
+
+module_exit(pkglist_exit);
+
+MODULE_AUTHOR("Daniel Rosenberg, Google");
+MODULE_DESCRIPTION("Configfs Pkglist implementation");
+MODULE_LICENSE("GPL v2");
diff -ruN a/drivers/pkglist/pkglist_none.c b/drivers/pkglist/pkglist_none.c
--- a/drivers/pkglist/pkglist_none.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/pkglist/pkglist_none.c	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,57 @@
+/*
+ * Copyright (C) 2017 Google Inc., Author: Daniel Rosenberg <drosen@google.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/ctype.h>
+#include <linux/dcache.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/pkglist.h>
+
+kuid_t pkglist_get_appid(const char *key)
+{
+	return make_kuid(&init_user_ns, 0);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_appid);
+
+kgid_t pkglist_get_ext_gid(const char *key)
+{
+	return make_kgid(&init_user_ns, 0);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_ext_gid);
+
+bool pkglist_user_is_excluded(const char *key, uint32_t user)
+{
+	return false;
+}
+EXPORT_SYMBOL_GPL(pkglist_user_is_excluded);
+
+kuid_t pkglist_get_allowed_appid(const char *key, uint32_t user)
+{
+	return make_kuid(&init_user_ns, 0);
+}
+EXPORT_SYMBOL_GPL(pkglist_get_allowed_appid);
+
+void pkglist_register_update_listener(struct pkg_list *pkg) { }
+EXPORT_SYMBOL_GPL(pkglist_register_update_listener);
+
+void pkglist_unregister_update_listener(struct pkg_list *pkg) { }
+EXPORT_SYMBOL_GPL(pkglist_unregister_update_listener);
+
+static int __init pkglist_init(void)
+{
+	return 0;
+}
+module_init(pkglist_init);
+
+static void pkglist_exit(void) { }
+
+module_exit(pkglist_exit);
+
+MODULE_AUTHOR("Daniel Rosenberg, Google");
+MODULE_DESCRIPTION("Empty Pkglist implementation");
+MODULE_LICENSE("GPL v2");
diff -ruN a/drivers/platform/chrome/chromeos.c b/drivers/platform/chrome/chromeos.c
--- a/drivers/platform/chrome/chromeos.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/platform/chrome/chromeos.c	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,120 @@
+/*
+ *  ChromeOS platform support code. Glue layer between higher level functions
+ *  and per-platform firmware interfaces.
+ *
+ *  Copyright (C) 2010 The Chromium OS Authors
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ */
+
+#include <linux/types.h>
+#include <linux/chromeos_platform.h>
+#include <linux/module.h>
+#include "chromeos.h"
+
+static struct chromeos_vbc *chromeos_vbc_ptr;
+
+static int vbc_read(u8 *buf, int buf_size);
+static int vbc_write_byte(unsigned offset, u8 value);
+
+/* the following defines are copied from
+ * vboot_reference:firmware/lib/vboot_nvstorage.c.
+ */
+#define RECOVERY_OFFSET              2
+#define VBNV_RECOVERY_RW_INVALID_OS  0x43
+
+int chromeos_set_need_recovery(void)
+{
+	if (!chromeos_legacy_set_need_recovery())
+		return 0;
+
+	return vbc_write_byte(RECOVERY_OFFSET, VBNV_RECOVERY_RW_INVALID_OS);
+}
+EXPORT_SYMBOL(chromeos_set_need_recovery);
+
+/*
+ * Lifted from vboot_reference:firmware/lib/vboot_nvstorage.c and formatted.
+ *
+ * Return CRC-8 of the data, using x^8 + x^2 + x + 1 polynomial. A table-based
+ * algorithm would be faster, but for only 15 bytes isn't worth the code size.
+ */
+static u8 crc8(const u8 *data, int len)
+{
+	unsigned crc = 0;
+	int i, j;
+
+	for (j = len; j; j--, data++) {
+		crc ^= (*data << 8);
+		for (i = 8; i; i--) {
+			if (crc & 0x8000)
+				crc ^= (0x1070 << 3);
+			crc <<= 1;
+		}
+	}
+	return (u8)(crc >> 8);
+}
+
+static int vbc_write_byte(unsigned offset, u8 value)
+{
+	u8 buf[MAX_VBOOT_CONTEXT_BUFFER_SIZE];
+	ssize_t size;
+
+	if (!chromeos_vbc_ptr)
+		return -ENOSYS;
+
+	size = vbc_read(buf, sizeof(buf));
+	if (size <= 0)
+		return -EINVAL;
+
+	if (offset >= (size - 1))
+		return -EINVAL;
+
+	if (buf[offset] == value)
+		return 0;
+
+	buf[offset] = value;
+	buf[size - 1] = crc8(buf, size - 1);
+
+	return chromeos_vbc_ptr->write(buf, size);
+}
+
+/*
+ * Read vboot context and verify it.  If everything checks out, return number
+ * of bytes in the vboot context buffer, -1 on any error (uninitialized
+ * subsystem, corrupted crc8 value, not enough room in the buffer, etc.).
+ */
+static int vbc_read(u8 *buf, int buf_size)
+{
+	ssize_t size;
+
+	if (!chromeos_vbc_ptr)
+		return -ENOSYS;
+
+	size = chromeos_vbc_ptr->read(buf, buf_size);
+	if (size <= 0)
+		return -1;
+
+	if (buf[size - 1] != crc8(buf, size - 1)) {
+		pr_err("%s: vboot context contents corrupted\n", __func__);
+		return -1;
+	}
+	return size;
+}
+
+int chromeos_vbc_register(struct chromeos_vbc *chromeos_vbc)
+{
+	chromeos_vbc_ptr = chromeos_vbc;
+	return 0;
+}
diff -ruN a/drivers/platform/chrome/chromeos.h b/drivers/platform/chrome/chromeos.h
--- a/drivers/platform/chrome/chromeos.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/platform/chrome/chromeos.h	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,61 @@
+/*
+ *  Copyright (C) 2011 The Chromium OS Authors
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ */
+#ifndef _DRIVERS_PLATFORM_CHROMEOS_H
+#define _DRIVERS_PLATFORM_CHROMEOS_H
+
+#define MAX_VBOOT_CONTEXT_BUFFER_SIZE 64  /* Should be enough for anything. */
+
+#ifdef CONFIG_ACPI_CHROMEOS
+extern int chromeos_legacy_set_need_recovery(void);
+#else
+static inline int chromeos_legacy_set_need_recovery(void) { return -ENODEV; }
+#endif
+
+struct chromeos_vbc {
+	/**
+	 * Read vboot context to buffer
+	 *
+	 * @param buf		Pointer to buffer for storing vboot context
+	 * @param count		Size of buffer
+	 * @return	on success, the number of bytes read is returned and
+	 *		on error, -err is returned.
+	 */
+	ssize_t (*read)(void *buf, size_t count);
+
+	/**
+	 * Write vboot context from buffer
+	 *
+	 * @param buf		Pointer to buffer of new vboot context content
+	 * @param count		Size of buffer
+	 * @return	on success, the number of bytes written is returned and
+	 *		on error, -err is returned.
+	 */
+	ssize_t (*write)(const void *buf, size_t count);
+
+	const char *name;
+};
+
+/**
+ * Register chromeos_vbc callbacks.
+ *
+ * @param chromeos_vbc	Pointer to struct holding callbacks
+ * @return	on success, return 0, on error, -err is returned.
+ */
+int chromeos_vbc_register(struct chromeos_vbc *chromeos_vbc);
+
+#endif /* _DRIVERS_PLATFORM_CHROMEOS_H */
diff -ruN a/drivers/platform/chrome/cros_ec_lpc.c b/drivers/platform/chrome/cros_ec_lpc.c
--- a/drivers/platform/chrome/cros_ec_lpc.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/chrome/cros_ec_lpc.c	2021-12-23 08:35:44.000000000 +0100
@@ -46,6 +46,8 @@
 
 static struct lpc_driver_ops cros_ec_lpc_ops = { };
 
+static struct platform_device *pdev_extcon;
+
 /*
  * A generic instance of the read function of struct lpc_driver_ops, used for
  * the LPC EC.
@@ -426,6 +428,13 @@
 				 status);
 	}
 
+	/* Revert this after we introduce Type C connector class driver. */
+	if (dmi_match(DMI_PRODUCT_FAMILY, "Google_Volteer") ||
+	    dmi_match(DMI_PRODUCT_NAME, "tglrvp"))
+		pdev_extcon = platform_device_register_data(dev,
+					"extcon-tcss-cros-ec",
+					PLATFORM_DEVID_NONE, NULL, 0);
+
 	return 0;
 }
 
@@ -434,6 +443,8 @@
 	struct cros_ec_device *ec_dev = platform_get_drvdata(pdev);
 	struct acpi_device *adev;
 
+	platform_device_unregister(pdev_extcon);
+
 	adev = ACPI_COMPANION(&pdev->dev);
 	if (adev)
 		acpi_remove_notify_handler(adev->handle, ACPI_ALL_NOTIFY,
diff -ruN a/drivers/platform/chrome/cros_ec_pd_sysfs.c b/drivers/platform/chrome/cros_ec_pd_sysfs.c
--- a/drivers/platform/chrome/cros_ec_pd_sysfs.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/platform/chrome/cros_ec_pd_sysfs.c	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,130 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * cros_ec_pd_sysfs - expose the Chrome OS EC PD update through sysfs
+ *
+ * Copyright 2019 Google, Inc.
+ */
+
+#include <linux/ctype.h>
+#include <linux/device.h>
+#include <linux/fs.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/platform_data/cros_ec_pd_update.h>
+#include <linux/platform_device.h>
+#include <linux/printk.h>
+#include <linux/stat.h>
+#include <linux/types.h>
+#include <linux/uaccess.h>
+
+/*
+ * Driver loaded on top of the EC object.
+ *
+ * It exposes a sysfs interface, but most importantly, set global cros_ec_pd_ec
+ * to let the real driver knows which cros_ec_pd_ec device to talk to.
+ */
+#define DRV_NAME "cros-ec-pd-sysfs"
+
+
+static umode_t cros_ec_pd_attrs_are_visible(struct kobject *kobj,
+					    struct attribute *a, int n)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct cros_ec_dev *ec = container_of(dev, struct cros_ec_dev,
+					      class_dev);
+	struct ec_params_usb_pd_rw_hash_entry hash_entry;
+	struct ec_params_usb_pd_discovery_entry discovery_entry;
+
+	/* Check if a PD MCU is present */
+	if (cros_ec_pd_get_status(dev,
+				  ec,
+				  0,
+				  &hash_entry,
+				  &discovery_entry) == EC_RES_SUCCESS) {
+		/*
+		 * Save our ec pointer so we can conduct transactions.
+		 * TODO(shawnn): Find a better way to access the ec pointer.
+		 */
+		if (!cros_ec_pd_ec)
+			cros_ec_pd_ec = ec;
+		return a->mode;
+	}
+
+	return 0;
+}
+
+static ssize_t firmware_images_show(struct device *dev,
+				    struct device_attribute *attr, char *buf)
+{
+	int size = 0;
+	int i;
+
+	for (i = 0; cros_ec_pd_firmware_images[i].rw_image_size > 0; i++) {
+		if (cros_ec_pd_firmware_images[i].filename == NULL)
+			size += scnprintf(
+				buf + size, PAGE_SIZE,
+				"%d: %d.%d NONE\n", i,
+				cros_ec_pd_firmware_images[i].id_major,
+				cros_ec_pd_firmware_images[i].id_minor);
+		else
+			size += scnprintf(
+				buf + size, PAGE_SIZE,
+				"%d: %d.%d %s\n", i,
+				cros_ec_pd_firmware_images[i].id_major,
+				cros_ec_pd_firmware_images[i].id_minor,
+				cros_ec_pd_firmware_images[i].filename);
+	}
+
+	return size;
+}
+
+static DEVICE_ATTR_RO(firmware_images);
+
+static struct attribute *__pd_attrs[] = {
+	&dev_attr_firmware_images.attr,
+	NULL,
+};
+
+static struct attribute_group cros_ec_pd_attr_group = {
+	.name = "pd_update",
+	.attrs = __pd_attrs,
+	.is_visible = cros_ec_pd_attrs_are_visible,
+};
+
+
+static int cros_ec_pd_sysfs_probe(struct platform_device *pd)
+{
+	struct cros_ec_dev *ec_dev = dev_get_drvdata(pd->dev.parent);
+	struct device *dev = &pd->dev;
+	int ret;
+
+	ret = sysfs_create_group(&ec_dev->class_dev.kobj,
+			&cros_ec_pd_attr_group);
+	if (ret < 0)
+		dev_err(dev, "failed to create attributes. err=%d\n", ret);
+
+	return ret;
+}
+
+static int cros_ec_pd_sysfs_remove(struct platform_device *pd)
+{
+	struct cros_ec_dev *ec_dev = dev_get_drvdata(pd->dev.parent);
+
+	sysfs_remove_group(&ec_dev->class_dev.kobj, &cros_ec_pd_attr_group);
+
+	return 0;
+}
+
+static struct platform_driver cros_ec_pd_sysfs_driver = {
+	.driver = {
+		.name = DRV_NAME,
+	},
+	.probe = cros_ec_pd_sysfs_probe,
+	.remove = cros_ec_pd_sysfs_remove,
+};
+
+module_platform_driver(cros_ec_pd_sysfs_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("ChromeOS EC PD update sysfs driver");
+MODULE_ALIAS("platform:" DRV_NAME);
diff -ruN a/drivers/platform/chrome/cros_ec_pd_update.c b/drivers/platform/chrome/cros_ec_pd_update.c
--- a/drivers/platform/chrome/cros_ec_pd_update.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/platform/chrome/cros_ec_pd_update.c	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,921 @@
+/*
+ * cros_ec_pd_update - Chrome OS EC Power Delivery Device FW Update Driver
+ *
+ * Copyright (C) 2014 Google, Inc
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * This driver communicates with a Chrome OS PD device and performs tasks
+ * related to auto-updating its firmware.
+ */
+
+#include <linux/delay.h>
+#include <linux/firmware.h>
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/platform_data/cros_ec_commands.h>
+#include <linux/platform_data/cros_ec_pd_update.h>
+#include <linux/platform_data/cros_ec_proto.h>
+#include <linux/platform_data/cros_usbpd_notify.h>
+#include <linux/platform_device.h>
+#include <linux/power_supply.h>
+
+/*
+ * Driver loaded when a Chrome OS PD device is found.
+ */
+#define DRV_NAME "cros-ec-pd-update"
+
+struct cros_ec_dev *cros_ec_pd_ec;
+EXPORT_SYMBOL_GPL(cros_ec_pd_ec);
+
+/* Allow disabling of the update for testing purposes */
+static int disable;
+
+/*
+ * $DEVICE_known_update_hashes - A list of old known RW hashes from which we
+ * wish to upgrade. When cros_ec_pd_firmware_images is updated, the old hash
+ * should probably be added here. The latest hash currently in
+ * cros_ec_pd_firmware_images should NOT appear here.
+ */
+static uint8_t zinger_known_update_hashes[][PD_RW_HASH_SIZE] = {
+	/* zinger_v1.7.509-e5bffd3.bin */
+	{ 0x02, 0xad, 0x4c, 0x95, 0x25,
+	  0x89, 0xe5, 0xe7, 0x1e, 0xc6,
+	  0xaf, 0x9c, 0x0e, 0xaa, 0xbb,
+	  0x6c, 0xa7, 0x52, 0x8c, 0x3a },
+	/* zinger_v1.7.262-9a5b8f4.bin */
+	{ 0x05, 0x94, 0xb8, 0x97, 0x8a,
+	  0x9a, 0xa0, 0x0a, 0x71, 0x07,
+	  0x37, 0xba, 0x8f, 0x4c, 0x01,
+	  0xe6, 0x45, 0x6d, 0xb0, 0x01 },
+};
+
+static uint8_t dingdong_known_update_hashes[][PD_RW_HASH_SIZE] = {
+	/* dingdong_v1.7.575-96b74f1.bin devid: 3.2 */
+	{ 0x64, 0xdb, 0x4e, 0x86, 0xd6,
+	  0x7d, 0x7a, 0xce, 0x41, 0xfd,
+	  0x09, 0x3b, 0xd4, 0x8b, 0x3f,
+	  0x1f, 0xba, 0x73, 0xcb, 0x73 },
+	/* dingdong_v1.7.489-8533e9d.bin devid: 3.2 */
+	{ 0x53, 0x20, 0x21, 0x34, 0xc2,
+	  0xee, 0x2f, 0x07, 0xbb, 0x24,
+	  0x94, 0xab, 0xbe, 0x1f, 0xee,
+	  0xf2, 0xb3, 0x7e, 0xff, 0x23 },
+	/* dingdong_v1.7.317-b0bb7c9.bin devid: 3.1 */
+	{ 0x0f, 0x1e, 0x93, 0x9f, 0xbc,
+	  0x23, 0x0a, 0x3f, 0x4f, 0x35,
+	  0xf8, 0xfe, 0xd8, 0xa9, 0x71,
+	  0x8f, 0xef, 0x15, 0xc8, 0xea },
+};
+
+static uint8_t hoho_known_update_hashes[][PD_RW_HASH_SIZE] = {
+	/* hoho_v1.7.575-96b74f1.bin devid: 4.2 */
+	{ 0x4b, 0x3d, 0x8b, 0xba, 0x8a,
+	  0x62, 0xae, 0x4f, 0x64, 0xd2,
+	  0x0f, 0x96, 0xf9, 0x4e, 0xc7,
+	  0xf6, 0x6a, 0x19, 0x84, 0x1c },
+	/* hoho_v1.7.489-8533e9d.bin devid: 4.2 */
+	{ 0xac, 0x00, 0xc1, 0x4c, 0x3a,
+	  0x77, 0xa6, 0x1f, 0xf9, 0xd5,
+	  0x59, 0x3a, 0x56, 0x06, 0x5c,
+	  0x86, 0x09, 0xe0, 0x03, 0xb3 },
+	/* hoho_v1.7.317-b0bb7c9.bin devid:4.1 */
+	{ 0x98, 0x19, 0xa6, 0x6b, 0x61,
+	  0x1f, 0x28, 0xba, 0xde, 0x80,
+	  0xa3, 0x88, 0x95, 0x67, 0x57,
+	  0xa2, 0x98, 0xe4, 0xf1, 0x62 },
+};
+
+const struct cros_ec_pd_firmware_image cros_ec_pd_firmware_images[] = {
+	/* PD_DEVICE_TYPE_ZINGER */
+	{
+		.id_major = PD_DEVICE_TYPE_ZINGER,
+		.id_minor = 1,
+		.usb_vid = USB_VID_GOOGLE,
+		.usb_pid = USB_PID_ZINGER,
+		.filename = "cros-pd/zinger_v1.7.539-91a0fa2.bin",
+		.rw_image_size = (16 * 1024),
+		.hash = { 0x3b, 0x2e, 0xe3, 0xf6, 0x1e,
+			  0x6a, 0x1d, 0x49, 0xd3, 0x1c,
+			  0xf5, 0x77, 0x5e, 0xa7, 0x19,
+			  0xdb, 0xde, 0xcd, 0xaa, 0xc2 },
+		.update_hashes = &zinger_known_update_hashes,
+		.update_hash_count = ARRAY_SIZE(zinger_known_update_hashes),
+	},
+	{
+		.id_major = PD_DEVICE_TYPE_DINGDONG,
+		.id_minor = 2,
+		.usb_vid = USB_VID_GOOGLE,
+		.usb_pid = USB_PID_DINGDONG,
+		.filename = "cros-pd/dingdong_v1.7.684-69498dd.bin",
+		.rw_image_size = (64 * 1024),
+		.hash = { 0xe6, 0x97, 0x90, 0xd9, 0xe5,
+			  0x01, 0x15, 0x22, 0xee, 0x1c,
+			  0x7e, 0x4d, 0x6c, 0x54, 0x78,
+			  0xd4, 0x7a, 0xa7, 0xda, 0x1d },
+		.update_hashes = &dingdong_known_update_hashes,
+		.update_hash_count = ARRAY_SIZE(dingdong_known_update_hashes),
+	},
+	{
+		.id_major = PD_DEVICE_TYPE_DINGDONG,
+		.id_minor = 1,
+		.usb_vid = USB_VID_GOOGLE,
+		.usb_pid = USB_PID_DINGDONG,
+		.filename = "cros-pd/dingdong_v1.7.684-69498dd.bin",
+		.rw_image_size = (64 * 1024),
+		.hash = { 0xe6, 0x97, 0x90, 0xd9, 0xe5,
+			  0x01, 0x15, 0x22, 0xee, 0x1c,
+			  0x7e, 0x4d, 0x6c, 0x54, 0x78,
+			  0xd4, 0x7a, 0xa7, 0xda, 0x1d },
+		.update_hashes = &dingdong_known_update_hashes,
+		.update_hash_count = ARRAY_SIZE(dingdong_known_update_hashes),
+	},
+	{
+		.id_major = PD_DEVICE_TYPE_HOHO,
+		.id_minor = 2,
+		.usb_vid = USB_VID_GOOGLE,
+		.usb_pid = USB_PID_HOHO,
+		.filename = "cros-pd/hoho_v1.7.684-69498dd.bin",
+		.rw_image_size = (64 * 1024),
+		.hash = { 0x43, 0x1b, 0x4e, 0x20, 0xe8,
+			  0x38, 0xdd, 0x29, 0x42, 0xbd,
+			  0x6d, 0xfc, 0x13, 0xf2, 0xb2,
+			  0x46, 0xa6, 0xf4, 0x98, 0x08 },
+		.update_hashes = &hoho_known_update_hashes,
+		.update_hash_count = ARRAY_SIZE(hoho_known_update_hashes),
+	},
+	{
+		.id_major = PD_DEVICE_TYPE_HOHO,
+		.id_minor = 1,
+		.usb_vid = USB_VID_GOOGLE,
+		.usb_pid = USB_PID_HOHO,
+		.filename = "cros-pd/hoho_v1.7.684-69498dd.bin",
+		.rw_image_size = (64 * 1024),
+		.hash = { 0x43, 0x1b, 0x4e, 0x20, 0xe8,
+			  0x38, 0xdd, 0x29, 0x42, 0xbd,
+			  0x6d, 0xfc, 0x13, 0xf2, 0xb2,
+			  0x46, 0xa6, 0xf4, 0x98, 0x08 },
+		.update_hashes = &hoho_known_update_hashes,
+		.update_hash_count = ARRAY_SIZE(hoho_known_update_hashes),
+	},
+	{
+		/* Empty image for termination. */
+	},
+};
+EXPORT_SYMBOL_GPL(cros_ec_pd_firmware_images);
+
+/**
+ * cros_ec_pd_command - Send a command to the EC. Returns 0 on success,
+ * <0 on failure.
+ *
+ * @dev: PD device
+ * @pd_dev: EC PD device
+ * @command: EC command
+ * @outdata: EC command output data
+ * @outsize: Size of outdata
+ * @indata: EC command input data
+ * @insize: Size of indata
+ */
+static int cros_ec_pd_command(struct device *dev,
+			      struct cros_ec_dev *pd_dev,
+			      int command,
+			      uint8_t *outdata,
+			      int outsize,
+			      uint8_t *indata,
+			      int insize)
+{
+	int ret;
+	struct cros_ec_command *msg;
+
+	msg = kzalloc(sizeof(*msg) + max(insize, outsize), GFP_KERNEL);
+	if (!msg)
+		return -EC_RES_ERROR;
+
+	msg->command = command | pd_dev->cmd_offset;
+	msg->outsize = outsize;
+	msg->insize = insize;
+
+	if (outsize)
+		memcpy(msg->data, outdata, outsize);
+
+	ret = cros_ec_cmd_xfer_status(pd_dev->ec_dev, msg);
+	if (ret < 0)
+		goto error;
+
+	if (insize)
+		memcpy(indata, msg->data, insize);
+	ret = EC_RES_SUCCESS;
+error:
+	kfree(msg);
+	return ret;
+}
+
+/**
+ * cros_ec_pd_enter_gfu - Enter GFU alternate mode.
+ * Returns 0 if ec command successful <0 on failure.
+ *
+ * Note, doesn't guarantee entry.
+ *
+ * @dev: PD device
+ * @pd_dev: EC PD device
+ * @port: Port # on device
+ */
+static int cros_ec_pd_enter_gfu(struct device *dev, struct cros_ec_dev *pd_dev,
+				int port)
+{
+	int rv;
+	struct ec_params_usb_pd_set_mode_request set_mode_request;
+
+	set_mode_request.port = port;
+	set_mode_request.svid = USB_VID_GOOGLE;
+	/* TODO(tbroch) Will GFU always be '1'? */
+	set_mode_request.opos = 1;
+	set_mode_request.cmd = PD_ENTER_MODE;
+	rv = cros_ec_pd_command(dev, pd_dev, EC_CMD_USB_PD_SET_AMODE,
+				(uint8_t *)&set_mode_request,
+				sizeof(set_mode_request),
+				NULL, 0);
+	if (!rv)
+		/* Allow time to enter GFU mode */
+		msleep(500);
+
+	return rv;
+}
+
+int cros_ec_pd_get_status(
+		struct device *dev,
+		struct cros_ec_dev *pd_dev,
+		int port,
+		struct ec_params_usb_pd_rw_hash_entry *hash_entry,
+		struct ec_params_usb_pd_discovery_entry *discovery_entry)
+{
+	struct ec_params_usb_pd_info_request info_request;
+	int ret;
+
+	info_request.port = port;
+	ret = cros_ec_pd_command(dev, pd_dev, EC_CMD_USB_PD_DEV_INFO,
+				 (uint8_t *)&info_request, sizeof(info_request),
+				 (uint8_t *)hash_entry, sizeof(*hash_entry));
+	/* Skip getting USB discovery data if no device present on port */
+	if (ret < 0 || hash_entry->dev_id == PD_DEVICE_TYPE_NONE)
+		return ret;
+
+	return cros_ec_pd_command(dev, pd_dev, EC_CMD_USB_PD_DISCOVERY,
+				  (uint8_t *)&info_request,
+				  sizeof(info_request),
+				  (uint8_t *)discovery_entry,
+				  sizeof(*discovery_entry));
+}
+EXPORT_SYMBOL_GPL(cros_ec_pd_get_status);
+
+/**
+ * cros_ec_pd_send_hash_entry - Inform the EC of a PD devices for which we
+ * have firmware available. EC typically will not store more than four hashes.
+ * Returns 0 on success, <0 on failure.
+ *
+ * @dev: PD device
+ * @pd_dev: EC PD device
+ * @fw: FW update image to inform the EC of
+ */
+static int cros_ec_pd_send_hash_entry(struct device *dev,
+				      struct cros_ec_dev *pd_dev,
+				      const struct cros_ec_pd_firmware_image
+						   *fw)
+{
+	struct ec_params_usb_pd_rw_hash_entry hash_entry;
+
+	hash_entry.dev_id = MAJOR_MINOR_TO_DEV_ID(fw->id_major, fw->id_minor);
+	memcpy(hash_entry.dev_rw_hash, fw->hash, PD_RW_HASH_SIZE);
+
+	return cros_ec_pd_command(dev, pd_dev, EC_CMD_USB_PD_RW_HASH_ENTRY,
+				  (uint8_t *)&hash_entry, sizeof(hash_entry),
+				  NULL, 0);
+}
+
+/**
+ * cros_ec_pd_send_fw_update_cmd - Send update-related EC command.
+ * Returns 0 on success, <0 on failure.
+ *
+ * @dev: PD device
+ * @pd_dev: EC PD device
+ * @pd_cmd: fw_update command
+ */
+static int cros_ec_pd_send_fw_update_cmd(struct device *dev,
+					 struct cros_ec_dev *pd_dev,
+					 struct ec_params_usb_pd_fw_update
+						*pd_cmd)
+{
+	return cros_ec_pd_command(dev, pd_dev, EC_CMD_USB_PD_FW_UPDATE,
+				  (uint8_t *)pd_cmd,
+				  pd_cmd->size + sizeof(*pd_cmd),
+				  NULL, 0);
+}
+
+/**
+ * cros_ec_pd_get_num_ports - Get number of EC charge ports.
+ * Returns 0 on success, <0 on failure.
+ *
+ * @dev: PD device
+ * @pd_dev: EC PD device
+ * @num_ports: Holds number of ports, on command success
+ */
+static int cros_ec_pd_get_num_ports(struct device *dev,
+				    struct cros_ec_dev *pd_dev,
+				    int *num_ports)
+{
+	struct ec_response_usb_pd_ports resp;
+	int ret;
+
+	ret = cros_ec_pd_command(dev, pd_dev, EC_CMD_USB_PD_PORTS,
+				 NULL, 0,
+				 (uint8_t *)&resp, sizeof(resp));
+	if (ret == EC_RES_SUCCESS)
+		*num_ports = resp.num_ports;
+	return ret;
+}
+
+
+/**
+ * cros_ec_pd_fw_update - Send EC_CMD_USB_PD_FW_UPDATE command to perform
+ * update-related operation.
+ * Returns 0 on success, <0 on failure.
+ *
+ * @dev: PD device
+ * @pd_dev: EC PD device
+ * @fw: RW FW update file
+ * @port: Port# to which update device is attached
+ */
+static int cros_ec_pd_fw_update(struct cros_ec_pd_update_data *drv_data,
+				struct cros_ec_dev *pd_dev,
+				const struct firmware *fw,
+				uint8_t port)
+{
+	uint8_t cmd_buf[sizeof(struct ec_params_usb_pd_fw_update) +
+			PD_FLASH_WRITE_STEP];
+	struct ec_params_usb_pd_fw_update *pd_cmd =
+		(struct ec_params_usb_pd_fw_update *)cmd_buf;
+	uint8_t *pd_cmd_data = cmd_buf + sizeof(*pd_cmd);
+	struct device *dev = drv_data->dev;
+	int i, ret;
+
+	if (drv_data->is_suspending)
+		return -EBUSY;
+
+	/* Common port */
+	pd_cmd->port = port;
+
+	/* Erase signature */
+	pd_cmd->cmd = USB_PD_FW_ERASE_SIG;
+	pd_cmd->size = 0;
+	ret = cros_ec_pd_send_fw_update_cmd(dev, pd_dev, pd_cmd);
+	if (ret < 0) {
+		dev_err(dev,
+			"Unable to clear Port%d PD signature (err:%d)\n",
+			port, ret);
+		return ret;
+	}
+
+	/* Reboot PD */
+	pd_cmd->cmd = USB_PD_FW_REBOOT;
+	pd_cmd->size = 0;
+	ret = cros_ec_pd_send_fw_update_cmd(dev, pd_dev, pd_cmd);
+	if (ret < 0) {
+		dev_err(dev, "Unable to reboot Port%d PD (err:%d)\n",
+			port, ret);
+		return ret;
+	}
+
+	/*
+	 * Wait for the charger to reboot.
+	 * TODO(shawnn): Instead of waiting for a fixed period of time, wait
+	 * to receive an interrupt that signals the charger is back online.
+	 */
+	msleep(4000);
+
+	if (drv_data->is_suspending)
+		return -EBUSY;
+
+	/*
+	 * Force re-entry into GFU mode for USBPD devices that don't enter
+	 * it by default.
+	 */
+	ret = cros_ec_pd_enter_gfu(dev, pd_dev, port);
+	if (ret < 0)
+		dev_warn(dev, "Unable to enter GFU (err:%d)\n", ret);
+
+	/* Erase RW flash */
+	pd_cmd->cmd = USB_PD_FW_FLASH_ERASE;
+	pd_cmd->size = 0;
+	ret = cros_ec_pd_send_fw_update_cmd(dev, pd_dev, pd_cmd);
+	if (ret < 0) {
+		dev_err(dev, "Unable to erase Port%d PD RW flash (err:%d)\n",
+			port, ret);
+		return ret;
+	}
+
+	/* Wait 3 seconds for the PD peripheral to finalize RW erase */
+	msleep(3000);
+
+	/* Write RW flash */
+	pd_cmd->cmd = USB_PD_FW_FLASH_WRITE;
+	for (i = 0; i < fw->size; i += PD_FLASH_WRITE_STEP) {
+		if (drv_data->is_suspending)
+			return -EBUSY;
+		pd_cmd->size = min(fw->size - i, (size_t)PD_FLASH_WRITE_STEP);
+		memcpy(pd_cmd_data, fw->data + i, pd_cmd->size);
+		ret = cros_ec_pd_send_fw_update_cmd(dev, pd_dev, pd_cmd);
+		if (ret < 0) {
+			dev_err(dev,
+				"Unable to write Port%d PD RW flash (err:%d)\n",
+				port, ret);
+			return ret;
+		}
+	}
+
+	/* Wait 100ms to guarantee that writes finish */
+	msleep(100);
+
+	/* Reboot PD into new RW */
+	pd_cmd->cmd = USB_PD_FW_REBOOT;
+	pd_cmd->size = 0;
+	ret = cros_ec_pd_send_fw_update_cmd(dev, pd_dev, pd_cmd);
+	if (ret < 0) {
+		dev_err(dev,
+			"Unable to reboot Port%d PD post-flash (err:%d)\n",
+			port, ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+/**
+ * cros_ec_find_update_firmware - Search firmware image table for an image
+ * matching the passed attributes, then decide whether an update should
+ * be performed.
+ * Returns PD_DO_UPDATE if an update should be performed, and writes the
+ * cros_ec_pd_firmware_image pointer to update_image.
+ * Returns reason for not updating otherwise.
+ *
+ * @dev: PD device
+ * @hash_entry: Pre-filled hash entry struct for matching
+ * @discovery_entry: Pre-filled discovery entry struct for matching
+ * @update_image: Stores update firmware image on success
+ */
+static enum cros_ec_pd_find_update_firmware_result cros_ec_find_update_firmware(
+	struct device *dev,
+	struct ec_params_usb_pd_rw_hash_entry *hash_entry,
+	struct ec_params_usb_pd_discovery_entry *discovery_entry,
+	const struct cros_ec_pd_firmware_image **update_image)
+{
+	const struct cros_ec_pd_firmware_image *img;
+	int i;
+
+	if (hash_entry->dev_id == PD_DEVICE_TYPE_NONE)
+		return PD_UNKNOWN_DEVICE;
+
+	/*
+	 * Search for a matching firmware update image.
+	 * TODO(shawnn): Replace sequential table search with modified binary
+	 * search on major / minor.
+	 */
+	for (i = 0; cros_ec_pd_firmware_images[i].rw_image_size > 0; i++) {
+		img = &cros_ec_pd_firmware_images[i];
+		if (MAJOR_MINOR_TO_DEV_ID(img->id_major, img->id_minor)
+					  == hash_entry->dev_id &&
+		    img->usb_vid == discovery_entry->vid &&
+		    img->usb_pid == discovery_entry->pid)
+			break;
+	}
+	*update_image = img;
+
+	if (cros_ec_pd_firmware_images[i].rw_image_size == 0)
+		return PD_UNKNOWN_DEVICE;
+
+	if (!memcmp(hash_entry->dev_rw_hash, img->hash, PD_RW_HASH_SIZE)) {
+		if (hash_entry->current_image != EC_IMAGE_RW)
+			/*
+			 * As signature isn't factored into the hash if we've
+			 * previously updated RW but subsequently invalidate
+			 * signature we can get into this situation.  Need to
+			 * reflash.
+			 */
+			return PD_DO_UPDATE;
+		/* Device is already updated */
+		return PD_ALREADY_HAVE_LATEST;
+	}
+
+	/* Always update if PD device is stuck in RO. */
+	if (hash_entry->current_image != EC_IMAGE_RW) {
+		dev_info(dev, "Updating FW since PD dev is in RO\n");
+		return PD_DO_UPDATE;
+	}
+
+	dev_info(dev, "Considering upgrade from existing RW: %x %x %x %x\n",
+		 hash_entry->dev_rw_hash[0],
+		 hash_entry->dev_rw_hash[1],
+		 hash_entry->dev_rw_hash[2],
+		 hash_entry->dev_rw_hash[3]);
+
+	/* Verify RW is a known update image so we don't roll-back. */
+	for (i = 0; i < img->update_hash_count; ++i)
+		if (memcmp(hash_entry->dev_rw_hash,
+			   (*img->update_hashes)[i],
+			   PD_RW_HASH_SIZE) == 0) {
+			dev_info(dev, "Updating FW since RW is known\n");
+			return PD_DO_UPDATE;
+		}
+
+	dev_info(dev, "Skipping FW update since RW is unknown\n");
+	return PD_UNKNOWN_RW;
+}
+
+/**
+ * cros_ec_pd_get_host_event_status - Get host event status and return.  If
+ * failure return 0.
+ *
+ * @dev: PD device
+ * @pd_dev: EC PD device
+ */
+static uint32_t cros_ec_pd_get_host_event_status(struct device *dev,
+						 struct cros_ec_dev *pd_dev)
+{
+	int ret;
+	struct ec_response_host_event_status host_event_status;
+
+	/* Check for host events on EC. */
+	ret = cros_ec_pd_command(dev, pd_dev, EC_CMD_PD_HOST_EVENT_STATUS,
+				 NULL, 0,
+				 (uint8_t *)&host_event_status,
+				 sizeof(host_event_status));
+	if (ret) {
+		dev_err(dev, "Can't get host event status (err: %d)\n", ret);
+		return 0;
+	}
+	dev_dbg(dev, "Got host event status %x\n", host_event_status.status);
+	return host_event_status.status;
+}
+
+/**
+ * cros_ec_pd_update_check - Probe the status of attached PD devices and kick
+ * off an RW firmware update if needed. This is run as a deferred task on
+ * module load, resume, and when an ACPI event is received (typically on
+ * PD device insertion).
+ *
+ * @work: Delayed work pointer
+ */
+static void cros_ec_pd_update_check(struct work_struct *work)
+{
+	const struct cros_ec_pd_firmware_image *img;
+	const struct firmware *fw;
+	struct ec_params_usb_pd_rw_hash_entry hash_entry;
+	struct ec_params_usb_pd_discovery_entry discovery_entry;
+	struct cros_ec_pd_update_data *drv_data =
+		container_of(to_delayed_work(work),
+		struct cros_ec_pd_update_data, work);
+	struct device *dev = drv_data->dev;
+	struct power_supply *charger;
+	enum cros_ec_pd_find_update_firmware_result result;
+	int ret, port;
+
+	if (disable) {
+		dev_info(dev, "Update is disabled\n");
+		return;
+	}
+
+	dev_dbg(dev, "Checking for updates\n");
+
+	/* Force GFU entry for devices not in GFU by default. */
+	for (port = 0; port < drv_data->num_ports; ++port) {
+		dev_dbg(dev, "Considering GFU entry on C%d\n", port);
+		ret = cros_ec_pd_get_status(dev, cros_ec_pd_ec,
+					    port, &hash_entry,
+					    &discovery_entry);
+		if (ret || (hash_entry.dev_id == PD_DEVICE_TYPE_NONE)) {
+			dev_dbg(dev, "Forcing GFU entry on C%d\n", port);
+			cros_ec_pd_enter_gfu(dev, cros_ec_pd_ec, port);
+		}
+	}
+
+	/*
+	 * Override status received from EC if update is forced, such as
+	 * after power-on or after resume.
+	 */
+	mutex_lock(&drv_data->lock);
+	if (drv_data->force_update) {
+		drv_data->pd_status =
+			PD_EVENT_POWER_CHANGE | PD_EVENT_UPDATE_DEVICE;
+		drv_data->force_update = 0;
+	}
+
+	/*
+	 * If there is an EC based charger, send a notification to it to
+	 * trigger a refresh of the power supply state.
+	 */
+	charger = cros_ec_pd_ec->ec_dev->charger;
+	if ((drv_data->pd_status & PD_EVENT_POWER_CHANGE) && charger)
+		charger->desc->external_power_changed(charger);
+
+	if (!(drv_data->pd_status & PD_EVENT_UPDATE_DEVICE)) {
+		drv_data->pd_status = 0;
+		mutex_unlock(&drv_data->lock);
+		return;
+	}
+
+	drv_data->pd_status = 0;
+	mutex_unlock(&drv_data->lock);
+
+	/* Received notification, send command to check on PD status. */
+	for (port = 0; port < drv_data->num_ports; ++port) {
+		/* Don't try to update if we're going to suspend. */
+		if (drv_data->is_suspending)
+			return;
+
+		ret = cros_ec_pd_get_status(dev, cros_ec_pd_ec,
+					    port, &hash_entry,
+					    &discovery_entry);
+		if (ret < 0) {
+			dev_err(dev,
+				"Can't get Port%d device status (err:%d)\n",
+				port, ret);
+			return;
+		}
+
+		result = cros_ec_find_update_firmware(dev,
+						      &hash_entry,
+						      &discovery_entry,
+						      &img);
+		dev_dbg(dev, "Find Port%d FW result: %d\n", port, result);
+
+		switch (result) {
+		case PD_DO_UPDATE:
+			if (request_firmware(&fw, img->filename, dev)) {
+				dev_err(dev,
+					"Error, Port%d can't load file %s\n",
+					port, img->filename);
+				break;
+			}
+
+			if (fw->size != img->rw_image_size) {
+				dev_err(dev,
+					"Port%d FW file %s size %zd != %zd\n",
+					port, img->filename, fw->size,
+					img->rw_image_size);
+				goto done;
+			}
+
+			/* Update firmware */
+			dev_info(dev, "Updating Port%d RW to %s\n", port,
+				 img->filename);
+			ret = cros_ec_pd_fw_update(drv_data, cros_ec_pd_ec, fw,
+						   port);
+			dev_info(dev,
+				 "Port%d FW update completed with status %d\n",
+				  port, ret);
+done:
+			release_firmware(fw);
+			break;
+		case PD_ALREADY_HAVE_LATEST:
+			/*
+			 * Device already has latest firmare. Send hash entry
+			 * to EC so we don't get subsequent FW update requests.
+			 */
+			dev_info(dev, "Port%d FW is already up-to-date %s\n",
+				 port, img->filename);
+			cros_ec_pd_send_hash_entry(dev, cros_ec_pd_ec, img);
+			break;
+		case PD_UNKNOWN_DEVICE:
+		case PD_UNKNOWN_RW:
+			/* Unknown PD device or RW -- don't update FW */
+			break;
+		}
+	}
+}
+
+/**
+ * cros_ec_pd_notify - Called upon receiving a PD MCU event (typically
+ * due to PD device insertion). Queue a delayed task to check if a PD
+ * device FW update is necessary.
+ */
+static void cros_ec_pd_notify(struct device *dev, u32 event)
+{
+	struct cros_ec_pd_update_data *drv_data =
+		(struct cros_ec_pd_update_data *)
+		dev_get_drvdata(dev);
+
+	if (drv_data) {
+		mutex_lock(&drv_data->lock);
+		if (event == 0)
+			drv_data->pd_status =
+				cros_ec_pd_get_host_event_status(dev,
+								 cros_ec_pd_ec);
+		else
+			drv_data->pd_status = event;
+		mutex_unlock(&drv_data->lock);
+		queue_delayed_work(drv_data->workqueue, &drv_data->work,
+				   PD_UPDATE_CHECK_DELAY);
+	} else {
+		dev_warn(dev, "PD notification skipped due to missing drv_data\n");
+	}
+}
+
+static ssize_t disable_firmware_update(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t count)
+{
+	int ret;
+	unsigned int val;
+	struct cros_ec_pd_update_data *drv_data;
+
+	ret = sscanf(buf, "%i", &val);
+	if (ret != 1)
+		return -EINVAL;
+
+	disable = !!val;
+	dev_info(dev, "FW update is %sabled\n", disable ? "dis" : "en");
+
+	drv_data = (struct cros_ec_pd_update_data *)dev_get_drvdata(dev);
+
+	/* If re-enabled then force update */
+	if (!disable && drv_data) {
+		drv_data->force_update = 1;
+		queue_delayed_work(drv_data->workqueue, &drv_data->work,
+				   PD_UPDATE_CHECK_DELAY);
+	}
+
+	return count;
+}
+
+static DEVICE_ATTR(disable, 0200, NULL, disable_firmware_update);
+
+static struct attribute *pd_attrs[] = {
+	&dev_attr_disable.attr,
+	NULL,
+};
+
+ATTRIBUTE_GROUPS(pd);
+
+static int cros_ec_pd_add(struct device *dev)
+{
+	struct cros_ec_pd_update_data *drv_data;
+	int ret, i;
+
+	/* If cros_ec_pd_ec is not initialized, try again later */
+	if (!cros_ec_pd_ec)
+		return -EPROBE_DEFER;
+
+	drv_data =
+		devm_kzalloc(dev, sizeof(*drv_data), GFP_KERNEL);
+	if (!drv_data)
+		return -ENOMEM;
+
+	mutex_init(&drv_data->lock);
+
+	drv_data->dev = dev;
+	INIT_DELAYED_WORK(&drv_data->work, cros_ec_pd_update_check);
+	drv_data->workqueue =
+		create_singlethread_workqueue("cros_ec_pd_update");
+	if (cros_ec_pd_get_num_ports(drv_data->dev,
+				     cros_ec_pd_ec,
+				     &drv_data->num_ports) < 0) {
+		dev_err(drv_data->dev, "Can't get num_ports\n");
+		return -EINVAL;
+	}
+	drv_data->force_update = 1;
+	drv_data->is_suspending = 0;
+	dev_set_drvdata(dev, drv_data);
+	ret = sysfs_create_groups(&dev->kobj, pd_groups);
+	if (ret) {
+		dev_err(dev, "failed to create sysfs attributes: %d\n", ret);
+		return ret;
+	}
+
+	/*
+	 * Send list of update FW hashes to PD MCU.
+	 * TODO(crosbug.com/p/35510): This won't scale past four update
+	 * devices. Find a better solution once we get there.
+	 */
+	for (i = 0; cros_ec_pd_firmware_images[i].rw_image_size > 0; i++)
+		cros_ec_pd_send_hash_entry(drv_data->dev,
+					   cros_ec_pd_ec,
+					   &cros_ec_pd_firmware_images[i]);
+
+	queue_delayed_work(drv_data->workqueue, &drv_data->work,
+		PD_UPDATE_CHECK_DELAY);
+	return 0;
+}
+
+static int cros_ec_pd_resume(struct device *dev)
+{
+	struct cros_ec_pd_update_data *drv_data =
+		(struct cros_ec_pd_update_data *)dev_get_drvdata(dev);
+
+	if (drv_data) {
+		drv_data->force_update = 1;
+		drv_data->is_suspending = 0;
+		queue_delayed_work(drv_data->workqueue, &drv_data->work,
+			PD_UPDATE_CHECK_DELAY);
+	}
+	return 0;
+}
+
+static int cros_ec_pd_remove(struct device *dev)
+{
+	struct cros_ec_pd_update_data *drv_data =
+		(struct cros_ec_pd_update_data *)
+		dev_get_drvdata(dev);
+
+	if (drv_data) {
+		drv_data->is_suspending = 1;
+		cancel_delayed_work_sync(&drv_data->work);
+		mutex_destroy(&drv_data->lock);
+	}
+
+
+	return 0;
+}
+
+static int cros_ec_pd_suspend(struct device *dev)
+{
+	struct cros_ec_pd_update_data *drv_data =
+		(struct cros_ec_pd_update_data *)dev_get_drvdata(dev);
+
+	if (drv_data) {
+		drv_data->is_suspending = 1;
+		cancel_delayed_work_sync(&drv_data->work);
+		disable = 0;
+	}
+	return 0;
+}
+
+static SIMPLE_DEV_PM_OPS(cros_ec_pd_pm,
+	cros_ec_pd_suspend, cros_ec_pd_resume);
+
+static int _ec_pd_notify(struct notifier_block *nb,
+	unsigned long host_event, void *_notify)
+{
+	struct cros_ec_pd_update_data *drv_data;
+	struct device *dev;
+
+	drv_data = container_of(nb, struct cros_ec_pd_update_data, notifier);
+	dev = drv_data->dev;
+
+	cros_ec_pd_notify(dev, host_event);
+	return NOTIFY_OK;
+}
+
+static int plat_cros_ec_pd_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct cros_ec_pd_update_data *drv_data =
+		(struct cros_ec_pd_update_data *)dev_get_drvdata(dev);
+	int ret;
+
+	ret = cros_ec_pd_add(dev);
+	if (ret < 0)
+		return ret;
+
+	drv_data = (struct cros_ec_pd_update_data *)dev_get_drvdata(dev);
+	/* Get PD events from the EC */
+	drv_data->notifier.notifier_call = _ec_pd_notify;
+	ret = cros_usbpd_register_notify(&drv_data->notifier);
+	if (ret < 0)
+		dev_warn(dev, "failed to register notifier\n");
+
+	return 0;
+}
+
+static int plat_cros_ec_pd_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct cros_ec_pd_update_data *drv_data =
+		(struct cros_ec_pd_update_data *)dev_get_drvdata(dev);
+
+	cros_usbpd_unregister_notify(&drv_data->notifier);
+
+	return cros_ec_pd_remove(dev);
+}
+
+static struct platform_driver cros_ec_pd_driver = {
+	.driver = {
+		.name  = DRV_NAME,
+		.pm = &cros_ec_pd_pm,
+	},
+	.remove  = plat_cros_ec_pd_remove,
+	.probe   = plat_cros_ec_pd_probe,
+};
+
+module_platform_driver(cros_ec_pd_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("ChromeOS power device FW update driver");
+MODULE_ALIAS("platform:" DRV_NAME);
diff -ruN a/drivers/platform/chrome/cros_ec_proto.c b/drivers/platform/chrome/cros_ec_proto.c
--- a/drivers/platform/chrome/cros_ec_proto.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/chrome/cros_ec_proto.c	2021-12-23 08:35:44.000000000 +0100
@@ -908,3 +908,51 @@
 	return sensor_count;
 }
 EXPORT_SYMBOL_GPL(cros_ec_get_sensor_count);
+
+/**
+ * cros_ec_command - Send a command to the EC.
+ *
+ * @ec_dev: EC device
+ * @version: EC command version
+ * @command: EC command
+ * @outdata: EC command output data
+ * @outsize: Size of outdata
+ * @indata: EC command input data
+ * @insize: Size of indata
+ *
+ * Return: >= 0 on success, negative error number on failure.
+ */
+int cros_ec_command(struct cros_ec_device *ec_dev,
+		    unsigned int version,
+		    int command,
+		    void *outdata,
+		    int outsize,
+		    void *indata,
+		    int insize)
+{
+	struct cros_ec_command *msg;
+	int ret;
+
+	msg = kzalloc(sizeof(*msg) + max(insize, outsize), GFP_KERNEL);
+	if (!msg)
+		return -ENOMEM;
+
+	msg->version = version;
+	msg->command = command;
+	msg->outsize = outsize;
+	msg->insize = insize;
+
+	if (outsize)
+		memcpy(msg->data, outdata, outsize);
+
+	ret = cros_ec_cmd_xfer_status(ec_dev, msg);
+	if (ret < 0)
+		goto error;
+
+	if (insize)
+		memcpy(indata, msg->data, insize);
+error:
+	kfree(msg);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(cros_ec_command);
diff -ruN a/drivers/platform/chrome/cros_ec_sensorhub.c b/drivers/platform/chrome/cros_ec_sensorhub.c
--- a/drivers/platform/chrome/cros_ec_sensorhub.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/chrome/cros_ec_sensorhub.c	2021-12-23 08:35:44.000000000 +0100
@@ -90,9 +90,15 @@
 		case MOTIONSENSE_TYPE_LIGHT:
 			name = "cros-ec-light";
 			break;
+		case MOTIONSENSE_TYPE_LIGHT_RGB:
+			/* Processed with cros-ec-light. */
+			continue;
 		case MOTIONSENSE_TYPE_ACTIVITY:
 			name = "cros-ec-activity";
 			break;
+		case MOTIONSENSE_TYPE_SYNC:
+			name = "cros-ec-sync";
+			break;
 		default:
 			dev_warn(dev, "unknown type %d\n",
 				 sensorhub->resp->info.type);
diff -ruN a/drivers/platform/chrome/cros_ec_typec.c b/drivers/platform/chrome/cros_ec_typec.c
--- a/drivers/platform/chrome/cros_ec_typec.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/chrome/cros_ec_typec.c	2021-12-23 08:35:44.000000000 +0100
@@ -379,37 +379,6 @@
 	return ret;
 }
 
-static int cros_typec_ec_command(struct cros_typec_data *typec,
-				 unsigned int version,
-				 unsigned int command,
-				 void *outdata,
-				 unsigned int outsize,
-				 void *indata,
-				 unsigned int insize)
-{
-	struct cros_ec_command *msg;
-	int ret;
-
-	msg = kzalloc(sizeof(*msg) + max(outsize, insize), GFP_KERNEL);
-	if (!msg)
-		return -ENOMEM;
-
-	msg->version = version;
-	msg->command = command;
-	msg->outsize = outsize;
-	msg->insize = insize;
-
-	if (outsize)
-		memcpy(msg->data, outdata, outsize);
-
-	ret = cros_ec_cmd_xfer_status(typec->ec, msg);
-	if (ret >= 0 && insize)
-		memcpy(indata, msg->data, insize);
-
-	kfree(msg);
-	return ret;
-}
-
 static int cros_typec_usb_safe_state(struct cros_typec_port *port)
 {
 	port->state.mode = TYPEC_STATE_SAFE;
@@ -596,8 +565,8 @@
 	/* Sending Acknowledgment to EC */
 	mux_ack.port = port_num;
 
-	if (cros_typec_ec_command(typec, 0, EC_CMD_USB_PD_MUX_ACK, &mux_ack,
-				  sizeof(mux_ack), NULL, 0) < 0)
+	if (cros_ec_command(typec->ec, 0, EC_CMD_USB_PD_MUX_ACK, &mux_ack,
+			    sizeof(mux_ack), NULL, 0) < 0)
 		dev_warn(typec->dev,
 			 "Failed to send Mux ACK to EC for port: %d\n",
 			 port_num);
@@ -668,8 +637,8 @@
 		.port = port_num,
 	};
 
-	return cros_typec_ec_command(typec, 0, EC_CMD_USB_PD_MUX_INFO, &req,
-				     sizeof(req), resp, sizeof(*resp));
+	return cros_ec_command(typec->ec, 0, EC_CMD_USB_PD_MUX_INFO, &req,
+			       sizeof(req), resp, sizeof(*resp));
 }
 
 /*
@@ -776,8 +745,8 @@
 	int ret = 0;
 
 	memset(disc, 0, EC_PROTO2_MAX_RESPONSE_SIZE);
-	ret = cros_typec_ec_command(typec, 0, EC_CMD_TYPEC_DISCOVERY, &req, sizeof(req),
-				    disc, EC_PROTO2_MAX_RESPONSE_SIZE);
+	ret = cros_ec_command(typec->ec, 0, EC_CMD_TYPEC_DISCOVERY, &req, sizeof(req),
+			      disc, EC_PROTO2_MAX_RESPONSE_SIZE);
 	if (ret < 0) {
 		dev_err(typec->dev, "Failed to get SOP' discovery data for port: %d\n", port_num);
 		goto sop_prime_disc_exit;
@@ -859,8 +828,8 @@
 	typec_partner_set_pd_revision(port->partner, pd_revision);
 
 	memset(sop_disc, 0, EC_PROTO2_MAX_RESPONSE_SIZE);
-	ret = cros_typec_ec_command(typec, 0, EC_CMD_TYPEC_DISCOVERY, &req, sizeof(req),
-				    sop_disc, EC_PROTO2_MAX_RESPONSE_SIZE);
+	ret = cros_ec_command(typec->ec, 0, EC_CMD_TYPEC_DISCOVERY, &req, sizeof(req),
+			      sop_disc, EC_PROTO2_MAX_RESPONSE_SIZE);
 	if (ret < 0) {
 		dev_err(typec->dev, "Failed to get SOP discovery data for port: %d\n", port_num);
 		goto disc_exit;
@@ -892,8 +861,8 @@
 		.clear_events_mask = events_mask,
 	};
 
-	return cros_typec_ec_command(typec, 0, EC_CMD_TYPEC_CONTROL, &req,
-				     sizeof(req), NULL, 0);
+	return cros_ec_command(typec->ec, 0, EC_CMD_TYPEC_CONTROL, &req,
+			       sizeof(req), NULL, 0);
 }
 
 static void cros_typec_handle_status(struct cros_typec_data *typec, int port_num)
@@ -904,8 +873,8 @@
 	};
 	int ret;
 
-	ret = cros_typec_ec_command(typec, 0, EC_CMD_TYPEC_STATUS, &req, sizeof(req),
-				    &resp, sizeof(resp));
+	ret = cros_ec_command(typec->ec, 0, EC_CMD_TYPEC_STATUS, &req, sizeof(req),
+			      &resp, sizeof(resp));
 	if (ret < 0) {
 		dev_warn(typec->dev, "EC_CMD_TYPEC_STATUS failed for port: %d\n", port_num);
 		return;
@@ -983,9 +952,9 @@
 	req.mux = USB_PD_CTRL_MUX_NO_CHANGE;
 	req.swap = USB_PD_CTRL_SWAP_NONE;
 
-	ret = cros_typec_ec_command(typec, typec->pd_ctrl_ver,
-				    EC_CMD_USB_PD_CONTROL, &req, sizeof(req),
-				    &resp, sizeof(resp));
+	ret = cros_ec_command(typec->ec, typec->pd_ctrl_ver,
+			      EC_CMD_USB_PD_CONTROL, &req, sizeof(req),
+			      &resp, sizeof(resp));
 	if (ret < 0)
 		return ret;
 
@@ -1035,8 +1004,8 @@
 
 	/* We're interested in the PD control command version. */
 	req_v1.cmd = EC_CMD_USB_PD_CONTROL;
-	ret = cros_typec_ec_command(typec, 1, EC_CMD_GET_CMD_VERSIONS,
-				    &req_v1, sizeof(req_v1), &resp,
+	ret = cros_ec_command(typec->ec, 1, EC_CMD_GET_CMD_VERSIONS,
+			      &req_v1, sizeof(req_v1), &resp,
 				    sizeof(resp));
 	if (ret < 0)
 		return ret;
@@ -1120,8 +1089,8 @@
 	typec->needs_mux_ack = !!cros_ec_check_features(ec_dev,
 							EC_FEATURE_TYPEC_MUX_REQUIRE_AP_ACK);
 
-	ret = cros_typec_ec_command(typec, 0, EC_CMD_USB_PD_PORTS, NULL, 0,
-				    &resp, sizeof(resp));
+	ret = cros_ec_command(typec->ec, 0, EC_CMD_USB_PD_PORTS, NULL, 0,
+			      &resp, sizeof(resp));
 	if (ret < 0)
 		return ret;
 
diff -ruN a/drivers/platform/chrome/cros_ec_uart.c b/drivers/platform/chrome/cros_ec_uart.c
--- a/drivers/platform/chrome/cros_ec_uart.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/platform/chrome/cros_ec_uart.c	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,415 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * UART interface for ChromeOS Embedded Controller
+ *
+ * Copyright 2020 Google LLC.
+ */
+
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/acpi.h>
+#include <linux/of.h>
+#include <linux/platform_data/cros_ec_commands.h>
+#include <linux/platform_data/cros_ec_proto.h>
+#include <linux/serdev.h>
+#include <linux/slab.h>
+#include <uapi/linux/sched/types.h>
+
+#include "cros_ec.h"
+
+/*
+ * EC sends contiguous bytes of response packet on UART AP RX.
+ * TTY driver in AP accumulates incoming bytes and calls the registered callback
+ * function. Byte count can range from 1 to MAX bytes supported by EC.
+ * This driver should wait for long time for all callbacks to be processed.
+ * Considering the worst case scenario, wait for 500 msec. This timeout should
+ * account for max latency and some additional guard time.
+ * Best case: Entire packet is received in ~200 ms, wait queue will be released
+ * and packet will be processed.
+ * Worst case: TTY driver sends bytes in multiple callbacks. In this case this
+ * driver will wait for ~1 sec beyond which it will timeout.
+ * This timeout value should not exceed ~500 msec because in case if
+ * EC_CMD_REBOOT_EC sent, high level driver should be able to intercept EC
+ * in RO.
+ */
+#define EC_MSG_DEADLINE_MS		500
+
+/**
+ * struct response_info - Encapsulate EC response related
+ *			information for passing between function
+ *			cros_ec_uart_pkt_xfer() and cros_ec_uart_rx_bytes()
+ *			callback.
+ * @data:		Copy the data received from EC here.
+ * @max_size:		Max size allocated for the @data buffer. If the
+ *			received data exceeds this value, we log an error.
+ * @size:		Actual size of data received from EC. This is also
+ *			used to accumulate byte count with response is received
+ *			in dma chunks.
+ * @exp_len:		Expected bytes of response from EC including header.
+ * @error:		0 for success, negative error code for a failure.
+ * @received:		Set to true on receiving a valid EC response.
+ * @wait_queue:		Wait queue EC response where the cros_ec sends request
+ *			to EC and waits
+ */
+struct response_info {
+	void *data;
+	size_t max_size;
+	size_t size;
+	int error;
+	size_t exp_len;
+	bool received;
+	wait_queue_head_t wait_queue;
+};
+
+/**
+ * struct cros_ec_uart - information about a uart-connected EC
+ *
+ * @serdev_device:	serdev uart device we are connected to.
+ * @baudrate:		UART baudrate of attached EC device.
+ * @flowcontrol:	UART flowcontrol of attached device.
+ * @irq:		Linux IRQ number of associated serial device.
+ * @response:		Response info passing between cros_ec_uart_pkt_xfer()
+ *			and cros_ec_uart_rx_bytes()
+ */
+struct cros_ec_uart {
+	struct serdev_device *serdev;
+	u32 baudrate;
+	u8  flowcontrol;
+	u32 irq;
+	struct response_info response;
+};
+
+static int cros_ec_uart_rx_bytes(struct serdev_device *serdev,
+				 const u8 *data,
+				 size_t count)
+{
+	struct ec_host_response *response;
+	struct cros_ec_device *ec_dev = serdev_device_get_drvdata(serdev);
+	struct cros_ec_uart *ec_uart = ec_dev->priv;
+
+	/* Check if bytes were sent out of band */
+	if (!ec_uart->response.data)
+		/* Discard all bytes */
+		return count;
+
+	/*
+	 * Check if incoming bytes + response.size are less than allocated
+	 * buffer in din by cros_ec. This will ensure that if EC sends more
+	 * bytes than max_size, waiting process will be notified with an error.
+	 */
+	if (ec_uart->response.size + count <= ec_uart->response.max_size) {
+		/* Copy bytes in data in buffer */
+		memcpy((void *)ec_uart->response.data + ec_uart->response.size,
+		       (void *)data, count);
+
+		/* Add incoming bytes in size */
+		ec_uart->response.size += count;
+
+		/*
+		 * Read data_len if we received response header and if exp_len
+		 * was not read before.
+		 */
+		if (ec_uart->response.size >= sizeof(*response) &&
+		    ec_uart->response.exp_len == 0) {
+			/* Get expected response length from response header */
+			response = (struct ec_host_response *)
+							ec_uart->response.data;
+
+			ec_uart->response.exp_len = response->data_len +
+				sizeof(*response);
+		}
+
+		/*
+		 * If driver received response header and payload from EC,
+		 * Wake up the wait queue.
+		 */
+		if (ec_uart->response.size >= sizeof(*response) &&
+		    ec_uart->response.size == ec_uart->response.exp_len) {
+			/* Set flag before waking up the caller */
+			ec_uart->response.received = true;
+
+			/* Wake the calling thread */
+			wake_up_interruptible(&ec_uart->response.wait_queue);
+		}
+	} else {
+		/* Received bytes are more the allocated buffer*/
+		ec_uart->response.error = -EMSGSIZE;
+
+		/* Wake the calling thread */
+		wake_up_interruptible(&ec_uart->response.wait_queue);
+	}
+
+	return count;
+}
+
+static int cros_ec_uart_pkt_xfer(struct cros_ec_device *ec_dev,
+				 struct cros_ec_command *ec_msg)
+{
+	struct cros_ec_uart *ec_uart = ec_dev->priv;
+	struct serdev_device *serdev = ec_uart->serdev;
+	struct ec_host_response *response;
+	unsigned int len;
+	int ret, i;
+	u8 sum = 0;
+
+	/* Prepare an outgoing message in the output buffer */
+	len = cros_ec_prepare_tx(ec_dev, ec_msg);
+	dev_dbg(ec_dev->dev, "Prepared len=%d\n", len);
+
+	/* Setup for incoming response */
+	ec_uart->response.data = ec_dev->din;
+	ec_uart->response.max_size = ec_dev->din_size;
+	ec_uart->response.size = 0;
+	ec_uart->response.error = 0;
+	ec_uart->response.exp_len = 0;
+	ec_uart->response.received = false;
+
+	/* Write serial device buffer */
+	ret = serdev_device_write_buf(serdev, ec_dev->dout, len);
+	if (ret < len) {
+		dev_err(&serdev->dev,
+			"Unable to write data to serial device %s",
+			dev_name(&serdev->dev));
+
+		/* Return EIO as controller had issues writing buffer */
+		ret = -EIO;
+		goto exit;
+	}
+
+	/* Once request is successfully sent to EC, wait to wait_queue */
+	wait_event_interruptible_timeout(ec_uart->response.wait_queue,
+					 ec_uart->response.received,
+					 msecs_to_jiffies(EC_MSG_DEADLINE_MS));
+
+	/* Check if wait_queue was interrupted due to an error */
+	if (ec_uart->response.error < 0) {
+		dev_warn(&serdev->dev, "Response error detected.\n");
+
+		ret = ec_uart->response.error;
+		goto exit;
+	}
+
+	/* Check if valid response was received or there was a timeout */
+	if (!ec_uart->response.received) {
+		dev_warn(&serdev->dev, "EC failed to respond in time.\n");
+
+		ret = -ETIMEDOUT;
+		goto exit;
+	}
+
+	/* Check response error code */
+	response = (struct ec_host_response *)ec_dev->din;
+	ec_msg->result = response->result;
+
+	/* Check if received response is longer than expected */
+	if (response->data_len > ec_msg->insize) {
+		dev_err(ec_dev->dev, "Resp too long (%d bytes, expected %d)",
+			response->data_len,
+			ec_msg->insize);
+		ret = -ENOSPC;
+		goto exit;
+	}
+
+	/* Copy response packet to ec_msg data buffer */
+	memcpy(ec_msg->data,
+	       ec_dev->din + sizeof(*response),
+	       response->data_len);
+
+	/* Add all response header bytes for checksum calculation */
+	for (i = 0; i < sizeof(*response); i++)
+		sum += ec_dev->din[i];
+
+	/* Copy response packet payload and compute checksum */
+	for (i = 0; i < response->data_len; i++)
+		sum += ec_msg->data[i];
+
+	if (sum) {
+		dev_err(ec_dev->dev,
+			"Bad packet checksum calculated %x\n",
+			sum);
+		ret = -EBADMSG;
+		goto exit;
+	}
+
+	/* Return data_len to cros_ec */
+	ret = response->data_len;
+
+exit:
+	/* Reset ec_uart */
+	ec_uart->response.data = NULL;
+	ec_uart->response.max_size = 0;
+	ec_uart->response.size = 0;
+	ec_uart->response.error = 0;
+	ec_uart->response.exp_len = 0;
+	ec_uart->response.received = false;
+
+	if (ec_msg->command == EC_CMD_REBOOT_EC)
+		msleep(EC_REBOOT_DELAY_MS);
+
+	return ret;
+}
+
+static int cros_ec_uart_resource(struct acpi_resource *ares, void *data)
+{
+	struct cros_ec_uart *ec_uart = data;
+	struct acpi_resource_uart_serialbus *sb;
+
+	switch (ares->type) {
+	case ACPI_RESOURCE_TYPE_SERIAL_BUS:
+		sb = &ares->data.uart_serial_bus;
+		if (sb->type == ACPI_RESOURCE_SERIAL_TYPE_UART) {
+			ec_uart->baudrate = sb->default_baud_rate;
+			dev_dbg(&ec_uart->serdev->dev, "Baudrate %d\n",
+				ec_uart->baudrate);
+
+			ec_uart->flowcontrol = sb->flow_control;
+			dev_dbg(&ec_uart->serdev->dev, "Flow control %d\n",
+				ec_uart->flowcontrol);
+		}
+		break;
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+static int cros_ec_uart_acpi_probe(struct cros_ec_uart *ec_uart)
+{
+	LIST_HEAD(resources);
+	struct acpi_device *adev = ACPI_COMPANION(&ec_uart->serdev->dev);
+	int ret;
+
+	/* Retrieve UART ACPI info */
+	ret = acpi_dev_get_resources(adev, &resources,
+				     cros_ec_uart_resource, ec_uart);
+	if (ret < 0)
+		return ret;
+
+	acpi_dev_free_resource_list(&resources);
+
+	/* Retrieve GpioInt and translate it to Linux IRQ number */
+	ret = acpi_dev_gpio_irq_get(adev, 0);
+	if (ret < 0)
+		return ret;
+
+	ec_uart->irq = ret;
+	dev_dbg(&ec_uart->serdev->dev, "IRQ number %d\n", ec_uart->irq);
+
+	return 0;
+}
+
+static const struct serdev_device_ops cros_ec_uart_client_ops = {
+	.receive_buf = cros_ec_uart_rx_bytes,
+};
+
+static int cros_ec_uart_probe(struct serdev_device *serdev)
+{
+	struct device *dev = &serdev->dev;
+	struct cros_ec_device *ec_dev;
+	struct cros_ec_uart *ec_uart;
+	int ret;
+
+	ec_uart = devm_kzalloc(dev, sizeof(*ec_uart), GFP_KERNEL);
+	if (!ec_uart)
+		return -ENOMEM;
+
+	ec_dev = devm_kzalloc(dev, sizeof(*ec_dev), GFP_KERNEL);
+	if (!ec_dev)
+		return -ENOMEM;
+
+	ec_uart->serdev = serdev;
+
+	/* Open the serial device */
+	ret = devm_serdev_device_open(dev, ec_uart->serdev);
+	if (ret) {
+		dev_err(dev, "Unable to open UART device %s",
+			dev_name(&serdev->dev));
+		return ret;
+	}
+
+	serdev_device_set_drvdata(serdev, ec_dev);
+
+	serdev_device_set_client_ops(serdev, &cros_ec_uart_client_ops);
+
+	/* Initialize wait queue */
+	init_waitqueue_head(&ec_uart->response.wait_queue);
+
+	ret = cros_ec_uart_acpi_probe(ec_uart);
+	if (ret < 0) {
+		dev_err(dev, "Failed to get ACPI info (%d)", ret);
+		return ret;
+	}
+
+	/* Set baud rate of serial device */
+	ret = serdev_device_set_baudrate(serdev, ec_uart->baudrate);
+	if (ret < 0) {
+		dev_err(dev, "Failed to set up host baud rate (%d)", ret);
+		return ret;
+	}
+
+	/* Set flow control of serial device */
+	serdev_device_set_flow_control(serdev, ec_uart->flowcontrol);
+
+	/* Initialize ec_dev for cros_ec  */
+	ec_dev->phys_name = dev_name(&ec_uart->serdev->dev);
+	ec_dev->dev = dev;
+	ec_dev->priv = ec_uart;
+	ec_dev->irq = ec_uart->irq;
+	ec_dev->cmd_xfer = NULL;
+	ec_dev->pkt_xfer = cros_ec_uart_pkt_xfer;
+	ec_dev->din_size = sizeof(struct ec_host_response) +
+			   sizeof(struct ec_response_get_protocol_info);
+	ec_dev->dout_size = sizeof(struct ec_host_request);
+
+	/* Register a new cros_ec device */
+	return cros_ec_register(ec_dev);
+}
+
+static void cros_ec_uart_remove(struct serdev_device *serdev)
+{
+	struct cros_ec_device *ec_dev = serdev_device_get_drvdata(serdev);
+
+	cros_ec_unregister(ec_dev);
+};
+
+static int __maybe_unused cros_ec_uart_suspend(struct device *dev)
+{
+	struct cros_ec_device *ec_dev = dev_get_drvdata(dev);
+
+	return cros_ec_suspend(ec_dev);
+}
+
+static int __maybe_unused cros_ec_uart_resume(struct device *dev)
+{
+	struct cros_ec_device *ec_dev = dev_get_drvdata(dev);
+
+	return cros_ec_resume(ec_dev);
+}
+
+static SIMPLE_DEV_PM_OPS(cros_ec_uart_pm_ops, cros_ec_uart_suspend,
+			 cros_ec_uart_resume);
+
+static const struct of_device_id cros_ec_uart_of_match[] = {
+	{ .compatible = "google,cros-ec-uart" },
+	{}
+};
+
+static struct serdev_device_driver cros_ec_uart_driver = {
+	.driver	= {
+		.name	= "cros-ec-uart",
+		.of_match_table = cros_ec_uart_of_match,
+		.pm	= &cros_ec_uart_pm_ops,
+	},
+	.probe		= cros_ec_uart_probe,
+	.remove		= cros_ec_uart_remove,
+};
+
+module_serdev_device_driver(cros_ec_uart_driver);
+
+MODULE_LICENSE("GPL v2");
+MODULE_DESCRIPTION("UART interface for ChromeOS Embedded Controller");
+MODULE_AUTHOR("Bhanu Prakash Maiya <bhanumaiya@chromium.org>");
diff -ruN a/drivers/platform/chrome/cros_usbpd_notify.c b/drivers/platform/chrome/cros_usbpd_notify.c
--- a/drivers/platform/chrome/cros_usbpd_notify.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/chrome/cros_usbpd_notify.c	2021-12-23 08:35:44.000000000 +0100
@@ -53,50 +53,6 @@
 }
 EXPORT_SYMBOL_GPL(cros_usbpd_unregister_notify);
 
-/**
- * cros_ec_pd_command - Send a command to the EC.
- *
- * @ec_dev: EC device
- * @command: EC command
- * @outdata: EC command output data
- * @outsize: Size of outdata
- * @indata: EC command input data
- * @insize: Size of indata
- *
- * Return: >= 0 on success, negative error number on failure.
- */
-static int cros_ec_pd_command(struct cros_ec_device *ec_dev,
-			      int command,
-			      uint8_t *outdata,
-			      int outsize,
-			      uint8_t *indata,
-			      int insize)
-{
-	struct cros_ec_command *msg;
-	int ret;
-
-	msg = kzalloc(sizeof(*msg) + max(insize, outsize), GFP_KERNEL);
-	if (!msg)
-		return -ENOMEM;
-
-	msg->command = command;
-	msg->outsize = outsize;
-	msg->insize = insize;
-
-	if (outsize)
-		memcpy(msg->data, outdata, outsize);
-
-	ret = cros_ec_cmd_xfer_status(ec_dev, msg);
-	if (ret < 0)
-		goto error;
-
-	if (insize)
-		memcpy(indata, msg->data, insize);
-error:
-	kfree(msg);
-	return ret;
-}
-
 static void cros_usbpd_get_event_and_notify(struct device  *dev,
 					    struct cros_ec_device *ec_dev)
 {
@@ -115,10 +71,8 @@
 	}
 
 	/* Check for PD host events on EC. */
-	ret = cros_ec_pd_command(ec_dev, EC_CMD_PD_HOST_EVENT_STATUS,
-				 NULL, 0,
-				 (uint8_t *)&host_event_status,
-				 sizeof(host_event_status));
+	ret = cros_ec_command(ec_dev, 0, EC_CMD_PD_HOST_EVENT_STATUS,
+			      NULL, 0, &host_event_status, sizeof(host_event_status));
 	if (ret < 0) {
 		dev_warn(dev, "Can't get host event status (err: %d)\n", ret);
 		goto send_notify;
diff -ruN a/drivers/platform/chrome/Kconfig b/drivers/platform/chrome/Kconfig
--- a/drivers/platform/chrome/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/chrome/Kconfig	2021-12-23 08:35:44.000000000 +0100
@@ -15,6 +15,14 @@
 
 if CHROME_PLATFORMS
 
+config CHROMEOS
+	bool
+	depends on NVRAM && ACPI_CHROMEOS
+	help
+	  Provides abstracted interfaces to the firmware features provided on
+	  ChromeOS devices. It depends on a lowlevel driver to implement the
+	  firmware interface on the platform.
+
 config CHROMEOS_LAPTOP
 	tristate "Chrome OS Laptop"
 	depends on I2C && DMI && X86
@@ -108,6 +116,16 @@
 	  response time cannot be guaranteed, we support ignoring
 	  'pre-amble' bytes before the response actually starts.
 
+config CROS_EC_UART
+	tristate "ChromeOS Embedded Controller (UART)"
+	depends on CROS_EC && ACPI && SERIAL_DEV_BUS
+	help
+	  If you say Y here, you get support for talking to the ChromeOS EC
+	  through a UART, using a byte-level protocol.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cros_ec_uart.
+
 config CROS_EC_LPC
 	tristate "ChromeOS Embedded Controller (LPC)"
 	depends on CROS_EC && ACPI && (X86 || COMPILE_TEST)
@@ -204,11 +222,24 @@
 	  To compile this driver as a module, choose M here: the
 	  module will be called cros_ec_sysfs.
 
+config CROS_EC_PD_UPDATE
+	tristate "ChromeOS Embedded Controller PD device update driver"
+	depends on MFD_CROS_EC_DEV
+	depends on CROS_USBPD_NOTIFY
+
+	help
+	  If you say Y here, you get support for updating ChromeOS
+	  PD device firmware.
+
+	  To compile this driver as a module, choose M here: the module will be
+	  called cros_ec_pd_update.
+
 config CROS_EC_TYPEC
 	tristate "ChromeOS EC Type-C Connector Control"
 	depends on MFD_CROS_EC_DEV && TYPEC
 	depends on CROS_USBPD_NOTIFY
 	depends on USB_ROLE_SWITCH
+	depends on !EXTCON_TCSS_CROS_EC
 	default MFD_CROS_EC_DEV
 	help
 	  If you say Y here, you get support for accessing Type C connector
diff -ruN a/drivers/platform/chrome/Makefile b/drivers/platform/chrome/Makefile
--- a/drivers/platform/chrome/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/chrome/Makefile	2021-12-23 08:35:44.000000000 +0100
@@ -3,6 +3,7 @@
 # tell define_trace.h where to find the cros ec trace header
 CFLAGS_cros_ec_trace.o:=		-I$(src)
 
+obj-$(CONFIG_CHROMEOS)			+= chromeos.o
 obj-$(CONFIG_CHROMEOS_LAPTOP)		+= chromeos_laptop.o
 obj-$(CONFIG_CHROMEOS_PSTORE)		+= chromeos_pstore.o
 obj-$(CONFIG_CHROMEOS_TBMC)		+= chromeos_tbmc.o
@@ -11,9 +12,11 @@
 obj-$(CONFIG_CROS_EC_ISHTP)		+= cros_ec_ishtp.o
 obj-$(CONFIG_CROS_EC_RPMSG)		+= cros_ec_rpmsg.o
 obj-$(CONFIG_CROS_EC_SPI)		+= cros_ec_spi.o
+obj-$(CONFIG_CROS_EC_UART)		+= cros_ec_uart.o
 cros_ec_lpcs-objs			:= cros_ec_lpc.o cros_ec_lpc_mec.o
 obj-$(CONFIG_CROS_EC_TYPEC)		+= cros_ec_typec.o
 obj-$(CONFIG_CROS_EC_LPC)		+= cros_ec_lpcs.o
+obj-$(CONFIG_CROS_EC_PD_UPDATE)		+= cros_ec_pd_update.o cros_ec_pd_sysfs.o
 obj-$(CONFIG_CROS_EC_PROTO)		+= cros_ec_proto.o cros_ec_trace.o
 obj-$(CONFIG_CROS_KBD_LED_BACKLIGHT)	+= cros_kbd_led_backlight.o
 obj-$(CONFIG_CROS_EC_CHARDEV)		+= cros_ec_chardev.o
diff -ruN a/drivers/platform/chrome/wilco_ec/charge_schedule.c b/drivers/platform/chrome/wilco_ec/charge_schedule.c
--- a/drivers/platform/chrome/wilco_ec/charge_schedule.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/platform/chrome/wilco_ec/charge_schedule.c	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,245 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * EC communication for Peak Shift and Advanced Battery Charging schedules.
+ *
+ * Copyright 2019 Google LLC
+ *
+ * See Documentation/ABI/testing/sysfs-platform-wilco-ec for more info.
+ */
+
+#include <linux/platform_data/wilco-ec.h>
+#include "charge_schedule.h"
+
+/* Property IDs and related EC constants */
+#define PID_PEAK_SHIFT				0x04EA
+#define PID_PEAK_SHIFT_BATTERY_THRESHOLD	0x04EB
+#define PID_PEAK_SHIFT_SUNDAY			0x04EE
+#define PID_ADV_CHARGING			0x04ED
+#define PID_ADV_CHARGING_SUNDAY			0x04F5
+
+/*
+ * Date and hour information is passed to/from the EC using packed bytes,
+ * where each byte represents an hour and a minute that some event occurs.
+ * The minute field supports quarter-hour intervals, so either
+ * 0, 15, 30, or 45. This allows this info to be packed within 2 bits.
+ * Along with the 5 bits of hour info [0-23], this gives us 7 used bits
+ * within each packed byte:
+ * +---------------+
+ * |7|6|5|4|3|2|1|0|
+ * +---------------+
+ * |X|  hour   |min|
+ * +---------------+
+ */
+
+#define MINUTE_POSITION	0	/* bits[0:1] */
+#define MINUTE_MASK	0x03	/* 0b00000011 */
+#define HOUR_POSITION	2	/* bits[2:6] */
+#define HOUR_MASK	0x7c	/* 0b01111100 */
+
+struct adv_charging_payload {
+	u8 start_time;
+	u8 duration_time;
+	u16 RESERVED;
+} __packed;
+
+struct peak_shift_payload {
+	u8 start_time;
+	u8 end_time;
+	u8 charge_start_time;
+	u8 RESERVED;
+} __packed;
+
+/* Pack hour and minute info into a byte. */
+static u8 pack_field(int hour, int minute)
+{
+	int result = 0;
+	int quarter_hour;
+
+	quarter_hour = minute / 15;
+	result |= hour << HOUR_POSITION;
+	result |= quarter_hour << MINUTE_POSITION;
+
+	return (u8)result;
+}
+
+/* Extract hour and minute info from a byte. */
+static void unpack_field(int *hour, int *minute, u8 field)
+{
+	int quarter_hour;
+
+	*hour =		(field & HOUR_MASK)	>> HOUR_POSITION;
+	quarter_hour =	(field & MINUTE_MASK)	>> MINUTE_POSITION;
+	*minute = quarter_hour * 15;
+}
+
+#define hour_valid(h)   (h >= 0 && h < 24)
+#define minute_valid(m) (m >= 0 && m < 60 && (m % 15 == 0))
+
+static bool
+is_adv_charging_sched_valid(const struct adv_charge_schedule *sched)
+{
+	return (hour_valid(sched->start_hour) &&
+		hour_valid(sched->duration_hour) &&
+		minute_valid(sched->start_minute) &&
+		minute_valid(sched->duration_minute));
+}
+
+static bool
+is_peak_shift_schedule_valid(const struct peak_shift_schedule *sched)
+{
+	return (hour_valid(sched->start_hour) &&
+		hour_valid(sched->end_hour) &&
+		hour_valid(sched->charge_start_hour) &&
+		minute_valid(sched->start_minute) &&
+		minute_valid(sched->end_minute) &&
+		minute_valid(sched->charge_start_minute));
+}
+
+int wilco_ec_get_adv_charge_schedule(struct wilco_ec_device *ec,
+				     struct adv_charge_schedule *sched)
+{
+	struct wilco_ec_property_msg msg;
+	struct adv_charging_payload *payload;
+	int ret;
+
+	msg.property_id = PID_ADV_CHARGING_SUNDAY + sched->day_of_week;
+	ret = wilco_ec_get_property(ec, &msg);
+	if (ret)
+		return ret;
+
+	payload = (struct adv_charging_payload *) msg.data;
+	unpack_field(&sched->start_hour, &sched->start_minute,
+		     payload->start_time);
+	unpack_field(&sched->duration_hour, &sched->duration_minute,
+		     payload->duration_time);
+
+	return 0;
+}
+
+int wilco_ec_set_adv_charge_schedule(struct wilco_ec_device *ec,
+				     const struct adv_charge_schedule *sched)
+{
+	struct adv_charging_payload *payload;
+	struct wilco_ec_property_msg msg;
+
+	if (!is_adv_charging_sched_valid(sched))
+		return -EINVAL;
+
+	payload = (struct adv_charging_payload *)msg.data;
+	memset(payload, 0, sizeof(*payload));
+	payload->start_time = pack_field(sched->start_hour,
+					 sched->start_minute);
+	payload->duration_time = pack_field(sched->duration_hour,
+					    sched->duration_minute);
+	msg.length = sizeof(*payload);
+	msg.property_id = PID_ADV_CHARGING_SUNDAY + sched->day_of_week;
+
+	return wilco_ec_set_property(ec, &msg);
+}
+
+int wilco_ec_get_peak_shift_schedule(struct wilco_ec_device *ec,
+				     struct peak_shift_schedule *sched)
+{
+	struct wilco_ec_property_msg msg;
+	struct peak_shift_payload *payload;
+	int ret;
+
+	msg.property_id = PID_PEAK_SHIFT_SUNDAY + sched->day_of_week;
+	ret = wilco_ec_get_property(ec, &msg);
+	if (ret)
+		return ret;
+
+	payload = (struct peak_shift_payload *) msg.data;
+	unpack_field(&sched->start_hour, &sched->start_minute,
+		     payload->start_time);
+	unpack_field(&sched->end_hour, &sched->end_minute, payload->end_time);
+	unpack_field(&sched->charge_start_hour, &sched->charge_start_minute,
+		     payload->charge_start_time);
+
+	return 0;
+}
+
+int wilco_ec_set_peak_shift_schedule(struct wilco_ec_device *ec,
+				     const struct peak_shift_schedule *sched)
+{
+	struct peak_shift_payload *payload;
+	struct wilco_ec_property_msg msg;
+
+	if (!is_peak_shift_schedule_valid(sched))
+		return -EINVAL;
+
+	payload = (struct peak_shift_payload *)msg.data;
+	memset(payload, 0, sizeof(*payload));
+	payload->start_time = pack_field(sched->start_hour,
+					 sched->start_minute);
+	payload->end_time = pack_field(sched->end_hour, sched->end_minute);
+	payload->charge_start_time = pack_field(sched->charge_start_hour,
+						sched->charge_start_minute);
+	msg.length = sizeof(*payload);
+	msg.property_id = PID_PEAK_SHIFT_SUNDAY + sched->day_of_week;
+
+	return wilco_ec_set_property(ec, &msg);
+}
+
+int wilco_ec_get_peak_shift_enable(struct wilco_ec_device *ec, bool *enable)
+{
+	u8 result;
+	int ret;
+
+	ret = wilco_ec_get_byte_property(ec, PID_PEAK_SHIFT, &result);
+	if (ret < 0)
+		return ret;
+
+	*enable = result;
+
+	return 0;
+}
+
+int wilco_ec_set_peak_shift_enable(struct wilco_ec_device *ec, bool enable)
+{
+	return wilco_ec_set_byte_property(ec, PID_PEAK_SHIFT, (u8)enable);
+}
+
+int wilco_ec_get_adv_charging_enable(struct wilco_ec_device *ec, bool *enable)
+{
+	u8 result;
+	int ret;
+
+	ret = wilco_ec_get_byte_property(ec, PID_ADV_CHARGING, &result);
+	if (ret < 0)
+		return ret;
+
+	*enable = result;
+
+	return 0;
+}
+
+int wilco_ec_set_adv_charging_enable(struct wilco_ec_device *ec, bool enable)
+{
+	return wilco_ec_set_byte_property(ec, PID_ADV_CHARGING, (u8)enable);
+}
+
+int wilco_ec_get_peak_shift_battery_threshold(struct wilco_ec_device *ec,
+					      int *percent)
+{
+	u8 result;
+	int ret;
+
+	ret = wilco_ec_get_byte_property(ec, PID_PEAK_SHIFT_BATTERY_THRESHOLD,
+					 &result);
+	if (ret < 0)
+		return ret;
+
+	*percent = result;
+
+	return 0;
+}
+int wilco_ec_set_peak_shift_battery_threshold(struct wilco_ec_device *ec,
+					      int percent)
+{
+	if (percent < WILCO_EC_PEAK_SHIFT_BATTERY_THRESHOLD_MIN ||
+	    percent > WILCO_EC_PEAK_SHIFT_BATTERY_THRESHOLD_MAX)
+		return -EINVAL;
+	return wilco_ec_set_byte_property(ec, PID_PEAK_SHIFT_BATTERY_THRESHOLD,
+					  (u8) percent);
+}
diff -ruN a/drivers/platform/chrome/wilco_ec/charge_schedule.h b/drivers/platform/chrome/wilco_ec/charge_schedule.h
--- a/drivers/platform/chrome/wilco_ec/charge_schedule.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/platform/chrome/wilco_ec/charge_schedule.h	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,65 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * EC communication for Peak Shift and Advanced Battery Charging schedules.
+ *
+ * Copyright 2019 Google LLC
+ *
+ * See Documentation/ABI/testing/sysfs-platform-wilco-ec for more info.
+ */
+#ifndef WILCO_EC_CHARGE_SCHEDULE_H
+#define WILCO_EC_CHARGE_SCHEDULE_H
+
+#include <linux/platform_data/wilco-ec.h>
+
+#define WILCO_EC_PEAK_SHIFT_BATTERY_THRESHOLD_MIN	15
+#define WILCO_EC_PEAK_SHIFT_BATTERY_THRESHOLD_MAX	100
+
+struct peak_shift_schedule {
+	int day_of_week;		/* 0==Sunday, 1==Monday, ... */
+	int start_hour;			/* 0..23 */
+	int start_minute;		/* One of {0, 15, 30, 45} */
+	int end_hour;			/* 0..23 */
+	int end_minute;			/* One of {0, 15, 30, 45} */
+	int charge_start_hour;		/* 0..23 */
+	int charge_start_minute;	/* One of {0, 15, 30, 45} */
+};
+
+struct adv_charge_schedule {
+	int day_of_week;	/* 0==Sunday, 1==Monday, ... */
+	int start_hour;		/* 0..23 */
+	int start_minute;	/* One of {0, 15, 30, 45} */
+	int duration_hour;	/* 0..23 */
+	int duration_minute;	/* One of {0, 15, 30, 45} */
+};
+
+/*
+ * Return 0 on success, negative error code on failure. For the getters()
+ * the sched.day_of_week field should be filled before use. For the setters()
+ * all of the sched fields should be filled before use.
+ */
+int wilco_ec_get_adv_charge_schedule(struct wilco_ec_device *ec,
+				     struct adv_charge_schedule *sched);
+int wilco_ec_set_adv_charge_schedule(struct wilco_ec_device *ec,
+				     const struct adv_charge_schedule *sched);
+int wilco_ec_get_peak_shift_schedule(struct wilco_ec_device *ec,
+				     struct peak_shift_schedule *sched);
+int wilco_ec_set_peak_shift_schedule(struct wilco_ec_device *ec,
+				     const struct peak_shift_schedule *sched);
+
+/* Return 0 on success, negative error code on failure. */
+int wilco_ec_get_peak_shift_enable(struct wilco_ec_device *ec, bool *enable);
+int wilco_ec_set_peak_shift_enable(struct wilco_ec_device *ec, bool enable);
+int wilco_ec_get_adv_charging_enable(struct wilco_ec_device *ec, bool *enable);
+int wilco_ec_set_adv_charging_enable(struct wilco_ec_device *ec, bool enable);
+
+/*
+ * Return 0 on success, negative error code on failure.
+ * Valid range for setting is from |WILCO_EC_PEAK_SHIFT_BATTERY_THRESHOLD_MIN|
+ * to |WILCO_EC_PEAK_SHIFT_BATTERY_THRESHOLD_MAX|, inclusive.
+ */
+int wilco_ec_get_peak_shift_battery_threshold(struct wilco_ec_device *ec,
+					      int *percent);
+int wilco_ec_set_peak_shift_battery_threshold(struct wilco_ec_device *ec,
+					      int percent);
+
+#endif /* WILCO_EC_CHARGE_SCHEDULE_H */
diff -ruN a/drivers/platform/chrome/wilco_ec/charge_schedule_sysfs.c b/drivers/platform/chrome/wilco_ec/charge_schedule_sysfs.c
--- a/drivers/platform/chrome/wilco_ec/charge_schedule_sysfs.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/platform/chrome/wilco_ec/charge_schedule_sysfs.c	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,319 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Sysfs interface for Peak Shift and Advanced Battery Charging schedules.
+ *
+ * Copyright 2019 Google LLC
+ *
+ * See Documentation/ABI/testing/sysfs-platform-wilco-ec for more info.
+ */
+
+#include <linux/device.h>
+#include <linux/module.h>
+#include <linux/platform_data/wilco-ec.h>
+#include <linux/platform_device.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+
+#include "charge_schedule.h"
+
+#define DRV_NAME "wilco-charge-schedule"
+
+static ssize_t peak_shift_enable_show(struct device *dev,
+				      struct device_attribute *attr, char *buf)
+{
+	struct wilco_ec_device *ec = dev_get_platdata(dev);
+	bool enabled;
+	int ret;
+
+	ret = wilco_ec_get_peak_shift_enable(ec, &enabled);
+	if (ret < 0)
+		return ret;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", enabled);
+}
+
+static ssize_t peak_shift_enable_store(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t count)
+{
+	struct wilco_ec_device *ec = dev_get_platdata(dev);
+	bool enable;
+	int ret;
+
+	if (strtobool(buf, &enable) < 0)
+		return -EINVAL;
+
+	ret = wilco_ec_set_peak_shift_enable(ec, enable);
+	if (ret < 0)
+		return ret;
+
+	return count;
+}
+
+static struct device_attribute dev_attr_peak_shift_enable =
+		__ATTR(enable, 0644,
+		       peak_shift_enable_show, peak_shift_enable_store);
+
+static ssize_t advanced_charging_enable_show(struct device *dev,
+					     struct device_attribute *attr,
+					     char *buf)
+{
+	struct wilco_ec_device *ec = dev_get_platdata(dev);
+	bool enabled;
+	int ret;
+
+	ret = wilco_ec_get_adv_charging_enable(ec, &enabled);
+	if (ret < 0)
+		return ret;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", enabled);
+}
+static ssize_t advanced_charging_enable_store(struct device *dev,
+					      struct device_attribute *attr,
+					      const char *buf, size_t count)
+{
+	struct wilco_ec_device *ec = dev_get_platdata(dev);
+	bool enable;
+	int ret;
+
+	if (strtobool(buf, &enable) < 0)
+		return -EINVAL;
+
+	ret = wilco_ec_set_adv_charging_enable(ec, enable);
+	if (ret < 0)
+		return ret;
+
+	return count;
+}
+
+static struct device_attribute dev_attr_advanced_charging_enable =
+		__ATTR(enable, 0644,
+		       advanced_charging_enable_show,
+		       advanced_charging_enable_store);
+
+static ssize_t
+peak_shift_battery_threshold_show(struct device *dev,
+				  struct device_attribute *attr, char *buf)
+{
+	struct wilco_ec_device *ec = dev_get_platdata(dev);
+	int ret, val;
+
+	ret = wilco_ec_get_peak_shift_battery_threshold(ec, &val);
+	if (ret < 0)
+		return ret;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", val);
+}
+
+static ssize_t
+peak_shift_battery_threshold_store(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t count)
+{
+	struct wilco_ec_device *ec = dev_get_platdata(dev);
+	int ret, val;
+
+	if (kstrtoint(buf, 10, &val) < 0)
+		return -EINVAL;
+
+	ret = wilco_ec_set_peak_shift_battery_threshold(ec, val);
+	if (ret < 0)
+		return ret;
+
+	return count;
+}
+
+struct device_attribute dev_attr_peak_shift_battery_threshold =
+		__ATTR(battery_threshold, 0644,
+		       peak_shift_battery_threshold_show,
+		       peak_shift_battery_threshold_store);
+
+struct wilco_schedule_attribute {
+	struct device_attribute dev_attr;
+	int day_of_week;	/* 0==Sunday, 1==Monday, ... */
+};
+
+#define to_wilco_schedule_attr(_dev_attr) \
+	container_of(_dev_attr, struct wilco_schedule_attribute, dev_attr)
+
+static ssize_t advanced_charging_schedule_show(struct device *dev,
+					       struct device_attribute *attr,
+					       char *buf)
+{
+	struct wilco_ec_device *ec = dev_get_platdata(dev);
+	struct wilco_schedule_attribute *wsa;
+	struct adv_charge_schedule sched;
+	int ret;
+
+	wsa = to_wilco_schedule_attr(attr);
+	sched.day_of_week = wsa->day_of_week;
+	ret = wilco_ec_get_adv_charge_schedule(ec, &sched);
+	if (ret < 0)
+		return ret;
+
+	return snprintf(buf, PAGE_SIZE, "%02d:%02d %02d:%02d\n",
+			sched.start_hour, sched.start_minute,
+			sched.duration_hour, sched.duration_minute);
+}
+
+static ssize_t advanced_charging_schedule_store(struct device *dev,
+						struct device_attribute *attr,
+						const char *buf, size_t count)
+{
+	struct wilco_ec_device *ec = dev_get_platdata(dev);
+	struct wilco_schedule_attribute *wsa;
+	struct adv_charge_schedule sched;
+	int ret;
+
+	ret = sscanf(buf, "%d:%d %d:%d",
+		     &sched.start_hour, &sched.start_minute,
+		     &sched.duration_hour, &sched.duration_minute);
+	if (ret != 4)
+		return -EINVAL;
+
+	wsa = to_wilco_schedule_attr(attr);
+	sched.day_of_week = wsa->day_of_week;
+	ret = wilco_ec_set_adv_charge_schedule(ec, &sched);
+	if (ret < 0)
+		return ret;
+
+	return count;
+}
+
+#define ADVANCED_CHARGING_SCHED_ATTR(_name, _day_of_week)		\
+	struct wilco_schedule_attribute adv_charging_sched_attr_##_name = { \
+		.dev_attr = __ATTR(_name, 0644,				\
+				   advanced_charging_schedule_show,	\
+				   advanced_charging_schedule_store),	\
+		.day_of_week = _day_of_week				\
+	}
+
+static ADVANCED_CHARGING_SCHED_ATTR(sunday, 0);
+static ADVANCED_CHARGING_SCHED_ATTR(monday, 1);
+static ADVANCED_CHARGING_SCHED_ATTR(tuesday, 2);
+static ADVANCED_CHARGING_SCHED_ATTR(wednesday, 3);
+static ADVANCED_CHARGING_SCHED_ATTR(thursday, 4);
+static ADVANCED_CHARGING_SCHED_ATTR(friday, 5);
+static ADVANCED_CHARGING_SCHED_ATTR(saturday, 6);
+
+static struct attribute *wilco_advanced_charging_attrs[] = {
+	&dev_attr_advanced_charging_enable.attr,
+	&adv_charging_sched_attr_sunday.dev_attr.attr,
+	&adv_charging_sched_attr_monday.dev_attr.attr,
+	&adv_charging_sched_attr_tuesday.dev_attr.attr,
+	&adv_charging_sched_attr_wednesday.dev_attr.attr,
+	&adv_charging_sched_attr_thursday.dev_attr.attr,
+	&adv_charging_sched_attr_friday.dev_attr.attr,
+	&adv_charging_sched_attr_saturday.dev_attr.attr,
+	NULL,
+};
+
+static struct attribute_group wilco_advanced_charging_attr_group = {
+	.name = "advanced_charging",
+	.attrs = wilco_advanced_charging_attrs,
+};
+
+static ssize_t peak_shift_schedule_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	struct wilco_ec_device *ec = dev_get_platdata(dev);
+	struct wilco_schedule_attribute *wsa;
+	struct peak_shift_schedule sched;
+	int ret;
+
+	wsa = to_wilco_schedule_attr(attr);
+	sched.day_of_week = wsa->day_of_week;
+	ret = wilco_ec_get_peak_shift_schedule(ec, &sched);
+	if (ret < 0)
+		return ret;
+
+	return snprintf(buf, PAGE_SIZE, "%02d:%02d %02d:%02d %02d:%02d\n",
+			sched.start_hour, sched.start_minute,
+			sched.end_hour, sched.end_minute,
+			sched.charge_start_hour, sched.charge_start_minute);
+}
+
+static ssize_t peak_shift_schedule_store(struct device *dev,
+					 struct device_attribute *attr,
+					 const char *buf, size_t count)
+{
+	struct wilco_ec_device *ec = dev_get_platdata(dev);
+	struct wilco_schedule_attribute *wsa;
+	struct peak_shift_schedule sched;
+	int ret;
+
+	ret = sscanf(buf, "%d:%d %d:%d %d:%d",
+		     &sched.start_hour, &sched.start_minute,
+		     &sched.end_hour, &sched.end_minute,
+		     &sched.charge_start_hour, &sched.charge_start_minute);
+	if (ret != 6)
+		return -EINVAL;
+
+	wsa = to_wilco_schedule_attr(attr);
+	sched.day_of_week = wsa->day_of_week;
+	ret = wilco_ec_set_peak_shift_schedule(ec, &sched);
+	if (ret < 0)
+		return ret;
+
+	return count;
+}
+
+#define PEAK_SHIFT_SCHED_ATTR(_name, _day_of_week)			\
+	struct wilco_schedule_attribute peak_shift_sched_attr_##_name = { \
+		.dev_attr = __ATTR(_name, 0644,				\
+				   peak_shift_schedule_show,		\
+				   peak_shift_schedule_store),		\
+		.day_of_week = _day_of_week				\
+	}
+
+static PEAK_SHIFT_SCHED_ATTR(sunday, 0);
+static PEAK_SHIFT_SCHED_ATTR(monday, 1);
+static PEAK_SHIFT_SCHED_ATTR(tuesday, 2);
+static PEAK_SHIFT_SCHED_ATTR(wednesday, 3);
+static PEAK_SHIFT_SCHED_ATTR(thursday, 4);
+static PEAK_SHIFT_SCHED_ATTR(friday, 5);
+static PEAK_SHIFT_SCHED_ATTR(saturday, 6);
+
+static struct attribute *wilco_peak_shift_attrs[] = {
+	&dev_attr_peak_shift_enable.attr,
+	&dev_attr_peak_shift_battery_threshold.attr,
+	&peak_shift_sched_attr_sunday.dev_attr.attr,
+	&peak_shift_sched_attr_monday.dev_attr.attr,
+	&peak_shift_sched_attr_tuesday.dev_attr.attr,
+	&peak_shift_sched_attr_wednesday.dev_attr.attr,
+	&peak_shift_sched_attr_thursday.dev_attr.attr,
+	&peak_shift_sched_attr_friday.dev_attr.attr,
+	&peak_shift_sched_attr_saturday.dev_attr.attr,
+	NULL,
+};
+
+static struct attribute_group wilco_peak_shift_attr_group = {
+	.name = "peak_shift",
+	.attrs = wilco_peak_shift_attrs,
+};
+
+static const struct attribute_group *wilco_charge_schedule_attr_groups[] = {
+	&wilco_advanced_charging_attr_group,
+	&wilco_peak_shift_attr_group,
+	NULL
+};
+
+static int wilco_charge_schedule_probe(struct platform_device *pdev)
+{
+	return devm_device_add_groups(&pdev->dev,
+				      wilco_charge_schedule_attr_groups);
+}
+
+static struct platform_driver wilco_charge_schedule_driver = {
+	.probe	= wilco_charge_schedule_probe,
+	.driver = {
+		.name = DRV_NAME,
+	}
+};
+module_platform_driver(wilco_charge_schedule_driver);
+
+MODULE_ALIAS("platform:" DRV_NAME);
+MODULE_AUTHOR("Nick Crews <ncrews@chromium.org>");
+MODULE_LICENSE("GPL v2");
+MODULE_DESCRIPTION("Wilco EC charge scheduling driver");
diff -ruN a/drivers/platform/chrome/wilco_ec/core.c b/drivers/platform/chrome/wilco_ec/core.c
--- a/drivers/platform/chrome/wilco_ec/core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/chrome/wilco_ec/core.c	2021-12-23 08:35:44.000000000 +0100
@@ -107,6 +107,16 @@
 		ret = PTR_ERR(ec->charger_pdev);
 		goto remove_sysfs;
 	}
+	/* Register child device to be found by charge scheduling driver. */
+	ec->charge_schedule_pdev = platform_device_register_data(dev,
+			"wilco-charge-schedule", PLATFORM_DEVID_NONE,
+			ec, sizeof(*ec));
+	if (IS_ERR(ec->charge_schedule_pdev)) {
+		dev_err(dev,
+			"Failed to create charge schedule platform device\n");
+		ret = PTR_ERR(ec->charge_schedule_pdev);
+		goto unregister_charge_config;
+	}
 
 	/* Register child device that will be found by the telemetry driver. */
 	ec->telem_pdev = platform_device_register_data(dev, "wilco_telem",
@@ -115,11 +125,13 @@
 	if (IS_ERR(ec->telem_pdev)) {
 		dev_err(dev, "Failed to create telemetry platform device\n");
 		ret = PTR_ERR(ec->telem_pdev);
-		goto unregister_charge_config;
+		goto unregister_charge_schedule;
 	}
 
 	return 0;
 
+unregister_charge_schedule:
+	platform_device_unregister(ec->charge_schedule_pdev);
 unregister_charge_config:
 	platform_device_unregister(ec->charger_pdev);
 remove_sysfs:
@@ -138,6 +150,7 @@
 	struct wilco_ec_device *ec = platform_get_drvdata(pdev);
 
 	platform_device_unregister(ec->telem_pdev);
+	platform_device_unregister(ec->charge_schedule_pdev);
 	platform_device_unregister(ec->charger_pdev);
 	wilco_ec_remove_sysfs(ec);
 	platform_device_unregister(ec->rtc_pdev);
diff -ruN a/drivers/platform/chrome/wilco_ec/Kconfig b/drivers/platform/chrome/wilco_ec/Kconfig
--- a/drivers/platform/chrome/wilco_ec/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/chrome/wilco_ec/Kconfig	2021-12-23 08:35:44.000000000 +0100
@@ -30,6 +30,18 @@
 	  over ACPI, and a driver queues up the events to be read by a
 	  userspace daemon from /dev/wilco_event using read() and poll().
 
+config WILCO_EC_CHARGE_SCHEDULE
+	tristate "Enable Peak Shift and Advanced Battery Charging support"
+	depends on WILCO_EC
+	help
+	  If you say Y here, you get support to control two charge-scheduling
+	  policies managed by the EC, Peak Shift and Advanced Charging. Peak
+	  Shift is a power saving policy that minimizes AC usage during the
+	  peak-usage times of the day. Advanced Charging Mode maximizes battery
+	  health by adjusting the charging algorithm throughout the day. For
+	  userspace interface and more info see
+	  Documentation/ABI/testing/sysfs-platform-wilco-ec
+
 config WILCO_EC_TELEMETRY
 	tristate "Enable querying telemetry data from EC"
 	depends on WILCO_EC
diff -ruN a/drivers/platform/chrome/wilco_ec/Makefile b/drivers/platform/chrome/wilco_ec/Makefile
--- a/drivers/platform/chrome/wilco_ec/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/chrome/wilco_ec/Makefile	2021-12-23 08:35:44.000000000 +0100
@@ -7,5 +7,8 @@
 obj-$(CONFIG_WILCO_EC_DEBUGFS)		+= wilco_ec_debugfs.o
 wilco_ec_events-objs			:= event.o
 obj-$(CONFIG_WILCO_EC_EVENTS)		+= wilco_ec_events.o
+wilco_charge_schedule-objs		:= charge_schedule.o \
+					   charge_schedule_sysfs.o
+obj-$(CONFIG_WILCO_EC_CHARGE_SCHEDULE)	+= wilco_charge_schedule.o
 wilco_ec_telem-objs			:= telemetry.o
 obj-$(CONFIG_WILCO_EC_TELEMETRY)	+= wilco_ec_telem.o
diff -ruN a/drivers/platform/x86/chromeos_acpi.c b/drivers/platform/x86/chromeos_acpi.c
--- a/drivers/platform/x86/chromeos_acpi.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/platform/x86/chromeos_acpi.c	2021-12-23 08:35:44.000000000 +0100
@@ -0,0 +1,794 @@
+ /*
+ *  chromeos_acpi.c - ChromeOS specific ACPI support
+ *
+ *
+ * Copyright (C) 2011 The Chromium OS Authors
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ *
+ * This driver attaches to the ChromeOS ACPI device and the exports the values
+ * reported by the ACPI in a sysfs directory
+ * (/sys/devices/platform/chromeos_acpi).
+ *
+ * The first version of the driver provides only static information; the
+ * values reported by the driver are the snapshot reported by the ACPI at
+ * driver installation time.
+ *
+ * All values are presented in the string form (numbers as decimal values) and
+ * can be accessed as the contents of the appropriate read only files in the
+ * sysfs directory tree originating in /sys/devices/platform/chromeos_acpi.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/nvram.h>
+#include <linux/platform_device.h>
+#include <linux/acpi.h>
+
+#include "../chrome/chromeos.h"
+
+#define CHNV_DEBUG_RESET_FLAG	0x40	     /* flag for S3 reboot */
+#define CHNV_RECOVERY_FLAG	0x80	     /* flag for recovery reboot */
+
+#define CHSW_RECOVERY_FW	0x00000002   /* recovery button depressed */
+#define CHSW_RECOVERY_EC	0x00000004   /* recovery button depressed */
+#define CHSW_DEVELOPER_MODE	0x00000020   /* developer switch set */
+#define CHSW_WP			0x00000200   /* write-protect (optional) */
+
+/*
+ * Structure containing one ACPI exported integer along with the validity
+ * flag.
+ */
+struct chromeos_acpi_datum {
+	unsigned cad_value;
+	bool	 cad_is_set;
+};
+
+/*
+ * Structure containing the set of ACPI exported integers required by chromeos
+ * wrapper.
+ */
+struct chromeos_acpi_if {
+	struct chromeos_acpi_datum	switch_state;
+
+	/* chnv is a single byte offset in nvram. exported by older firmware */
+	struct chromeos_acpi_datum	chnv;
+
+	/* vbnv is an address range in nvram, exported by newer firmware */
+	struct chromeos_acpi_datum	nv_base;
+	struct chromeos_acpi_datum	nv_size;
+};
+
+#define MY_LOGPREFIX "chromeos_acpi: "
+#define MY_ERR KERN_ERR MY_LOGPREFIX
+#define MY_NOTICE KERN_NOTICE MY_LOGPREFIX
+#define MY_INFO KERN_INFO MY_LOGPREFIX
+
+/* ACPI method name for MLST; the response for this method is a
+ * package of strings listing the methods which should be reflected in
+ * sysfs. */
+#define MLST_METHOD "MLST"
+
+static const struct acpi_device_id chromeos_device_ids[] = {
+	{"GGL0001", 0}, /* Google's own */
+	{"", 0},
+};
+
+MODULE_DEVICE_TABLE(acpi, chromeos_device_ids);
+
+static int chromeos_device_add(struct acpi_device *device);
+static int chromeos_device_remove(struct acpi_device *device);
+
+static struct chromeos_acpi_if chromeos_acpi_if_data;
+static struct acpi_driver chromeos_acpi_driver = {
+	.name = "ChromeOS Device",
+	.class = "ChromeOS",
+	.ids = chromeos_device_ids,
+	.ops = {
+		.add = chromeos_device_add,
+		.remove = chromeos_device_remove,
+		},
+	.owner = THIS_MODULE,
+};
+
+/* The default list of methods the chromeos ACPI device is supposed to export,
+ * if the MLST method is not present or is poorly formed.  The MLST method
+ * itself is included, to aid in debugging. */
+static char *default_methods[] = {
+	"CHSW", "HWID", "BINF", "GPIO", "CHNV", "FWID", "FRID", MLST_METHOD
+};
+
+/*
+ * Representation of a single sys fs attribute. In addition to the standard
+ * device_attribute structure has a link field, allowing to create a list of
+ * these structures (to keep track for de-allocation when removing the driver)
+ * and a pointer to the actual attribute value, reported when accessing the
+ * appropriate sys fs file
+ */
+struct acpi_attribute {
+	struct device_attribute dev_attr;
+	struct acpi_attribute *next_acpi_attr;
+	char *value;
+};
+
+/*
+ * Representation of a sys fs attribute group (a sub directory in the device's
+ * sys fs directory). In addition to the standard structure has a link to
+ * allow to keep track of the allocated structures.
+ */
+struct acpi_attribute_group {
+	struct attribute_group ag;
+	struct acpi_attribute_group *next_acpi_attr_group;
+};
+
+/*
+ * ChromeOS ACPI device wrapper adds links pointing at lists of allocated
+ * attributes and attribute groups.
+ */
+struct chromeos_acpi_dev {
+	struct platform_device *p_dev;
+	struct acpi_attribute *attributes;
+	struct acpi_attribute_group *groups;
+};
+
+static struct chromeos_acpi_dev chromeos_acpi = { };
+
+static bool chromeos_on_legacy_firmware(void)
+{
+	/*
+	 * Presense of the CHNV ACPI element implies running on a legacy
+	 * firmware
+	 */
+	return chromeos_acpi_if_data.chnv.cad_is_set;
+}
+
+/*
+ * This function operates on legacy BIOSes which do not export VBNV element
+ * through ACPI. These BIOSes use a fixed location in NVRAM to contain a
+ * bitmask of known flags.
+ *
+ * @flag - the bitmask to set, it is the responsibility of the caller to set
+ *         the proper bits.
+ *
+ * returns 0 on success (is running in legacy mode and chnv is initialized) or
+ *         -1 otherwise.
+ */
+static int chromeos_set_nvram_flag(u8 flag)
+{
+	u8 cur;
+	unsigned index = chromeos_acpi_if_data.chnv.cad_value;
+
+	if (!chromeos_on_legacy_firmware())
+		return -ENODEV;
+
+	cur = nvram_read_byte(index);
+
+	if ((cur & flag) != flag)
+		nvram_write_byte(cur | flag, index);
+	return 0;
+}
+
+int chromeos_legacy_set_need_recovery(void)
+{
+	return chromeos_set_nvram_flag(CHNV_RECOVERY_FLAG);
+}
+
+/*
+ * Read the nvram buffer contents into the user provided space.
+ *
+ * retrun number of bytes copied, or -1 on any error.
+ */
+static ssize_t chromeos_vbc_nvram_read(void *buf, size_t count)
+{
+
+	int base, size, i;
+
+	if (!chromeos_acpi_if_data.nv_base.cad_is_set ||
+	    !chromeos_acpi_if_data.nv_size.cad_is_set) {
+		printk(MY_ERR "%s: NVRAM not configured!\n", __func__);
+		return -ENODEV;
+	}
+
+	base = chromeos_acpi_if_data.nv_base.cad_value;
+	size = chromeos_acpi_if_data.nv_size.cad_value;
+
+	if (count < size) {
+		pr_err("%s: not enough room to read nvram (%zd < %d)\n",
+		       __func__, count, size);
+		return -EINVAL;
+	}
+
+	for (i = 0; i < size; i++)
+		((u8 *)buf)[i] = nvram_read_byte(base++);
+
+	return size;
+}
+
+static ssize_t chromeos_vbc_nvram_write(const void *buf, size_t count)
+{
+	unsigned base, size, i;
+
+	if (!chromeos_acpi_if_data.nv_base.cad_is_set ||
+	    !chromeos_acpi_if_data.nv_size.cad_is_set) {
+		printk(MY_ERR "%s: NVRAM not configured!\n", __func__);
+		return -ENODEV;
+	}
+
+	size = chromeos_acpi_if_data.nv_size.cad_value;
+	base = chromeos_acpi_if_data.nv_base.cad_value;
+
+	if (count != size) {
+		printk(MY_ERR "%s: wrong buffer size (%zd != %d)!\n", __func__,
+		       count, size);
+		return -EINVAL;
+	}
+
+	for (i = 0; i < size; i++) {
+		u8 c;
+
+		c = nvram_read_byte(base + i);
+		if (c == ((u8 *)buf)[i])
+			continue;
+		nvram_write_byte(((u8 *)buf)[i], base + i);
+	}
+	return size;
+}
+
+/*
+ * To show attribute value just access the container structure's `value'
+ * field.
+ */
+static ssize_t show_acpi_attribute(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	struct acpi_attribute *paa;
+
+	paa = container_of(attr, struct acpi_attribute, dev_attr);
+	return snprintf(buf, PAGE_SIZE, "%s", paa->value);
+}
+
+/*
+ * create_sysfs_attribute() create and initialize an ACPI sys fs attribute
+ *			    structure.
+ * @value: attribute value
+ * @name: base attribute name
+ * @count: total number of instances of this attribute
+ * @instance: instance number of this particular attribute
+ *
+ * This function allocates and initializes the structure containing all
+ * information necessary to add a sys fs attribute. In case the attribute has
+ * just a single instance, the attribute file name is equal to the @name
+ * parameter . In case the attribute has several instances, the attribute
+ * file name is @name.@instance.
+ *
+ * Returns: a pointer to the allocated and initialized structure, or null if
+ * allocation failed.
+ *
+ * As a side effect, the allocated structure is added to the list in the
+ * chromeos_acpi structure. Note that the actual attribute creation is not
+ * attempted yet, in case of creation error the structure would not have an
+ * actual attribute associated with it, so when de-installing the driver this
+ * structure would be used to try to remove an attribute which does not exist.
+ * This is considered acceptable, as there is no reason for sys fs attribute
+ * creation failure.
+ */
+static struct acpi_attribute *create_sysfs_attribute(char *value, char *name,
+						     int count, int instance)
+{
+	struct acpi_attribute *paa;
+	int total_size, room_left;
+	int value_len = strlen(value);
+
+	if (!value_len)
+		return NULL;
+
+	value_len++; /* include the terminating zero */
+
+	/*
+	 * total allocation size includes (all strings with including
+	 * terminating zeros):
+	 *
+	 * - value string
+	 * - attribute structure size
+	 * - name string
+	 * - suffix string (in case there are multiple instances)
+	 * - dot separating the instance suffix
+	 */
+
+	total_size = value_len + sizeof(struct acpi_attribute) +
+			strlen(name) + 1;
+
+	if (count != 1) {
+		if (count >= 1000) {
+			printk(MY_ERR "%s: too many (%d) instances of %s\n",
+			       __func__, count, name);
+			return NULL;
+		}
+		/* allow up to three digits and the dot */
+		total_size += 4;
+	}
+
+	paa = kzalloc(total_size, GFP_KERNEL);
+	if (!paa) {
+		printk(MY_ERR "out of memory in %s!\n", __func__);
+		return NULL;
+	}
+
+	sysfs_attr_init(&paa->dev_attr.attr);
+	paa->dev_attr.attr.mode = 0444;  /* read only */
+	paa->dev_attr.show = show_acpi_attribute;
+	paa->value = (char *)(paa + 1);
+	strcpy(paa->value, value);
+	paa->dev_attr.attr.name = paa->value + value_len;
+
+	room_left = total_size - value_len -
+			offsetof(struct acpi_attribute, value);
+
+	if (count == 1) {
+		snprintf((char *)paa->dev_attr.attr.name, room_left, name);
+	} else {
+		snprintf((char *)paa->dev_attr.attr.name, room_left,
+			 "%s.%d", name, instance);
+	}
+
+	paa->next_acpi_attr = chromeos_acpi.attributes;
+	chromeos_acpi.attributes = paa;
+
+	return paa;
+}
+
+/*
+ * add_sysfs_attribute() create and initialize an ACPI sys fs attribute
+ *			    structure and create the attribute.
+ * @value: attribute value
+ * @name: base attribute name
+ * @count: total number of instances of this attribute
+ * @instance: instance number of this particular attribute
+ */
+
+static void add_sysfs_attribute(char *value, char *name,
+				int count, int instance)
+{
+	struct acpi_attribute *paa =
+	    create_sysfs_attribute(value, name, count, instance);
+
+	if (!paa)
+		return;
+
+	if (device_create_file(&chromeos_acpi.p_dev->dev, &paa->dev_attr))
+		printk(MY_ERR "failed to create attribute for %s\n", name);
+}
+
+/*
+ * handle_nested_acpi_package() create sysfs group including attributes
+ *				representing a nested ACPI package.
+ *
+ * @po: package contents as returned by ACPI
+ * @pm: name of the group
+ * @total: number of instances of this package
+ * @instance: instance number of this particular group
+ *
+ * The created group is called @pm in case there is a single instance, or
+ * @pm.@instance otherwise.
+ *
+ * All group and attribute storage allocations are included in the lists for
+ * tracking of allocated memory.
+ */
+static void handle_nested_acpi_package(union acpi_object *po, char *pm,
+				       int total, int instance)
+{
+	int i, size, count, j;
+	struct acpi_attribute_group *aag;
+
+	count = po->package.count;
+
+	size = strlen(pm) + 1 + sizeof(struct acpi_attribute_group) +
+	    sizeof(struct attribute *) * (count + 1);
+
+	if (total != 1) {
+		if (total >= 1000) {
+			printk(MY_ERR "%s: too many (%d) instances of %s\n",
+			       __func__, total, pm);
+			return;
+		}
+		/* allow up to three digits and the dot */
+		size += 4;
+	}
+
+	aag = kzalloc(size, GFP_KERNEL);
+	if (!aag) {
+		printk(MY_ERR "out of memory in %s!\n", __func__);
+		return;
+	}
+
+	aag->next_acpi_attr_group = chromeos_acpi.groups;
+	chromeos_acpi.groups = aag->next_acpi_attr_group;
+	aag->ag.attrs = (struct attribute **)(aag + 1);
+	aag->ag.name = (const char *)&aag->ag.attrs[count + 1];
+
+	/* room left in the buffer */
+	size = size - (aag->ag.name - (char *)aag);
+
+	if (total != 1)
+		snprintf((char *)aag->ag.name, size, "%s.%d", pm, instance);
+	else
+		snprintf((char *)aag->ag.name, size, "%s", pm);
+
+	j = 0;			/* attribute index */
+	for (i = 0; i < count; i++) {
+		union acpi_object *element = po->package.elements + i;
+		int copy_size = 0;
+		char attr_value[40];	/* 40 chars be enough for names */
+		struct acpi_attribute *paa;
+
+		switch (element->type) {
+		case ACPI_TYPE_INTEGER:
+			copy_size = snprintf(attr_value, sizeof(attr_value),
+					     "%d", (int)element->integer.value);
+			paa = create_sysfs_attribute(attr_value, pm, count, i);
+			break;
+
+		case ACPI_TYPE_STRING:
+			copy_size = min(element->string.length,
+					(u32)(sizeof(attr_value)) - 1);
+			memcpy(attr_value, element->string.pointer, copy_size);
+			attr_value[copy_size] = '\0';
+			paa = create_sysfs_attribute(attr_value, pm, count, i);
+			break;
+
+		default:
+			printk(MY_ERR "ignoring nested type %d\n",
+			       element->type);
+			continue;
+		}
+		aag->ag.attrs[j++] = &paa->dev_attr.attr;
+	}
+
+	if (sysfs_create_group(&chromeos_acpi.p_dev->dev.kobj, &aag->ag))
+		printk(MY_ERR "failed to create group %s.%d\n", pm, instance);
+}
+
+/*
+ * maybe_export_acpi_int() export a single int value when required
+ *
+ * @pm: name of the package
+ * @index: index of the element of the package
+ * @value: value of the element
+ */
+static void maybe_export_acpi_int(const char *pm, int index, unsigned value)
+{
+	int i;
+	struct chromeos_acpi_exported_ints {
+		const char *acpi_name;
+		int acpi_index;
+		struct chromeos_acpi_datum *cad;
+	} exported_ints[] = {
+		{ "VBNV", 0, &chromeos_acpi_if_data.nv_base },
+		{ "VBNV", 1, &chromeos_acpi_if_data.nv_size },
+		{ "CHSW", 0, &chromeos_acpi_if_data.switch_state },
+		{ "CHNV", 0, &chromeos_acpi_if_data.chnv }
+	};
+
+	for (i = 0; i < ARRAY_SIZE(exported_ints); i++) {
+		struct chromeos_acpi_exported_ints *exported_int;
+
+		exported_int = exported_ints + i;
+
+		if (!strncmp(pm, exported_int->acpi_name, 4) &&
+		    (exported_int->acpi_index == index)) {
+			printk(MY_NOTICE "registering %s %d\n", pm, index);
+			exported_int->cad->cad_value = value;
+			exported_int->cad->cad_is_set = true;
+			return;
+		}
+	}
+}
+
+/*
+ * acpi_buffer_to_string() convert contents of an ACPI buffer element into a
+ *		hex string truncating it if necessary to fit into one page.
+ *
+ * @element: an acpi element known to contain an ACPI buffer.
+ *
+ * Returns: pointer to an ASCII string containing the buffer representation
+ *	    (whatever fit into PAGE_SIZE). The caller is responsible for
+ *	    freeing the memory.
+ */
+static char *acpi_buffer_to_string(union acpi_object *element)
+{
+	char *base, *p;
+	int i;
+	unsigned room_left;
+	/* Include this many characters per line */
+	unsigned char_per_line = 16;
+	unsigned blob_size;
+	unsigned string_buffer_size;
+
+	/*
+	 * As of now the VDAT structure can supply as much as 3700 bytes. When
+	 * expressed as a hex dump it becomes 3700 * 3 + 3700/16 + .. which
+	 * clearly exceeds the maximum allowed sys fs buffer size of one page
+	 * (4k).
+	 *
+	 * What this means is that we can't keep the entire blob in one sysfs
+	 * file. Currently verified boot (the consumer of the VDAT contents)
+	 * does not care about the most of the data, so as a quick fix we will
+	 * truncate it here. Once the blob data beyond the 4K boundary is
+	 * required this approach will have to be reworked.
+	 *
+	 * TODO(vbendeb): Split the data into multiple VDAT instances, each
+	 * not exceeding 4K or consider exporting as a binary using
+	 * sysfs_create_bin_file().
+	 */
+
+	/*
+	 * X, the maximum number of bytes which will fit into a sysfs file
+	 * (one memory page) can be derived from the following equation (where
+	 * N is number of bytes included in every hex string):
+	 *
+	 * 3X + X/N + 4 <= PAGE_SIZE.
+	 *
+	 * Solving this for X gives the following
+	 */
+	blob_size = ((PAGE_SIZE - 4) * char_per_line) / (char_per_line * 3 + 1);
+
+	if (element->buffer.length > blob_size)
+		printk(MY_INFO "truncating buffer from %d to %d\n",
+		       element->buffer.length, blob_size);
+	else
+		blob_size = element->buffer.length;
+
+	string_buffer_size =
+		/* three characters to display one byte */
+		blob_size * 3 +
+		/* one newline per line, all rounded up, plus
+		 * extra newline in the end, plus terminating
+		 * zero, hence + 4
+		 */
+		blob_size/char_per_line + 4;
+
+	p = kzalloc(string_buffer_size, GFP_KERNEL);
+	if (!p) {
+		printk(MY_ERR "out of memory in %s!\n", __func__);
+		return NULL;
+	}
+
+	base = p;
+	room_left = string_buffer_size;
+	for (i = 0; i < blob_size; i++) {
+		int printed;
+		printed = snprintf(p, room_left, " %2.2x",
+				   element->buffer.pointer[i]);
+		room_left -= printed;
+		p += printed;
+		if (((i + 1) % char_per_line) == 0) {
+			if (!room_left)
+				break;
+			room_left--;
+			*p++ = '\n';
+		}
+	}
+	if (room_left < 2) {
+		printk(MY_ERR "%s: no room in the buffer!\n", __func__);
+		*p = '\0';
+	} else {
+		*p++ = '\n';
+		*p++ = '\0';
+	}
+	return base;
+}
+
+/*
+ * handle_acpi_package() create sysfs group including attributes
+ *			 representing an ACPI package.
+ *
+ * @po: package contents as returned by ACPI
+ * @pm: name of the group
+ *
+ * Scalar objects included in the package get sys fs attributes created for
+ * them. Nested packages are passed to a function creating a sys fs group per
+ * package.
+ */
+static void handle_acpi_package(union acpi_object *po, char *pm)
+{
+	int j;
+	int count = po->package.count;
+	for (j = 0; j < count; j++) {
+		union acpi_object *element = po->package.elements + j;
+		int copy_size = 0;
+		char attr_value[256];	/* strings could be this long */
+
+		switch (element->type) {
+		case ACPI_TYPE_INTEGER:
+			copy_size = snprintf(attr_value, sizeof(attr_value),
+					     "%d", (int)element->integer.value);
+			add_sysfs_attribute(attr_value, pm, count, j);
+			maybe_export_acpi_int(pm, j, (unsigned)
+					      element->integer.value);
+			break;
+
+		case ACPI_TYPE_STRING:
+			copy_size = min(element->string.length,
+					(u32)(sizeof(attr_value)) - 1);
+			memcpy(attr_value, element->string.pointer, copy_size);
+			attr_value[copy_size] = '\0';
+			add_sysfs_attribute(attr_value, pm, count, j);
+			break;
+
+		case ACPI_TYPE_BUFFER: {
+			char *buf_str;
+			buf_str = acpi_buffer_to_string(element);
+			if (buf_str) {
+				add_sysfs_attribute(buf_str, pm, count, j);
+				kfree(buf_str);
+			}
+			break;
+		}
+		case ACPI_TYPE_PACKAGE:
+			handle_nested_acpi_package(element, pm, count, j);
+			break;
+
+		default:
+			printk(MY_ERR "ignoring type %d (%s)\n",
+			       element->type, pm);
+			break;
+		}
+	}
+}
+
+
+/*
+ * add_acpi_method() evaluate an ACPI method and create sysfs attributes.
+ *
+ * @device: ACPI device
+ * @pm: name of the method to evaluate
+ */
+static void add_acpi_method(struct acpi_device *device, char *pm)
+{
+	acpi_status status;
+	struct acpi_buffer output;
+	union acpi_object *po;
+
+	output.length = ACPI_ALLOCATE_BUFFER;
+	output.pointer = NULL;
+
+	status = acpi_evaluate_object(device->handle, pm, NULL, &output);
+
+	if (!ACPI_SUCCESS(status)) {
+		printk(MY_ERR "failed to retrieve %s (%d)\n", pm, status);
+		return;
+	}
+
+	po = output.pointer;
+
+	if (po->type != ACPI_TYPE_PACKAGE)
+		printk(MY_ERR "%s is not a package, ignored\n", pm);
+	else
+		handle_acpi_package(po, pm);
+	kfree(output.pointer);
+}
+
+/*
+ * chromeos_process_mlst() Evaluate the MLST method and add methods listed
+ *                         in the response.
+ *
+ * @device: ACPI device
+ *
+ * Returns: 0 if successful, non-zero if error.
+ */
+static int chromeos_process_mlst(struct acpi_device *device)
+{
+	acpi_status status;
+	struct acpi_buffer output;
+	union acpi_object *po;
+	int j;
+
+	output.length = ACPI_ALLOCATE_BUFFER;
+	output.pointer = NULL;
+
+	status = acpi_evaluate_object(device->handle, MLST_METHOD, NULL,
+				      &output);
+	if (!ACPI_SUCCESS(status)) {
+		pr_debug(MY_LOGPREFIX "failed to retrieve MLST (%d)\n",
+			 status);
+		return 1;
+	}
+
+	po = output.pointer;
+	if (po->type != ACPI_TYPE_PACKAGE) {
+		printk(MY_ERR MLST_METHOD "is not a package, ignored\n");
+		kfree(output.pointer);
+		return -EINVAL;
+	}
+
+	for (j = 0; j < po->package.count; j++) {
+		union acpi_object *element = po->package.elements + j;
+		int copy_size = 0;
+		char method[ACPI_NAMESEG_SIZE + 1];
+
+		if (element->type == ACPI_TYPE_STRING) {
+			copy_size = min(element->string.length,
+					(u32)ACPI_NAMESEG_SIZE);
+			memcpy(method, element->string.pointer, copy_size);
+			method[copy_size] = '\0';
+			add_acpi_method(device, method);
+		} else {
+			pr_debug(MY_LOGPREFIX "ignoring type %d\n",
+				 element->type);
+		}
+	}
+
+	kfree(output.pointer);
+	return 0;
+}
+
+static int chromeos_device_add(struct acpi_device *device)
+{
+	int i;
+
+	/* Attempt to add methods by querying the device's MLST method
+	 * for the list of methods. */
+	if (!chromeos_process_mlst(device))
+		return 0;
+
+	printk(MY_INFO "falling back to default list of methods\n");
+	for (i = 0; i < ARRAY_SIZE(default_methods); i++)
+		add_acpi_method(device, default_methods[i]);
+	return 0;
+}
+
+static int chromeos_device_remove(struct acpi_device *device)
+{
+	return 0;
+}
+
+static struct chromeos_vbc chromeos_vbc_nvram = {
+	.name = "chromeos_vbc_nvram",
+	.read = chromeos_vbc_nvram_read,
+	.write = chromeos_vbc_nvram_write,
+};
+
+static int __init chromeos_acpi_init(void)
+{
+	int ret = 0;
+
+	if (acpi_disabled)
+		return -ENODEV;
+
+	ret = chromeos_vbc_register(&chromeos_vbc_nvram);
+	if (ret)
+		return ret;
+
+	chromeos_acpi.p_dev = platform_device_register_simple("chromeos_acpi",
+							      -1, NULL, 0);
+	if (IS_ERR(chromeos_acpi.p_dev)) {
+		printk(MY_ERR "unable to register platform device\n");
+		return PTR_ERR(chromeos_acpi.p_dev);
+	}
+
+	ret = acpi_bus_register_driver(&chromeos_acpi_driver);
+	if (ret < 0) {
+		printk(MY_ERR "failed to register driver (%d)\n", ret);
+		platform_device_unregister(chromeos_acpi.p_dev);
+		chromeos_acpi.p_dev = NULL;
+		return ret;
+	}
+	printk(MY_INFO "installed%s\n",
+	       chromeos_on_legacy_firmware() ? " (legacy mode)" : "");
+
+	return 0;
+}
+
+subsys_initcall(chromeos_acpi_init);
diff -ruN a/drivers/platform/x86/Kconfig b/drivers/platform/x86/Kconfig
--- a/drivers/platform/x86/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/x86/Kconfig	2021-12-23 08:35:44.000000000 +0100
@@ -965,6 +965,16 @@
 config FW_ATTR_CLASS
 	tristate
 
+config ACPI_CHROMEOS
+	bool "ChromeOS specific ACPI extensions"
+	depends on ACPI
+	depends on CHROME_PLATFORMS
+	select NVRAM
+	select CHROMEOS
+	help
+	  This driver provides the firmware interface for the services exported
+	  through the CHROMEOS interfaces when using ChromeOS ACPI firmware.
+
 config INTEL_IMR
 	bool "Intel Isolated Memory Region support"
 	depends on X86_INTEL_QUARK && IOSF_MBI
diff -ruN a/drivers/platform/x86/Makefile b/drivers/platform/x86/Makefile
--- a/drivers/platform/x86/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/platform/x86/Makefile	2021-12-23 08:35:44.000000000 +0100
@@ -94,6 +94,7 @@
 obj-$(CONFIG_ACPI_TOSHIBA)	+= toshiba_acpi.o
 
 # Laptop drivers
+obj-$(CONFIG_ACPI_CHROMEOS)	+= chromeos_acpi.o
 obj-$(CONFIG_ACPI_CMPC)		+= classmate-laptop.o
 obj-$(CONFIG_COMPAL_LAPTOP)	+= compal-laptop.o
 obj-$(CONFIG_LG_LAPTOP)		+= lg-laptop.o
diff -ruN a/drivers/power/supply/cros_usbpd-charger.c b/drivers/power/supply/cros_usbpd-charger.c
--- a/drivers/power/supply/cros_usbpd-charger.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/power/supply/cros_usbpd-charger.c	2021-12-23 08:35:44.000000000 +0100
@@ -61,6 +61,7 @@
 	POWER_SUPPLY_PROP_CURRENT_MAX,
 	POWER_SUPPLY_PROP_VOLTAGE_MAX_DESIGN,
 	POWER_SUPPLY_PROP_VOLTAGE_NOW,
+	POWER_SUPPLY_PROP_CHARGE_CONTROL_LIMIT_MAX,
 	POWER_SUPPLY_PROP_MODEL_NAME,
 	POWER_SUPPLY_PROP_MANUFACTURER,
 	POWER_SUPPLY_PROP_USB_TYPE
@@ -124,6 +125,27 @@
 	return ret;
 }
 
+static int cros_usbpd_set_override_ports(struct charger_data *charger,
+					 int port_num)
+{
+	struct device *dev = charger->dev;
+	struct ec_params_charge_port_override req;
+	int ret;
+
+	req.override_port = port_num;
+
+	ret = cros_usbpd_charger_ec_command(charger, 0,
+		EC_CMD_PD_CHARGE_PORT_OVERRIDE,
+		(uint8_t *)&req, sizeof(req),
+		NULL, 0);
+	if (ret < 0) {
+		dev_warn(dev, "Port Override command returned 0x%x\n", ret);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
 static int cros_usbpd_charger_get_num_ports(struct charger_data *charger)
 {
 	struct ec_response_charge_port_count resp;
@@ -387,6 +409,7 @@
 	case POWER_SUPPLY_PROP_CURRENT_MAX:
 	case POWER_SUPPLY_PROP_VOLTAGE_MAX_DESIGN:
 	case POWER_SUPPLY_PROP_VOLTAGE_NOW:
+	case POWER_SUPPLY_PROP_CHARGE_CONTROL_LIMIT_MAX:
 		ret = cros_usbpd_charger_get_port_status(port, true);
 		if (ret < 0) {
 			dev_err(dev, "Failed to get port status (err:0x%x)\n",
@@ -414,6 +437,9 @@
 	case POWER_SUPPLY_PROP_VOLTAGE_NOW:
 		val->intval = port->psy_voltage_now * 1000;
 		break;
+	case POWER_SUPPLY_PROP_CHARGE_CONTROL_LIMIT_MAX:
+		val->intval = 0;
+		break;
 	case POWER_SUPPLY_PROP_USB_TYPE:
 		val->intval = port->psy_usb_type;
 		break;
@@ -449,8 +475,8 @@
 	struct port_data *port = power_supply_get_drvdata(psy);
 	struct charger_data *charger = port->charger;
 	struct device *dev = charger->dev;
+	int port_number, ret;
 	u16 intval;
-	int ret;
 
 	/* U16_MAX in mV/mA is the maximum supported value */
 	if (val->intval >= U16_MAX * 1000)
@@ -462,6 +488,17 @@
 		intval = val->intval / 1000;
 
 	switch (psp) {
+	case POWER_SUPPLY_PROP_CHARGE_CONTROL_LIMIT_MAX:
+		/*
+		 * A value of -1 implies switching to battery as the power
+		 * source. Any other value implies using this port as the
+		 * power source.
+		 */
+		port_number = val->intval;
+		if (port_number != -1)
+			port_number = port->port_number;
+		ret = cros_usbpd_set_override_ports(charger, port_number);
+		break;
 	case POWER_SUPPLY_PROP_INPUT_CURRENT_LIMIT:
 		ret = cros_usbpd_charger_set_ext_power_limit(charger, intval,
 							input_voltage_limit);
@@ -506,6 +543,7 @@
 	int ret;
 
 	switch (psp) {
+	case POWER_SUPPLY_PROP_CHARGE_CONTROL_LIMIT_MAX:
 	case POWER_SUPPLY_PROP_INPUT_CURRENT_LIMIT:
 	case POWER_SUPPLY_PROP_INPUT_VOLTAGE_LIMIT:
 		ret = 1;
@@ -658,6 +696,7 @@
 		port->psy = psy;
 
 		charger->ports[charger->num_registered_psy++] = port;
+		ec_device->charger = psy;
 	}
 
 	if (!charger->num_registered_psy) {
@@ -682,6 +721,7 @@
 	return 0;
 
 fail:
+	ec_device->charger = NULL;
 	WARN(1, "%s: Failing probe (err:0x%x)\n", dev_name(dev), ret);
 
 fail_nowarn:
diff -ruN a/drivers/pwm/core.c b/drivers/pwm/core.c
--- a/drivers/pwm/core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pwm/core.c	2021-12-23 08:35:44.000000000 +0100
@@ -271,6 +271,7 @@
 		pwm->chip = chip;
 		pwm->pwm = chip->base + i;
 		pwm->hwpwm = i;
+		pwm->state.output_type = PWM_OUTPUT_FIXED;
 
 		radix_tree_insert(&pwm_tree, pwm->pwm, pwm);
 	}
diff -ruN a/drivers/pwm/sysfs.c b/drivers/pwm/sysfs.c
--- a/drivers/pwm/sysfs.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/pwm/sysfs.c	2021-12-23 08:35:44.000000000 +0100
@@ -215,11 +215,35 @@
 	return sprintf(buf, "%u %u\n", result.period, result.duty_cycle);
 }
 
+static ssize_t output_type_show(struct device *child,
+			     struct device_attribute *attr,
+			     char *buf)
+{
+	const struct pwm_device *pwm = child_to_pwm_device(child);
+	const char *output_type = "unknown";
+	struct pwm_state state;
+
+	pwm_get_state(pwm, &state);
+	switch (state.output_type) {
+	case PWM_OUTPUT_FIXED:
+		output_type = "fixed";
+		break;
+	case PWM_OUTPUT_MODULATED:
+		output_type = "modulated";
+		break;
+	default:
+		break;
+	}
+
+	return snprintf(buf, PAGE_SIZE, "%s\n", output_type);
+}
+
 static DEVICE_ATTR_RW(period);
 static DEVICE_ATTR_RW(duty_cycle);
 static DEVICE_ATTR_RW(enable);
 static DEVICE_ATTR_RW(polarity);
 static DEVICE_ATTR_RO(capture);
+static DEVICE_ATTR_RO(output_type);
 
 static struct attribute *pwm_attrs[] = {
 	&dev_attr_period.attr,
@@ -227,6 +251,7 @@
 	&dev_attr_enable.attr,
 	&dev_attr_polarity.attr,
 	&dev_attr_capture.attr,
+	&dev_attr_output_type.attr,
 	NULL
 };
 ATTRIBUTE_GROUPS(pwm);
diff -ruN a/drivers/regulator/core.c b/drivers/regulator/core.c
--- a/drivers/regulator/core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/regulator/core.c	2021-12-23 08:35:44.000000000 +0100
@@ -1151,9 +1151,10 @@
 		}
 
 		if (current_uV < 0) {
-			rdev_err(rdev,
-				 "failed to get the current voltage: %pe\n",
-				 ERR_PTR(current_uV));
+			if (current_uV != -EPROBE_DEFER)
+				rdev_err(rdev,
+					 "failed to get the current voltage: %pe\n",
+					 ERR_PTR(current_uV));
 			return current_uV;
 		}
 
diff -ruN a/drivers/rpmsg/mtk_rpmsg.c b/drivers/rpmsg/mtk_rpmsg.c
--- a/drivers/rpmsg/mtk_rpmsg.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/rpmsg/mtk_rpmsg.c	2021-12-23 08:35:44.000000000 +0100
@@ -183,7 +183,7 @@
 	int ret;
 
 	for_each_available_child_of_node(node, child) {
-		ret = of_property_read_string(child, "mtk,rpmsg-name", &name);
+		ret = of_property_read_string(child, "mediatek,rpmsg-name", &name);
 		if (ret)
 			continue;
 
diff -ruN a/drivers/rpmsg/qcom_glink_native.c b/drivers/rpmsg/qcom_glink_native.c
--- a/drivers/rpmsg/qcom_glink_native.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/rpmsg/qcom_glink_native.c	2021-12-23 08:35:44.000000000 +0100
@@ -92,6 +92,8 @@
  * @rcids:	idr of all channels with a known remote channel id
  * @features:	remote features
  * @intentless:	flag to indicate that there is no intent
+ * @tx_avail_notify: Waitqueue for pending tx tasks
+ * @sent_read_notify: flag to check cmd sent or not
  */
 struct qcom_glink {
 	struct device *dev;
@@ -118,6 +120,8 @@
 	unsigned long features;
 
 	bool intentless;
+	wait_queue_head_t tx_avail_notify;
+	bool sent_read_notify;
 };
 
 enum {
@@ -301,6 +305,20 @@
 	glink->tx_pipe->write(glink->tx_pipe, hdr, hlen, data, dlen);
 }
 
+static void qcom_glink_send_read_notify(struct qcom_glink *glink)
+{
+	struct glink_msg msg;
+
+	msg.cmd = cpu_to_le16(RPM_CMD_READ_NOTIF);
+	msg.param1 = 0;
+	msg.param2 = 0;
+
+	qcom_glink_tx_write(glink, &msg, sizeof(msg), NULL, 0);
+
+	mbox_send_message(glink->mbox_chan, NULL);
+	mbox_client_txdone(glink->mbox_chan, 0);
+}
+
 static int qcom_glink_tx(struct qcom_glink *glink,
 			 const void *hdr, size_t hlen,
 			 const void *data, size_t dlen, bool wait)
@@ -321,12 +339,21 @@
 			goto out;
 		}
 
+		if (!glink->sent_read_notify) {
+			glink->sent_read_notify = true;
+			qcom_glink_send_read_notify(glink);
+		}
+
 		/* Wait without holding the tx_lock */
 		spin_unlock_irqrestore(&glink->tx_lock, flags);
 
-		usleep_range(10000, 15000);
+		wait_event_timeout(glink->tx_avail_notify,
+				   qcom_glink_tx_avail(glink) >= tlen, 10 * HZ);
 
 		spin_lock_irqsave(&glink->tx_lock, flags);
+
+		if (qcom_glink_tx_avail(glink) >= tlen)
+			glink->sent_read_notify = false;
 	}
 
 	qcom_glink_tx_write(glink, hdr, hlen, data, dlen);
@@ -986,6 +1013,9 @@
 	unsigned int cmd;
 	int ret = 0;
 
+	/* To wakeup any blocking writers */
+	wake_up_all(&glink->tx_avail_notify);
+
 	for (;;) {
 		avail = qcom_glink_rx_avail(glink);
 		if (avail < sizeof(msg))
@@ -1271,6 +1301,8 @@
 	} __packed req;
 	int ret;
 	unsigned long flags;
+	int chunk_size = len;
+	int left_size = 0;
 
 	if (!glink->intentless) {
 		while (!intent) {
@@ -1304,18 +1336,46 @@
 		iid = intent->id;
 	}
 
+	if (wait && chunk_size > SZ_8K) {
+		chunk_size = SZ_8K;
+		left_size = len - chunk_size;
+	}
 	req.msg.cmd = cpu_to_le16(RPM_CMD_TX_DATA);
 	req.msg.param1 = cpu_to_le16(channel->lcid);
 	req.msg.param2 = cpu_to_le32(iid);
-	req.chunk_size = cpu_to_le32(len);
-	req.left_size = cpu_to_le32(0);
+	req.chunk_size = cpu_to_le32(chunk_size);
+	req.left_size = cpu_to_le32(left_size);
 
-	ret = qcom_glink_tx(glink, &req, sizeof(req), data, len, wait);
+	ret = qcom_glink_tx(glink, &req, sizeof(req), data, chunk_size, wait);
 
 	/* Mark intent available if we failed */
-	if (ret && intent)
+	if (ret && intent) {
 		intent->in_use = false;
+		return ret;
+	}
 
+	while (left_size > 0) {
+		data = (void *)((char *)data + chunk_size);
+		chunk_size = left_size;
+		if (chunk_size > SZ_8K)
+			chunk_size = SZ_8K;
+		left_size -= chunk_size;
+
+		req.msg.cmd = cpu_to_le16(RPM_CMD_TX_DATA_CONT);
+		req.msg.param1 = cpu_to_le16(channel->lcid);
+		req.msg.param2 = cpu_to_le32(iid);
+		req.chunk_size = cpu_to_le32(chunk_size);
+		req.left_size = cpu_to_le32(left_size);
+
+		ret = qcom_glink_tx(glink, &req, sizeof(req), data,
+				    chunk_size, wait);
+
+		/* Mark intent available if we failed */
+		if (ret && intent) {
+			intent->in_use = false;
+			break;
+		}
+	}
 	return ret;
 }
 
@@ -1387,9 +1447,7 @@
 static void qcom_glink_rpdev_release(struct device *dev)
 {
 	struct rpmsg_device *rpdev = to_rpmsg_device(dev);
-	struct glink_channel *channel = to_glink_channel(rpdev->ept);
 
-	channel->rpdev = NULL;
 	kfree(rpdev);
 }
 
@@ -1494,6 +1552,7 @@
 
 		rpmsg_unregister_device(glink->dev, &chinfo);
 	}
+	channel->rpdev = NULL;
 
 	qcom_glink_send_close_ack(glink, channel->rcid);
 
@@ -1507,9 +1566,13 @@
 
 static void qcom_glink_rx_close_ack(struct qcom_glink *glink, unsigned int lcid)
 {
+	struct rpmsg_channel_info chinfo;
 	struct glink_channel *channel;
 	unsigned long flags;
 
+	/* To wakeup any blocking writers */
+	wake_up_all(&glink->tx_avail_notify);
+
 	spin_lock_irqsave(&glink->idr_lock, flags);
 	channel = idr_find(&glink->lcids, lcid);
 	if (WARN(!channel, "close ack on unknown channel\n")) {
@@ -1521,6 +1584,16 @@
 	channel->lcid = 0;
 	spin_unlock_irqrestore(&glink->idr_lock, flags);
 
+	/* Decouple the potential rpdev from the channel */
+	if (channel->rpdev) {
+		strscpy(chinfo.name, channel->name, sizeof(chinfo.name));
+		chinfo.src = RPMSG_ADDR_ANY;
+		chinfo.dst = RPMSG_ADDR_ANY;
+
+		rpmsg_unregister_device(glink->dev, &chinfo);
+	}
+	channel->rpdev = NULL;
+
 	kref_put(&channel->refcount, qcom_glink_channel_release);
 }
 
@@ -1670,6 +1743,7 @@
 	spin_lock_init(&glink->rx_lock);
 	INIT_LIST_HEAD(&glink->rx_queue);
 	INIT_WORK(&glink->rx_work, qcom_glink_work);
+	init_waitqueue_head(&glink->tx_avail_notify);
 
 	spin_lock_init(&glink->idr_lock);
 	idr_init(&glink->lcids);
diff -ruN a/drivers/scsi/scsi_pm.c b/drivers/scsi/scsi_pm.c
--- a/drivers/scsi/scsi_pm.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/scsi/scsi_pm.c	2021-12-23 08:35:46.000000000 +0100
@@ -179,7 +179,7 @@
 		/* Wait until async scanning is finished */
 		scsi_complete_async_scans();
 	}
-	return 0;
+	return 1;
 }
 
 static int scsi_bus_suspend(struct device *dev)
diff -ruN a/drivers/scsi/ufs/ufshcd.c b/drivers/scsi/ufs/ufshcd.c
--- a/drivers/scsi/ufs/ufshcd.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/scsi/ufs/ufshcd.c	2021-12-23 08:35:46.000000000 +0100
@@ -224,7 +224,6 @@
 static void ufshcd_hba_exit(struct ufs_hba *hba);
 static int ufshcd_probe_hba(struct ufs_hba *hba, bool init_dev_params);
 static int ufshcd_setup_clocks(struct ufs_hba *hba, bool on);
-static int ufshcd_uic_hibern8_enter(struct ufs_hba *hba);
 static inline void ufshcd_add_delay_before_dme_cmd(struct ufs_hba *hba);
 static int ufshcd_host_reset_and_restore(struct ufs_hba *hba);
 static void ufshcd_resume_clkscaling(struct ufs_hba *hba);
@@ -4077,7 +4076,7 @@
 }
 EXPORT_SYMBOL_GPL(ufshcd_link_recovery);
 
-static int ufshcd_uic_hibern8_enter(struct ufs_hba *hba)
+int ufshcd_uic_hibern8_enter(struct ufs_hba *hba)
 {
 	int ret;
 	struct uic_command uic_cmd = {0};
@@ -4099,6 +4098,7 @@
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(ufshcd_uic_hibern8_enter);
 
 int ufshcd_uic_hibern8_exit(struct ufs_hba *hba)
 {
diff -ruN a/drivers/scsi/ufs/ufshcd.h b/drivers/scsi/ufs/ufshcd.h
--- a/drivers/scsi/ufs/ufshcd.h	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/scsi/ufs/ufshcd.h	2021-12-23 08:35:46.000000000 +0100
@@ -1003,6 +1003,7 @@
 int ufshcd_link_recovery(struct ufs_hba *hba);
 int ufshcd_make_hba_operational(struct ufs_hba *hba);
 void ufshcd_remove(struct ufs_hba *);
+int ufshcd_uic_hibern8_enter(struct ufs_hba *hba);
 int ufshcd_uic_hibern8_exit(struct ufs_hba *hba);
 void ufshcd_delay_us(unsigned long us, unsigned long tolerance);
 int ufshcd_wait_for_register(struct ufs_hba *hba, u32 reg, u32 mask,
diff -ruN a/drivers/scsi/ufs/ufs-qcom.c b/drivers/scsi/ufs/ufs-qcom.c
--- a/drivers/scsi/ufs/ufs-qcom.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/scsi/ufs/ufs-qcom.c	2021-12-23 08:35:46.000000000 +0100
@@ -1213,24 +1213,34 @@
 	int err = 0;
 
 	if (status == PRE_CHANGE) {
+		err = ufshcd_uic_hibern8_enter(hba);
+		if (err)
+			return err;
 		if (scale_up)
 			err = ufs_qcom_clk_scale_up_pre_change(hba);
 		else
 			err = ufs_qcom_clk_scale_down_pre_change(hba);
+		if (err)
+			ufshcd_uic_hibern8_exit(hba);
+
 	} else {
 		if (scale_up)
 			err = ufs_qcom_clk_scale_up_post_change(hba);
 		else
 			err = ufs_qcom_clk_scale_down_post_change(hba);
 
-		if (err || !dev_req_params)
+
+		if (err || !dev_req_params) {
+			ufshcd_uic_hibern8_exit(hba);
 			goto out;
+		}
 
 		ufs_qcom_cfg_timers(hba,
 				    dev_req_params->gear_rx,
 				    dev_req_params->pwr_rx,
 				    dev_req_params->hs_rate,
 				    false);
+		ufshcd_uic_hibern8_exit(hba);
 	}
 
 out:
diff -ruN a/drivers/tty/serial/8250/8250_dw.c b/drivers/tty/serial/8250/8250_dw.c
--- a/drivers/tty/serial/8250/8250_dw.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/tty/serial/8250/8250_dw.c	2021-12-23 08:35:48.000000000 +0100
@@ -9,6 +9,7 @@
  * LCR is written whilst busy.  If it is, then a busy detect interrupt is
  * raised, the LCR needs to be rewritten and the uart status register read.
  */
+#include <linux/console.h>
 #include <linux/delay.h>
 #include <linux/device.h>
 #include <linux/io.h>
@@ -26,6 +27,7 @@
 #include <linux/clk.h>
 #include <linux/reset.h>
 #include <linux/pm_runtime.h>
+#include <linux/pci.h>
 
 #include <asm/byteorder.h>
 
@@ -647,10 +649,34 @@
 }
 
 #ifdef CONFIG_PM_SLEEP
+static void dw8250_configure_no_d3(struct dw8250_data *data, bool dev_flag)
+{
+	struct uart_8250_port *up = serial8250_get_port(data->data.line);
+	struct pci_dev *p_dev;
+
+	/*
+	 *	For Platforms with LPSS PCI UARTs, the parent device should
+	 *	be prevented from going into D3 for the no_console_suspend
+	 *  	flag to work as expected.
+	 */
+	if (platform_get_resource_byname(to_platform_device(up->port.dev),
+					IORESOURCE_MEM, "lpss_dev")) {
+		p_dev = (to_pci_dev(up->port.dev->parent));
+		if (p_dev && !console_suspend_enabled && uart_console(&up->port)) {
+			if (dev_flag)
+				p_dev->dev_flags |= PCI_DEV_FLAGS_NO_D3;
+			else
+				p_dev->dev_flags &= ~PCI_DEV_FLAGS_NO_D3;
+		}
+
+	}
+}
+
 static int dw8250_suspend(struct device *dev)
 {
 	struct dw8250_data *data = dev_get_drvdata(dev);
 
+	dw8250_configure_no_d3(data, true);
 	serial8250_suspend_port(data->data.line);
 
 	return 0;
@@ -661,6 +687,7 @@
 	struct dw8250_data *data = dev_get_drvdata(dev);
 
 	serial8250_resume_port(data->data.line);
+	dw8250_configure_no_d3(data, false);
 
 	return 0;
 }
diff -ruN a/drivers/tty/serial/kgdboc.c b/drivers/tty/serial/kgdboc.c
--- a/drivers/tty/serial/kgdboc.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/tty/serial/kgdboc.c	2021-12-23 08:35:48.000000000 +0100
@@ -53,6 +53,16 @@
 				struct input_dev *dev,
 				const struct input_device_id *id)
 {
+	/*
+	 * Pretend that SysRq key was never pressed (in case we got here
+	 * via SysRq), otherwise as we release all they keys we'll
+	 * end up sending release events for Alt and SysRq, potentially
+	 * triggering print screen function.
+	 */
+	spin_lock_irq(&dev->event_lock);
+	clear_bit(KEY_SYSRQ, dev->key);
+	spin_unlock_irq(&dev->event_lock);
+
 	input_reset_device(dev);
 
 	/* Return an error - we do not want to bind, just to reset */
diff -ruN a/drivers/tty/sysrq.c b/drivers/tty/sysrq.c
--- a/drivers/tty/sysrq.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/tty/sysrq.c	2021-12-23 08:35:48.000000000 +0100
@@ -51,6 +51,7 @@
 #include <linux/syscalls.h>
 #include <linux/of.h>
 #include <linux/rcupdate.h>
+#include <linux/delay.h>
 
 #include <asm/ptrace.h>
 #include <asm/irq_regs.h>
@@ -439,6 +440,64 @@
 	.enable_mask	= SYSRQ_ENABLE_RTNICE,
 };
 
+/* send a signal to a process named comm if it has a certain parent */
+/* if parent is NULL, send to the first matching process */
+static void sysrq_x_cros_signal_process(char *comm, char *parent, int sig)
+{
+	struct task_struct *p;
+
+	read_lock(&tasklist_lock);
+	for_each_process(p) {
+		if (p->flags & (PF_KTHREAD | PF_EXITING))
+			continue;
+		if (is_global_init(p))
+			continue;
+		if (strncmp(p->comm, comm, TASK_COMM_LEN))
+			continue;
+		if (parent && strncmp(p->parent->comm, parent, TASK_COMM_LEN))
+			continue;
+
+		printk(KERN_INFO "%s: signal %d %s pid %u tgid %u\n",
+		       __func__, sig, comm, p->pid, p->tgid);
+		do_send_sig_info(sig, SEND_SIG_PRIV, p, true);
+	}
+	read_unlock(&tasklist_lock);
+}
+
+/* how many seconds do we wait for subsequent keypresses after the first */
+#define CROS_SYSRQ_WAIT 20
+
+static void sysrq_handle_cros_xkey(int key)
+{
+	static unsigned long first_jiffies = INITIAL_JIFFIES - CROS_SYSRQ_WAIT * HZ;
+	static unsigned int xkey_iteration;
+
+	if (time_after(jiffies, first_jiffies + CROS_SYSRQ_WAIT * HZ)) {
+		first_jiffies = jiffies;
+		xkey_iteration = 0;
+	} else {
+		xkey_iteration++;
+	}
+
+	if (!xkey_iteration) {
+		sysrq_x_cros_signal_process("chrome", "session_manager",
+					    SIGABRT);
+	} else {
+		sysrq_handle_showstate_blocked(key);
+		sysrq_handle_sync(key);
+		/* Delay for a bit to give time for sync to complete */
+		mdelay(1000);
+		panic("ChromeOS X Key");
+	}
+}
+
+static struct sysrq_key_op sysrq_cros_xkey = {
+	.handler	= sysrq_handle_cros_xkey,
+	.help_msg	= "Cros-dump-and-crash",
+	.action_msg	= "Cros dump and crash",
+	.enable_mask	= SYSRQ_ENABLE_CROS_XKEY,
+};
+
 /* Key Operations table and lock */
 static DEFINE_SPINLOCK(sysrq_key_table_lock);
 
@@ -495,7 +554,8 @@
 	/* x: May be registered on mips for TLB dump */
 	/* x: May be registered on ppc/powerpc for xmon */
 	/* x: May be registered on sparc64 for global PMU dump */
-	NULL,				/* x */
+	/* x: On Chrome OS, this is the dump and crash key */
+	&sysrq_cros_xkey,		/* x */
 	/* y: May be registered on sparc64 for global register dump */
 	NULL,				/* y */
 	&sysrq_ftrace_dump_op,		/* z */
@@ -653,8 +713,10 @@
 	unsigned int alt_use;
 	unsigned int shift;
 	unsigned int shift_use;
+	unsigned int sysrq_use;
 	bool active;
 	bool need_reinject;
+	bool reinject_release_alt;
 	bool reinjecting;
 
 	/* reset sequence handling */
@@ -794,24 +856,55 @@
 			container_of(work, struct sysrq_state, reinject_work);
 	struct input_handle *handle = &sysrq->handle;
 	unsigned int alt_code = sysrq->alt_use;
+	unsigned int sysrq_code = sysrq->sysrq_use;
+
+	/*
+	 * Try to "restore" the events that we suppressed when user
+	 * activated SysRq mode. We start by sending the SysRq press,
+	 * followed by release of either SysRq or Alt, depending on
+	 * what has been actually released.
+	 */
 
-	if (sysrq->need_reinject) {
-		/* we do not want the assignment to be reordered */
-		sysrq->reinjecting = true;
-		mb();
-
-		/* Simulate press and release of Alt + SysRq */
-		input_inject_event(handle, EV_KEY, alt_code, 1);
-		input_inject_event(handle, EV_KEY, KEY_SYSRQ, 1);
-		input_inject_event(handle, EV_SYN, SYN_REPORT, 1);
+	/* we do not want the assignment to be reordered */
+	sysrq->reinjecting = true;
+	mb();
 
-		input_inject_event(handle, EV_KEY, KEY_SYSRQ, 0);
-		input_inject_event(handle, EV_KEY, alt_code, 0);
-		input_inject_event(handle, EV_SYN, SYN_REPORT, 1);
+	if (sysrq->reinject_release_alt) {
+		/*
+		 * Alt was released, which means that SysRq is still
+		 * down. Force it's state to be "released" so our
+		 * "press" event isn't swallowed by the input core.
+		 */
+		spin_lock_irq(&handle->dev->event_lock);
+		clear_bit(sysrq_code, handle->dev->key);
+		spin_unlock_irq(&handle->dev->event_lock);
+	}
+
+	/* Now "restore" previously suppressed SysRq press event */
+	input_inject_event(handle, EV_KEY, sysrq_code, 1);
+	input_inject_event(handle, EV_SYN, SYN_REPORT, 1);
 
-		mb();
-		sysrq->reinjecting = false;
+	if (sysrq->reinject_release_alt) {
+		/*
+		 * Force alt key state to be "pressed" since the key
+		 * actually been released, but event was suppressed,
+		 * and we want to re-send the event.
+		 */
+		spin_lock_irq(&handle->dev->event_lock);
+		set_bit(alt_code, handle->dev->key);
+		spin_unlock_irq(&handle->dev->event_lock);
+
+		/* And now release it */
+		input_inject_event(handle, EV_KEY, alt_code, 0);
+	} else {
+		/* And release SysRq key */
+		input_inject_event(handle, EV_KEY, sysrq_code, 0);
 	}
+
+	input_inject_event(handle, EV_SYN, SYN_REPORT, 1);
+
+	mb();
+	sysrq->reinjecting = false;
 }
 
 static bool sysrq_handle_keypress(struct sysrq_state *sysrq,
@@ -846,29 +939,35 @@
 		break;
 
 	case KEY_SYSRQ:
-		if (value == 1 && sysrq->alt != KEY_RESERVED) {
+	case KEY_F10:
+	case KEY_VOLUMEUP:
+		if (!value) {
+			if (code == sysrq->sysrq_use) {
+				/* SysRq is being released */
+				sysrq->active = false;
+				sysrq->alt = KEY_RESERVED;
+			}
+		} else if (value != 1) {
+			/* Ignore autorepeats */
+		} else if (sysrq->active && code != sysrq->sysrq_use) {
+			/*
+			 * We pressed the *other* SysRq, which means the
+			 * sequence is not "pure" and we no longer want to
+			 * re-inject it.
+			 */
+			sysrq->need_reinject = false;
+		} else if (sysrq->alt != KEY_RESERVED) {
 			sysrq->active = true;
 			sysrq->alt_use = sysrq->alt;
 			/* either RESERVED (for released) or actual code */
 			sysrq->shift_use = sysrq->shift;
+			sysrq->sysrq_use = code;
 			/*
 			 * If nothing else will be pressed we'll need
 			 * to re-inject Alt-SysRq keysroke.
 			 */
 			sysrq->need_reinject = true;
 		}
-
-		/*
-		 * Pretend that sysrq was never pressed at all. This
-		 * is needed to properly handle KGDB which will try
-		 * to release all keys after exiting debugger. If we
-		 * do not clear key bit it KGDB will end up sending
-		 * release events for Alt and SysRq, potentially
-		 * triggering print screen function.
-		 */
-		if (sysrq->active)
-			clear_bit(KEY_SYSRQ, sysrq->handle.dev->key);
-
 		break;
 
 	default:
@@ -883,8 +982,6 @@
 		break;
 	}
 
-	suppress = sysrq->active;
-
 	if (!sysrq->active) {
 
 		/*
@@ -903,18 +1000,28 @@
 		else
 			clear_bit(code, sysrq->key_down);
 
-		if (was_active)
-			schedule_work(&sysrq->reinject_work);
+		if (was_active) {
+			clear_bit(sysrq->sysrq_use, sysrq->handle.dev->key);
+			suppress = true;
+
+			if (sysrq->need_reinject) {
+				sysrq->reinject_release_alt =
+					code == sysrq->alt_use;
+				schedule_work(&sysrq->reinject_work);
+			}
+		} else {
+			suppress = false;
+		}
 
 		/* Check for reset sequence */
 		sysrq_detect_reset_sequence(sysrq, code, value);
-
-	} else if (value == 0 && test_and_clear_bit(code, sysrq->key_down)) {
+	} else {
 		/*
 		 * Pass on release events for keys that was pressed before
 		 * entering SysRq mode.
 		 */
-		suppress = false;
+		suppress = value != 0 ||
+			   !test_and_clear_bit(code, sysrq->key_down);
 	}
 
 	return suppress;
diff -ruN a/drivers/usb/core/quirks.c b/drivers/usb/core/quirks.c
--- a/drivers/usb/core/quirks.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/usb/core/quirks.c	2021-12-23 08:35:49.000000000 +0100
@@ -444,6 +444,9 @@
 	/* Lenovo ThinkPad USB-C Dock Gen2 Ethernet (RTL8153 GigE) */
 	{ USB_DEVICE(0x17ef, 0xa387), .driver_info = USB_QUIRK_NO_LPM },
 
+	/* Google - Plankton */
+	{ USB_DEVICE(0x18d1, 0x501e), .driver_info = USB_QUIRK_NO_LPM },
+
 	/* BUILDWIN Photo Frame */
 	{ USB_DEVICE(0x1908, 0x1315), .driver_info =
 			USB_QUIRK_HONOR_BNUMINTERFACES },
diff -ruN a/drivers/usb/dwc3/core.c b/drivers/usb/dwc3/core.c
--- a/drivers/usb/dwc3/core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/usb/dwc3/core.c	2021-12-23 08:35:49.000000000 +0100
@@ -31,12 +31,14 @@
 #include <linux/usb/gadget.h>
 #include <linux/usb/of.h>
 #include <linux/usb/otg.h>
+#include <linux/usb/hcd.h>
 
 #include "core.h"
 #include "gadget.h"
 #include "io.h"
 
 #include "debug.h"
+#include "../host/xhci.h"
 
 #define DWC3_DEFAULT_AUTOSUSPEND_DELAY	5000 /* ms */
 
@@ -1744,10 +1746,36 @@
 	return ret;
 }
 
+static void dwc3_set_phy_speed_flags(struct dwc3 *dwc)
+{
+
+	int i, num_ports;
+	u32 reg;
+	struct usb_hcd	*hcd = platform_get_drvdata(dwc->xhci);
+	struct xhci_hcd	*xhci_hcd = hcd_to_xhci(hcd);
+
+	dwc->hs_phy_flags &= ~(PHY_MODE_USB_HOST_HS | PHY_MODE_USB_HOST_LS);
+
+	reg = readl(&xhci_hcd->cap_regs->hcs_params1);
+
+	num_ports = HCS_MAX_PORTS(reg);
+	for (i = 0; i < num_ports; i++) {
+		reg = readl(&xhci_hcd->op_regs->port_status_base + i * 0x04);
+		if (reg & PORT_PE) {
+			if (DEV_HIGHSPEED(reg) || DEV_FULLSPEED(reg))
+				dwc->hs_phy_flags |= PHY_MODE_USB_HOST_HS;
+			else if (DEV_LOWSPEED(reg))
+				dwc->hs_phy_flags |= PHY_MODE_USB_HOST_LS;
+		}
+	}
+	phy_set_mode(dwc->usb2_generic_phy, dwc->hs_phy_flags);
+}
+
 static int dwc3_suspend_common(struct dwc3 *dwc, pm_message_t msg)
 {
 	unsigned long	flags;
 	u32 reg;
+	struct usb_hcd  *hcd = platform_get_drvdata(dwc->xhci);
 
 	switch (dwc->current_dr_role) {
 	case DWC3_GCTL_PRTCAP_DEVICE:
@@ -1760,10 +1788,7 @@
 		dwc3_core_exit(dwc);
 		break;
 	case DWC3_GCTL_PRTCAP_HOST:
-		if (!PMSG_IS_AUTO(msg)) {
-			dwc3_core_exit(dwc);
-			break;
-		}
+		dwc3_set_phy_speed_flags(dwc);
 
 		/* Let controller to suspend HSPHY before PHY driver suspends */
 		if (dwc->dis_u2_susphy_quirk ||
@@ -1779,6 +1804,16 @@
 
 		phy_pm_runtime_put_sync(dwc->usb2_generic_phy);
 		phy_pm_runtime_put_sync(dwc->usb3_generic_phy);
+
+		if (!PMSG_IS_AUTO(msg)) {
+			if (device_may_wakeup(&dwc->xhci->dev) &&
+			    usb_wakeup_enabled_descendants(hcd->self.root_hub)) {
+				dwc->need_phy_for_wakeup = true;
+			} else {
+				dwc->need_phy_for_wakeup = false;
+				dwc3_core_exit(dwc);
+			}
+		}
 		break;
 	case DWC3_GCTL_PRTCAP_OTG:
 		/* do nothing during runtime_suspend */
@@ -1822,11 +1857,12 @@
 		break;
 	case DWC3_GCTL_PRTCAP_HOST:
 		if (!PMSG_IS_AUTO(msg)) {
-			ret = dwc3_core_init_for_resume(dwc);
-			if (ret)
-				return ret;
-			dwc3_set_prtcap(dwc, DWC3_GCTL_PRTCAP_HOST);
-			break;
+			if (!dwc->need_phy_for_wakeup) {
+				ret = dwc3_core_init_for_resume(dwc);
+				if (ret)
+					return ret;
+				dwc3_set_prtcap(dwc, DWC3_GCTL_PRTCAP_HOST);
+			}
 		}
 		/* Restore GUSB2PHYCFG bits that were modified in suspend */
 		reg = dwc3_readl(dwc->regs, DWC3_GUSB2PHYCFG(0));
diff -ruN a/drivers/usb/dwc3/core.h b/drivers/usb/dwc3/core.h
--- a/drivers/usb/dwc3/core.h	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/usb/dwc3/core.h	2021-12-23 08:35:49.000000000 +0100
@@ -1131,6 +1131,8 @@
 	struct phy		*usb3_generic_phy;
 
 	bool			phys_ready;
+	bool                    need_phy_for_wakeup;
+	unsigned int            hs_phy_flags;
 
 	struct ulpi		*ulpi;
 	bool			ulpi_ready;
diff -ruN a/drivers/usb/dwc3/dwc3-qcom.c b/drivers/usb/dwc3/dwc3-qcom.c
--- a/drivers/usb/dwc3/dwc3-qcom.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/usb/dwc3/dwc3-qcom.c	2021-12-23 08:35:49.000000000 +0100
@@ -17,9 +17,11 @@
 #include <linux/of_platform.h>
 #include <linux/platform_device.h>
 #include <linux/phy/phy.h>
+#include <linux/pm_domain.h>
 #include <linux/usb/of.h>
 #include <linux/reset.h>
 #include <linux/iopoll.h>
+#include <linux/usb/hcd.h>
 
 #include "core.h"
 
@@ -298,21 +300,34 @@
 
 static void dwc3_qcom_disable_interrupts(struct dwc3_qcom *qcom)
 {
+	struct dwc3 *dwc = platform_get_drvdata(qcom->dwc3);
+
 	if (qcom->hs_phy_irq) {
 		disable_irq_wake(qcom->hs_phy_irq);
 		disable_irq_nosync(qcom->hs_phy_irq);
 	}
+	if (dwc->hs_phy_flags & PHY_MODE_USB_HOST_LS) {
+		if (qcom->dp_hs_phy_irq) {
+			disable_irq_wake(qcom->dp_hs_phy_irq);
+			disable_irq_nosync(qcom->dp_hs_phy_irq);
+		}
+	} else if (dwc->hs_phy_flags & PHY_MODE_USB_HOST_HS) {
+		if (qcom->dm_hs_phy_irq) {
+			disable_irq_wake(qcom->dm_hs_phy_irq);
+			disable_irq_nosync(qcom->dm_hs_phy_irq);
+		}
+	} else {
 
-	if (qcom->dp_hs_phy_irq) {
-		disable_irq_wake(qcom->dp_hs_phy_irq);
-		disable_irq_nosync(qcom->dp_hs_phy_irq);
-	}
-
-	if (qcom->dm_hs_phy_irq) {
-		disable_irq_wake(qcom->dm_hs_phy_irq);
-		disable_irq_nosync(qcom->dm_hs_phy_irq);
+		if (qcom->dp_hs_phy_irq) {
+			disable_irq_wake(qcom->dp_hs_phy_irq);
+			disable_irq_nosync(qcom->dp_hs_phy_irq);
+		}
+
+		if (qcom->dm_hs_phy_irq) {
+			disable_irq_wake(qcom->dm_hs_phy_irq);
+			disable_irq_nosync(qcom->dm_hs_phy_irq);
+		}
 	}
-
 	if (qcom->ss_phy_irq) {
 		disable_irq_wake(qcom->ss_phy_irq);
 		disable_irq_nosync(qcom->ss_phy_irq);
@@ -321,21 +336,34 @@
 
 static void dwc3_qcom_enable_interrupts(struct dwc3_qcom *qcom)
 {
+	struct dwc3 *dwc = platform_get_drvdata(qcom->dwc3);
+
 	if (qcom->hs_phy_irq) {
 		enable_irq(qcom->hs_phy_irq);
 		enable_irq_wake(qcom->hs_phy_irq);
 	}
+	if (dwc->hs_phy_flags & PHY_MODE_USB_HOST_LS) {
+		if (qcom->dp_hs_phy_irq) {
+			enable_irq(qcom->dp_hs_phy_irq);
+			enable_irq_wake(qcom->dp_hs_phy_irq);
+		}
+	} else if (dwc->hs_phy_flags & PHY_MODE_USB_HOST_HS) {
+		if (qcom->dm_hs_phy_irq) {
+			enable_irq(qcom->dm_hs_phy_irq);
+			enable_irq_wake(qcom->dm_hs_phy_irq);
+		}
+	} else {
 
-	if (qcom->dp_hs_phy_irq) {
-		enable_irq(qcom->dp_hs_phy_irq);
-		enable_irq_wake(qcom->dp_hs_phy_irq);
-	}
-
-	if (qcom->dm_hs_phy_irq) {
-		enable_irq(qcom->dm_hs_phy_irq);
-		enable_irq_wake(qcom->dm_hs_phy_irq);
+		if (qcom->dp_hs_phy_irq) {
+			enable_irq(qcom->dp_hs_phy_irq);
+			enable_irq_wake(qcom->dp_hs_phy_irq);
+		}
+
+		if (qcom->dm_hs_phy_irq) {
+			enable_irq(qcom->dm_hs_phy_irq);
+			enable_irq_wake(qcom->dm_hs_phy_irq);
+		}
 	}
-
 	if (qcom->ss_phy_irq) {
 		enable_irq(qcom->ss_phy_irq);
 		enable_irq_wake(qcom->ss_phy_irq);
@@ -346,6 +374,14 @@
 {
 	u32 val;
 	int i, ret;
+	struct dwc3 *dwc = platform_get_drvdata(qcom->dwc3);
+	struct usb_hcd  *hcd = platform_get_drvdata(dwc->xhci);
+	struct generic_pm_domain *genpd;
+
+	genpd = pd_to_genpd(qcom->dev->pm_domain);
+
+	if (genpd && usb_wakeup_enabled_descendants(hcd->self.root_hub))
+		genpd->flags |= GENPD_FLAG_ACTIVE_WAKEUP;
 
 	if (qcom->is_suspended)
 		return 0;
@@ -373,6 +409,11 @@
 {
 	int ret;
 	int i;
+	struct generic_pm_domain *genpd;
+
+	genpd = pd_to_genpd(qcom->dev->pm_domain);
+	if (genpd)
+		genpd->flags &= ~GENPD_FLAG_ACTIVE_WAKEUP;
 
 	if (!qcom->is_suspended)
 		return 0;
diff -ruN a/drivers/usb/dwc3/Kconfig b/drivers/usb/dwc3/Kconfig
--- a/drivers/usb/dwc3/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/usb/dwc3/Kconfig	2021-12-23 08:35:49.000000000 +0100
@@ -2,7 +2,7 @@
 
 config USB_DWC3
 	tristate "DesignWare USB3 DRD Core Support"
-	depends on (USB || USB_GADGET) && HAS_DMA
+	depends on USB && HAS_DMA
 	select USB_XHCI_PLATFORM if USB_XHCI_HCD
 	select USB_ROLE_SWITCH if USB_DWC3_DUAL_ROLE
 	help
diff -ruN a/drivers/usb/gadget/configfs.c b/drivers/usb/gadget/configfs.c
--- a/drivers/usb/gadget/configfs.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/usb/gadget/configfs.c	2021-12-23 08:35:49.000000000 +0100
@@ -10,6 +10,36 @@
 #include "u_f.h"
 #include "u_os_desc.h"
 
+#ifdef CONFIG_USB_CONFIGFS_UEVENT
+#include <linux/platform_device.h>
+#include <linux/kdev_t.h>
+#include <linux/usb/ch9.h>
+
+#ifdef CONFIG_USB_CONFIGFS_F_ACC
+extern int acc_ctrlrequest(struct usb_composite_dev *cdev,
+				const struct usb_ctrlrequest *ctrl);
+void acc_disconnect(void);
+#endif
+static struct class *android_class;
+static struct device *android_device;
+static int index;
+static int gadget_index;
+
+struct device *create_function_device(char *name)
+{
+	if (android_device && !IS_ERR(android_device))
+		return device_create(android_class, android_device,
+			MKDEV(0, index++), NULL, name);
+	else
+		return ERR_PTR(-EINVAL);
+}
+EXPORT_SYMBOL_GPL(create_function_device);
+#else
+#ifdef CONFIG_USB_CONFIGFS_F_ACC
+static inline void acc_disconnect(void) {}
+#endif
+#endif
+
 int check_user_usb_string(const char *name,
 		struct usb_gadget_strings *stringtab_dev)
 {
@@ -51,6 +81,12 @@
 	char qw_sign[OS_STRING_QW_SIGN_LEN];
 	spinlock_t spinlock;
 	bool unbind;
+#ifdef CONFIG_USB_CONFIGFS_UEVENT
+	bool connected;
+	bool sw_connected;
+	struct work_struct work;
+	struct device *dev;
+#endif
 };
 
 static inline struct gadget_info *to_gadget_info(struct config_item *item)
@@ -272,7 +308,7 @@
 
 	mutex_lock(&gi->lock);
 
-	if (!strlen(name)) {
+	if (!strlen(name) || strcmp(name, "none") == 0) {
 		ret = unregister_gadget(gi);
 		if (ret)
 			goto err;
@@ -1426,6 +1462,57 @@
 	return ret;
 }
 
+#ifdef CONFIG_USB_CONFIGFS_UEVENT
+static void android_work(struct work_struct *data)
+{
+	struct gadget_info *gi = container_of(data, struct gadget_info, work);
+	struct usb_composite_dev *cdev = &gi->cdev;
+	char *disconnected[2] = { "USB_STATE=DISCONNECTED", NULL };
+	char *connected[2]    = { "USB_STATE=CONNECTED", NULL };
+	char *configured[2]   = { "USB_STATE=CONFIGURED", NULL };
+	/* 0-connected 1-configured 2-disconnected*/
+	bool status[3] = { false, false, false };
+	unsigned long flags;
+	bool uevent_sent = false;
+
+	spin_lock_irqsave(&cdev->lock, flags);
+	if (cdev->config)
+		status[1] = true;
+
+	if (gi->connected != gi->sw_connected) {
+		if (gi->connected)
+			status[0] = true;
+		else
+			status[2] = true;
+		gi->sw_connected = gi->connected;
+	}
+	spin_unlock_irqrestore(&cdev->lock, flags);
+
+	if (status[0]) {
+		kobject_uevent_env(&gi->dev->kobj, KOBJ_CHANGE, connected);
+		pr_info("%s: sent uevent %s\n", __func__, connected[0]);
+		uevent_sent = true;
+	}
+
+	if (status[1]) {
+		kobject_uevent_env(&gi->dev->kobj, KOBJ_CHANGE, configured);
+		pr_info("%s: sent uevent %s\n", __func__, configured[0]);
+		uevent_sent = true;
+	}
+
+	if (status[2]) {
+		kobject_uevent_env(&gi->dev->kobj, KOBJ_CHANGE, disconnected);
+		pr_info("%s: sent uevent %s\n", __func__, disconnected[0]);
+		uevent_sent = true;
+	}
+
+	if (!uevent_sent) {
+		pr_info("%s: did not send uevent (%d %d %p)\n", __func__,
+			gi->connected, gi->sw_connected, cdev->config);
+	}
+}
+#endif
+
 static void configfs_composite_unbind(struct usb_gadget *gadget)
 {
 	struct usb_composite_dev	*cdev;
@@ -1451,6 +1538,50 @@
 	spin_unlock_irqrestore(&gi->spinlock, flags);
 }
 
+#ifdef CONFIG_USB_CONFIGFS_UEVENT
+static int android_setup(struct usb_gadget *gadget,
+			const struct usb_ctrlrequest *c)
+{
+	struct usb_composite_dev *cdev = get_gadget_data(gadget);
+	unsigned long flags;
+	struct gadget_info *gi = container_of(cdev, struct gadget_info, cdev);
+	int value = -EOPNOTSUPP;
+	struct usb_function_instance *fi;
+
+	spin_lock_irqsave(&cdev->lock, flags);
+	if (!gi->connected) {
+		gi->connected = 1;
+		schedule_work(&gi->work);
+	}
+	spin_unlock_irqrestore(&cdev->lock, flags);
+	list_for_each_entry(fi, &gi->available_func, cfs_list) {
+		if (fi != NULL && fi->f != NULL && fi->f->setup != NULL) {
+			value = fi->f->setup(fi->f, c);
+			if (value >= 0)
+				break;
+		}
+	}
+
+#ifdef CONFIG_USB_CONFIGFS_F_ACC
+	if (value < 0)
+		value = acc_ctrlrequest(cdev, c);
+#endif
+
+	if (value < 0)
+		value = composite_setup(gadget, c);
+
+	spin_lock_irqsave(&cdev->lock, flags);
+	if (c->bRequest == USB_REQ_SET_CONFIGURATION &&
+						cdev->config) {
+		schedule_work(&gi->work);
+	}
+	spin_unlock_irqrestore(&cdev->lock, flags);
+
+	return value;
+}
+
+#else // CONFIG_USB_CONFIGFS_UEVENT
+
 static int configfs_composite_setup(struct usb_gadget *gadget,
 		const struct usb_ctrlrequest *ctrl)
 {
@@ -1476,6 +1607,8 @@
 	return ret;
 }
 
+#endif // CONFIG_USB_CONFIGFS_UEVENT
+
 static void configfs_composite_disconnect(struct usb_gadget *gadget)
 {
 	struct usb_composite_dev *cdev;
@@ -1486,6 +1619,14 @@
 	if (!cdev)
 		return;
 
+#ifdef CONFIG_USB_CONFIGFS_F_ACC
+	/*
+	 * accessory HID support can be active while the
+	 * accessory function is not actually enabled,
+	 * so we need to inform it when we are disconnected.
+	 */
+	acc_disconnect();
+#endif
 	gi = container_of(cdev, struct gadget_info, cdev);
 	spin_lock_irqsave(&gi->spinlock, flags);
 	cdev = get_gadget_data(gadget);
@@ -1494,6 +1635,10 @@
 		return;
 	}
 
+#ifdef CONFIG_USB_CONFIGFS_UEVENT
+	gi->connected = 0;
+	schedule_work(&gi->work);
+#endif
 	composite_disconnect(gadget);
 	spin_unlock_irqrestore(&gi->spinlock, flags);
 }
@@ -1568,10 +1713,13 @@
 	.bind           = configfs_composite_bind,
 	.unbind         = configfs_composite_unbind,
 
+#ifdef CONFIG_USB_CONFIGFS_UEVENT
+	.setup          = android_setup,
+#else
 	.setup          = configfs_composite_setup,
+#endif
 	.reset          = configfs_composite_reset,
 	.disconnect     = configfs_composite_disconnect,
-
 	.suspend	= configfs_composite_suspend,
 	.resume		= configfs_composite_resume,
 
@@ -1583,6 +1731,91 @@
 	.match_existing_only = 1,
 };
 
+#ifdef CONFIG_USB_CONFIGFS_UEVENT
+static ssize_t state_show(struct device *pdev, struct device_attribute *attr,
+			char *buf)
+{
+	struct gadget_info *dev = dev_get_drvdata(pdev);
+	struct usb_composite_dev *cdev;
+	char *state = "DISCONNECTED";
+	unsigned long flags;
+
+	if (!dev)
+		goto out;
+
+	cdev = &dev->cdev;
+
+	if (!cdev)
+		goto out;
+
+	spin_lock_irqsave(&cdev->lock, flags);
+	if (cdev->config)
+		state = "CONFIGURED";
+	else if (dev->connected)
+		state = "CONNECTED";
+	spin_unlock_irqrestore(&cdev->lock, flags);
+out:
+	return sprintf(buf, "%s\n", state);
+}
+
+static DEVICE_ATTR(state, S_IRUGO, state_show, NULL);
+
+static struct device_attribute *android_usb_attributes[] = {
+	&dev_attr_state,
+	NULL
+};
+
+static int android_device_create(struct gadget_info *gi)
+{
+	struct device_attribute **attrs;
+	struct device_attribute *attr;
+
+	INIT_WORK(&gi->work, android_work);
+	gi->dev = device_create(android_class, NULL,
+			MKDEV(0, 0), NULL, "android%d", gadget_index++);
+	if (IS_ERR(gi->dev))
+		return PTR_ERR(gi->dev);
+
+	dev_set_drvdata(gi->dev, gi);
+	if (!android_device)
+		android_device = gi->dev;
+
+	attrs = android_usb_attributes;
+	while ((attr = *attrs++)) {
+		int err;
+
+		err = device_create_file(gi->dev, attr);
+		if (err) {
+			device_destroy(gi->dev->class,
+				       gi->dev->devt);
+			return err;
+		}
+	}
+
+	return 0;
+}
+
+static void android_device_destroy(struct gadget_info *gi)
+{
+	struct device_attribute **attrs;
+	struct device_attribute *attr;
+
+	attrs = android_usb_attributes;
+	while ((attr = *attrs++))
+		device_remove_file(gi->dev, attr);
+	device_destroy(gi->dev->class, gi->dev->devt);
+}
+#else
+static inline int android_device_create(struct gadget_info *gi)
+{
+	return 0;
+}
+
+static inline void android_device_destroy(struct gadget_info *gi)
+{
+}
+#endif
+
 static struct config_group *gadgets_make(
 		struct config_group *group,
 		const char *name)
@@ -1635,7 +1868,11 @@
 	if (!gi->composite.gadget_driver.function)
 		goto err;
 
+	if (android_device_create(gi) < 0)
+		goto err;
+
 	return &gi->group;
+
 err:
 	kfree(gi);
 	return ERR_PTR(-ENOMEM);
@@ -1643,7 +1880,11 @@
 
 static void gadgets_drop(struct config_group *group, struct config_item *item)
 {
+	struct gadget_info *gi;
+
+	gi = container_of(to_config_group(item), struct gadget_info, group);
 	config_item_put(item);
+	android_device_destroy(gi);
 }
 
 static struct configfs_group_operations gadgets_ops = {
@@ -1683,6 +1924,13 @@
 	config_group_init(&gadget_subsys.su_group);
 
 	ret = configfs_register_subsystem(&gadget_subsys);
+
+#ifdef CONFIG_USB_CONFIGFS_UEVENT
+	android_class = class_create(THIS_MODULE, "android_usb");
+	if (IS_ERR(android_class))
+		return PTR_ERR(android_class);
+#endif
+
 	return ret;
 }
 module_init(gadget_cfs_init);
@@ -1690,5 +1938,10 @@
 static void __exit gadget_cfs_exit(void)
 {
 	configfs_unregister_subsystem(&gadget_subsys);
+#ifdef CONFIG_USB_CONFIGFS_UEVENT
+	if (!IS_ERR(android_class))
+		class_destroy(android_class);
+#endif
+
 }
 module_exit(gadget_cfs_exit);
diff -ruN a/drivers/usb/gadget/function/f_accessory.c b/drivers/usb/gadget/function/f_accessory.c
--- a/drivers/usb/gadget/function/f_accessory.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/usb/gadget/function/f_accessory.c	2021-12-23 08:35:49.000000000 +0100
@@ -0,0 +1,1532 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Gadget Function Driver for Android USB accessories
+ *
+ * Copyright (C) 2011 Google, Inc.
+ * Author: Mike Lockwood <lockwood@android.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+/* #define DEBUG */
+/* #define VERBOSE_DEBUG */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/poll.h>
+#include <linux/delay.h>
+#include <linux/wait.h>
+#include <linux/err.h>
+#include <linux/interrupt.h>
+#include <linux/kthread.h>
+#include <linux/freezer.h>
+#include <linux/kref.h>
+
+#include <linux/types.h>
+#include <linux/file.h>
+#include <linux/device.h>
+#include <linux/miscdevice.h>
+
+#include <linux/hid.h>
+#include <linux/hiddev.h>
+#include <linux/usb.h>
+#include <linux/usb/ch9.h>
+#include <linux/usb/f_accessory.h>
+
+#include <linux/configfs.h>
+#include <linux/usb/composite.h>
+
+#define MAX_INST_NAME_LEN        40
+#define BULK_BUFFER_SIZE    16384
+#define ACC_STRING_SIZE     256
+
+#define PROTOCOL_VERSION    2
+
+/* String IDs */
+#define INTERFACE_STRING_INDEX	0
+
+/* number of tx and rx requests to allocate */
+#define TX_REQ_MAX 4
+#define RX_REQ_MAX 2
+
+struct acc_hid_dev {
+	struct list_head	list;
+	struct hid_device *hid;
+	struct acc_dev *dev;
+	/* accessory defined ID */
+	int id;
+	/* HID report descriptor */
+	u8 *report_desc;
+	/* length of HID report descriptor */
+	int report_desc_len;
+	/* number of bytes of report_desc we have received so far */
+	int report_desc_offset;
+};
+
+struct acc_dev {
+	struct usb_function function;
+	struct usb_composite_dev *cdev;
+	spinlock_t lock;
+	struct acc_dev_ref *ref;
+
+	struct usb_ep *ep_in;
+	struct usb_ep *ep_out;
+
+	/* online indicates state of function_set_alt & function_unbind
+	 * set to 1 when we connect
+	 */
+	int online;
+
+	/* disconnected indicates state of open & release
+	 * Set to 1 when we disconnect.
+	 * Not cleared until our file is closed.
+	 */
+	int disconnected;
+
+	/* strings sent by the host */
+	char manufacturer[ACC_STRING_SIZE];
+	char model[ACC_STRING_SIZE];
+	char description[ACC_STRING_SIZE];
+	char version[ACC_STRING_SIZE];
+	char uri[ACC_STRING_SIZE];
+	char serial[ACC_STRING_SIZE];
+
+	/* for acc_complete_set_string */
+	int string_index;
+
+	/* set to 1 if we have a pending start request */
+	int start_requested;
+
+	int audio_mode;
+
+	/* synchronize access to our device file */
+	atomic_t open_excl;
+
+	struct list_head tx_idle;
+
+	wait_queue_head_t read_wq;
+	wait_queue_head_t write_wq;
+	struct usb_request *rx_req[RX_REQ_MAX];
+	int rx_done;
+
+	/* delayed work for handling ACCESSORY_START */
+	struct delayed_work start_work;
+
+	/* work for handling ACCESSORY GET PROTOCOL */
+	struct work_struct getprotocol_work;
+
+	/* work for handling ACCESSORY SEND STRING */
+	struct work_struct sendstring_work;
+
+	/* worker for registering and unregistering hid devices */
+	struct work_struct hid_work;
+
+	/* list of active HID devices */
+	struct list_head	hid_list;
+
+	/* list of new HID devices to register */
+	struct list_head	new_hid_list;
+
+	/* list of dead HID devices to unregister */
+	struct list_head	dead_hid_list;
+};
+
+static struct usb_interface_descriptor acc_interface_desc = {
+	.bLength                = USB_DT_INTERFACE_SIZE,
+	.bDescriptorType        = USB_DT_INTERFACE,
+	.bInterfaceNumber       = 0,
+	.bNumEndpoints          = 2,
+	.bInterfaceClass        = USB_CLASS_VENDOR_SPEC,
+	.bInterfaceSubClass     = USB_SUBCLASS_VENDOR_SPEC,
+	.bInterfaceProtocol     = 0,
+};
+
+static struct usb_endpoint_descriptor acc_superspeedplus_in_desc = {
+	.bLength                = USB_DT_ENDPOINT_SIZE,
+	.bDescriptorType        = USB_DT_ENDPOINT,
+	.bEndpointAddress       = USB_DIR_IN,
+	.bmAttributes           = USB_ENDPOINT_XFER_BULK,
+	.wMaxPacketSize         = cpu_to_le16(1024),
+};
+
+static struct usb_endpoint_descriptor acc_superspeedplus_out_desc = {
+	.bLength                = USB_DT_ENDPOINT_SIZE,
+	.bDescriptorType        = USB_DT_ENDPOINT,
+	.bEndpointAddress       = USB_DIR_OUT,
+	.bmAttributes           = USB_ENDPOINT_XFER_BULK,
+	.wMaxPacketSize         = cpu_to_le16(1024),
+};
+
+static struct usb_ss_ep_comp_descriptor acc_superspeedplus_comp_desc = {
+	.bLength                = sizeof(acc_superspeedplus_comp_desc),
+	.bDescriptorType        = USB_DT_SS_ENDPOINT_COMP,
+
+	/* the following 2 values can be tweaked if necessary */
+	/* .bMaxBurst =         0, */
+	/* .bmAttributes =      0, */
+};
+
+static struct usb_endpoint_descriptor acc_superspeed_in_desc = {
+	.bLength                = USB_DT_ENDPOINT_SIZE,
+	.bDescriptorType        = USB_DT_ENDPOINT,
+	.bEndpointAddress       = USB_DIR_IN,
+	.bmAttributes           = USB_ENDPOINT_XFER_BULK,
+	.wMaxPacketSize         = cpu_to_le16(1024),
+};
+
+static struct usb_endpoint_descriptor acc_superspeed_out_desc = {
+	.bLength                = USB_DT_ENDPOINT_SIZE,
+	.bDescriptorType        = USB_DT_ENDPOINT,
+	.bEndpointAddress       = USB_DIR_OUT,
+	.bmAttributes           = USB_ENDPOINT_XFER_BULK,
+	.wMaxPacketSize         = cpu_to_le16(1024),
+};
+
+static struct usb_ss_ep_comp_descriptor acc_superspeed_comp_desc = {
+	.bLength                = sizeof(acc_superspeed_comp_desc),
+	.bDescriptorType        = USB_DT_SS_ENDPOINT_COMP,
+
+	/* the following 2 values can be tweaked if necessary */
+	/* .bMaxBurst =         0, */
+	/* .bmAttributes =      0, */
+};
+
+static struct usb_endpoint_descriptor acc_highspeed_in_desc = {
+	.bLength                = USB_DT_ENDPOINT_SIZE,
+	.bDescriptorType        = USB_DT_ENDPOINT,
+	.bEndpointAddress       = USB_DIR_IN,
+	.bmAttributes           = USB_ENDPOINT_XFER_BULK,
+	.wMaxPacketSize         = cpu_to_le16(512),
+};
+
+static struct usb_endpoint_descriptor acc_highspeed_out_desc = {
+	.bLength                = USB_DT_ENDPOINT_SIZE,
+	.bDescriptorType        = USB_DT_ENDPOINT,
+	.bEndpointAddress       = USB_DIR_OUT,
+	.bmAttributes           = USB_ENDPOINT_XFER_BULK,
+	.wMaxPacketSize         = cpu_to_le16(512),
+};
+
+static struct usb_endpoint_descriptor acc_fullspeed_in_desc = {
+	.bLength                = USB_DT_ENDPOINT_SIZE,
+	.bDescriptorType        = USB_DT_ENDPOINT,
+	.bEndpointAddress       = USB_DIR_IN,
+	.bmAttributes           = USB_ENDPOINT_XFER_BULK,
+};
+
+static struct usb_endpoint_descriptor acc_fullspeed_out_desc = {
+	.bLength                = USB_DT_ENDPOINT_SIZE,
+	.bDescriptorType        = USB_DT_ENDPOINT,
+	.bEndpointAddress       = USB_DIR_OUT,
+	.bmAttributes           = USB_ENDPOINT_XFER_BULK,
+};
+
+static struct usb_descriptor_header *fs_acc_descs[] = {
+	(struct usb_descriptor_header *) &acc_interface_desc,
+	(struct usb_descriptor_header *) &acc_fullspeed_in_desc,
+	(struct usb_descriptor_header *) &acc_fullspeed_out_desc,
+	NULL,
+};
+
+static struct usb_descriptor_header *hs_acc_descs[] = {
+	(struct usb_descriptor_header *) &acc_interface_desc,
+	(struct usb_descriptor_header *) &acc_highspeed_in_desc,
+	(struct usb_descriptor_header *) &acc_highspeed_out_desc,
+	NULL,
+};
+
+static struct usb_descriptor_header *ss_acc_descs[] = {
+	(struct usb_descriptor_header *) &acc_interface_desc,
+	(struct usb_descriptor_header *) &acc_superspeed_in_desc,
+	(struct usb_descriptor_header *) &acc_superspeed_comp_desc,
+	(struct usb_descriptor_header *) &acc_superspeed_out_desc,
+	(struct usb_descriptor_header *) &acc_superspeed_comp_desc,
+	NULL,
+};
+
+static struct usb_descriptor_header *ssp_acc_descs[] = {
+	(struct usb_descriptor_header *) &acc_interface_desc,
+	(struct usb_descriptor_header *) &acc_superspeedplus_in_desc,
+	(struct usb_descriptor_header *) &acc_superspeedplus_comp_desc,
+	(struct usb_descriptor_header *) &acc_superspeedplus_out_desc,
+	(struct usb_descriptor_header *) &acc_superspeedplus_comp_desc,
+	NULL,
+};
+
+static struct usb_string acc_string_defs[] = {
+	[INTERFACE_STRING_INDEX].s	= "Android Accessory Interface",
+	{  },	/* end of list */
+};
+
+static struct usb_gadget_strings acc_string_table = {
+	.language		= 0x0409,	/* en-US */
+	.strings		= acc_string_defs,
+};
+
+static struct usb_gadget_strings *acc_strings[] = {
+	&acc_string_table,
+	NULL,
+};
+
+struct acc_dev_ref {
+	struct kref	kref;
+	struct acc_dev	*acc_dev;
+};
+
+static struct acc_dev_ref _acc_dev_ref = {
+	.kref = KREF_INIT(0),
+};
+
+struct acc_instance {
+	struct usb_function_instance func_inst;
+	const char *name;
+};
+
+static struct acc_dev *get_acc_dev(void)
+{
+	struct acc_dev_ref *ref = &_acc_dev_ref;
+
+	return kref_get_unless_zero(&ref->kref) ? ref->acc_dev : NULL;
+}
+
+static void __put_acc_dev(struct kref *kref)
+{
+	struct acc_dev_ref *ref = container_of(kref, struct acc_dev_ref, kref);
+	struct acc_dev *dev = ref->acc_dev;
+
+	/* Cancel any async work */
+	cancel_delayed_work_sync(&dev->start_work);
+	cancel_work_sync(&dev->getprotocol_work);
+	cancel_work_sync(&dev->sendstring_work);
+	cancel_work_sync(&dev->hid_work);
+
+	ref->acc_dev = NULL;
+	kfree(dev);
+}
+
+static void put_acc_dev(struct acc_dev *dev)
+{
+	struct acc_dev_ref *ref = dev->ref;
+
+	WARN_ON(ref->acc_dev != dev);
+	kref_put(&ref->kref, __put_acc_dev);
+}
+
+static inline struct acc_dev *func_to_dev(struct usb_function *f)
+{
+	return container_of(f, struct acc_dev, function);
+}
+
+static struct usb_request *acc_request_new(struct usb_ep *ep, int buffer_size)
+{
+	struct usb_request *req = usb_ep_alloc_request(ep, GFP_KERNEL);
+
+	if (!req)
+		return NULL;
+
+	/* now allocate buffers for the requests */
+	req->buf = kmalloc(buffer_size, GFP_KERNEL);
+	if (!req->buf) {
+		usb_ep_free_request(ep, req);
+		return NULL;
+	}
+
+	return req;
+}
+
+static void acc_request_free(struct usb_request *req, struct usb_ep *ep)
+{
+	if (req) {
+		kfree(req->buf);
+		usb_ep_free_request(ep, req);
+	}
+}
+
+/* add a request to the tail of a list */
+static void req_put(struct acc_dev *dev, struct list_head *head,
+		struct usb_request *req)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&dev->lock, flags);
+	list_add_tail(&req->list, head);
+	spin_unlock_irqrestore(&dev->lock, flags);
+}
+
+/* remove a request from the head of a list */
+static struct usb_request *req_get(struct acc_dev *dev, struct list_head *head)
+{
+	unsigned long flags;
+	struct usb_request *req;
+
+	spin_lock_irqsave(&dev->lock, flags);
+	if (list_empty(head)) {
+		req = 0;
+	} else {
+		req = list_first_entry(head, struct usb_request, list);
+		list_del(&req->list);
+	}
+	spin_unlock_irqrestore(&dev->lock, flags);
+	return req;
+}
+
+static void acc_set_disconnected(struct acc_dev *dev)
+{
+	dev->disconnected = 1;
+}
+
+static void acc_complete_in(struct usb_ep *ep, struct usb_request *req)
+{
+	struct acc_dev *dev = get_acc_dev();
+
+	if (!dev)
+		return;
+
+	if (req->status == -ESHUTDOWN) {
+		pr_debug("acc_complete_in set disconnected");
+		acc_set_disconnected(dev);
+	}
+
+	req_put(dev, &dev->tx_idle, req);
+
+	wake_up(&dev->write_wq);
+	put_acc_dev(dev);
+}
+
+static void acc_complete_out(struct usb_ep *ep, struct usb_request *req)
+{
+	struct acc_dev *dev = get_acc_dev();
+
+	if (!dev)
+		return;
+
+	dev->rx_done = 1;
+	if (req->status == -ESHUTDOWN) {
+		pr_debug("acc_complete_out set disconnected");
+		acc_set_disconnected(dev);
+	}
+
+	wake_up(&dev->read_wq);
+	put_acc_dev(dev);
+}
+
+static void acc_complete_set_string(struct usb_ep *ep, struct usb_request *req)
+{
+	struct acc_dev	*dev = ep->driver_data;
+	char *string_dest = NULL;
+	int length = req->actual;
+
+	if (req->status != 0) {
+		pr_err("acc_complete_set_string, err %d\n", req->status);
+		return;
+	}
+
+	switch (dev->string_index) {
+	case ACCESSORY_STRING_MANUFACTURER:
+		string_dest = dev->manufacturer;
+		break;
+	case ACCESSORY_STRING_MODEL:
+		string_dest = dev->model;
+		break;
+	case ACCESSORY_STRING_DESCRIPTION:
+		string_dest = dev->description;
+		break;
+	case ACCESSORY_STRING_VERSION:
+		string_dest = dev->version;
+		break;
+	case ACCESSORY_STRING_URI:
+		string_dest = dev->uri;
+		break;
+	case ACCESSORY_STRING_SERIAL:
+		string_dest = dev->serial;
+		break;
+	}
+	if (string_dest) {
+		unsigned long flags;
+
+		if (length >= ACC_STRING_SIZE)
+			length = ACC_STRING_SIZE - 1;
+
+		spin_lock_irqsave(&dev->lock, flags);
+		memcpy(string_dest, req->buf, length);
+		/* ensure zero termination */
+		string_dest[length] = 0;
+		spin_unlock_irqrestore(&dev->lock, flags);
+	} else {
+		pr_err("unknown accessory string index %d\n",
+			dev->string_index);
+	}
+}
+
+static void acc_complete_set_hid_report_desc(struct usb_ep *ep,
+		struct usb_request *req)
+{
+	struct acc_hid_dev *hid = req->context;
+	struct acc_dev *dev = hid->dev;
+	int length = req->actual;
+
+	if (req->status != 0) {
+		pr_err("acc_complete_set_hid_report_desc, err %d\n",
+			req->status);
+		return;
+	}
+
+	memcpy(hid->report_desc + hid->report_desc_offset, req->buf, length);
+	hid->report_desc_offset += length;
+	if (hid->report_desc_offset == hid->report_desc_len) {
+		/* After we have received the entire report descriptor
+		 * we schedule work to initialize the HID device
+		 */
+		schedule_work(&dev->hid_work);
+	}
+}
+
+static void acc_complete_send_hid_event(struct usb_ep *ep,
+		struct usb_request *req)
+{
+	struct acc_hid_dev *hid = req->context;
+	int length = req->actual;
+
+	if (req->status != 0) {
+		pr_err("acc_complete_send_hid_event, err %d\n", req->status);
+		return;
+	}
+
+	hid_report_raw_event(hid->hid, HID_INPUT_REPORT, req->buf, length, 1);
+}
+
+static int acc_hid_parse(struct hid_device *hid)
+{
+	struct acc_hid_dev *hdev = hid->driver_data;
+
+	hid_parse_report(hid, hdev->report_desc, hdev->report_desc_len);
+	return 0;
+}
+
+static int acc_hid_start(struct hid_device *hid)
+{
+	return 0;
+}
+
+static void acc_hid_stop(struct hid_device *hid)
+{
+}
+
+static int acc_hid_open(struct hid_device *hid)
+{
+	return 0;
+}
+
+static void acc_hid_close(struct hid_device *hid)
+{
+}
+
+static int acc_hid_raw_request(struct hid_device *hid, unsigned char reportnum,
+	__u8 *buf, size_t len, unsigned char rtype, int reqtype)
+{
+	return 0;
+}
+
+static struct hid_ll_driver acc_hid_ll_driver = {
+	.parse = acc_hid_parse,
+	.start = acc_hid_start,
+	.stop = acc_hid_stop,
+	.open = acc_hid_open,
+	.close = acc_hid_close,
+	.raw_request = acc_hid_raw_request,
+};
+
+static struct acc_hid_dev *acc_hid_new(struct acc_dev *dev,
+		int id, int desc_len)
+{
+	struct acc_hid_dev *hdev;
+
+	hdev = kzalloc(sizeof(*hdev), GFP_ATOMIC);
+	if (!hdev)
+		return NULL;
+	hdev->report_desc = kzalloc(desc_len, GFP_ATOMIC);
+	if (!hdev->report_desc) {
+		kfree(hdev);
+		return NULL;
+	}
+	hdev->dev = dev;
+	hdev->id = id;
+	hdev->report_desc_len = desc_len;
+
+	return hdev;
+}
+
+static struct acc_hid_dev *acc_hid_get(struct list_head *list, int id)
+{
+	struct acc_hid_dev *hid;
+
+	list_for_each_entry(hid, list, list) {
+		if (hid->id == id)
+			return hid;
+	}
+	return NULL;
+}
+
+static int acc_register_hid(struct acc_dev *dev, int id, int desc_length)
+{
+	struct acc_hid_dev *hid;
+	unsigned long flags;
+
+	/* report descriptor length must be > 0 */
+	if (desc_length <= 0)
+		return -EINVAL;
+
+	spin_lock_irqsave(&dev->lock, flags);
+	/* replace HID if one already exists with this ID */
+	hid = acc_hid_get(&dev->hid_list, id);
+	if (!hid)
+		hid = acc_hid_get(&dev->new_hid_list, id);
+	if (hid)
+		list_move(&hid->list, &dev->dead_hid_list);
+
+	hid = acc_hid_new(dev, id, desc_length);
+	if (!hid) {
+		spin_unlock_irqrestore(&dev->lock, flags);
+		return -ENOMEM;
+	}
+
+	list_add(&hid->list, &dev->new_hid_list);
+	spin_unlock_irqrestore(&dev->lock, flags);
+
+	/* schedule work to register the HID device */
+	schedule_work(&dev->hid_work);
+	return 0;
+}
+
+static int acc_unregister_hid(struct acc_dev *dev, int id)
+{
+	struct acc_hid_dev *hid;
+	unsigned long flags;
+
+	spin_lock_irqsave(&dev->lock, flags);
+	hid = acc_hid_get(&dev->hid_list, id);
+	if (!hid)
+		hid = acc_hid_get(&dev->new_hid_list, id);
+	if (!hid) {
+		spin_unlock_irqrestore(&dev->lock, flags);
+		return -EINVAL;
+	}
+
+	list_move(&hid->list, &dev->dead_hid_list);
+	spin_unlock_irqrestore(&dev->lock, flags);
+
+	schedule_work(&dev->hid_work);
+	return 0;
+}
+
+static int create_bulk_endpoints(struct acc_dev *dev,
+				struct usb_endpoint_descriptor *in_desc,
+				struct usb_endpoint_descriptor *out_desc)
+{
+	struct usb_composite_dev *cdev = dev->cdev;
+	struct usb_request *req;
+	struct usb_ep *ep;
+	int i;
+
+	DBG(cdev, "create_bulk_endpoints dev: %p\n", dev);
+
+	ep = usb_ep_autoconfig(cdev->gadget, in_desc);
+	if (!ep) {
+		DBG(cdev, "usb_ep_autoconfig for ep_in failed\n");
+		return -ENODEV;
+	}
+	DBG(cdev, "usb_ep_autoconfig for ep_in got %s\n", ep->name);
+	ep->driver_data = dev;		/* claim the endpoint */
+	dev->ep_in = ep;
+
+	ep = usb_ep_autoconfig(cdev->gadget, out_desc);
+	if (!ep) {
+		DBG(cdev, "usb_ep_autoconfig for ep_out failed\n");
+		return -ENODEV;
+	}
+	DBG(cdev, "usb_ep_autoconfig for ep_out got %s\n", ep->name);
+	ep->driver_data = dev;		/* claim the endpoint */
+	dev->ep_out = ep;
+
+	/* now allocate requests for our endpoints */
+	for (i = 0; i < TX_REQ_MAX; i++) {
+		req = acc_request_new(dev->ep_in, BULK_BUFFER_SIZE);
+		if (!req)
+			goto fail;
+		req->complete = acc_complete_in;
+		req_put(dev, &dev->tx_idle, req);
+	}
+	for (i = 0; i < RX_REQ_MAX; i++) {
+		req = acc_request_new(dev->ep_out, BULK_BUFFER_SIZE);
+		if (!req)
+			goto fail;
+		req->complete = acc_complete_out;
+		dev->rx_req[i] = req;
+	}
+
+	return 0;
+
+fail:
+	pr_err("acc_bind() could not allocate requests\n");
+	while ((req = req_get(dev, &dev->tx_idle)))
+		acc_request_free(req, dev->ep_in);
+	for (i = 0; i < RX_REQ_MAX; i++)
+		acc_request_free(dev->rx_req[i], dev->ep_out);
+	return -1;
+}
+
+static ssize_t acc_read(struct file *fp, char __user *buf,
+	size_t count, loff_t *pos)
+{
+	struct acc_dev *dev = fp->private_data;
+	struct usb_request *req;
+	ssize_t r = count;
+	ssize_t data_length;
+	unsigned xfer;
+	int ret = 0;
+
+	pr_debug("acc_read(%zu)\n", count);
+
+	if (dev->disconnected) {
+		pr_debug("acc_read disconnected");
+		return -ENODEV;
+	}
+
+	if (count > BULK_BUFFER_SIZE)
+		count = BULK_BUFFER_SIZE;
+
+	/* we will block until we're online */
+	pr_debug("acc_read: waiting for online\n");
+	ret = wait_event_interruptible(dev->read_wq, dev->online);
+	if (ret < 0) {
+		r = ret;
+		goto done;
+	}
+
+	/*
+	 * Calculate the data length by considering termination character.
+	 * Then compansite the difference of rounding up to
+	 * integer multiple of maxpacket size.
+	 */
+	data_length = count;
+	data_length += dev->ep_out->maxpacket - 1;
+	data_length -= data_length % dev->ep_out->maxpacket;
+
+	if (dev->rx_done) {
+		// last req cancelled. try to get it.
+		req = dev->rx_req[0];
+		goto copy_data;
+	}
+
+requeue_req:
+	/* queue a request */
+	req = dev->rx_req[0];
+	req->length = data_length;
+	dev->rx_done = 0;
+	ret = usb_ep_queue(dev->ep_out, req, GFP_KERNEL);
+	if (ret < 0) {
+		r = -EIO;
+		goto done;
+	} else {
+		pr_debug("rx %p queue\n", req);
+	}
+
+	/* wait for a request to complete */
+	ret = wait_event_interruptible(dev->read_wq, dev->rx_done);
+	if (ret < 0) {
+		r = ret;
+		ret = usb_ep_dequeue(dev->ep_out, req);
+		if (ret != 0) {
+			// cancel failed. There can be a data already received.
+			// it will be retrieved in the next read.
+			pr_debug("acc_read: cancelling failed %d", ret);
+		}
+		goto done;
+	}
+
+copy_data:
+	dev->rx_done = 0;
+	if (dev->online) {
+		/* If we got a 0-len packet, throw it back and try again. */
+		if (req->actual == 0)
+			goto requeue_req;
+
+		pr_debug("rx %p %u\n", req, req->actual);
+		xfer = (req->actual < count) ? req->actual : count;
+		r = xfer;
+		if (copy_to_user(buf, req->buf, xfer))
+			r = -EFAULT;
+	} else
+		r = -EIO;
+
+done:
+	pr_debug("acc_read returning %zd\n", r);
+	return r;
+}
+
+static ssize_t acc_write(struct file *fp, const char __user *buf,
+	size_t count, loff_t *pos)
+{
+	struct acc_dev *dev = fp->private_data;
+	struct usb_request *req = 0;
+	ssize_t r = count;
+	unsigned xfer;
+	int ret;
+
+	pr_debug("acc_write(%zu)\n", count);
+
+	if (!dev->online || dev->disconnected) {
+		pr_debug("acc_write disconnected or not online");
+		return -ENODEV;
+	}
+
+	while (count > 0) {
+		/* get an idle tx request to use */
+		req = 0;
+		ret = wait_event_interruptible(dev->write_wq,
+			((req = req_get(dev, &dev->tx_idle)) || !dev->online));
+		if (!dev->online || dev->disconnected) {
+			pr_debug("acc_write dev->error\n");
+			r = -EIO;
+			break;
+		}
+
+		if (!req) {
+			r = ret;
+			break;
+		}
+
+		if (count > BULK_BUFFER_SIZE) {
+			xfer = BULK_BUFFER_SIZE;
+			/* ZLP, They will be more TX requests so not yet. */
+			req->zero = 0;
+		} else {
+			xfer = count;
+			/* If the data length is a multple of the
+			 * maxpacket size then send a zero length packet(ZLP).
+			*/
+			req->zero = ((xfer % dev->ep_in->maxpacket) == 0);
+		}
+		if (copy_from_user(req->buf, buf, xfer)) {
+			r = -EFAULT;
+			break;
+		}
+
+		req->length = xfer;
+		ret = usb_ep_queue(dev->ep_in, req, GFP_KERNEL);
+		if (ret < 0) {
+			pr_debug("acc_write: xfer error %d\n", ret);
+			r = -EIO;
+			break;
+		}
+
+		buf += xfer;
+		count -= xfer;
+
+		/* zero this so we don't try to free it on error exit */
+		req = 0;
+	}
+
+	if (req)
+		req_put(dev, &dev->tx_idle, req);
+
+	pr_debug("acc_write returning %zd\n", r);
+	return r;
+}
+
+static long acc_ioctl(struct file *fp, unsigned code, unsigned long value)
+{
+	struct acc_dev *dev = fp->private_data;
+	char *src = NULL;
+	int ret;
+
+	switch (code) {
+	case ACCESSORY_GET_STRING_MANUFACTURER:
+		src = dev->manufacturer;
+		break;
+	case ACCESSORY_GET_STRING_MODEL:
+		src = dev->model;
+		break;
+	case ACCESSORY_GET_STRING_DESCRIPTION:
+		src = dev->description;
+		break;
+	case ACCESSORY_GET_STRING_VERSION:
+		src = dev->version;
+		break;
+	case ACCESSORY_GET_STRING_URI:
+		src = dev->uri;
+		break;
+	case ACCESSORY_GET_STRING_SERIAL:
+		src = dev->serial;
+		break;
+	case ACCESSORY_IS_START_REQUESTED:
+		return dev->start_requested;
+	case ACCESSORY_GET_AUDIO_MODE:
+		return dev->audio_mode;
+	}
+	if (!src)
+		return -EINVAL;
+
+	ret = strlen(src) + 1;
+	if (copy_to_user((void __user *)value, src, ret))
+		ret = -EFAULT;
+	return ret;
+}
+
+static int acc_open(struct inode *ip, struct file *fp)
+{
+	struct acc_dev *dev = get_acc_dev();
+
+	if (!dev)
+		return -ENODEV;
+
+	if (atomic_xchg(&dev->open_excl, 1)) {
+		put_acc_dev(dev);
+		return -EBUSY;
+	}
+
+	dev->disconnected = 0;
+	fp->private_data = dev;
+	return 0;
+}
+
+static int acc_release(struct inode *ip, struct file *fp)
+{
+	struct acc_dev *dev = fp->private_data;
+
+	if (!dev)
+		return -ENOENT;
+
+	/* indicate that we are disconnected
+	 * still could be online so don't touch online flag
+	 */
+	dev->disconnected = 1;
+
+	fp->private_data = NULL;
+	WARN_ON(!atomic_xchg(&dev->open_excl, 0));
+	put_acc_dev(dev);
+	return 0;
+}
+
+/* file operations for /dev/usb_accessory */
+static const struct file_operations acc_fops = {
+	.owner = THIS_MODULE,
+	.read = acc_read,
+	.write = acc_write,
+	.unlocked_ioctl = acc_ioctl,
+	.open = acc_open,
+	.release = acc_release,
+};
+
+static int acc_hid_probe(struct hid_device *hdev,
+		const struct hid_device_id *id)
+{
+	int ret;
+
+	ret = hid_parse(hdev);
+	if (ret)
+		return ret;
+	return hid_hw_start(hdev, HID_CONNECT_DEFAULT);
+}
+
+static struct miscdevice acc_device = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "usb_accessory",
+	.fops = &acc_fops,
+};
+
+static const struct hid_device_id acc_hid_table[] = {
+	{ HID_USB_DEVICE(HID_ANY_ID, HID_ANY_ID) },
+	{ }
+};
+
+static struct hid_driver acc_hid_driver = {
+	.name = "USB accessory",
+	.id_table = acc_hid_table,
+	.probe = acc_hid_probe,
+};
+
+static void acc_complete_setup_noop(struct usb_ep *ep, struct usb_request *req)
+{
+	/*
+	 * Default no-op function when nothing needs to be done for the
+	 * setup request
+	 */
+}
+
+int acc_ctrlrequest(struct usb_composite_dev *cdev,
+				const struct usb_ctrlrequest *ctrl)
+{
+	struct acc_dev	*dev = get_acc_dev();
+	int	value = -EOPNOTSUPP;
+	struct acc_hid_dev *hid;
+	int offset;
+	u8 b_requestType = ctrl->bRequestType;
+	u8 b_request = ctrl->bRequest;
+	u16	w_index = le16_to_cpu(ctrl->wIndex);
+	u16	w_value = le16_to_cpu(ctrl->wValue);
+	u16	w_length = le16_to_cpu(ctrl->wLength);
+	unsigned long flags;
+
+	/*
+	 * If instance is not created which is the case in power off charging
+	 * mode, dev will be NULL. Hence return error if it is the case.
+	 */
+	if (!dev)
+		return -ENODEV;
+
+	if (b_requestType == (USB_DIR_OUT | USB_TYPE_VENDOR)) {
+		if (b_request == ACCESSORY_START) {
+			dev->start_requested = 1;
+			schedule_delayed_work(
+				&dev->start_work, msecs_to_jiffies(10));
+			value = 0;
+			cdev->req->complete = acc_complete_setup_noop;
+		} else if (b_request == ACCESSORY_SEND_STRING) {
+			schedule_work(&dev->sendstring_work);
+			dev->string_index = w_index;
+			cdev->gadget->ep0->driver_data = dev;
+			cdev->req->complete = acc_complete_set_string;
+			value = w_length;
+		} else if (b_request == ACCESSORY_SET_AUDIO_MODE &&
+				w_index == 0 && w_length == 0) {
+			dev->audio_mode = w_value;
+			cdev->req->complete = acc_complete_setup_noop;
+			value = 0;
+		} else if (b_request == ACCESSORY_REGISTER_HID) {
+			cdev->req->complete = acc_complete_setup_noop;
+			value = acc_register_hid(dev, w_value, w_index);
+		} else if (b_request == ACCESSORY_UNREGISTER_HID) {
+			cdev->req->complete = acc_complete_setup_noop;
+			value = acc_unregister_hid(dev, w_value);
+		} else if (b_request == ACCESSORY_SET_HID_REPORT_DESC) {
+			spin_lock_irqsave(&dev->lock, flags);
+			hid = acc_hid_get(&dev->new_hid_list, w_value);
+			spin_unlock_irqrestore(&dev->lock, flags);
+			if (!hid) {
+				value = -EINVAL;
+				goto err;
+			}
+			offset = w_index;
+			if (offset != hid->report_desc_offset
+				|| offset + w_length > hid->report_desc_len) {
+				value = -EINVAL;
+				goto err;
+			}
+			cdev->req->context = hid;
+			cdev->req->complete = acc_complete_set_hid_report_desc;
+			value = w_length;
+		} else if (b_request == ACCESSORY_SEND_HID_EVENT) {
+			spin_lock_irqsave(&dev->lock, flags);
+			hid = acc_hid_get(&dev->hid_list, w_value);
+			spin_unlock_irqrestore(&dev->lock, flags);
+			if (!hid) {
+				value = -EINVAL;
+				goto err;
+			}
+			cdev->req->context = hid;
+			cdev->req->complete = acc_complete_send_hid_event;
+			value = w_length;
+		}
+	} else if (b_requestType == (USB_DIR_IN | USB_TYPE_VENDOR)) {
+		if (b_request == ACCESSORY_GET_PROTOCOL) {
+			schedule_work(&dev->getprotocol_work);
+			*((u16 *)cdev->req->buf) = PROTOCOL_VERSION;
+			value = sizeof(u16);
+			cdev->req->complete = acc_complete_setup_noop;
+			/* clear any string left over from a previous session */
+			memset(dev->manufacturer, 0, sizeof(dev->manufacturer));
+			memset(dev->model, 0, sizeof(dev->model));
+			memset(dev->description, 0, sizeof(dev->description));
+			memset(dev->version, 0, sizeof(dev->version));
+			memset(dev->uri, 0, sizeof(dev->uri));
+			memset(dev->serial, 0, sizeof(dev->serial));
+			dev->start_requested = 0;
+			dev->audio_mode = 0;
+		}
+	}
+
+	if (value >= 0) {
+		cdev->req->zero = 0;
+		cdev->req->length = value;
+		value = usb_ep_queue(cdev->gadget->ep0, cdev->req, GFP_ATOMIC);
+		if (value < 0)
+			ERROR(cdev, "%s setup response queue error\n",
+				__func__);
+	}
+
+err:
+	if (value == -EOPNOTSUPP)
+		VDBG(cdev,
+			"unknown class-specific control req "
+			"%02x.%02x v%04x i%04x l%u\n",
+			ctrl->bRequestType, ctrl->bRequest,
+			w_value, w_index, w_length);
+	put_acc_dev(dev);
+	return value;
+}
+EXPORT_SYMBOL_GPL(acc_ctrlrequest);
+
+static int
+__acc_function_bind(struct usb_configuration *c,
+			struct usb_function *f, bool configfs)
+{
+	struct usb_composite_dev *cdev = c->cdev;
+	struct acc_dev	*dev = func_to_dev(f);
+	int			id;
+	int			ret;
+
+	DBG(cdev, "acc_function_bind dev: %p\n", dev);
+
+	if (configfs) {
+		if (acc_string_defs[INTERFACE_STRING_INDEX].id == 0) {
+			ret = usb_string_id(c->cdev);
+			if (ret < 0)
+				return ret;
+			acc_string_defs[INTERFACE_STRING_INDEX].id = ret;
+			acc_interface_desc.iInterface = ret;
+		}
+		dev->cdev = c->cdev;
+	}
+	ret = hid_register_driver(&acc_hid_driver);
+	if (ret)
+		return ret;
+
+	dev->start_requested = 0;
+
+	/* allocate interface ID(s) */
+	id = usb_interface_id(c, f);
+	if (id < 0)
+		return id;
+	acc_interface_desc.bInterfaceNumber = id;
+
+	/* allocate endpoints */
+	ret = create_bulk_endpoints(dev, &acc_fullspeed_in_desc,
+			&acc_fullspeed_out_desc);
+	if (ret)
+		return ret;
+
+	/* support high speed hardware */
+	if (gadget_is_dualspeed(c->cdev->gadget)) {
+		acc_highspeed_in_desc.bEndpointAddress =
+			acc_fullspeed_in_desc.bEndpointAddress;
+		acc_highspeed_out_desc.bEndpointAddress =
+			acc_fullspeed_out_desc.bEndpointAddress;
+	}
+
+	DBG(cdev, "%s speed %s: IN/%s, OUT/%s\n",
+			gadget_is_dualspeed(c->cdev->gadget) ? "dual" : "full",
+			f->name, dev->ep_in->name, dev->ep_out->name);
+	return 0;
+}
+
+static int
+acc_function_bind_configfs(struct usb_configuration *c,
+			struct usb_function *f) {
+	return __acc_function_bind(c, f, true);
+}
+
+static void
+kill_all_hid_devices(struct acc_dev *dev)
+{
+	struct acc_hid_dev *hid;
+	struct list_head *entry, *temp;
+	unsigned long flags;
+
+	spin_lock_irqsave(&dev->lock, flags);
+	list_for_each_safe(entry, temp, &dev->hid_list) {
+		hid = list_entry(entry, struct acc_hid_dev, list);
+		list_del(&hid->list);
+		list_add(&hid->list, &dev->dead_hid_list);
+	}
+	list_for_each_safe(entry, temp, &dev->new_hid_list) {
+		hid = list_entry(entry, struct acc_hid_dev, list);
+		list_del(&hid->list);
+		list_add(&hid->list, &dev->dead_hid_list);
+	}
+	spin_unlock_irqrestore(&dev->lock, flags);
+
+	schedule_work(&dev->hid_work);
+}
+
+static void
+acc_hid_unbind(struct acc_dev *dev)
+{
+	hid_unregister_driver(&acc_hid_driver);
+	kill_all_hid_devices(dev);
+}
+
+static void
+acc_function_unbind(struct usb_configuration *c, struct usb_function *f)
+{
+	struct acc_dev	*dev = func_to_dev(f);
+	struct usb_request *req;
+	int i;
+
+	dev->online = 0;		/* clear online flag */
+	wake_up(&dev->read_wq);		/* unblock reads on closure */
+	wake_up(&dev->write_wq);	/* likewise for writes */
+
+	while ((req = req_get(dev, &dev->tx_idle)))
+		acc_request_free(req, dev->ep_in);
+	for (i = 0; i < RX_REQ_MAX; i++)
+		acc_request_free(dev->rx_req[i], dev->ep_out);
+
+	acc_hid_unbind(dev);
+}
+
+static void acc_getprotocol_work(struct work_struct *data)
+{
+	char *envp[2] = { "ACCESSORY=GETPROTOCOL", NULL };
+
+	kobject_uevent_env(&acc_device.this_device->kobj, KOBJ_CHANGE, envp);
+}
+
+static void acc_sendstring_work(struct work_struct *data)
+{
+	char *envp[2] = { "ACCESSORY=SENDSTRING", NULL };
+
+	kobject_uevent_env(&acc_device.this_device->kobj, KOBJ_CHANGE, envp);
+}
+
+static void acc_start_work(struct work_struct *data)
+{
+	char *envp[2] = { "ACCESSORY=START", NULL };
+
+	kobject_uevent_env(&acc_device.this_device->kobj, KOBJ_CHANGE, envp);
+}
+
+static int acc_hid_init(struct acc_hid_dev *hdev)
+{
+	struct hid_device *hid;
+	int ret;
+
+	hid = hid_allocate_device();
+	if (IS_ERR(hid))
+		return PTR_ERR(hid);
+
+	hid->ll_driver = &acc_hid_ll_driver;
+	hid->dev.parent = acc_device.this_device;
+
+	hid->bus = BUS_USB;
+	hid->vendor = HID_ANY_ID;
+	hid->product = HID_ANY_ID;
+	hid->driver_data = hdev;
+	ret = hid_add_device(hid);
+	if (ret) {
+		pr_err("can't add hid device: %d\n", ret);
+		hid_destroy_device(hid);
+		return ret;
+	}
+
+	hdev->hid = hid;
+	return 0;
+}
+
+static void acc_hid_delete(struct acc_hid_dev *hid)
+{
+	kfree(hid->report_desc);
+	kfree(hid);
+}
+
+static void acc_hid_work(struct work_struct *data)
+{
+	struct acc_dev *dev = get_acc_dev();
+	struct list_head	*entry, *temp;
+	struct acc_hid_dev *hid;
+	struct list_head	new_list, dead_list;
+	unsigned long flags;
+
+	if (!dev)
+		return;
+
+	INIT_LIST_HEAD(&new_list);
+
+	spin_lock_irqsave(&dev->lock, flags);
+
+	/* copy hids that are ready for initialization to new_list */
+	list_for_each_safe(entry, temp, &dev->new_hid_list) {
+		hid = list_entry(entry, struct acc_hid_dev, list);
+		if (hid->report_desc_offset == hid->report_desc_len)
+			list_move(&hid->list, &new_list);
+	}
+
+	if (list_empty(&dev->dead_hid_list)) {
+		INIT_LIST_HEAD(&dead_list);
+	} else {
+		/* move all of dev->dead_hid_list to dead_list */
+		dead_list.prev = dev->dead_hid_list.prev;
+		dead_list.next = dev->dead_hid_list.next;
+		dead_list.next->prev = &dead_list;
+		dead_list.prev->next = &dead_list;
+		INIT_LIST_HEAD(&dev->dead_hid_list);
+	}
+
+	spin_unlock_irqrestore(&dev->lock, flags);
+
+	/* register new HID devices */
+	list_for_each_safe(entry, temp, &new_list) {
+		hid = list_entry(entry, struct acc_hid_dev, list);
+		if (acc_hid_init(hid)) {
+			pr_err("can't add HID device %p\n", hid);
+			acc_hid_delete(hid);
+		} else {
+			spin_lock_irqsave(&dev->lock, flags);
+			list_move(&hid->list, &dev->hid_list);
+			spin_unlock_irqrestore(&dev->lock, flags);
+		}
+	}
+
+	/* remove dead HID devices */
+	list_for_each_safe(entry, temp, &dead_list) {
+		hid = list_entry(entry, struct acc_hid_dev, list);
+		list_del(&hid->list);
+		if (hid->hid)
+			hid_destroy_device(hid->hid);
+		acc_hid_delete(hid);
+	}
+
+	put_acc_dev(dev);
+}
+
+static int acc_function_set_alt(struct usb_function *f,
+		unsigned intf, unsigned alt)
+{
+	struct acc_dev	*dev = func_to_dev(f);
+	struct usb_composite_dev *cdev = f->config->cdev;
+	int ret;
+
+	DBG(cdev, "acc_function_set_alt intf: %d alt: %d\n", intf, alt);
+
+	ret = config_ep_by_speed(cdev->gadget, f, dev->ep_in);
+	if (ret)
+		return ret;
+
+	ret = usb_ep_enable(dev->ep_in);
+	if (ret)
+		return ret;
+
+	ret = config_ep_by_speed(cdev->gadget, f, dev->ep_out);
+	if (ret)
+		return ret;
+
+	ret = usb_ep_enable(dev->ep_out);
+	if (ret) {
+		usb_ep_disable(dev->ep_in);
+		return ret;
+	}
+
+	dev->online = 1;
+	dev->disconnected = 0; /* if online then not disconnected */
+
+	/* readers may be blocked waiting for us to go online */
+	wake_up(&dev->read_wq);
+	return 0;
+}
+
+static void acc_function_disable(struct usb_function *f)
+{
+	struct acc_dev	*dev = func_to_dev(f);
+	struct usb_composite_dev	*cdev = dev->cdev;
+
+	DBG(cdev, "acc_function_disable\n");
+	acc_set_disconnected(dev); /* this now only sets disconnected */
+	dev->online = 0; /* so now need to clear online flag here too */
+	usb_ep_disable(dev->ep_in);
+	usb_ep_disable(dev->ep_out);
+
+	/* readers may be blocked waiting for us to go online */
+	wake_up(&dev->read_wq);
+
+	VDBG(cdev, "%s disabled\n", dev->function.name);
+}
+
+static int acc_setup(void)
+{
+	struct acc_dev_ref *ref = &_acc_dev_ref;
+	struct acc_dev *dev;
+	int ret;
+
+	if (kref_read(&ref->kref))
+		return -EBUSY;
+
+	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
+	if (!dev)
+		return -ENOMEM;
+
+	spin_lock_init(&dev->lock);
+	init_waitqueue_head(&dev->read_wq);
+	init_waitqueue_head(&dev->write_wq);
+	atomic_set(&dev->open_excl, 0);
+	INIT_LIST_HEAD(&dev->tx_idle);
+	INIT_LIST_HEAD(&dev->hid_list);
+	INIT_LIST_HEAD(&dev->new_hid_list);
+	INIT_LIST_HEAD(&dev->dead_hid_list);
+	INIT_DELAYED_WORK(&dev->start_work, acc_start_work);
+	INIT_WORK(&dev->hid_work, acc_hid_work);
+	INIT_WORK(&dev->getprotocol_work, acc_getprotocol_work);
+	INIT_WORK(&dev->sendstring_work, acc_sendstring_work);
+
+	dev->ref = ref;
+	if (cmpxchg_relaxed(&ref->acc_dev, NULL, dev)) {
+		ret = -EBUSY;
+		goto err_free_dev;
+	}
+
+	ret = misc_register(&acc_device);
+	if (ret)
+		goto err_zap_ptr;
+
+	kref_init(&ref->kref);
+	return 0;
+
+err_zap_ptr:
+	ref->acc_dev = NULL;
+err_free_dev:
+	kfree(dev);
+	pr_err("USB accessory gadget driver failed to initialize\n");
+	return ret;
+}
+
+void acc_disconnect(void)
+{
+	struct acc_dev *dev = get_acc_dev();
+
+	if (!dev)
+		return;
+
+	/* unregister all HID devices if USB is disconnected */
+	kill_all_hid_devices(dev);
+	put_acc_dev(dev);
+}
+EXPORT_SYMBOL_GPL(acc_disconnect);
+
+static void acc_cleanup(void)
+{
+	struct acc_dev *dev = get_acc_dev();
+
+	misc_deregister(&acc_device);
+	put_acc_dev(dev);
+	put_acc_dev(dev); /* Pairs with kref_init() in acc_setup() */
+}
+static struct acc_instance *to_acc_instance(struct config_item *item)
+{
+	return container_of(to_config_group(item), struct acc_instance,
+		func_inst.group);
+}
+
+static void acc_attr_release(struct config_item *item)
+{
+	struct acc_instance *fi_acc = to_acc_instance(item);
+
+	usb_put_function_instance(&fi_acc->func_inst);
+}
+
+static struct configfs_item_operations acc_item_ops = {
+	.release        = acc_attr_release,
+};
+
+static struct config_item_type acc_func_type = {
+	.ct_item_ops    = &acc_item_ops,
+	.ct_owner       = THIS_MODULE,
+};
+
+static struct acc_instance *to_fi_acc(struct usb_function_instance *fi)
+{
+	return container_of(fi, struct acc_instance, func_inst);
+}
+
+static int acc_set_inst_name(struct usb_function_instance *fi, const char *name)
+{
+	struct acc_instance *fi_acc;
+	char *ptr;
+	int name_len;
+
+	name_len = strlen(name) + 1;
+	if (name_len > MAX_INST_NAME_LEN)
+		return -ENAMETOOLONG;
+
+	ptr = kstrndup(name, name_len, GFP_KERNEL);
+	if (!ptr)
+		return -ENOMEM;
+
+	fi_acc = to_fi_acc(fi);
+	fi_acc->name = ptr;
+	return 0;
+}
+
+static void acc_free_inst(struct usb_function_instance *fi)
+{
+	struct acc_instance *fi_acc;
+
+	fi_acc = to_fi_acc(fi);
+	kfree(fi_acc->name);
+	acc_cleanup();
+}
+
+static struct usb_function_instance *acc_alloc_inst(void)
+{
+	struct acc_instance *fi_acc;
+	int err;
+
+	fi_acc = kzalloc(sizeof(*fi_acc), GFP_KERNEL);
+	if (!fi_acc)
+		return ERR_PTR(-ENOMEM);
+	fi_acc->func_inst.set_inst_name = acc_set_inst_name;
+	fi_acc->func_inst.free_func_inst = acc_free_inst;
+
+	err = acc_setup();
+	if (err) {
+		kfree(fi_acc);
+		return ERR_PTR(err);
+	}
+
+	config_group_init_type_name(&fi_acc->func_inst.group,
+					"", &acc_func_type);
+	return  &fi_acc->func_inst;
+}
+
+static void acc_free(struct usb_function *f)
+{
+	struct acc_dev *dev = func_to_dev(f);
+
+	put_acc_dev(dev);
+}
+
+int acc_ctrlrequest_configfs(struct usb_function *f,
+			const struct usb_ctrlrequest *ctrl) {
+	if (f->config != NULL && f->config->cdev != NULL)
+		return acc_ctrlrequest(f->config->cdev, ctrl);
+	else
+		return -1;
+}
+
+static struct usb_function *acc_alloc(struct usb_function_instance *fi)
+{
+	struct acc_dev *dev = get_acc_dev();
+
+	dev->function.name = "accessory";
+	dev->function.strings = acc_strings,
+	dev->function.fs_descriptors = fs_acc_descs;
+	dev->function.hs_descriptors = hs_acc_descs;
+	dev->function.ss_descriptors = ss_acc_descs;
+	dev->function.ssp_descriptors = ssp_acc_descs;
+	dev->function.bind = acc_function_bind_configfs;
+	dev->function.unbind = acc_function_unbind;
+	dev->function.set_alt = acc_function_set_alt;
+	dev->function.disable = acc_function_disable;
+	dev->function.free_func = acc_free;
+	dev->function.setup = acc_ctrlrequest_configfs;
+
+	return &dev->function;
+}
+DECLARE_USB_FUNCTION_INIT(accessory, acc_alloc_inst, acc_alloc);
+MODULE_LICENSE("GPL");
diff -ruN a/drivers/usb/gadget/function/f_audio_source.c b/drivers/usb/gadget/function/f_audio_source.c
--- a/drivers/usb/gadget/function/f_audio_source.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/usb/gadget/function/f_audio_source.c	2021-12-23 08:35:49.000000000 +0100
@@ -0,0 +1,1071 @@
+/*
+ * Gadget Function Driver for USB audio source device
+ *
+ * Copyright (C) 2012 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/device.h>
+#include <linux/usb/audio.h>
+#include <linux/wait.h>
+#include <linux/pm_qos.h>
+#include <sound/core.h>
+#include <sound/initval.h>
+#include <sound/pcm.h>
+
+#include <linux/usb.h>
+#include <linux/usb_usual.h>
+#include <linux/usb/ch9.h>
+#include <linux/configfs.h>
+#include <linux/usb/composite.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#define SAMPLE_RATE 44100
+#define FRAMES_PER_MSEC (SAMPLE_RATE / 1000)
+
+#define IN_EP_MAX_PACKET_SIZE 256
+
+/* Number of requests to allocate */
+#define IN_EP_REQ_COUNT 4
+
+#define AUDIO_AC_INTERFACE	0
+#define AUDIO_AS_INTERFACE	1
+#define AUDIO_NUM_INTERFACES	2
+#define MAX_INST_NAME_LEN     40
+
+/* B.3.1  Standard AC Interface Descriptor */
+static struct usb_interface_descriptor ac_interface_desc = {
+	.bLength =		USB_DT_INTERFACE_SIZE,
+	.bDescriptorType =	USB_DT_INTERFACE,
+	.bNumEndpoints =	0,
+	.bInterfaceClass =	USB_CLASS_AUDIO,
+	.bInterfaceSubClass =	USB_SUBCLASS_AUDIOCONTROL,
+};
+
+DECLARE_UAC_AC_HEADER_DESCRIPTOR(2);
+
+#define UAC_DT_AC_HEADER_LENGTH	UAC_DT_AC_HEADER_SIZE(AUDIO_NUM_INTERFACES)
+/* 1 input terminal, 1 output terminal and 1 feature unit */
+#define UAC_DT_TOTAL_LENGTH (UAC_DT_AC_HEADER_LENGTH \
+	+ UAC_DT_INPUT_TERMINAL_SIZE + UAC_DT_OUTPUT_TERMINAL_SIZE \
+	+ UAC_DT_FEATURE_UNIT_SIZE(0))
+/* B.3.2  Class-Specific AC Interface Descriptor */
+static struct uac1_ac_header_descriptor_2 ac_header_desc = {
+	.bLength =		UAC_DT_AC_HEADER_LENGTH,
+	.bDescriptorType =	USB_DT_CS_INTERFACE,
+	.bDescriptorSubtype =	UAC_HEADER,
+	.bcdADC =		__constant_cpu_to_le16(0x0100),
+	.wTotalLength =		__constant_cpu_to_le16(UAC_DT_TOTAL_LENGTH),
+	.bInCollection =	AUDIO_NUM_INTERFACES,
+	.baInterfaceNr = {
+		[0] =		AUDIO_AC_INTERFACE,
+		[1] =		AUDIO_AS_INTERFACE,
+	}
+};
+
+#define INPUT_TERMINAL_ID	1
+static struct uac_input_terminal_descriptor input_terminal_desc = {
+	.bLength =		UAC_DT_INPUT_TERMINAL_SIZE,
+	.bDescriptorType =	USB_DT_CS_INTERFACE,
+	.bDescriptorSubtype =	UAC_INPUT_TERMINAL,
+	.bTerminalID =		INPUT_TERMINAL_ID,
+	.wTerminalType =	UAC_INPUT_TERMINAL_MICROPHONE,
+	.bAssocTerminal =	0,
+	.wChannelConfig =	0x3,
+};
+
+DECLARE_UAC_FEATURE_UNIT_DESCRIPTOR(0);
+
+#define FEATURE_UNIT_ID		2
+static struct uac_feature_unit_descriptor_0 feature_unit_desc = {
+	.bLength		= UAC_DT_FEATURE_UNIT_SIZE(0),
+	.bDescriptorType	= USB_DT_CS_INTERFACE,
+	.bDescriptorSubtype	= UAC_FEATURE_UNIT,
+	.bUnitID		= FEATURE_UNIT_ID,
+	.bSourceID		= INPUT_TERMINAL_ID,
+	.bControlSize		= 2,
+};
+
+#define OUTPUT_TERMINAL_ID	3
+static struct uac1_output_terminal_descriptor output_terminal_desc = {
+	.bLength		= UAC_DT_OUTPUT_TERMINAL_SIZE,
+	.bDescriptorType	= USB_DT_CS_INTERFACE,
+	.bDescriptorSubtype	= UAC_OUTPUT_TERMINAL,
+	.bTerminalID		= OUTPUT_TERMINAL_ID,
+	.wTerminalType		= UAC_TERMINAL_STREAMING,
+	.bAssocTerminal		= FEATURE_UNIT_ID,
+	.bSourceID		= FEATURE_UNIT_ID,
+};
+
+/* B.4.1  Standard AS Interface Descriptor */
+static struct usb_interface_descriptor as_interface_alt_0_desc = {
+	.bLength =		USB_DT_INTERFACE_SIZE,
+	.bDescriptorType =	USB_DT_INTERFACE,
+	.bAlternateSetting =	0,
+	.bNumEndpoints =	0,
+	.bInterfaceClass =	USB_CLASS_AUDIO,
+	.bInterfaceSubClass =	USB_SUBCLASS_AUDIOSTREAMING,
+};
+
+static struct usb_interface_descriptor as_interface_alt_1_desc = {
+	.bLength =		USB_DT_INTERFACE_SIZE,
+	.bDescriptorType =	USB_DT_INTERFACE,
+	.bAlternateSetting =	1,
+	.bNumEndpoints =	1,
+	.bInterfaceClass =	USB_CLASS_AUDIO,
+	.bInterfaceSubClass =	USB_SUBCLASS_AUDIOSTREAMING,
+};
+
+/* B.4.2  Class-Specific AS Interface Descriptor */
+static struct uac1_as_header_descriptor as_header_desc = {
+	.bLength =		UAC_DT_AS_HEADER_SIZE,
+	.bDescriptorType =	USB_DT_CS_INTERFACE,
+	.bDescriptorSubtype =	UAC_AS_GENERAL,
+	.bTerminalLink =	INPUT_TERMINAL_ID,
+	.bDelay =		1,
+	.wFormatTag =		UAC_FORMAT_TYPE_I_PCM,
+};
+
+DECLARE_UAC_FORMAT_TYPE_I_DISCRETE_DESC(1);
+
+static struct uac_format_type_i_discrete_descriptor_1 as_type_i_desc = {
+	.bLength =		UAC_FORMAT_TYPE_I_DISCRETE_DESC_SIZE(1),
+	.bDescriptorType =	USB_DT_CS_INTERFACE,
+	.bDescriptorSubtype =	UAC_FORMAT_TYPE,
+	.bFormatType =		UAC_FORMAT_TYPE_I,
+	.bSubframeSize =	2,
+	.bBitResolution =	16,
+	.bSamFreqType =		1,
+};
+
+/* Standard ISO IN Endpoint Descriptor for highspeed */
+static struct usb_endpoint_descriptor hs_as_in_ep_desc  = {
+	.bLength =		USB_DT_ENDPOINT_AUDIO_SIZE,
+	.bDescriptorType =	USB_DT_ENDPOINT,
+	.bEndpointAddress =	USB_DIR_IN,
+	.bmAttributes =		USB_ENDPOINT_SYNC_SYNC
+				| USB_ENDPOINT_XFER_ISOC,
+	.wMaxPacketSize =	__constant_cpu_to_le16(IN_EP_MAX_PACKET_SIZE),
+	.bInterval =		4, /* poll 1 per millisecond */
+};
+
+/* Standard ISO IN Endpoint Descriptor for highspeed */
+static struct usb_endpoint_descriptor fs_as_in_ep_desc  = {
+	.bLength =		USB_DT_ENDPOINT_AUDIO_SIZE,
+	.bDescriptorType =	USB_DT_ENDPOINT,
+	.bEndpointAddress =	USB_DIR_IN,
+	.bmAttributes =		USB_ENDPOINT_SYNC_SYNC
+				| USB_ENDPOINT_XFER_ISOC,
+	.wMaxPacketSize =	__constant_cpu_to_le16(IN_EP_MAX_PACKET_SIZE),
+	.bInterval =		1, /* poll 1 per millisecond */
+};
+
+/* Class-specific AS ISO OUT Endpoint Descriptor */
+static struct uac_iso_endpoint_descriptor as_iso_in_desc = {
+	.bLength =		UAC_ISO_ENDPOINT_DESC_SIZE,
+	.bDescriptorType =	USB_DT_CS_ENDPOINT,
+	.bDescriptorSubtype =	UAC_EP_GENERAL,
+	.bmAttributes =		1,
+	.bLockDelayUnits =	1,
+	.wLockDelay =		__constant_cpu_to_le16(1),
+};
+
+static struct usb_descriptor_header *hs_audio_desc[] = {
+	(struct usb_descriptor_header *)&ac_interface_desc,
+	(struct usb_descriptor_header *)&ac_header_desc,
+
+	(struct usb_descriptor_header *)&input_terminal_desc,
+	(struct usb_descriptor_header *)&output_terminal_desc,
+	(struct usb_descriptor_header *)&feature_unit_desc,
+
+	(struct usb_descriptor_header *)&as_interface_alt_0_desc,
+	(struct usb_descriptor_header *)&as_interface_alt_1_desc,
+	(struct usb_descriptor_header *)&as_header_desc,
+
+	(struct usb_descriptor_header *)&as_type_i_desc,
+
+	(struct usb_descriptor_header *)&hs_as_in_ep_desc,
+	(struct usb_descriptor_header *)&as_iso_in_desc,
+	NULL,
+};
+
+static struct usb_descriptor_header *fs_audio_desc[] = {
+	(struct usb_descriptor_header *)&ac_interface_desc,
+	(struct usb_descriptor_header *)&ac_header_desc,
+
+	(struct usb_descriptor_header *)&input_terminal_desc,
+	(struct usb_descriptor_header *)&output_terminal_desc,
+	(struct usb_descriptor_header *)&feature_unit_desc,
+
+	(struct usb_descriptor_header *)&as_interface_alt_0_desc,
+	(struct usb_descriptor_header *)&as_interface_alt_1_desc,
+	(struct usb_descriptor_header *)&as_header_desc,
+
+	(struct usb_descriptor_header *)&as_type_i_desc,
+
+	(struct usb_descriptor_header *)&fs_as_in_ep_desc,
+	(struct usb_descriptor_header *)&as_iso_in_desc,
+	NULL,
+};
+
+static struct snd_pcm_hardware audio_hw_info = {
+	.info =			SNDRV_PCM_INFO_MMAP |
+				SNDRV_PCM_INFO_MMAP_VALID |
+				SNDRV_PCM_INFO_BATCH |
+				SNDRV_PCM_INFO_INTERLEAVED |
+				SNDRV_PCM_INFO_BLOCK_TRANSFER,
+
+	.formats		= SNDRV_PCM_FMTBIT_S16_LE,
+	.channels_min		= 2,
+	.channels_max		= 2,
+	.rate_min		= SAMPLE_RATE,
+	.rate_max		= SAMPLE_RATE,
+
+	.buffer_bytes_max =	1024 * 1024,
+	.period_bytes_min =	64,
+	.period_bytes_max =	512 * 1024,
+	.periods_min =		2,
+	.periods_max =		1024,
+};
+
+/*-------------------------------------------------------------------------*/
+
+struct audio_source_config {
+	int	card;
+	int	device;
+};
+
+struct audio_dev {
+	struct usb_function		func;
+	struct snd_card			*card;
+	struct snd_pcm			*pcm;
+	struct snd_pcm_substream *substream;
+
+	struct list_head		idle_reqs;
+	struct usb_ep			*in_ep;
+
+	spinlock_t			lock;
+
+	/* beginning, end and current position in our buffer */
+	void				*buffer_start;
+	void				*buffer_end;
+	void				*buffer_pos;
+
+	/* byte size of a "period" */
+	unsigned int			period;
+	/* bytes sent since last call to snd_pcm_period_elapsed */
+	unsigned int			period_offset;
+	/* time we started playing */
+	ktime_t				start_time;
+	/* number of frames sent since start_time */
+	s64				frames_sent;
+	struct audio_source_config	*config;
+	/* for creating and issuing QoS requests */
+	struct pm_qos_request pm_qos;
+};
+
+static inline struct audio_dev *func_to_audio(struct usb_function *f)
+{
+	return container_of(f, struct audio_dev, func);
+}
+
+/*-------------------------------------------------------------------------*/
+
+struct audio_source_instance {
+	struct usb_function_instance func_inst;
+	const char *name;
+	struct audio_source_config *config;
+	struct device *audio_device;
+};
+
+static void audio_source_attr_release(struct config_item *item);
+
+static struct configfs_item_operations audio_source_item_ops = {
+	.release        = audio_source_attr_release,
+};
+
+static struct config_item_type audio_source_func_type = {
+	.ct_item_ops    = &audio_source_item_ops,
+	.ct_owner       = THIS_MODULE,
+};
+
+static ssize_t audio_source_pcm_show(struct device *dev,
+		struct device_attribute *attr, char *buf);
+
+static DEVICE_ATTR(pcm, S_IRUGO, audio_source_pcm_show, NULL);
+
+static struct device_attribute *audio_source_function_attributes[] = {
+	&dev_attr_pcm,
+	NULL
+};
+
+/*--------------------------------------------------------------------------*/
+
+static struct usb_request *audio_request_new(struct usb_ep *ep, int buffer_size)
+{
+	struct usb_request *req = usb_ep_alloc_request(ep, GFP_KERNEL);
+
+	if (!req)
+		return NULL;
+
+	req->buf = kmalloc(buffer_size, GFP_KERNEL);
+	if (!req->buf) {
+		usb_ep_free_request(ep, req);
+		return NULL;
+	}
+	req->length = buffer_size;
+	return req;
+}
+
+static void audio_request_free(struct usb_request *req, struct usb_ep *ep)
+{
+	if (req) {
+		kfree(req->buf);
+		usb_ep_free_request(ep, req);
+	}
+}
+
+static void audio_req_put(struct audio_dev *audio, struct usb_request *req)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&audio->lock, flags);
+	list_add_tail(&req->list, &audio->idle_reqs);
+	spin_unlock_irqrestore(&audio->lock, flags);
+}
+
+static struct usb_request *audio_req_get(struct audio_dev *audio)
+{
+	unsigned long flags;
+	struct usb_request *req;
+
+	spin_lock_irqsave(&audio->lock, flags);
+	if (list_empty(&audio->idle_reqs)) {
+		req = 0;
+	} else {
+		req = list_first_entry(&audio->idle_reqs, struct usb_request,
+				list);
+		list_del(&req->list);
+	}
+	spin_unlock_irqrestore(&audio->lock, flags);
+	return req;
+}
+
+/* send the appropriate number of packets to match our bitrate */
+static void audio_send(struct audio_dev *audio)
+{
+	struct snd_pcm_runtime *runtime;
+	struct usb_request *req;
+	int length, length1, length2, ret;
+	s64 msecs;
+	s64 frames;
+	ktime_t now;
+
+	/* audio->substream will be null if we have been closed */
+	if (!audio->substream)
+		return;
+	/* audio->buffer_pos will be null if we have been stopped */
+	if (!audio->buffer_pos)
+		return;
+
+	runtime = audio->substream->runtime;
+
+	/* compute number of frames to send */
+	now = ktime_get();
+	msecs = div_s64((ktime_to_ns(now) - ktime_to_ns(audio->start_time)),
+			1000000);
+	frames = div_s64((msecs * SAMPLE_RATE), 1000);
+
+	/* Readjust our frames_sent if we fall too far behind.
+	 * If we get too far behind it is better to drop some frames than
+	 * to keep sending data too fast in an attempt to catch up.
+	 */
+	if (frames - audio->frames_sent > 10 * FRAMES_PER_MSEC)
+		audio->frames_sent = frames - FRAMES_PER_MSEC;
+
+	frames -= audio->frames_sent;
+
+	/* We need to send something to keep the pipeline going */
+	if (frames <= 0)
+		frames = FRAMES_PER_MSEC;
+
+	while (frames > 0) {
+		req = audio_req_get(audio);
+		if (!req)
+			break;
+
+		length = frames_to_bytes(runtime, frames);
+		if (length > IN_EP_MAX_PACKET_SIZE)
+			length = IN_EP_MAX_PACKET_SIZE;
+
+		if (audio->buffer_pos + length > audio->buffer_end)
+			length1 = audio->buffer_end - audio->buffer_pos;
+		else
+			length1 = length;
+		memcpy(req->buf, audio->buffer_pos, length1);
+		if (length1 < length) {
+			/* Wrap around and copy remaining length
+			 * at beginning of buffer.
+			 */
+			length2 = length - length1;
+			memcpy(req->buf + length1, audio->buffer_start,
+					length2);
+			audio->buffer_pos = audio->buffer_start + length2;
+		} else {
+			audio->buffer_pos += length1;
+			if (audio->buffer_pos >= audio->buffer_end)
+				audio->buffer_pos = audio->buffer_start;
+		}
+
+		req->length = length;
+		ret = usb_ep_queue(audio->in_ep, req, GFP_ATOMIC);
+		if (ret < 0) {
+			pr_err("usb_ep_queue failed ret: %d\n", ret);
+			audio_req_put(audio, req);
+			break;
+		}
+
+		frames -= bytes_to_frames(runtime, length);
+		audio->frames_sent += bytes_to_frames(runtime, length);
+	}
+}
+
+static void audio_control_complete(struct usb_ep *ep, struct usb_request *req)
+{
+	/* nothing to do here */
+}
+
+static void audio_data_complete(struct usb_ep *ep, struct usb_request *req)
+{
+	struct audio_dev *audio = req->context;
+
+	pr_debug("audio_data_complete req->status %d req->actual %d\n",
+		req->status, req->actual);
+
+	audio_req_put(audio, req);
+
+	if (!audio->buffer_start || req->status)
+		return;
+
+	audio->period_offset += req->actual;
+	if (audio->period_offset >= audio->period) {
+		snd_pcm_period_elapsed(audio->substream);
+		audio->period_offset = 0;
+	}
+	audio_send(audio);
+}
+
+static int audio_set_endpoint_req(struct usb_function *f,
+		const struct usb_ctrlrequest *ctrl)
+{
+	int value = -EOPNOTSUPP;
+	u16 ep = le16_to_cpu(ctrl->wIndex);
+	u16 len = le16_to_cpu(ctrl->wLength);
+	u16 w_value = le16_to_cpu(ctrl->wValue);
+
+	pr_debug("bRequest 0x%x, w_value 0x%04x, len %d, endpoint %d\n",
+			ctrl->bRequest, w_value, len, ep);
+
+	switch (ctrl->bRequest) {
+	case UAC_SET_CUR:
+	case UAC_SET_MIN:
+	case UAC_SET_MAX:
+	case UAC_SET_RES:
+		value = len;
+		break;
+	default:
+		break;
+	}
+
+	return value;
+}
+
+static int audio_get_endpoint_req(struct usb_function *f,
+		const struct usb_ctrlrequest *ctrl)
+{
+	struct usb_composite_dev *cdev = f->config->cdev;
+	int value = -EOPNOTSUPP;
+	u8 ep = ((le16_to_cpu(ctrl->wIndex) >> 8) & 0xFF);
+	u16 len = le16_to_cpu(ctrl->wLength);
+	u16 w_value = le16_to_cpu(ctrl->wValue);
+	u8 *buf = cdev->req->buf;
+
+	pr_debug("bRequest 0x%x, w_value 0x%04x, len %d, endpoint %d\n",
+			ctrl->bRequest, w_value, len, ep);
+
+	if (w_value == UAC_EP_CS_ATTR_SAMPLE_RATE << 8) {
+		switch (ctrl->bRequest) {
+		case UAC_GET_CUR:
+		case UAC_GET_MIN:
+		case UAC_GET_MAX:
+		case UAC_GET_RES:
+			/* return our sample rate */
+			buf[0] = (u8)SAMPLE_RATE;
+			buf[1] = (u8)(SAMPLE_RATE >> 8);
+			buf[2] = (u8)(SAMPLE_RATE >> 16);
+			value = 3;
+			break;
+		default:
+			break;
+		}
+	}
+
+	return value;
+}
+
+static int
+audio_setup(struct usb_function *f, const struct usb_ctrlrequest *ctrl)
+{
+	struct usb_composite_dev *cdev = f->config->cdev;
+	struct usb_request *req = cdev->req;
+	int value = -EOPNOTSUPP;
+	u16 w_index = le16_to_cpu(ctrl->wIndex);
+	u16 w_value = le16_to_cpu(ctrl->wValue);
+	u16 w_length = le16_to_cpu(ctrl->wLength);
+
+	/* composite driver infrastructure handles everything; interface
+	 * activation uses set_alt().
+	 */
+	switch (ctrl->bRequestType) {
+	case USB_DIR_OUT | USB_TYPE_CLASS | USB_RECIP_ENDPOINT:
+		value = audio_set_endpoint_req(f, ctrl);
+		break;
+
+	case USB_DIR_IN | USB_TYPE_CLASS | USB_RECIP_ENDPOINT:
+		value = audio_get_endpoint_req(f, ctrl);
+		break;
+	}
+
+	/* respond with data transfer or status phase? */
+	if (value >= 0) {
+		pr_debug("audio req%02x.%02x v%04x i%04x l%d\n",
+			ctrl->bRequestType, ctrl->bRequest,
+			w_value, w_index, w_length);
+		req->zero = 0;
+		req->length = value;
+		req->complete = audio_control_complete;
+		value = usb_ep_queue(cdev->gadget->ep0, req, GFP_ATOMIC);
+		if (value < 0)
+			pr_err("audio response on err %d\n", value);
+	}
+
+	/* device either stalls (value < 0) or reports success */
+	return value;
+}
+
+static int audio_set_alt(struct usb_function *f, unsigned intf, unsigned alt)
+{
+	struct audio_dev *audio = func_to_audio(f);
+	struct usb_composite_dev *cdev = f->config->cdev;
+	int ret;
+
+	pr_debug("audio_set_alt intf %d, alt %d\n", intf, alt);
+
+	ret = config_ep_by_speed(cdev->gadget, f, audio->in_ep);
+	if (ret)
+		return ret;
+
+	usb_ep_enable(audio->in_ep);
+	return 0;
+}
+
+static void audio_disable(struct usb_function *f)
+{
+	struct audio_dev	*audio = func_to_audio(f);
+
+	pr_debug("audio_disable\n");
+	usb_ep_disable(audio->in_ep);
+}
+
+static void audio_free_func(struct usb_function *f)
+{
+	/* no-op */
+}
+
+/*-------------------------------------------------------------------------*/
+
+static void audio_build_desc(struct audio_dev *audio)
+{
+	u8 *sam_freq;
+	int rate;
+
+	/* Set channel numbers */
+	input_terminal_desc.bNrChannels = 2;
+	as_type_i_desc.bNrChannels = 2;
+
+	/* Set sample rates */
+	rate = SAMPLE_RATE;
+	sam_freq = as_type_i_desc.tSamFreq[0];
+	memcpy(sam_freq, &rate, 3);
+}
+
+
+static int snd_card_setup(struct usb_configuration *c,
+	struct audio_source_config *config);
+static struct audio_source_instance *to_fi_audio_source(
+	const struct usb_function_instance *fi);
+
+
+/* audio function driver setup/binding */
+static int
+audio_bind(struct usb_configuration *c, struct usb_function *f)
+{
+	struct usb_composite_dev *cdev = c->cdev;
+	struct audio_dev *audio = func_to_audio(f);
+	int status;
+	struct usb_ep *ep;
+	struct usb_request *req;
+	int i;
+	int err;
+
+	if (IS_ENABLED(CONFIG_USB_CONFIGFS)) {
+		struct audio_source_instance *fi_audio =
+				to_fi_audio_source(f->fi);
+		struct audio_source_config *config =
+				fi_audio->config;
+
+		err = snd_card_setup(c, config);
+		if (err)
+			return err;
+	}
+
+	audio_build_desc(audio);
+
+	/* allocate instance-specific interface IDs, and patch descriptors */
+	status = usb_interface_id(c, f);
+	if (status < 0)
+		goto fail;
+	ac_interface_desc.bInterfaceNumber = status;
+
+	/* AUDIO_AC_INTERFACE */
+	ac_header_desc.baInterfaceNr[0] = status;
+
+	status = usb_interface_id(c, f);
+	if (status < 0)
+		goto fail;
+	as_interface_alt_0_desc.bInterfaceNumber = status;
+	as_interface_alt_1_desc.bInterfaceNumber = status;
+
+	/* AUDIO_AS_INTERFACE */
+	ac_header_desc.baInterfaceNr[1] = status;
+
+	status = -ENODEV;
+
+	/* allocate our endpoint */
+	ep = usb_ep_autoconfig(cdev->gadget, &fs_as_in_ep_desc);
+	if (!ep)
+		goto fail;
+	audio->in_ep = ep;
+	ep->driver_data = audio; /* claim */
+
+	if (gadget_is_dualspeed(c->cdev->gadget))
+		hs_as_in_ep_desc.bEndpointAddress =
+			fs_as_in_ep_desc.bEndpointAddress;
+
+	f->fs_descriptors = fs_audio_desc;
+	f->hs_descriptors = hs_audio_desc;
+
+	for (i = 0, status = 0; i < IN_EP_REQ_COUNT && status == 0; i++) {
+		req = audio_request_new(ep, IN_EP_MAX_PACKET_SIZE);
+		if (req) {
+			req->context = audio;
+			req->complete = audio_data_complete;
+			audio_req_put(audio, req);
+		} else
+			status = -ENOMEM;
+	}
+
+fail:
+	return status;
+}
+
+static void
+audio_unbind(struct usb_configuration *c, struct usb_function *f)
+{
+	struct audio_dev *audio = func_to_audio(f);
+	struct usb_request *req;
+
+	while ((req = audio_req_get(audio)))
+		audio_request_free(req, audio->in_ep);
+
+	snd_card_free_when_closed(audio->card);
+	audio->card = NULL;
+	audio->pcm = NULL;
+	audio->substream = NULL;
+	audio->in_ep = NULL;
+
+	if (IS_ENABLED(CONFIG_USB_CONFIGFS)) {
+		struct audio_source_instance *fi_audio =
+				to_fi_audio_source(f->fi);
+		struct audio_source_config *config =
+				fi_audio->config;
+
+		config->card = -1;
+		config->device = -1;
+	}
+}
+
+static void audio_pcm_playback_start(struct audio_dev *audio)
+{
+	audio->start_time = ktime_get();
+	audio->frames_sent = 0;
+	audio_send(audio);
+}
+
+static void audio_pcm_playback_stop(struct audio_dev *audio)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&audio->lock, flags);
+	audio->buffer_start = 0;
+	audio->buffer_end = 0;
+	audio->buffer_pos = 0;
+	spin_unlock_irqrestore(&audio->lock, flags);
+}
+
+static int audio_pcm_open(struct snd_pcm_substream *substream)
+{
+	struct snd_pcm_runtime *runtime = substream->runtime;
+	struct audio_dev *audio = substream->private_data;
+
+	runtime->private_data = audio;
+	runtime->hw = audio_hw_info;
+	snd_pcm_limit_hw_rates(runtime);
+	runtime->hw.channels_max = 2;
+
+	audio->substream = substream;
+
+	/* Add the QoS request and set the latency to 0 */
+	cpu_latency_qos_add_request(&audio->pm_qos, 0);
+
+	return 0;
+}
+
+static int audio_pcm_close(struct snd_pcm_substream *substream)
+{
+	struct audio_dev *audio = substream->private_data;
+	unsigned long flags;
+
+	spin_lock_irqsave(&audio->lock, flags);
+
+	/* Remove the QoS request */
+	cpu_latency_qos_remove_request(&audio->pm_qos);
+
+	audio->substream = NULL;
+	spin_unlock_irqrestore(&audio->lock, flags);
+
+	return 0;
+}
+
+static int audio_pcm_hw_params(struct snd_pcm_substream *substream,
+				struct snd_pcm_hw_params *params)
+{
+	unsigned int channels = params_channels(params);
+	unsigned int rate = params_rate(params);
+
+	if (rate != SAMPLE_RATE)
+		return -EINVAL;
+	if (channels != 2)
+		return -EINVAL;
+
+	return snd_pcm_lib_alloc_vmalloc_buffer(substream,
+		params_buffer_bytes(params));
+}
+
+static int audio_pcm_hw_free(struct snd_pcm_substream *substream)
+{
+	return snd_pcm_lib_free_vmalloc_buffer(substream);
+}
+
+static int audio_pcm_prepare(struct snd_pcm_substream *substream)
+{
+	struct snd_pcm_runtime *runtime = substream->runtime;
+	struct audio_dev *audio = runtime->private_data;
+
+	audio->period = snd_pcm_lib_period_bytes(substream);
+	audio->period_offset = 0;
+	audio->buffer_start = runtime->dma_area;
+	audio->buffer_end = audio->buffer_start
+		+ snd_pcm_lib_buffer_bytes(substream);
+	audio->buffer_pos = audio->buffer_start;
+
+	return 0;
+}
+
+static snd_pcm_uframes_t audio_pcm_pointer(struct snd_pcm_substream *substream)
+{
+	struct snd_pcm_runtime *runtime = substream->runtime;
+	struct audio_dev *audio = runtime->private_data;
+	ssize_t bytes = audio->buffer_pos - audio->buffer_start;
+
+	/* return offset of next frame to fill in our buffer */
+	return bytes_to_frames(runtime, bytes);
+}
+
+static int audio_pcm_playback_trigger(struct snd_pcm_substream *substream,
+					int cmd)
+{
+	struct audio_dev *audio = substream->runtime->private_data;
+	int ret = 0;
+
+	switch (cmd) {
+	case SNDRV_PCM_TRIGGER_START:
+	case SNDRV_PCM_TRIGGER_RESUME:
+		audio_pcm_playback_start(audio);
+		break;
+
+	case SNDRV_PCM_TRIGGER_STOP:
+	case SNDRV_PCM_TRIGGER_SUSPEND:
+		audio_pcm_playback_stop(audio);
+		break;
+
+	default:
+		ret = -EINVAL;
+	}
+
+	return ret;
+}
+
+static struct audio_dev _audio_dev = {
+	.func = {
+		.name = "audio_source",
+		.bind = audio_bind,
+		.unbind = audio_unbind,
+		.set_alt = audio_set_alt,
+		.setup = audio_setup,
+		.disable = audio_disable,
+		.free_func = audio_free_func,
+	},
+	.lock = __SPIN_LOCK_UNLOCKED(_audio_dev.lock),
+	.idle_reqs = LIST_HEAD_INIT(_audio_dev.idle_reqs),
+};
+
+static struct snd_pcm_ops audio_playback_ops = {
+	.open		= audio_pcm_open,
+	.close		= audio_pcm_close,
+	.ioctl		= snd_pcm_lib_ioctl,
+	.hw_params	= audio_pcm_hw_params,
+	.hw_free	= audio_pcm_hw_free,
+	.prepare	= audio_pcm_prepare,
+	.trigger	= audio_pcm_playback_trigger,
+	.pointer	= audio_pcm_pointer,
+};
+
+int audio_source_bind_config(struct usb_configuration *c,
+		struct audio_source_config *config)
+{
+	struct audio_dev *audio;
+	int err;
+
+	config->card = -1;
+	config->device = -1;
+
+	audio = &_audio_dev;
+
+	err = snd_card_setup(c, config);
+	if (err)
+		return err;
+
+	err = usb_add_function(c, &audio->func);
+	if (err)
+		goto add_fail;
+
+	return 0;
+
+add_fail:
+	snd_card_free(audio->card);
+	return err;
+}
+
+static int snd_card_setup(struct usb_configuration *c,
+		struct audio_source_config *config)
+{
+	struct audio_dev *audio;
+	struct snd_card *card;
+	struct snd_pcm *pcm;
+	int err;
+
+	audio = &_audio_dev;
+
+	err = snd_card_new(&c->cdev->gadget->dev,
+			SNDRV_DEFAULT_IDX1, SNDRV_DEFAULT_STR1,
+			THIS_MODULE, 0, &card);
+	if (err)
+		return err;
+
+	err = snd_pcm_new(card, "USB audio source", 0, 1, 0, &pcm);
+	if (err)
+		goto pcm_fail;
+
+	pcm->private_data = audio;
+	pcm->info_flags = 0;
+	audio->pcm = pcm;
+
+	strlcpy(pcm->name, "USB gadget audio", sizeof(pcm->name));
+
+	snd_pcm_set_ops(pcm, SNDRV_PCM_STREAM_PLAYBACK, &audio_playback_ops);
+	snd_pcm_lib_preallocate_pages_for_all(pcm, SNDRV_DMA_TYPE_DEV,
+				NULL, 0, 64 * 1024);
+
+	strlcpy(card->driver, "audio_source", sizeof(card->driver));
+	strlcpy(card->shortname, card->driver, sizeof(card->shortname));
+	strlcpy(card->longname, "USB accessory audio source",
+		sizeof(card->longname));
+
+	err = snd_card_register(card);
+	if (err)
+		goto register_fail;
+
+	config->card = pcm->card->number;
+	config->device = pcm->device;
+	audio->card = card;
+	return 0;
+
+register_fail:
+pcm_fail:
+	snd_card_free(audio->card);
+	return err;
+}
+
+static struct audio_source_instance *to_audio_source_instance(
+					struct config_item *item)
+{
+	return container_of(to_config_group(item), struct audio_source_instance,
+		func_inst.group);
+}
+
+static struct audio_source_instance *to_fi_audio_source(
+					const struct usb_function_instance *fi)
+{
+	return container_of(fi, struct audio_source_instance, func_inst);
+}
+
+static void audio_source_attr_release(struct config_item *item)
+{
+	struct audio_source_instance *fi_audio = to_audio_source_instance(item);
+
+	usb_put_function_instance(&fi_audio->func_inst);
+}
+
+static int audio_source_set_inst_name(struct usb_function_instance *fi,
+					const char *name)
+{
+	struct audio_source_instance *fi_audio;
+	char *ptr;
+	int name_len;
+
+	name_len = strlen(name) + 1;
+	if (name_len > MAX_INST_NAME_LEN)
+		return -ENAMETOOLONG;
+
+	ptr = kstrndup(name, name_len, GFP_KERNEL);
+	if (!ptr)
+		return -ENOMEM;
+
+	fi_audio = to_fi_audio_source(fi);
+	fi_audio->name = ptr;
+
+	return 0;
+}
+
+static void audio_source_free_inst(struct usb_function_instance *fi)
+{
+	struct audio_source_instance *fi_audio;
+
+	fi_audio = to_fi_audio_source(fi);
+	device_destroy(fi_audio->audio_device->class,
+			fi_audio->audio_device->devt);
+	kfree(fi_audio->name);
+	kfree(fi_audio->config);
+}
+
+static ssize_t audio_source_pcm_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct audio_source_instance *fi_audio = dev_get_drvdata(dev);
+	struct audio_source_config *config = fi_audio->config;
+
+	/* print PCM card and device numbers */
+	return sprintf(buf, "%d %d\n", config->card, config->device);
+}
+
+struct device *create_function_device(char *name);
+
+static struct usb_function_instance *audio_source_alloc_inst(void)
+{
+	struct audio_source_instance *fi_audio;
+	struct device_attribute **attrs;
+	struct device_attribute *attr;
+	struct device *dev;
+	void *err_ptr;
+	int err = 0;
+
+	fi_audio = kzalloc(sizeof(*fi_audio), GFP_KERNEL);
+	if (!fi_audio)
+		return ERR_PTR(-ENOMEM);
+
+	fi_audio->func_inst.set_inst_name = audio_source_set_inst_name;
+	fi_audio->func_inst.free_func_inst = audio_source_free_inst;
+
+	fi_audio->config = kzalloc(sizeof(struct audio_source_config),
+							GFP_KERNEL);
+	if (!fi_audio->config) {
+		err_ptr = ERR_PTR(-ENOMEM);
+		goto fail_audio;
+	}
+
+	config_group_init_type_name(&fi_audio->func_inst.group, "",
+						&audio_source_func_type);
+	dev = create_function_device("f_audio_source");
+
+	if (IS_ERR(dev)) {
+		err_ptr = dev;
+		goto fail_audio_config;
+	}
+
+	fi_audio->config->card = -1;
+	fi_audio->config->device = -1;
+	fi_audio->audio_device = dev;
+
+	attrs = audio_source_function_attributes;
+	if (attrs) {
+		while ((attr = *attrs++) && !err)
+			err = device_create_file(dev, attr);
+		if (err) {
+			err_ptr = ERR_PTR(-EINVAL);
+			goto fail_device;
+		}
+	}
+
+	dev_set_drvdata(dev, fi_audio);
+	_audio_dev.config = fi_audio->config;
+
+	return  &fi_audio->func_inst;
+
+fail_device:
+	device_destroy(dev->class, dev->devt);
+fail_audio_config:
+	kfree(fi_audio->config);
+fail_audio:
+	kfree(fi_audio);
+	return err_ptr;
+
+}
+
+static struct usb_function *audio_source_alloc(struct usb_function_instance *fi)
+{
+	return &_audio_dev.func;
+}
+
+DECLARE_USB_FUNCTION_INIT(audio_source, audio_source_alloc_inst,
+			audio_source_alloc);
+MODULE_LICENSE("GPL");
diff -ruN a/drivers/usb/gadget/function/f_midi.c b/drivers/usb/gadget/function/f_midi.c
--- a/drivers/usb/gadget/function/f_midi.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/usb/gadget/function/f_midi.c	2021-12-23 08:35:49.000000000 +0100
@@ -1224,6 +1224,65 @@
 	}
 }
 
+#ifdef CONFIG_USB_CONFIGFS_UEVENT
+extern struct device *create_function_device(char *name);
+static ssize_t alsa_show(struct device *dev,
+		struct device_attribute *attr, char *buf)
+{
+	struct usb_function_instance *fi_midi = dev_get_drvdata(dev);
+	struct f_midi *midi;
+
+	if (!fi_midi->f)
+		dev_warn(dev, "f_midi: function not set\n");
+
+	if (fi_midi && fi_midi->f) {
+		midi = func_to_midi(fi_midi->f);
+		if (midi->rmidi && midi->card && midi->rmidi->card)
+			return sprintf(buf, "%d %d\n",
+			midi->rmidi->card->number, midi->rmidi->device);
+	}
+
+	/* print PCM card and device numbers */
+	return sprintf(buf, "%d %d\n", -1, -1);
+}
+
+static DEVICE_ATTR(alsa, S_IRUGO, alsa_show, NULL);
+
+static struct device_attribute *alsa_function_attributes[] = {
+	&dev_attr_alsa,
+	NULL
+};
+
+static int create_alsa_device(struct usb_function_instance *fi)
+{
+	struct device *dev;
+	struct device_attribute **attrs;
+	struct device_attribute *attr;
+	int err = 0;
+
+	dev = create_function_device("f_midi");
+	if (IS_ERR(dev))
+		return PTR_ERR(dev);
+
+	attrs = alsa_function_attributes;
+	if (attrs) {
+		while ((attr = *attrs++) && !err)
+			err = device_create_file(dev, attr);
+		if (err) {
+			device_destroy(dev->class, dev->devt);
+			return -EINVAL;
+		}
+	}
+	dev_set_drvdata(dev, fi);
+	return 0;
+}
+#else
+static int create_alsa_device(struct usb_function_instance *fi)
+{
+	return 0;
+}
+#endif
+
 static struct usb_function_instance *f_midi_alloc_inst(void)
 {
 	struct f_midi_opts *opts;
@@ -1242,6 +1301,11 @@
 	opts->out_ports = 1;
 	opts->refcnt = 1;
 
+	if (create_alsa_device(&opts->func_inst)) {
+		kfree(opts);
+		return ERR_PTR(-ENODEV);
+	}
+
 	config_group_init_type_name(&opts->func_inst.group, "",
 				    &midi_func_type);
 
@@ -1262,6 +1326,7 @@
 		kfifo_free(&midi->in_req_fifo);
 		kfree(midi);
 		free = true;
+		opts->func_inst.f = NULL;
 	}
 	mutex_unlock(&opts->lock);
 
@@ -1349,6 +1414,7 @@
 	midi->func.disable	= f_midi_disable;
 	midi->func.free_func	= f_midi_free;
 
+	fi->f = &midi->func;
 	return &midi->func;
 
 midi_free:
diff -ruN a/drivers/usb/gadget/function/Makefile b/drivers/usb/gadget/function/Makefile
--- a/drivers/usb/gadget/function/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/usb/gadget/function/Makefile	2021-12-23 08:35:49.000000000 +0100
@@ -50,3 +50,7 @@
 obj-$(CONFIG_USB_F_PRINTER)	+= usb_f_printer.o
 usb_f_tcm-y			:= f_tcm.o
 obj-$(CONFIG_USB_F_TCM)		+= usb_f_tcm.o
+usb_f_accessory-y		:= f_accessory.o
+obj-$(CONFIG_USB_F_ACC)		+= usb_f_accessory.o
+usb_f_audio_source-y		:= f_audio_source.o
+obj-$(CONFIG_USB_F_AUDIO_SRC)	+= usb_f_audio_source.o
diff -ruN a/drivers/usb/gadget/Kconfig b/drivers/usb/gadget/Kconfig
--- a/drivers/usb/gadget/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/usb/gadget/Kconfig	2021-12-23 08:35:49.000000000 +0100
@@ -216,6 +216,12 @@
 config USB_F_TCM
 	tristate
 
+config USB_F_ACC
+	tristate
+
+config USB_F_AUDIO_SRC
+	tristate
+
 # this first set of drivers all depend on bulk-capable hardware.
 
 config USB_CONFIGFS
@@ -230,6 +236,14 @@
 	  appropriate symbolic links.
 	  For more information see Documentation/usb/gadget_configfs.rst.
 
+config USB_CONFIGFS_UEVENT
+	bool "Uevent notification of Gadget state"
+	depends on USB_CONFIGFS
+	help
+	  Enable uevent notifications to userspace when the gadget
+	  state changes. The gadget can be in any of the following
+	  three states: "CONNECTED/DISCONNECTED/CONFIGURED"
+
 config USB_CONFIGFS_SERIAL
 	bool "Generic serial bulk in/out"
 	depends on USB_CONFIGFS
@@ -371,6 +385,24 @@
 	  implemented in kernel space (for instance Ethernet, serial or
 	  mass storage) and other are implemented in user space.
 
+config USB_CONFIGFS_F_ACC
+	bool "Accessory gadget"
+	depends on USB_CONFIGFS
+	depends on HID=y
+	select USB_F_ACC
+	help
+	  USB gadget Accessory support
+
+config USB_CONFIGFS_F_AUDIO_SRC
+	bool "Audio Source gadget"
+	depends on USB_CONFIGFS
+	depends on USB_CONFIGFS_UEVENT
+	depends on SND
+	select SND_PCM
+	select USB_F_AUDIO_SRC
+	help
+	  USB gadget Audio Source support
+
 config USB_CONFIGFS_F_UAC1
 	bool "Audio Class 1.0"
 	depends on USB_CONFIGFS
diff -ruN a/drivers/usb/host/xhci-ring.c b/drivers/usb/host/xhci-ring.c
--- a/drivers/usb/host/xhci-ring.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/usb/host/xhci-ring.c	2021-12-23 08:35:49.000000000 +0100
@@ -152,6 +152,54 @@
 	}
 }
 
+/* Forward dequeue pointer to the specific position,
+ * walk through the ring and reclaim free trb slots to num_trbs_free
+ */
+static int move_deq(struct xhci_hcd *xhci, struct xhci_ring *ep_ring,
+		    struct xhci_segment *new_seg, union xhci_trb *new_deq)
+{
+	unsigned int steps;
+	union xhci_trb *deq;
+	struct xhci_segment *seg = ep_ring->deq_seg;
+
+	/* direct paths */
+	if (ep_ring->dequeue == new_deq) {
+		return 0;
+	} else if ((ep_ring->deq_seg == new_seg) &&
+	    (ep_ring->dequeue <= new_deq)) {
+		steps = new_deq - ep_ring->dequeue;
+		deq = new_deq;
+		goto found;
+	}
+
+	/* fast walk to the next segment */
+	seg = seg->next;
+	steps = (TRBS_PER_SEGMENT - 1) -
+		(ep_ring->dequeue - ep_ring->deq_seg->trbs);
+	deq = &seg->trbs[0];
+
+	while (deq != new_deq) {
+		if (trb_is_link(deq)) {
+			seg = seg->next;
+			deq = seg->trbs;
+		} else {
+			steps++;
+			deq++;
+		}
+		if (deq == ep_ring->dequeue) {
+			xhci_warn(xhci, "Unable to find new dequeue pointer\n");
+			return -ENOENT;
+		}
+	}
+
+found:
+	ep_ring->deq_seg = seg;
+	ep_ring->dequeue = deq;
+	ep_ring->num_trbs_free += steps;
+
+	return 0;
+}
+
 /*
  * See Cycle bit rules. SW is the consumer for the event ring only.
  */
@@ -1303,52 +1351,6 @@
 			"xHCI host controller is dead.");
 }
 
-static void update_ring_for_set_deq_completion(struct xhci_hcd *xhci,
-		struct xhci_virt_device *dev,
-		struct xhci_ring *ep_ring,
-		unsigned int ep_index)
-{
-	union xhci_trb *dequeue_temp;
-	int num_trbs_free_temp;
-	bool revert = false;
-
-	num_trbs_free_temp = ep_ring->num_trbs_free;
-	dequeue_temp = ep_ring->dequeue;
-
-	/* If we get two back-to-back stalls, and the first stalled transfer
-	 * ends just before a link TRB, the dequeue pointer will be left on
-	 * the link TRB by the code in the while loop.  So we have to update
-	 * the dequeue pointer one segment further, or we'll jump off
-	 * the segment into la-la-land.
-	 */
-	if (trb_is_link(ep_ring->dequeue)) {
-		ep_ring->deq_seg = ep_ring->deq_seg->next;
-		ep_ring->dequeue = ep_ring->deq_seg->trbs;
-	}
-
-	while (ep_ring->dequeue != dev->eps[ep_index].queued_deq_ptr) {
-		/* We have more usable TRBs */
-		ep_ring->num_trbs_free++;
-		ep_ring->dequeue++;
-		if (trb_is_link(ep_ring->dequeue)) {
-			if (ep_ring->dequeue ==
-					dev->eps[ep_index].queued_deq_ptr)
-				break;
-			ep_ring->deq_seg = ep_ring->deq_seg->next;
-			ep_ring->dequeue = ep_ring->deq_seg->trbs;
-		}
-		if (ep_ring->dequeue == dequeue_temp) {
-			revert = true;
-			break;
-		}
-	}
-
-	if (revert) {
-		xhci_dbg(xhci, "Unable to find new dequeue pointer\n");
-		ep_ring->num_trbs_free = num_trbs_free_temp;
-	}
-}
-
 /*
  * When we get a completion for a Set Transfer Ring Dequeue Pointer command,
  * we need to clear the set deq pending flag in the endpoint ring state, so that
@@ -1435,8 +1437,8 @@
 			/* Update the ring's dequeue segment and dequeue pointer
 			 * to reflect the new position.
 			 */
-			update_ring_for_set_deq_completion(xhci, ep->vdev,
-				ep_ring, ep_index);
+			move_deq(xhci, ep_ring, ep->queued_deq_seg,
+				 ep->queued_deq_ptr);
 		} else {
 			xhci_warn(xhci, "Mismatch between completed Set TR Deq Ptr command & xHCI internal state.\n");
 			xhci_warn(xhci, "ep deq seg = %p, deq ptr = %p\n",
@@ -2276,9 +2278,7 @@
 	}
 
 	/* Update ring dequeue pointer */
-	ep_ring->dequeue = td->last_trb;
-	ep_ring->deq_seg = td->last_trb_seg;
-	ep_ring->num_trbs_free += td->num_trbs - 1;
+	move_deq(xhci, ep_ring, td->last_trb_seg, td->last_trb);
 	inc_deq(xhci, ep_ring);
 
 	return xhci_td_cleanup(xhci, td, ep_ring, td->status);
@@ -2498,9 +2498,7 @@
 	frame->actual_length = 0;
 
 	/* Update ring dequeue pointer */
-	ep->ring->dequeue = td->last_trb;
-	ep->ring->deq_seg = td->last_trb_seg;
-	ep->ring->num_trbs_free += td->num_trbs - 1;
+	move_deq(xhci, ep->ring, td->last_trb_seg, td->last_trb);
 	inc_deq(xhci, ep->ring);
 
 	return xhci_td_cleanup(xhci, td, ep->ring, status);
diff -ruN a/drivers/vfio/pci/vfio_pci_igd.c b/drivers/vfio/pci/vfio_pci_igd.c
--- a/drivers/vfio/pci/vfio_pci_igd.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/vfio/pci/vfio_pci_igd.c	2021-12-23 08:35:50.000000000 +0100
@@ -25,20 +25,121 @@
 #define OPREGION_RVDS		0x3c2
 #define OPREGION_VERSION	0x16
 
+struct igd_opregion_vbt {
+	void *opregion;
+	void *vbt_ex;
+};
+
+/**
+ * igd_opregion_shift_copy() - Copy OpRegion to user buffer and shift position.
+ * @dst: User buffer ptr to copy to.
+ * @off: Offset to user buffer ptr. Increased by bytes on return.
+ * @src: Source buffer to copy from.
+ * @pos: Increased by bytes on return.
+ * @remaining: Decreased by bytes on return.
+ * @bytes: Bytes to copy and adjust off, pos and remaining.
+ *
+ * Copy OpRegion to offset from specific source ptr and shift the offset.
+ *
+ * Return: 0 on success, -EFAULT otherwise.
+ *
+ */
+static inline unsigned long igd_opregion_shift_copy(char __user *dst,
+						    loff_t *off,
+						    void *src,
+						    loff_t *pos,
+						    size_t *remaining,
+						    size_t bytes)
+{
+	if (copy_to_user(dst + (*off), src, bytes))
+		return -EFAULT;
+
+	*off += bytes;
+	*pos += bytes;
+	*remaining -= bytes;
+
+	return 0;
+}
+
 static ssize_t vfio_pci_igd_rw(struct vfio_pci_core_device *vdev,
 			       char __user *buf, size_t count, loff_t *ppos,
 			       bool iswrite)
 {
 	unsigned int i = VFIO_PCI_OFFSET_TO_INDEX(*ppos) - VFIO_PCI_NUM_REGIONS;
-	void *base = vdev->region[i].data;
-	loff_t pos = *ppos & VFIO_PCI_OFFSET_MASK;
+	struct igd_opregion_vbt *opregionvbt = vdev->region[i].data;
+	loff_t pos = *ppos & VFIO_PCI_OFFSET_MASK, off = 0;
+	size_t remaining;
 
 	if (pos >= vdev->region[i].size || iswrite)
 		return -EINVAL;
 
-	count = min(count, (size_t)(vdev->region[i].size - pos));
+	count = min_t(size_t, count, vdev->region[i].size - pos);
+	remaining = count;
+
+	/* Copy until OpRegion version */
+	if (remaining && pos < OPREGION_VERSION) {
+		size_t bytes = min_t(size_t, remaining, OPREGION_VERSION - pos);
+
+		if (igd_opregion_shift_copy(buf, &off,
+					    opregionvbt->opregion + pos, &pos,
+					    &remaining, bytes))
+			return -EFAULT;
+	}
+
+	/* Copy patched (if necessary) OpRegion version */
+	if (remaining && pos < OPREGION_VERSION + sizeof(__le16)) {
+		size_t bytes = min_t(size_t, remaining,
+				     OPREGION_VERSION + sizeof(__le16) - pos);
+		__le16 version = *(__le16 *)(opregionvbt->opregion +
+					     OPREGION_VERSION);
+
+		/* Patch to 2.1 if OpRegion 2.0 has extended VBT */
+		if (le16_to_cpu(version) == 0x0200 && opregionvbt->vbt_ex)
+			version = cpu_to_le16(0x0201);
+
+		if (igd_opregion_shift_copy(buf, &off,
+					    &version + (pos - OPREGION_VERSION),
+					    &pos, &remaining, bytes))
+			return -EFAULT;
+	}
+
+	/* Copy until RVDA */
+	if (remaining && pos < OPREGION_RVDA) {
+		size_t bytes = min_t(size_t, remaining, OPREGION_RVDA - pos);
+
+		if (igd_opregion_shift_copy(buf, &off,
+					    opregionvbt->opregion + pos, &pos,
+					    &remaining, bytes))
+			return -EFAULT;
+	}
+
+	/* Copy modified (if necessary) RVDA */
+	if (remaining && pos < OPREGION_RVDA + sizeof(__le64)) {
+		size_t bytes = min_t(size_t, remaining,
+				     OPREGION_RVDA + sizeof(__le64) - pos);
+		__le64 rvda = cpu_to_le64(opregionvbt->vbt_ex ?
+					  OPREGION_SIZE : 0);
+
+		if (igd_opregion_shift_copy(buf, &off,
+					    &rvda + (pos - OPREGION_RVDA),
+					    &pos, &remaining, bytes))
+			return -EFAULT;
+	}
 
-	if (copy_to_user(buf, base + pos, count))
+	/* Copy the rest of OpRegion */
+	if (remaining && pos < OPREGION_SIZE) {
+		size_t bytes = min_t(size_t, remaining, OPREGION_SIZE - pos);
+
+		if (igd_opregion_shift_copy(buf, &off,
+					    opregionvbt->opregion + pos, &pos,
+					    &remaining, bytes))
+			return -EFAULT;
+	}
+
+	/* Copy extended VBT if exists */
+	if (remaining &&
+	    copy_to_user(buf + off, opregionvbt->vbt_ex + (pos - OPREGION_SIZE),
+			 remaining))
 		return -EFAULT;
 
 	*ppos += count;
@@ -49,7 +150,13 @@
 static void vfio_pci_igd_release(struct vfio_pci_core_device *vdev,
 				 struct vfio_pci_region *region)
 {
-	memunmap(region->data);
+	struct igd_opregion_vbt *opregionvbt = region->data;
+
+	if (opregionvbt->vbt_ex)
+		memunmap(opregionvbt->vbt_ex);
+
+	memunmap(opregionvbt->opregion);
+	kfree(opregionvbt);
 }
 
 static const struct vfio_pci_regops vfio_pci_igd_regops = {
@@ -61,7 +168,7 @@
 {
 	__le32 *dwordp = (__le32 *)(vdev->vconfig + OPREGION_PCI_ADDR);
 	u32 addr, size;
-	void *base;
+	struct igd_opregion_vbt *opregionvbt;
 	int ret;
 	u16 version;
 
@@ -72,84 +179,93 @@
 	if (!addr || !(~addr))
 		return -ENODEV;
 
-	base = memremap(addr, OPREGION_SIZE, MEMREMAP_WB);
-	if (!base)
+	opregionvbt = kzalloc(sizeof(*opregionvbt), GFP_KERNEL);
+	if (!opregionvbt)
 		return -ENOMEM;
 
-	if (memcmp(base, OPREGION_SIGNATURE, 16)) {
-		memunmap(base);
+	opregionvbt->opregion = memremap(addr, OPREGION_SIZE, MEMREMAP_WB);
+	if (!opregionvbt->opregion) {
+		kfree(opregionvbt);
+		return -ENOMEM;
+	}
+
+	if (memcmp(opregionvbt->opregion, OPREGION_SIGNATURE, 16)) {
+		memunmap(opregionvbt->opregion);
+		kfree(opregionvbt);
 		return -EINVAL;
 	}
 
-	size = le32_to_cpu(*(__le32 *)(base + 16));
+	size = le32_to_cpu(*(__le32 *)(opregionvbt->opregion + 16));
 	if (!size) {
-		memunmap(base);
+		memunmap(opregionvbt->opregion);
+		kfree(opregionvbt);
 		return -EINVAL;
 	}
 
 	size *= 1024; /* In KB */
 
 	/*
-	 * Support opregion v2.1+
-	 * When VBT data exceeds 6KB size and cannot be within mailbox #4, then
-	 * the Extended VBT region next to opregion is used to hold the VBT data.
-	 * RVDA (Relative Address of VBT Data from Opregion Base) and RVDS
-	 * (Raw VBT Data Size) from opregion structure member are used to hold the
-	 * address from region base and size of VBT data. RVDA/RVDS are not
-	 * defined before opregion 2.0.
-	 *
-	 * opregion 2.1+: RVDA is unsigned, relative offset from
-	 * opregion base, and should point to the end of opregion.
-	 * otherwise, exposing to userspace to allow read access to everything between
-	 * the OpRegion and VBT is not safe.
-	 * RVDS is defined as size in bytes.
+	 * OpRegion and VBT:
+	 * When VBT data doesn't exceed 6KB, it's stored in Mailbox #4.
+	 * When VBT data exceeds 6KB size, Mailbox #4 is no longer large enough
+	 * to hold the VBT data, the Extended VBT region is introduced since
+	 * OpRegion 2.0 to hold the VBT data. Since OpRegion 2.0, RVDA/RVDS are
+	 * introduced to define the extended VBT data location and size.
+	 * OpRegion 2.0: RVDA defines the absolute physical address of the
+	 *   extended VBT data, RVDS defines the VBT data size.
+	 * OpRegion 2.1 and above: RVDA defines the relative address of the
+	 *   extended VBT data to OpRegion base, RVDS defines the VBT data size.
 	 *
-	 * opregion 2.0: rvda is the physical VBT address.
-	 * Since rvda is HPA it cannot be directly used in guest.
-	 * And it should not be practically available for end user,so it is not supported.
+	 * Due to the RVDA definition diff in OpRegion VBT (also the only diff
+	 * between 2.0 and 2.1), exposing OpRegion and VBT as a contiguous range
+	 * for OpRegion 2.0 and above makes it possible to support the
+	 * non-contiguous VBT through a single vfio region. From r/w ops view,
+	 * only contiguous VBT after OpRegion with version 2.1+ is exposed,
+	 * regardless the host OpRegion is 2.0 or non-contiguous 2.1+. The r/w
+	 * ops will on-the-fly shift the actural offset into VBT so that data at
+	 * correct position can be returned to the requester.
 	 */
-	version = le16_to_cpu(*(__le16 *)(base + OPREGION_VERSION));
+	version = le16_to_cpu(*(__le16 *)(opregionvbt->opregion +
+					  OPREGION_VERSION));
 	if (version >= 0x0200) {
-		u64 rvda;
-		u32 rvds;
+		u64 rvda = le64_to_cpu(*(__le64 *)(opregionvbt->opregion +
+						   OPREGION_RVDA));
+		u32 rvds = le32_to_cpu(*(__le32 *)(opregionvbt->opregion +
+						   OPREGION_RVDS));
 
-		rvda = le64_to_cpu(*(__le64 *)(base + OPREGION_RVDA));
-		rvds = le32_to_cpu(*(__le32 *)(base + OPREGION_RVDS));
+		/* The extended VBT is valid only when RVDA/RVDS are non-zero */
 		if (rvda && rvds) {
-			/* no support for opregion v2.0 with physical VBT address */
-			if (version == 0x0200) {
-				memunmap(base);
-				pci_err(vdev->pdev,
-					"IGD assignment does not support opregion v2.0 with an extended VBT region\n");
-				return -EINVAL;
-			}
+			size += rvds;
 
-			if (rvda != size) {
-				memunmap(base);
-				pci_err(vdev->pdev,
-					"Extended VBT does not follow opregion on version 0x%04x\n",
-					version);
-				return -EINVAL;
+			/*
+			 * Extended VBT location by RVDA:
+			 * Absolute physical addr for 2.0.
+			 * Relative addr to OpRegion header for 2.1+.
+			 */
+			if (version == 0x0200)
+				addr = rvda;
+			else
+				addr += rvda;
+
+			opregionvbt->vbt_ex = memremap(addr, rvds, MEMREMAP_WB);
+			if (!opregionvbt->vbt_ex) {
+				memunmap(opregionvbt->opregion);
+				kfree(opregionvbt);
+				return -ENOMEM;
 			}
-
-			/* region size for opregion v2.0+: opregion and VBT size. */
-			size += rvds;
 		}
 	}
 
-	if (size != OPREGION_SIZE) {
-		memunmap(base);
-		base = memremap(addr, size, MEMREMAP_WB);
-		if (!base)
-			return -ENOMEM;
-	}
-
 	ret = vfio_pci_register_dev_region(vdev,
 		PCI_VENDOR_ID_INTEL | VFIO_REGION_TYPE_PCI_VENDOR_TYPE,
-		VFIO_REGION_SUBTYPE_INTEL_IGD_OPREGION,
-		&vfio_pci_igd_regops, size, VFIO_REGION_INFO_FLAG_READ, base);
+		VFIO_REGION_SUBTYPE_INTEL_IGD_OPREGION, &vfio_pci_igd_regops,
+		size, VFIO_REGION_INFO_FLAG_READ, opregionvbt);
 	if (ret) {
-		memunmap(base);
+		if (opregionvbt->vbt_ex)
+			memunmap(opregionvbt->vbt_ex);
+
+		memunmap(opregionvbt->opregion);
+		kfree(opregionvbt);
 		return ret;
 	}
 
diff -ruN a/drivers/virtio/Kconfig b/drivers/virtio/Kconfig
--- a/drivers/virtio/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/virtio/Kconfig	2021-12-23 08:35:50.000000000 +0100
@@ -148,4 +148,12 @@
 	 This option adds a flavor of dma buffers that are backed by
 	 virtio resources.
 
+config VIRTIO_WL
+	bool "Virtio Wayland driver"
+	depends on VIRTIO && MMU
+	help
+	 This driver supports proxying of a wayland socket from host to guest.
+
+	 If unsure, say 'N'.
+
 endif # VIRTIO_MENU
diff -ruN a/drivers/virtio/Makefile b/drivers/virtio/Makefile
--- a/drivers/virtio/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/virtio/Makefile	2021-12-23 08:35:50.000000000 +0100
@@ -10,3 +10,4 @@
 obj-$(CONFIG_VIRTIO_VDPA) += virtio_vdpa.o
 obj-$(CONFIG_VIRTIO_MEM) += virtio_mem.o
 obj-$(CONFIG_VIRTIO_DMA_SHARED_BUFFER) += virtio_dma_buf.o
+obj-$(CONFIG_VIRTIO_WL) += virtio_wl.o
diff -ruN a/drivers/virtio/virtio_wl.c b/drivers/virtio/virtio_wl.c
--- a/drivers/virtio/virtio_wl.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/virtio/virtio_wl.c	2021-12-23 08:35:50.000000000 +0100
@@ -0,0 +1,1598 @@
+/*
+ *  Wayland Virtio Driver
+ *  Copyright (C) 2017 Google, Inc.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ */
+
+/*
+ * Virtio Wayland (virtio_wl or virtwl) is a virtual device that allows a guest
+ * virtual machine to use a wayland server on the host transparently (to the
+ * host).  This is done by proxying the wayland protocol socket stream verbatim
+ * between the host and guest over 2 (recv and send) virtio queues. The guest
+ * can request new wayland server connections to give each guest wayland client
+ * a different server context. Each host connection's file descriptor is exposed
+ * to the guest as a virtual file descriptor (VFD). Additionally, the guest can
+ * request shared memory file descriptors which are also exposed as VFDs. These
+ * shared memory VFDs are directly writable by the guest via device memory
+ * injected by the host. Each VFD is sendable along a connection context VFD and
+ * will appear as ancillary data to the wayland server, just like a message from
+ * an ordinary wayland client. When the wayland server sends a shared memory
+ * file descriptor to the client (such as when sending a keymap), a VFD is
+ * allocated by the device automatically and its memory is injected into as
+ * device memory.
+ *
+ * This driver is intended to be paired with the `virtwl_guest_proxy` program
+ * which is run in the guest system and acts like a wayland server. It accepts
+ * wayland client connections and converts their socket messages to ioctl
+ * messages exposed by this driver via the `/dev/wl` device file. While it would
+ * be possible to expose a unix stream socket from this driver, the user space
+ * helper is much cleaner to write.
+ */
+
+#include <linux/anon_inodes.h>
+#include <linux/cdev.h>
+#include <linux/compat.h>
+#include <linux/completion.h>
+#include <linux/dma-buf.h>
+#include <linux/err.h>
+#include <linux/fdtable.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/idr.h>
+#include <linux/kfifo.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/poll.h>
+#include <linux/scatterlist.h>
+#include <linux/syscalls.h>
+#include <linux/uaccess.h>
+#include <linux/virtio.h>
+#include <linux/virtio_dma_buf.h>
+#include <linux/virtio_wl.h>
+#include <linux/vmalloc.h>
+
+#include <uapi/linux/dma-buf.h>
+
+#ifdef CONFIG_DRM_VIRTIO_GPU
+#define SEND_VIRTGPU_RESOURCES
+#include <linux/sync_file.h>
+#endif
+
+#define VFD_ILLEGAL_SIGN_BIT 0x80000000
+#define VFD_HOST_VFD_ID_BIT 0x40000000
+
+struct virtwl_vfd_qentry {
+	struct list_head list;
+	struct virtio_wl_ctrl_hdr *hdr;
+	unsigned int len; /* total byte length of ctrl_vfd_* + vfds + data */
+	unsigned int vfd_offset; /* int offset into vfds */
+	unsigned int data_offset; /* byte offset into data */
+};
+
+struct virtwl_vfd {
+	struct kobject kobj;
+	struct mutex lock;
+
+	struct virtwl_info *vi;
+	uint32_t id;
+	uint32_t flags;
+	uint64_t pfn;
+	uint32_t size;
+	bool hungup;
+
+	struct list_head in_queue; /* list of virtwl_vfd_qentry */
+	wait_queue_head_t in_waitq;
+};
+
+struct virtwl_info {
+	dev_t dev_num;
+	struct device *dev;
+	struct class *class;
+	struct cdev cdev;
+
+	struct mutex vq_locks[VIRTWL_QUEUE_COUNT];
+	struct virtqueue *vqs[VIRTWL_QUEUE_COUNT];
+	struct work_struct in_vq_work;
+	struct work_struct out_vq_work;
+
+	wait_queue_head_t out_waitq;
+
+	struct mutex vfds_lock;
+	struct idr vfds;
+
+	bool use_send_vfd_v2;
+};
+
+static struct virtwl_vfd *virtwl_vfd_alloc(struct virtwl_info *vi);
+static void virtwl_vfd_free(struct virtwl_vfd *vfd);
+
+static const struct file_operations virtwl_vfd_fops;
+
+static int virtwl_resp_err(unsigned int type)
+{
+	switch (type) {
+	case VIRTIO_WL_RESP_OK:
+	case VIRTIO_WL_RESP_VFD_NEW:
+	case VIRTIO_WL_RESP_VFD_NEW_DMABUF:
+		return 0;
+	case VIRTIO_WL_RESP_ERR:
+		return -ENODEV; /* Device is no longer reliable */
+	case VIRTIO_WL_RESP_OUT_OF_MEMORY:
+		return -ENOMEM;
+	case VIRTIO_WL_RESP_INVALID_ID:
+		return -ENOENT;
+	case VIRTIO_WL_RESP_INVALID_TYPE:
+		return -EINVAL;
+	case VIRTIO_WL_RESP_INVALID_FLAGS:
+		return -EPERM;
+	case VIRTIO_WL_RESP_INVALID_CMD:
+		return -ENOTTY;
+	default:
+		return -EPROTO;
+	}
+}
+
+static int vq_return_inbuf_locked(struct virtqueue *vq, void *buffer)
+{
+	int ret;
+	struct scatterlist sg[1];
+
+	sg_init_one(sg, buffer, PAGE_SIZE);
+
+	ret = virtqueue_add_inbuf(vq, sg, 1, buffer, GFP_KERNEL);
+	if (ret) {
+		pr_warn("virtwl: failed to give inbuf to host: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int vq_queue_out(struct virtwl_info *vi, struct scatterlist *out_sg,
+			struct scatterlist *in_sg,
+			struct completion *finish_completion,
+			bool nonblock)
+{
+	struct virtqueue *vq = vi->vqs[VIRTWL_VQ_OUT];
+	struct mutex *vq_lock = &vi->vq_locks[VIRTWL_VQ_OUT];
+	struct scatterlist *sgs[] = { out_sg, in_sg };
+	int ret = 0;
+
+	mutex_lock(vq_lock);
+	while ((ret = virtqueue_add_sgs(vq, sgs, 1, 1, finish_completion,
+					GFP_KERNEL)) == -ENOSPC) {
+		mutex_unlock(vq_lock);
+		if (nonblock)
+			return -EAGAIN;
+		if (!wait_event_timeout(vi->out_waitq, vq->num_free > 0, HZ))
+			return -EBUSY;
+		mutex_lock(vq_lock);
+	}
+	if (!ret)
+		virtqueue_kick(vq);
+	mutex_unlock(vq_lock);
+
+	return ret;
+}
+
+static int vq_fill_locked(struct virtqueue *vq)
+{
+	void *buffer;
+	int ret = 0;
+
+	while (vq->num_free > 0) {
+		buffer = kmalloc(PAGE_SIZE, GFP_KERNEL);
+		if (!buffer) {
+			ret = -ENOMEM;
+			goto clear_queue;
+		}
+
+		ret = vq_return_inbuf_locked(vq, buffer);
+		if (ret)
+			goto clear_queue;
+	}
+
+	return 0;
+
+clear_queue:
+	while ((buffer = virtqueue_detach_unused_buf(vq)))
+		kfree(buffer);
+	return ret;
+}
+
+static bool vq_handle_new(struct virtwl_info *vi,
+			  struct virtio_wl_ctrl_vfd_new *new, unsigned int len)
+{
+	struct virtwl_vfd *vfd;
+	u32 id = new->vfd_id;
+	int ret;
+
+	if (id == 0)
+		return true; /* return the inbuf to vq */
+
+	if (!(id & VFD_HOST_VFD_ID_BIT) || (id & VFD_ILLEGAL_SIGN_BIT)) {
+		pr_warn("virtwl: received a vfd with invalid id: %u\n", id);
+		return true; /* return the inbuf to vq */
+	}
+
+	vfd = virtwl_vfd_alloc(vi);
+	if (!vfd)
+		return true; /* return the inbuf to vq */
+
+	mutex_lock(&vi->vfds_lock);
+	ret = idr_alloc(&vi->vfds, vfd, id, id + 1, GFP_KERNEL);
+	mutex_unlock(&vi->vfds_lock);
+
+	if (ret <= 0) {
+		virtwl_vfd_free(vfd);
+		pr_warn("virtwl: failed to place received vfd: %d\n", ret);
+		return true; /* return the inbuf to vq */
+	}
+
+	vfd->id = id;
+	vfd->size = new->size;
+	vfd->pfn = new->pfn;
+	vfd->flags = new->flags;
+
+	return true; /* return the inbuf to vq */
+}
+
+static bool vq_handle_recv(struct virtwl_info *vi,
+			   struct virtio_wl_ctrl_vfd_recv *recv,
+			   unsigned int len)
+{
+	struct virtwl_vfd *vfd;
+	struct virtwl_vfd_qentry *qentry;
+
+	mutex_lock(&vi->vfds_lock);
+	vfd = idr_find(&vi->vfds, recv->vfd_id);
+	if (vfd)
+		mutex_lock(&vfd->lock);
+	mutex_unlock(&vi->vfds_lock);
+
+	if (!vfd) {
+		pr_warn("virtwl: recv for unknown vfd_id %u\n", recv->vfd_id);
+		return true; /* return the inbuf to vq */
+	}
+
+	qentry = kzalloc(sizeof(*qentry), GFP_KERNEL);
+	if (!qentry) {
+		mutex_unlock(&vfd->lock);
+		pr_warn("virtwl: failed to allocate qentry for vfd\n");
+		return true; /* return the inbuf to vq */
+	}
+
+	qentry->hdr = &recv->hdr;
+	qentry->len = len;
+
+	list_add_tail(&qentry->list, &vfd->in_queue);
+	wake_up_interruptible_all(&vfd->in_waitq);
+	mutex_unlock(&vfd->lock);
+
+	return false; /* no return the inbuf to vq */
+}
+
+static bool vq_handle_hup(struct virtwl_info *vi,
+			   struct virtio_wl_ctrl_vfd *vfd_hup,
+			   unsigned int len)
+{
+	struct virtwl_vfd *vfd;
+
+	mutex_lock(&vi->vfds_lock);
+	vfd = idr_find(&vi->vfds, vfd_hup->vfd_id);
+	if (vfd)
+		mutex_lock(&vfd->lock);
+	mutex_unlock(&vi->vfds_lock);
+
+	if (!vfd) {
+		pr_warn("virtwl: hup for unknown vfd_id %u\n", vfd_hup->vfd_id);
+		return true; /* return the inbuf to vq */
+	}
+
+	if (vfd->hungup)
+		pr_warn("virtwl: hup for hungup vfd_id %u\n", vfd_hup->vfd_id);
+
+	vfd->hungup = true;
+	wake_up_interruptible_all(&vfd->in_waitq);
+	mutex_unlock(&vfd->lock);
+
+	return true;
+}
+
+static bool vq_dispatch_hdr(struct virtwl_info *vi, unsigned int len,
+			    struct virtio_wl_ctrl_hdr *hdr)
+{
+	struct virtqueue *vq = vi->vqs[VIRTWL_VQ_IN];
+	struct mutex *vq_lock = &vi->vq_locks[VIRTWL_VQ_IN];
+	bool return_vq = true;
+	int ret;
+
+	switch (hdr->type) {
+	case VIRTIO_WL_CMD_VFD_NEW:
+		return_vq = vq_handle_new(vi,
+					  (struct virtio_wl_ctrl_vfd_new *)hdr,
+					  len);
+		break;
+	case VIRTIO_WL_CMD_VFD_RECV:
+		return_vq = vq_handle_recv(vi,
+			(struct virtio_wl_ctrl_vfd_recv *)hdr, len);
+		break;
+	case VIRTIO_WL_CMD_VFD_HUP:
+		return_vq = vq_handle_hup(vi, (struct virtio_wl_ctrl_vfd *)hdr,
+					  len);
+		break;
+	default:
+		pr_warn("virtwl: unhandled ctrl command: %u\n", hdr->type);
+		break;
+	}
+
+	if (!return_vq)
+		return false; /* no kick the vq */
+
+	mutex_lock(vq_lock);
+	ret = vq_return_inbuf_locked(vq, hdr);
+	mutex_unlock(vq_lock);
+	if (ret) {
+		pr_warn("virtwl: failed to return inbuf to host: %d\n", ret);
+		kfree(hdr);
+	}
+
+	return true; /* kick the vq */
+}
+
+static void vq_in_work_handler(struct work_struct *work)
+{
+	struct virtwl_info *vi = container_of(work, struct virtwl_info,
+					      in_vq_work);
+	struct virtqueue *vq = vi->vqs[VIRTWL_VQ_IN];
+	struct mutex *vq_lock = &vi->vq_locks[VIRTWL_VQ_IN];
+	void *buffer;
+	unsigned int len;
+	bool kick_vq = false;
+
+	mutex_lock(vq_lock);
+	while ((buffer = virtqueue_get_buf(vq, &len)) != NULL) {
+		struct virtio_wl_ctrl_hdr *hdr = buffer;
+
+		mutex_unlock(vq_lock);
+		kick_vq |= vq_dispatch_hdr(vi, len, hdr);
+		mutex_lock(vq_lock);
+	}
+	mutex_unlock(vq_lock);
+
+	if (kick_vq)
+		virtqueue_kick(vq);
+}
+
+static void vq_out_work_handler(struct work_struct *work)
+{
+	struct virtwl_info *vi = container_of(work, struct virtwl_info,
+					      out_vq_work);
+	struct virtqueue *vq = vi->vqs[VIRTWL_VQ_OUT];
+	struct mutex *vq_lock = &vi->vq_locks[VIRTWL_VQ_OUT];
+	unsigned int len;
+	struct completion *finish_completion;
+	bool wake_waitq = false;
+
+	mutex_lock(vq_lock);
+	while ((finish_completion = virtqueue_get_buf(vq, &len)) != NULL) {
+		wake_waitq = true;
+		complete(finish_completion);
+	}
+	mutex_unlock(vq_lock);
+
+	if (wake_waitq)
+		wake_up_interruptible_all(&vi->out_waitq);
+}
+
+static void vq_in_cb(struct virtqueue *vq)
+{
+	struct virtwl_info *vi = vq->vdev->priv;
+
+	schedule_work(&vi->in_vq_work);
+}
+
+static void vq_out_cb(struct virtqueue *vq)
+{
+	struct virtwl_info *vi = vq->vdev->priv;
+
+	schedule_work(&vi->out_vq_work);
+}
+
+static struct virtwl_vfd *virtwl_vfd_alloc(struct virtwl_info *vi)
+{
+	struct virtwl_vfd *vfd = kzalloc(sizeof(struct virtwl_vfd), GFP_KERNEL);
+
+	if (!vfd)
+		return ERR_PTR(-ENOMEM);
+
+	vfd->vi = vi;
+
+	mutex_init(&vfd->lock);
+	INIT_LIST_HEAD(&vfd->in_queue);
+	init_waitqueue_head(&vfd->in_waitq);
+
+	return vfd;
+}
+
+static int virtwl_vfd_file_flags(struct virtwl_vfd *vfd)
+{
+	int flags = 0;
+	int rw_mask = VIRTIO_WL_VFD_WRITE | VIRTIO_WL_VFD_READ;
+
+	if ((vfd->flags & rw_mask) == rw_mask)
+		flags |= O_RDWR;
+	else if (vfd->flags & VIRTIO_WL_VFD_WRITE)
+		flags |= O_WRONLY;
+	else if (vfd->flags & VIRTIO_WL_VFD_READ)
+		flags |= O_RDONLY;
+	if (vfd->pfn)
+		flags |= O_RDWR;
+	return flags;
+}
+
+/* Locks the vfd and unlinks its id from vi */
+static void virtwl_vfd_lock_unlink(struct virtwl_vfd *vfd)
+{
+	struct virtwl_info *vi = vfd->vi;
+
+	/* this order is important to avoid deadlock */
+	mutex_lock(&vi->vfds_lock);
+	mutex_lock(&vfd->lock);
+	idr_remove(&vi->vfds, vfd->id);
+	mutex_unlock(&vfd->lock);
+	mutex_unlock(&vi->vfds_lock);
+}
+
+/*
+ * Only used to free a vfd that is not referenced any place else and contains
+ * no queed virtio buffers. This must not be called while vfd is included in a
+ * vi->vfd.
+ */
+static void virtwl_vfd_free(struct virtwl_vfd *vfd)
+{
+	kfree(vfd);
+}
+
+/*
+ * Thread safe and also removes vfd from vi as well as any queued virtio buffers
+ */
+static void virtwl_vfd_remove(struct virtwl_vfd *vfd)
+{
+	struct virtwl_info *vi = vfd->vi;
+	struct virtqueue *vq = vi->vqs[VIRTWL_VQ_IN];
+	struct mutex *vq_lock = &vi->vq_locks[VIRTWL_VQ_IN];
+	struct virtwl_vfd_qentry *qentry, *next;
+
+	virtwl_vfd_lock_unlink(vfd);
+
+	mutex_lock(vq_lock);
+	list_for_each_entry_safe(qentry, next, &vfd->in_queue, list) {
+		vq_return_inbuf_locked(vq, qentry->hdr);
+		list_del(&qentry->list);
+		kfree(qentry);
+	}
+	mutex_unlock(vq_lock);
+	virtqueue_kick(vq);
+
+	virtwl_vfd_free(vfd);
+}
+
+static void vfd_qentry_free_if_empty(struct virtwl_vfd *vfd,
+				     struct virtwl_vfd_qentry *qentry)
+{
+	struct virtwl_info *vi = vfd->vi;
+	struct virtqueue *vq = vi->vqs[VIRTWL_VQ_IN];
+	struct mutex *vq_lock = &vi->vq_locks[VIRTWL_VQ_IN];
+
+	if (qentry->hdr->type == VIRTIO_WL_CMD_VFD_RECV) {
+		struct virtio_wl_ctrl_vfd_recv *recv =
+			(struct virtio_wl_ctrl_vfd_recv *)qentry->hdr;
+		ssize_t data_len =
+			(ssize_t)qentry->len - (ssize_t)sizeof(*recv) -
+			(ssize_t)recv->vfd_count * (ssize_t)sizeof(__le32);
+
+		if (qentry->vfd_offset < recv->vfd_count)
+			return;
+
+		if ((s64)qentry->data_offset < data_len)
+			return;
+	}
+
+	mutex_lock(vq_lock);
+	vq_return_inbuf_locked(vq, qentry->hdr);
+	mutex_unlock(vq_lock);
+	list_del(&qentry->list);
+	kfree(qentry);
+	virtqueue_kick(vq);
+}
+
+static ssize_t vfd_out_locked(struct virtwl_vfd *vfd, char __user *buffer,
+			      size_t len)
+{
+	struct virtwl_vfd_qentry *qentry, *next;
+	ssize_t read_count = 0;
+
+	list_for_each_entry_safe(qentry, next, &vfd->in_queue, list) {
+		struct virtio_wl_ctrl_vfd_recv *recv =
+			(struct virtio_wl_ctrl_vfd_recv *)qentry->hdr;
+		size_t recv_offset = sizeof(*recv) + recv->vfd_count *
+				     sizeof(__le32) + qentry->data_offset;
+		u8 *buf = (u8 *)recv + recv_offset;
+		ssize_t to_read = (ssize_t)qentry->len - (ssize_t)recv_offset;
+
+		if (qentry->hdr->type != VIRTIO_WL_CMD_VFD_RECV)
+			continue;
+
+		if ((to_read + read_count) > len)
+			to_read = len - read_count;
+
+		if (copy_to_user(buffer + read_count, buf, to_read)) {
+			read_count = -EFAULT;
+			break;
+		}
+
+		read_count += to_read;
+
+		qentry->data_offset += to_read;
+		vfd_qentry_free_if_empty(vfd, qentry);
+
+		if (read_count >= len)
+			break;
+	}
+
+	return read_count;
+}
+
+/* must hold both vfd->lock and vi->vfds_lock */
+static size_t vfd_out_vfds_locked(struct virtwl_vfd *vfd,
+				  struct virtwl_vfd **vfds, size_t count)
+{
+	struct virtwl_info *vi = vfd->vi;
+	struct virtwl_vfd_qentry *qentry, *next;
+	size_t i;
+	size_t read_count = 0;
+
+	list_for_each_entry_safe(qentry, next, &vfd->in_queue, list) {
+		struct virtio_wl_ctrl_vfd_recv *recv =
+			(struct virtio_wl_ctrl_vfd_recv *)qentry->hdr;
+		size_t vfd_offset = sizeof(*recv) + qentry->vfd_offset *
+				    sizeof(__le32);
+		__le32 *vfds_le = (__le32 *)((void *)recv + vfd_offset);
+		ssize_t vfds_to_read = recv->vfd_count - qentry->vfd_offset;
+
+		if (read_count >= count)
+			break;
+		if (vfds_to_read <= 0)
+			continue;
+		if (qentry->hdr->type != VIRTIO_WL_CMD_VFD_RECV)
+			continue;
+
+		if ((vfds_to_read + read_count) > count)
+			vfds_to_read = count - read_count;
+
+		for (i = 0; i < vfds_to_read; i++) {
+			uint32_t vfd_id = le32_to_cpu(vfds_le[i]);
+			vfds[read_count] = idr_find(&vi->vfds, vfd_id);
+			if (vfds[read_count]) {
+				read_count++;
+			} else {
+				pr_warn("virtwl: received a vfd with unrecognized id: %u\n",
+					vfd_id);
+			}
+			qentry->vfd_offset++;
+		}
+
+		vfd_qentry_free_if_empty(vfd, qentry);
+	}
+
+	return read_count;
+}
+
+/* this can only be called if the caller has unique ownership of the vfd */
+static int do_vfd_close(struct virtwl_vfd *vfd)
+{
+	struct virtio_wl_ctrl_vfd *ctrl_close;
+	struct virtwl_info *vi = vfd->vi;
+	struct completion finish_completion;
+	struct scatterlist out_sg;
+	struct scatterlist in_sg;
+	int ret = 0;
+
+	ctrl_close = kzalloc(sizeof(*ctrl_close), GFP_KERNEL);
+	if (!ctrl_close)
+		return -ENOMEM;
+
+	ctrl_close->hdr.type = VIRTIO_WL_CMD_VFD_CLOSE;
+	ctrl_close->vfd_id = vfd->id;
+
+	sg_init_one(&out_sg, &ctrl_close->hdr,
+		    sizeof(struct virtio_wl_ctrl_vfd));
+	sg_init_one(&in_sg, &ctrl_close->hdr,
+		    sizeof(struct virtio_wl_ctrl_hdr));
+
+	init_completion(&finish_completion);
+	ret = vq_queue_out(vi, &out_sg, &in_sg, &finish_completion,
+			   false /* block */);
+	if (ret) {
+		pr_warn("virtwl: failed to queue close vfd id %u: %d\n",
+			vfd->id,
+			ret);
+		goto free_ctrl_close;
+	}
+
+	wait_for_completion(&finish_completion);
+	virtwl_vfd_remove(vfd);
+
+free_ctrl_close:
+	kfree(ctrl_close);
+	return ret;
+}
+
+static ssize_t virtwl_vfd_recv(struct file *filp, char __user *buffer,
+			       size_t len, struct virtwl_vfd **vfds,
+			       size_t *vfd_count)
+{
+	struct virtwl_vfd *vfd = filp->private_data;
+	struct virtwl_info *vi = vfd->vi;
+	ssize_t read_count = 0;
+	size_t vfd_read_count = 0;
+	bool force_to_wait = false;
+
+	mutex_lock(&vi->vfds_lock);
+	mutex_lock(&vfd->lock);
+
+	while (read_count == 0 && vfd_read_count == 0) {
+		while (force_to_wait || list_empty(&vfd->in_queue)) {
+			force_to_wait = false;
+			if (vfd->hungup)
+				goto out_unlock;
+
+			mutex_unlock(&vfd->lock);
+			mutex_unlock(&vi->vfds_lock);
+			if (filp->f_flags & O_NONBLOCK)
+				return -EAGAIN;
+
+			if (wait_event_interruptible(vfd->in_waitq,
+				!list_empty(&vfd->in_queue) || vfd->hungup))
+				return -ERESTARTSYS;
+
+			mutex_lock(&vi->vfds_lock);
+			mutex_lock(&vfd->lock);
+		}
+
+		read_count = vfd_out_locked(vfd, buffer, len);
+		if (read_count < 0)
+			goto out_unlock;
+		if (vfds && vfd_count && *vfd_count)
+			vfd_read_count = vfd_out_vfds_locked(vfd, vfds,
+							     *vfd_count);
+		else if (read_count == 0 && !list_empty(&vfd->in_queue))
+			/*
+			 * Indicates a corner case where the in_queue has ONLY
+			 * incoming VFDs but the caller has given us no space to
+			 * store them. We force a wait for more activity on the
+			 * in_queue to prevent busy waiting.
+			 */
+			force_to_wait = true;
+	}
+
+out_unlock:
+	mutex_unlock(&vfd->lock);
+	mutex_unlock(&vi->vfds_lock);
+	if (vfd_count)
+		*vfd_count = vfd_read_count;
+	return read_count;
+}
+
+static int encode_vfd_ids(struct virtwl_vfd **vfds, size_t vfd_count,
+			  __le32 *vfd_ids)
+{
+	size_t i;
+
+	for (i = 0; i < vfd_count; i++) {
+		if (vfds[i])
+			vfd_ids[i] = cpu_to_le32(vfds[i]->id);
+		else
+			return -EBADFD;
+	}
+	return 0;
+}
+
+#ifdef SEND_VIRTGPU_RESOURCES
+static int get_dma_buf_id(struct dma_buf *dma_buf, u32 *id)
+{
+	uuid_t uuid;
+	int ret = 0;
+
+	ret = virtio_dma_buf_get_uuid(dma_buf, &uuid);
+	*id = be32_to_cpu(*(__be32 *)(uuid.b + 12));
+
+	return ret;
+}
+
+static int encode_fence(struct dma_fence *fence,
+			struct virtio_wl_ctrl_vfd_send_vfd_v2 *vfd_id)
+{
+	const char *name = fence->ops->get_driver_name(fence);
+
+	// We only support virtgpu based fences. Since all virtgpu fences are
+	// in the same context, merging sync_files will always reduce to a
+	// single virtgpu fence.
+	if (strcmp(name, "virtio_gpu") != 0)
+		return -EBADFD;
+
+	if (dma_fence_is_signaled(fence)) {
+		vfd_id->kind =
+			VIRTIO_WL_CTRL_VFD_SEND_KIND_VIRTGPU_SIGNALED_FENCE;
+	} else {
+		vfd_id->kind = VIRTIO_WL_CTRL_VFD_SEND_KIND_VIRTGPU_FENCE;
+		vfd_id->seqno = cpu_to_le32(fence->seqno);
+	}
+	return 0;
+}
+
+static int encode_vfd_ids_foreign(struct virtwl_vfd **vfds,
+				  struct dma_buf **virtgpu_dma_bufs,
+				  struct dma_fence **virtgpu_dma_fence,
+				  size_t vfd_count,
+				  struct virtio_wl_ctrl_vfd_send_vfd *ids,
+				  struct virtio_wl_ctrl_vfd_send_vfd_v2 *ids_v2)
+{
+	size_t i;
+	int ret;
+
+	for (i = 0; i < vfd_count; i++) {
+		uint32_t kind = UINT_MAX;
+		uint32_t id = 0;
+
+		if (vfds[i]) {
+			kind = VIRTIO_WL_CTRL_VFD_SEND_KIND_LOCAL;
+			id = vfds[i]->id;
+		} else if (virtgpu_dma_bufs[i]) {
+			ret = get_dma_buf_id(virtgpu_dma_bufs[i],
+					     &id);
+			if (ret)
+				return ret;
+			kind = VIRTIO_WL_CTRL_VFD_SEND_KIND_VIRTGPU;
+		} else if (virtgpu_dma_fence[i]) {
+			ret = encode_fence(virtgpu_dma_fence[i],
+					   ids_v2 + i);
+			if (ret)
+				return ret;
+		} else {
+			return -EBADFD;
+		}
+		if (kind != UINT_MAX) {
+			if (ids) {
+				ids[i].kind = kind;
+				ids[i].id = cpu_to_le32(id);
+			} else {
+				ids_v2[i].kind = kind;
+				ids_v2[i].id = cpu_to_le32(id);
+			}
+		}
+	}
+	return 0;
+}
+#endif
+
+static int virtwl_vfd_send(struct file *filp, const char __user *buffer,
+					       u32 len, int *vfd_fds)
+{
+	struct virtwl_vfd *vfd = filp->private_data;
+	struct virtwl_info *vi = vfd->vi;
+	struct fd vfd_files[VIRTWL_SEND_MAX_ALLOCS] = { { 0 } };
+	struct virtwl_vfd *vfds[VIRTWL_SEND_MAX_ALLOCS] = { 0 };
+#ifdef SEND_VIRTGPU_RESOURCES
+	struct dma_buf *virtgpu_dma_bufs[VIRTWL_SEND_MAX_ALLOCS] = { 0 };
+	struct dma_fence *virtgpu_dma_fence[VIRTWL_SEND_MAX_ALLOCS] = { 0 };
+	bool foreign_id = false;
+#endif
+	size_t vfd_count = 0;
+	size_t vfd_ids_size;
+	size_t ctrl_send_size;
+	struct virtio_wl_ctrl_vfd_send *ctrl_send;
+	u8 *vfd_ids;
+	u8 *out_buffer;
+	struct completion finish_completion;
+	struct scatterlist out_sg;
+	struct scatterlist in_sg;
+	struct sg_table sgt;
+	struct vm_struct *area;
+	bool vmalloced;
+	int ret;
+	int i;
+
+	if (vfd_fds) {
+		for (i = 0; i < VIRTWL_SEND_MAX_ALLOCS; i++) {
+			struct fd vfd_file;
+			int fd = vfd_fds[i];
+
+			if (fd < 0)
+				break;
+
+			vfd_file = fdget(vfd_fds[i]);
+			if (!vfd_file.file) {
+				ret = -EBADFD;
+				goto put_files;
+			}
+
+			if (vfd_file.file->f_op == &virtwl_vfd_fops) {
+				vfd_files[i] = vfd_file;
+
+				vfds[i] = vfd_file.file->private_data;
+				if (vfds[i] && vfds[i]->id) {
+					vfd_count++;
+					continue;
+				}
+
+				ret = -EINVAL;
+				goto put_files;
+			} else {
+				struct dma_buf *dma_buf = ERR_PTR(-EINVAL);
+				struct dma_fence *dma_fence = ERR_PTR(-EINVAL);
+				bool handled = false;
+
+#ifdef SEND_VIRTGPU_RESOURCES
+				dma_buf = dma_buf_get(vfd_fds[i]);
+				dma_fence = vi->use_send_vfd_v2
+					? sync_file_get_fence(vfd_fds[i])
+					: ERR_PTR(-EINVAL);
+				handled = !IS_ERR(dma_buf) ||
+					  !IS_ERR(dma_fence);
+
+				if (!IS_ERR(dma_buf)) {
+					virtgpu_dma_bufs[i] = dma_buf;
+				} else {
+					virtgpu_dma_fence[i] = dma_fence;
+				}
+
+				foreign_id = true;
+				vfd_count++;
+#endif
+				fdput(vfd_file);
+				if (!handled) {
+					ret = IS_ERR(dma_buf) ?
+						PTR_ERR(dma_buf) :
+						PTR_ERR(dma_fence);
+					goto put_files;
+				}
+			}
+		}
+	}
+
+	/* Empty writes always succeed. */
+	if (len == 0 && vfd_count == 0)
+		return 0;
+
+	vfd_ids_size = vfd_count * sizeof(__le32);
+#ifdef SEND_VIRTGPU_RESOURCES
+	if (foreign_id) {
+		vfd_ids_size = vfd_count * (vi->use_send_vfd_v2
+			? sizeof(struct virtio_wl_ctrl_vfd_send_vfd_v2)
+			: sizeof(struct virtio_wl_ctrl_vfd_send_vfd));
+	}
+#endif
+	ctrl_send_size = sizeof(*ctrl_send) + vfd_ids_size + len;
+	vmalloced = false;
+	if (ctrl_send_size < PAGE_SIZE)
+		ctrl_send = kzalloc(ctrl_send_size, GFP_KERNEL);
+	else {
+		vmalloced = true;
+		ctrl_send = vzalloc(ctrl_send_size);
+	}
+	if (!ctrl_send) {
+		ret = -ENOMEM;
+		goto put_files;
+	}
+
+	vfd_ids = (u8 *)ctrl_send + sizeof(*ctrl_send);
+	out_buffer = (u8 *)ctrl_send + ctrl_send_size - len;
+
+	ctrl_send->hdr.type = VIRTIO_WL_CMD_VFD_SEND;
+#ifdef SEND_VIRTGPU_RESOURCES
+	if (foreign_id) {
+		struct virtio_wl_ctrl_vfd_send_vfd *v1 = NULL;
+		struct virtio_wl_ctrl_vfd_send_vfd_v2 *v2 = NULL;
+
+		if (vi->use_send_vfd_v2)
+			v2 = (struct virtio_wl_ctrl_vfd_send_vfd_v2 *) vfd_ids;
+		else
+			v1 = (struct virtio_wl_ctrl_vfd_send_vfd *) vfd_ids;
+
+		ctrl_send->hdr.type = VIRTIO_WL_CMD_VFD_SEND_FOREIGN_ID;
+		ret = encode_vfd_ids_foreign(vfds,
+			virtgpu_dma_bufs, virtgpu_dma_fence, vfd_count,
+			v1, v2);
+	} else {
+		ret = encode_vfd_ids(vfds, vfd_count, (__le32 *)vfd_ids);
+	}
+#else
+	ret = encode_vfd_ids(vfds, vfd_count, (__le32 *)vfd_ids);
+#endif
+	if (ret)
+		goto free_ctrl_send;
+	ctrl_send->vfd_id = vfd->id;
+	ctrl_send->vfd_count = vfd_count;
+
+	if (copy_from_user(out_buffer, buffer, len)) {
+		ret = -EFAULT;
+		goto free_ctrl_send;
+	}
+
+	init_completion(&finish_completion);
+	if (!vmalloced) {
+		sg_init_one(&out_sg, ctrl_send, ctrl_send_size);
+		sg_init_one(&in_sg, ctrl_send,
+		    sizeof(struct virtio_wl_ctrl_hdr));
+		ret = vq_queue_out(vi, &out_sg, &in_sg, &finish_completion,
+		    filp->f_flags & O_NONBLOCK);
+	} else {
+		area = find_vm_area(ctrl_send);
+		ret = sg_alloc_table_from_pages(&sgt, area->pages,
+		    area->nr_pages, 0, ctrl_send_size, GFP_KERNEL);
+		if (ret)
+			goto free_ctrl_send;
+
+		sg_init_table(&in_sg, 1);
+		sg_set_page(&in_sg, area->pages[0],
+		    sizeof(struct virtio_wl_ctrl_hdr), 0);
+
+		ret = vq_queue_out(vi, sgt.sgl, &in_sg, &finish_completion,
+		    filp->f_flags & O_NONBLOCK);
+	}
+	if (ret)
+		goto free_sgt;
+
+	wait_for_completion(&finish_completion);
+
+	ret = virtwl_resp_err(ctrl_send->hdr.type);
+
+free_sgt:
+	if (vmalloced)
+		sg_free_table(&sgt);
+free_ctrl_send:
+	kvfree(ctrl_send);
+put_files:
+	for (i = 0; i < VIRTWL_SEND_MAX_ALLOCS; i++) {
+		if (vfd_files[i].file)
+			fdput(vfd_files[i]);
+#ifdef SEND_VIRTGPU_RESOURCES
+		if (virtgpu_dma_bufs[i])
+			dma_buf_put(virtgpu_dma_bufs[i]);
+		if (virtgpu_dma_fence[i])
+			dma_fence_put(virtgpu_dma_fence[i]);
+#endif
+	}
+	return ret;
+}
+
+static int virtwl_vfd_dmabuf_sync(struct file *filp, u32 flags)
+{
+	struct virtio_wl_ctrl_vfd_dmabuf_sync *ctrl_dmabuf_sync;
+	struct virtwl_vfd *vfd = filp->private_data;
+	struct virtwl_info *vi = vfd->vi;
+	struct completion finish_completion;
+	struct scatterlist out_sg;
+	struct scatterlist in_sg;
+	int ret = 0;
+
+	ctrl_dmabuf_sync = kzalloc(sizeof(*ctrl_dmabuf_sync), GFP_KERNEL);
+	if (!ctrl_dmabuf_sync)
+		return -ENOMEM;
+
+	ctrl_dmabuf_sync->hdr.type = VIRTIO_WL_CMD_VFD_DMABUF_SYNC;
+	ctrl_dmabuf_sync->vfd_id = vfd->id;
+	ctrl_dmabuf_sync->flags = flags;
+
+	sg_init_one(&out_sg, &ctrl_dmabuf_sync->hdr,
+		    sizeof(struct virtio_wl_ctrl_vfd_dmabuf_sync));
+	sg_init_one(&in_sg, &ctrl_dmabuf_sync->hdr,
+		    sizeof(struct virtio_wl_ctrl_hdr));
+
+	init_completion(&finish_completion);
+	ret = vq_queue_out(vi, &out_sg, &in_sg, &finish_completion,
+			   false /* block */);
+	if (ret) {
+		pr_warn("virtwl: failed to queue dmabuf sync vfd id %u: %d\n",
+			vfd->id,
+			ret);
+		goto free_ctrl_dmabuf_sync;
+	}
+
+	wait_for_completion(&finish_completion);
+
+free_ctrl_dmabuf_sync:
+	kfree(ctrl_dmabuf_sync);
+	return ret;
+}
+
+static ssize_t virtwl_vfd_read(struct file *filp, char __user *buffer,
+			       size_t size, loff_t *pos)
+{
+	return virtwl_vfd_recv(filp, buffer, size, NULL, NULL);
+}
+
+static ssize_t virtwl_vfd_write(struct file *filp, const char __user *buffer,
+				size_t size, loff_t *pos)
+{
+	int ret = 0;
+
+	if (size > U32_MAX)
+		size = U32_MAX;
+
+	ret = virtwl_vfd_send(filp, buffer, size, NULL);
+	if (ret)
+		return ret;
+
+	return size;
+}
+
+static int virtwl_vfd_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct virtwl_vfd *vfd = filp->private_data;
+	unsigned long vm_size = vma->vm_end - vma->vm_start;
+	int ret = 0;
+
+	mutex_lock(&vfd->lock);
+
+	if (!vfd->pfn) {
+		ret = -EACCES;
+		goto out_unlock;
+	}
+
+	if (vm_size + (vma->vm_pgoff << PAGE_SHIFT) > PAGE_ALIGN(vfd->size)) {
+		ret = -EINVAL;
+		goto out_unlock;
+	}
+
+	ret = io_remap_pfn_range(vma, vma->vm_start, vfd->pfn, vm_size,
+				 vma->vm_page_prot);
+	if (ret)
+		goto out_unlock;
+
+	vma->vm_flags |= VM_PFNMAP | VM_IO | VM_DONTEXPAND | VM_DONTDUMP;
+
+out_unlock:
+	mutex_unlock(&vfd->lock);
+	return ret;
+}
+
+static unsigned int virtwl_vfd_poll(struct file *filp,
+				    struct poll_table_struct *wait)
+{
+	struct virtwl_vfd *vfd = filp->private_data;
+	struct virtwl_info *vi = vfd->vi;
+	unsigned int mask = 0;
+
+	mutex_lock(&vi->vq_locks[VIRTWL_VQ_OUT]);
+	poll_wait(filp, &vi->out_waitq, wait);
+	if (vi->vqs[VIRTWL_VQ_OUT]->num_free)
+		mask |= POLLOUT | POLLWRNORM;
+	mutex_unlock(&vi->vq_locks[VIRTWL_VQ_OUT]);
+
+	mutex_lock(&vfd->lock);
+	poll_wait(filp, &vfd->in_waitq, wait);
+	if (!list_empty(&vfd->in_queue))
+		mask |= POLLIN | POLLRDNORM;
+	if (vfd->hungup)
+		mask |= POLLHUP;
+	mutex_unlock(&vfd->lock);
+
+	return mask;
+}
+
+static int virtwl_vfd_release(struct inode *inodep, struct file *filp)
+{
+	struct virtwl_vfd *vfd = filp->private_data;
+	uint32_t vfd_id = vfd->id;
+	int ret;
+
+	/*
+	 * If release is called, filp must be out of references and we have the
+	 * last reference.
+	 */
+	ret = do_vfd_close(vfd);
+	if (ret)
+		pr_warn("virtwl: failed to release vfd id %u: %d\n", vfd_id,
+			ret);
+	return 0;
+}
+
+static int virtwl_open(struct inode *inodep, struct file *filp)
+{
+	struct virtwl_info *vi = container_of(inodep->i_cdev,
+					      struct virtwl_info, cdev);
+
+	filp->private_data = vi;
+
+	return 0;
+}
+
+static struct virtwl_vfd *do_new(struct virtwl_info *vi,
+				 struct virtwl_ioctl_new *ioctl_new,
+				 size_t ioctl_new_size, bool nonblock)
+{
+	struct virtio_wl_ctrl_vfd_new *ctrl_new;
+	struct virtwl_vfd *vfd;
+	struct completion finish_completion;
+	struct scatterlist out_sg;
+	struct scatterlist in_sg;
+	int ret = 0;
+
+	if (ioctl_new->type != VIRTWL_IOCTL_NEW_CTX &&
+		ioctl_new->type != VIRTWL_IOCTL_NEW_CTX_NAMED &&
+		ioctl_new->type != VIRTWL_IOCTL_NEW_ALLOC &&
+		ioctl_new->type != VIRTWL_IOCTL_NEW_PIPE_READ &&
+		ioctl_new->type != VIRTWL_IOCTL_NEW_PIPE_WRITE &&
+		ioctl_new->type != VIRTWL_IOCTL_NEW_DMABUF)
+		return ERR_PTR(-EINVAL);
+
+	ctrl_new = kzalloc(sizeof(*ctrl_new), GFP_KERNEL);
+	if (!ctrl_new)
+		return ERR_PTR(-ENOMEM);
+
+	vfd = virtwl_vfd_alloc(vi);
+	if (!vfd) {
+		ret = -ENOMEM;
+		goto free_ctrl_new;
+	}
+
+	mutex_lock(&vi->vfds_lock);
+	/*
+	 * Take the lock before adding it to the vfds list where others might
+	 * reference it.
+	 */
+	mutex_lock(&vfd->lock);
+	ret = idr_alloc(&vi->vfds, vfd, 1, VIRTWL_MAX_ALLOC, GFP_KERNEL);
+	mutex_unlock(&vi->vfds_lock);
+	if (ret <= 0)
+		goto remove_vfd;
+
+	vfd->id = ret;
+	ret = 0;
+
+	ctrl_new->vfd_id = vfd->id;
+	switch (ioctl_new->type) {
+	case VIRTWL_IOCTL_NEW_CTX:
+		ctrl_new->hdr.type = VIRTIO_WL_CMD_VFD_NEW_CTX;
+		ctrl_new->flags = VIRTIO_WL_VFD_WRITE | VIRTIO_WL_VFD_READ;
+		break;
+	case VIRTWL_IOCTL_NEW_CTX_NAMED:
+		ctrl_new->hdr.type = VIRTIO_WL_CMD_VFD_NEW_CTX_NAMED;
+		ctrl_new->flags = VIRTIO_WL_VFD_WRITE | VIRTIO_WL_VFD_READ;
+		memcpy(ctrl_new->name, ioctl_new->name, sizeof(ctrl_new->name));
+		break;
+	case VIRTWL_IOCTL_NEW_ALLOC:
+		ctrl_new->hdr.type = VIRTIO_WL_CMD_VFD_NEW;
+		ctrl_new->size = PAGE_ALIGN(ioctl_new->size);
+		break;
+	case VIRTWL_IOCTL_NEW_PIPE_READ:
+		ctrl_new->hdr.type = VIRTIO_WL_CMD_VFD_NEW_PIPE;
+		ctrl_new->flags = VIRTIO_WL_VFD_READ;
+		break;
+	case VIRTWL_IOCTL_NEW_PIPE_WRITE:
+		ctrl_new->hdr.type = VIRTIO_WL_CMD_VFD_NEW_PIPE;
+		ctrl_new->flags = VIRTIO_WL_VFD_WRITE;
+		break;
+	case VIRTWL_IOCTL_NEW_DMABUF:
+		/* Make sure ioctl_new contains enough data for NEW_DMABUF. */
+		if (ioctl_new_size == sizeof(*ioctl_new)) {
+			ctrl_new->hdr.type = VIRTIO_WL_CMD_VFD_NEW_DMABUF;
+			/* FIXME: convert from host byte order. */
+			memcpy(&ctrl_new->dmabuf, &ioctl_new->dmabuf,
+			       sizeof(ioctl_new->dmabuf));
+			break;
+		}
+		fallthrough;
+	default:
+		ret = -EINVAL;
+		goto remove_vfd;
+	}
+
+	init_completion(&finish_completion);
+	sg_init_one(&out_sg, ctrl_new, sizeof(*ctrl_new));
+	sg_init_one(&in_sg, ctrl_new, sizeof(*ctrl_new));
+
+	ret = vq_queue_out(vi, &out_sg, &in_sg, &finish_completion, nonblock);
+	if (ret)
+		goto remove_vfd;
+
+	wait_for_completion(&finish_completion);
+
+	ret = virtwl_resp_err(ctrl_new->hdr.type);
+	if (ret)
+		goto remove_vfd;
+
+	vfd->size = ctrl_new->size;
+	vfd->pfn = ctrl_new->pfn;
+	vfd->flags = ctrl_new->flags;
+
+	mutex_unlock(&vfd->lock);
+
+	if (ioctl_new->type == VIRTWL_IOCTL_NEW_DMABUF) {
+		/* FIXME: convert to host byte order. */
+		memcpy(&ioctl_new->dmabuf, &ctrl_new->dmabuf,
+		       sizeof(ctrl_new->dmabuf));
+	}
+
+	kfree(ctrl_new);
+	return vfd;
+
+remove_vfd:
+	/*
+	 * unlock the vfd to avoid deadlock when unlinking it
+	 * or freeing a held lock
+	 */
+	mutex_unlock(&vfd->lock);
+	/* this is safe since the id cannot change after the vfd is created */
+	if (vfd->id)
+		virtwl_vfd_lock_unlink(vfd);
+	virtwl_vfd_free(vfd);
+free_ctrl_new:
+	kfree(ctrl_new);
+	return ERR_PTR(ret);
+}
+
+static long virtwl_ioctl_send(struct file *filp, void __user *ptr)
+{
+	struct virtwl_ioctl_txn ioctl_send;
+	void __user *user_data = ptr + sizeof(struct virtwl_ioctl_txn);
+	int ret;
+
+	ret = copy_from_user(&ioctl_send, ptr, sizeof(struct virtwl_ioctl_txn));
+	if (ret)
+		return -EFAULT;
+
+	/* Early check for user error; do_send still uses copy_from_user. */
+	ret = !access_ok(user_data, ioctl_send.len);
+	if (ret)
+		return -EFAULT;
+
+	return virtwl_vfd_send(filp, user_data, ioctl_send.len, ioctl_send.fds);
+}
+
+static long virtwl_ioctl_recv(struct file *filp, void __user *ptr)
+{
+	struct virtwl_ioctl_txn ioctl_recv;
+	void __user *user_data = ptr + sizeof(struct virtwl_ioctl_txn);
+	int __user *user_fds = (int __user *)ptr;
+	size_t vfd_count = VIRTWL_SEND_MAX_ALLOCS;
+	struct virtwl_vfd *vfds[VIRTWL_SEND_MAX_ALLOCS] = { 0 };
+	int fds[VIRTWL_SEND_MAX_ALLOCS];
+	size_t i;
+	int ret = 0;
+
+	for (i = 0; i < VIRTWL_SEND_MAX_ALLOCS; i++)
+		fds[i] = -1;
+
+	ret = copy_from_user(&ioctl_recv, ptr, sizeof(struct virtwl_ioctl_txn));
+	if (ret)
+		return -EFAULT;
+
+	/* Early check for user error. */
+	ret = !access_ok(user_data, ioctl_recv.len);
+	if (ret)
+		return -EFAULT;
+
+	ret = virtwl_vfd_recv(filp, user_data, ioctl_recv.len, vfds,
+			      &vfd_count);
+	if (ret < 0)
+		return ret;
+
+	ret = copy_to_user(&((struct virtwl_ioctl_txn __user *)ptr)->len, &ret,
+			   sizeof(ioctl_recv.len));
+	if (ret) {
+		ret = -EFAULT;
+		goto free_vfds;
+	}
+
+	for (i = 0; i < vfd_count; i++) {
+		ret = anon_inode_getfd("[virtwl_vfd]", &virtwl_vfd_fops,
+				       vfds[i], virtwl_vfd_file_flags(vfds[i])
+				       | O_CLOEXEC);
+		if (ret < 0)
+			goto free_vfds;
+
+		vfds[i] = NULL;
+		fds[i] = ret;
+	}
+
+	ret = copy_to_user(user_fds, fds, sizeof(int) * VIRTWL_SEND_MAX_ALLOCS);
+	if (ret) {
+		ret = -EFAULT;
+		goto free_vfds;
+	}
+
+	return 0;
+
+free_vfds:
+	for (i = 0; i < vfd_count; i++) {
+		if (vfds[i])
+			do_vfd_close(vfds[i]);
+		if (fds[i] >= 0)
+			close_fd(fds[i]);
+	}
+	return ret;
+}
+
+static long virtwl_ioctl_dmabuf_sync(struct file *filp, void __user *ptr)
+{
+	struct virtwl_ioctl_dmabuf_sync ioctl_dmabuf_sync;
+	int ret;
+
+	ret = copy_from_user(&ioctl_dmabuf_sync, ptr,
+			     sizeof(struct virtwl_ioctl_dmabuf_sync));
+	if (ret)
+		return -EFAULT;
+
+	if (ioctl_dmabuf_sync.flags & ~DMA_BUF_SYNC_VALID_FLAGS_MASK)
+		return -EINVAL;
+
+	return virtwl_vfd_dmabuf_sync(filp, ioctl_dmabuf_sync.flags);
+}
+
+static long virtwl_vfd_ioctl(struct file *filp, unsigned int cmd,
+			     void __user *ptr)
+{
+	switch (cmd) {
+	case VIRTWL_IOCTL_SEND:
+		return virtwl_ioctl_send(filp, ptr);
+	case VIRTWL_IOCTL_RECV:
+		return virtwl_ioctl_recv(filp, ptr);
+	case VIRTWL_IOCTL_DMABUF_SYNC:
+		return virtwl_ioctl_dmabuf_sync(filp, ptr);
+	default:
+		return -ENOTTY;
+	}
+}
+
+static long virtwl_ioctl_new(struct file *filp, void __user *ptr,
+			     size_t in_size)
+{
+	struct virtwl_info *vi = filp->private_data;
+	struct virtwl_vfd *vfd;
+	struct virtwl_ioctl_new ioctl_new = {};
+	size_t size = min(in_size, sizeof(ioctl_new));
+	int ret;
+
+	/* Early check for user error. */
+	ret = !access_ok(ptr, size);
+	if (ret)
+		return -EFAULT;
+
+	ret = copy_from_user(&ioctl_new, ptr, size);
+	if (ret)
+		return -EFAULT;
+
+	vfd = do_new(vi, &ioctl_new, size, filp->f_flags & O_NONBLOCK);
+	if (IS_ERR(vfd))
+		return PTR_ERR(vfd);
+
+	ret = anon_inode_getfd("[virtwl_vfd]", &virtwl_vfd_fops, vfd,
+			       virtwl_vfd_file_flags(vfd) | O_CLOEXEC);
+	if (ret < 0) {
+		do_vfd_close(vfd);
+		return ret;
+	}
+
+	ioctl_new.fd = ret;
+	ret = copy_to_user(ptr, &ioctl_new, size);
+	if (ret) {
+		/* The release operation will handle freeing this alloc */
+		close_fd(ioctl_new.fd);
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static long virtwl_ioctl_ptr(struct file *filp, unsigned int cmd,
+			     void __user *ptr)
+{
+	if (filp->f_op == &virtwl_vfd_fops)
+		return virtwl_vfd_ioctl(filp, cmd, ptr);
+
+	switch (_IOC_NR(cmd)) {
+	case _IOC_NR(VIRTWL_IOCTL_NEW):
+		return virtwl_ioctl_new(filp, ptr, _IOC_SIZE(cmd));
+	default:
+		return -ENOTTY;
+	}
+}
+
+static long virtwl_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	return virtwl_ioctl_ptr(filp, cmd, (void __user *)arg);
+}
+
+#ifdef CONFIG_COMPAT
+static long virtwl_ioctl_compat(struct file *filp, unsigned int cmd,
+				unsigned long arg)
+{
+	return virtwl_ioctl_ptr(filp, cmd, compat_ptr(arg));
+}
+#else
+#define virtwl_ioctl_compat NULL
+#endif
+
+static int virtwl_release(struct inode *inodep, struct file *filp)
+{
+	return 0;
+}
+
+static const struct file_operations virtwl_fops = {
+	.open = virtwl_open,
+	.unlocked_ioctl = virtwl_ioctl,
+	.compat_ioctl = virtwl_ioctl_compat,
+	.release = virtwl_release,
+};
+
+static const struct file_operations virtwl_vfd_fops = {
+	.read = virtwl_vfd_read,
+	.write = virtwl_vfd_write,
+	.mmap = virtwl_vfd_mmap,
+	.poll = virtwl_vfd_poll,
+	.unlocked_ioctl = virtwl_ioctl,
+	.compat_ioctl = virtwl_ioctl_compat,
+	.release = virtwl_vfd_release,
+};
+
+static int probe_common(struct virtio_device *vdev)
+{
+	int i;
+	int ret;
+	struct virtwl_info *vi = NULL;
+	vq_callback_t *vq_callbacks[] = { vq_in_cb, vq_out_cb };
+	static const char * const vq_names[] = { "in", "out" };
+
+	vi = kzalloc(sizeof(struct virtwl_info), GFP_KERNEL);
+	if (!vi)
+		return -ENOMEM;
+
+	vdev->priv = vi;
+
+	ret = alloc_chrdev_region(&vi->dev_num, 0, 1, "wl");
+	if (ret) {
+		ret = -ENOMEM;
+		pr_warn("virtwl: failed to allocate wl chrdev region: %d\n",
+			ret);
+		goto free_vi;
+	}
+
+	vi->class = class_create(THIS_MODULE, "wl");
+	if (IS_ERR(vi->class)) {
+		ret = PTR_ERR(vi->class);
+		pr_warn("virtwl: failed to create wl class: %d\n", ret);
+		goto unregister_region;
+
+	}
+
+	vi->dev = device_create(vi->class, NULL, vi->dev_num, vi, "wl%d", 0);
+	if (IS_ERR(vi->dev)) {
+		ret = PTR_ERR(vi->dev);
+		pr_warn("virtwl: failed to create wl0 device: %d\n", ret);
+		goto destroy_class;
+	}
+
+	cdev_init(&vi->cdev, &virtwl_fops);
+	ret = cdev_add(&vi->cdev, vi->dev_num, 1);
+	if (ret) {
+		pr_warn("virtwl: failed to add virtio wayland character device to system: %d\n",
+			ret);
+		goto destroy_device;
+	}
+
+	for (i = 0; i < VIRTWL_QUEUE_COUNT; i++)
+		mutex_init(&vi->vq_locks[i]);
+
+	ret = virtio_find_vqs(vdev, VIRTWL_QUEUE_COUNT, vi->vqs, vq_callbacks,
+			      vq_names, NULL);
+	if (ret) {
+		pr_warn("virtwl: failed to find virtio wayland queues: %d\n",
+			ret);
+		goto del_cdev;
+	}
+
+	INIT_WORK(&vi->in_vq_work, vq_in_work_handler);
+	INIT_WORK(&vi->out_vq_work, vq_out_work_handler);
+	init_waitqueue_head(&vi->out_waitq);
+
+	mutex_init(&vi->vfds_lock);
+	idr_init(&vi->vfds);
+
+	vi->use_send_vfd_v2 = virtio_has_feature(vdev, VIRTIO_WL_F_SEND_FENCES);
+
+	/* lock is unneeded as we have unique ownership */
+	ret = vq_fill_locked(vi->vqs[VIRTWL_VQ_IN]);
+	if (ret) {
+		pr_warn("virtwl: failed to fill in virtqueue: %d", ret);
+		goto del_cdev;
+	}
+
+	virtio_device_ready(vdev);
+	virtqueue_kick(vi->vqs[VIRTWL_VQ_IN]);
+
+
+	return 0;
+
+del_cdev:
+	cdev_del(&vi->cdev);
+destroy_device:
+	put_device(vi->dev);
+destroy_class:
+	class_destroy(vi->class);
+unregister_region:
+	unregister_chrdev_region(vi->dev_num, 0);
+free_vi:
+	kfree(vi);
+	return ret;
+}
+
+static void remove_common(struct virtio_device *vdev)
+{
+	struct virtwl_info *vi = vdev->priv;
+
+	cdev_del(&vi->cdev);
+	put_device(vi->dev);
+	class_destroy(vi->class);
+	unregister_chrdev_region(vi->dev_num, 0);
+	kfree(vi);
+}
+
+static int virtwl_probe(struct virtio_device *vdev)
+{
+	return probe_common(vdev);
+}
+
+static void virtwl_remove(struct virtio_device *vdev)
+{
+	remove_common(vdev);
+}
+
+static void virtwl_scan(struct virtio_device *vdev)
+{
+}
+
+static struct virtio_device_id id_table[] = {
+	{ VIRTIO_ID_WL, VIRTIO_DEV_ANY_ID },
+	{ 0 },
+};
+
+static unsigned int features_legacy[] = {
+	VIRTIO_WL_F_TRANS_FLAGS
+};
+
+static unsigned int features[] = {
+	VIRTIO_WL_F_TRANS_FLAGS,
+	VIRTIO_WL_F_SEND_FENCES,
+};
+
+static struct virtio_driver virtio_wl_driver = {
+	.driver.name =	KBUILD_MODNAME,
+	.driver.owner =	THIS_MODULE,
+	.id_table =	id_table,
+	.feature_table = features,
+	.feature_table_size = ARRAY_SIZE(features),
+	.feature_table_legacy = features_legacy,
+	.feature_table_size_legacy = ARRAY_SIZE(features_legacy),
+	.probe =	virtwl_probe,
+	.remove =	virtwl_remove,
+	.scan =		virtwl_scan,
+};
+
+module_virtio_driver(virtio_wl_driver);
+MODULE_DEVICE_TABLE(virtio, id_table);
+MODULE_DESCRIPTION("Virtio wayland driver");
+MODULE_LICENSE("GPL");
diff -ruN a/drivers/watchdog/mtk_wdt.c b/drivers/watchdog/mtk_wdt.c
--- a/drivers/watchdog/mtk_wdt.c	2021-12-08 09:04:57.000000000 +0100
+++ b/drivers/watchdog/mtk_wdt.c	2021-12-23 08:35:50.000000000 +0100
@@ -65,6 +65,7 @@
 	void __iomem *wdt_base;
 	spinlock_t lock; /* protects WDT_SWSYSRST reg */
 	struct reset_controller_dev rcdev;
+	bool disable_wdt_extrst;
 };
 
 struct mtk_wdt_data {
@@ -256,6 +257,8 @@
 		reg |= (WDT_MODE_IRQ_EN | WDT_MODE_DUAL_EN);
 	else
 		reg &= ~(WDT_MODE_IRQ_EN | WDT_MODE_DUAL_EN);
+	if (mtk_wdt->disable_wdt_extrst)
+		reg &= ~WDT_MODE_EXRST_EN;
 	reg |= (WDT_MODE_EN | WDT_MODE_KEY);
 	iowrite32(reg, wdt_base + WDT_MODE);
 
@@ -381,6 +384,10 @@
 		if (err)
 			return err;
 	}
+
+	mtk_wdt->disable_wdt_extrst =
+		of_property_read_bool(dev->of_node, "mediatek,disable-extrst");
+
 	return 0;
 }
 
diff -ruN a/fs/9p/acl.c b/fs/9p/acl.c
--- a/fs/9p/acl.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/9p/acl.c	2021-12-23 08:35:51.000000000 +0100
@@ -217,7 +217,8 @@
 
 static int v9fs_xattr_get_acl(const struct xattr_handler *handler,
 			      struct dentry *dentry, struct inode *inode,
-			      const char *name, void *buffer, size_t size)
+			      const char *name, void *buffer, size_t size,
+			      int flags)
 {
 	struct v9fs_session_info *v9ses;
 	struct posix_acl *acl;
diff -ruN a/fs/9p/xattr.c b/fs/9p/xattr.c
--- a/fs/9p/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/9p/xattr.c	2021-12-23 08:35:51.000000000 +0100
@@ -149,7 +149,8 @@
 
 static int v9fs_xattr_handler_get(const struct xattr_handler *handler,
 				  struct dentry *dentry, struct inode *inode,
-				  const char *name, void *buffer, size_t size)
+				  const char *name, void *buffer, size_t size,
+				  int flags)
 {
 	const char *full_name = xattr_full_name(handler, name);
 
diff -ruN a/fs/afs/xattr.c b/fs/afs/xattr.c
--- a/fs/afs/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/afs/xattr.c	2021-12-23 08:35:51.000000000 +0100
@@ -36,7 +36,7 @@
 static int afs_xattr_get_acl(const struct xattr_handler *handler,
 			     struct dentry *dentry,
 			     struct inode *inode, const char *name,
-			     void *buffer, size_t size)
+			     void *buffer, size_t size, int flags)
 {
 	struct afs_operation *op;
 	struct afs_vnode *vnode = AFS_FS_I(inode);
@@ -138,7 +138,7 @@
 static int afs_xattr_get_yfs(const struct xattr_handler *handler,
 			     struct dentry *dentry,
 			     struct inode *inode, const char *name,
-			     void *buffer, size_t size)
+			     void *buffer, size_t size, int flags)
 {
 	struct afs_operation *op;
 	struct afs_vnode *vnode = AFS_FS_I(inode);
@@ -268,7 +268,7 @@
 static int afs_xattr_get_cell(const struct xattr_handler *handler,
 			      struct dentry *dentry,
 			      struct inode *inode, const char *name,
-			      void *buffer, size_t size)
+			      void *buffer, size_t size, int flags)
 {
 	struct afs_vnode *vnode = AFS_FS_I(inode);
 	struct afs_cell *cell = vnode->volume->cell;
@@ -295,7 +295,7 @@
 static int afs_xattr_get_fid(const struct xattr_handler *handler,
 			     struct dentry *dentry,
 			     struct inode *inode, const char *name,
-			     void *buffer, size_t size)
+			     void *buffer, size_t size, int flags)
 {
 	struct afs_vnode *vnode = AFS_FS_I(inode);
 	char text[16 + 1 + 24 + 1 + 8 + 1];
@@ -333,7 +333,7 @@
 static int afs_xattr_get_volume(const struct xattr_handler *handler,
 			      struct dentry *dentry,
 			      struct inode *inode, const char *name,
-			      void *buffer, size_t size)
+			      void *buffer, size_t size, int flags)
 {
 	struct afs_vnode *vnode = AFS_FS_I(inode);
 	const char *volname = vnode->volume->name;
diff -ruN a/fs/btrfs/xattr.c b/fs/btrfs/xattr.c
--- a/fs/btrfs/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/btrfs/xattr.c	2021-12-23 08:35:51.000000000 +0100
@@ -378,7 +378,8 @@
 
 static int btrfs_xattr_handler_get(const struct xattr_handler *handler,
 				   struct dentry *unused, struct inode *inode,
-				   const char *name, void *buffer, size_t size)
+				   const char *name, void *buffer, size_t size,
+				   int flags)
 {
 	name = xattr_full_name(handler, name);
 	return btrfs_getxattr(inode, name, buffer, size);
diff -ruN a/fs/ceph/xattr.c b/fs/ceph/xattr.c
--- a/fs/ceph/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ceph/xattr.c	2021-12-23 08:35:51.000000000 +0100
@@ -1256,7 +1256,8 @@
 
 static int ceph_get_xattr_handler(const struct xattr_handler *handler,
 				  struct dentry *dentry, struct inode *inode,
-				  const char *name, void *value, size_t size)
+				  const char *name, void *value, size_t size,
+				  int flags)
 {
 	if (!ceph_is_valid_xattr(name))
 		return -EOPNOTSUPP;
diff -ruN a/fs/cifs/xattr.c b/fs/cifs/xattr.c
--- a/fs/cifs/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/cifs/xattr.c	2021-12-23 08:35:51.000000000 +0100
@@ -279,7 +279,7 @@
 
 static int cifs_xattr_get(const struct xattr_handler *handler,
 			  struct dentry *dentry, struct inode *inode,
-			  const char *name, void *value, size_t size)
+			  const char *name, void *value, size_t size, int flags)
 {
 	ssize_t rc = -EOPNOTSUPP;
 	unsigned int xid;
diff -ruN a/fs/configfs/dir.c b/fs/configfs/dir.c
--- a/fs/configfs/dir.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/configfs/dir.c	2021-12-23 08:35:51.000000000 +0100
@@ -1384,6 +1384,22 @@
 	else
 		ret = configfs_attach_item(parent_item, item, dentry, frag);
 
+	/* inherit uid/gid from process creating the directory */
+	if (!uid_eq(current_fsuid(), GLOBAL_ROOT_UID) ||
+	    !gid_eq(current_fsgid(), GLOBAL_ROOT_GID)) {
+		struct inode *inode = d_inode(dentry);
+		struct iattr ia = {
+			.ia_uid = current_fsuid(),
+			.ia_gid = current_fsgid(),
+			.ia_valid = ATTR_UID | ATTR_GID,
+		};
+
+		inode->i_uid = ia.ia_uid;
+		inode->i_gid = ia.ia_gid;
+		/* the above manual assignments skip the permission checks */
+		configfs_setattr(mnt_userns, dentry, &ia);
+	}
+
 	spin_lock(&configfs_dirent_lock);
 	sd->s_type &= ~CONFIGFS_USET_IN_MKDIR;
 	if (!ret)
diff -ruN a/fs/configfs/inode.c b/fs/configfs/inode.c
--- a/fs/configfs/inode.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/configfs/inode.c	2021-12-23 08:35:51.000000000 +0100
@@ -32,6 +32,28 @@
 	.setattr	= configfs_setattr,
 };
 
+static struct iattr *configfs_alloc_iattr(struct configfs_dirent *sd_parent,
+					  struct configfs_dirent *sd, unsigned int s_time_gran)
+{
+	struct iattr *sd_iattr;
+
+	sd_iattr = kzalloc(sizeof(struct iattr), GFP_KERNEL);
+	if (!sd_iattr)
+		return NULL;
+	/* assign default attributes */
+	sd_iattr->ia_mode = sd->s_mode;
+	if (sd_parent && sd_parent->s_iattr) {
+		sd_iattr->ia_uid = sd_parent->s_iattr->ia_uid;
+		sd_iattr->ia_gid = sd_parent->s_iattr->ia_gid;
+	} else {
+		sd_iattr->ia_uid = GLOBAL_ROOT_UID;
+		sd_iattr->ia_gid = GLOBAL_ROOT_GID;
+	}
+	ktime_get_coarse_real_ts64(&sd_iattr->ia_ctime);
+	sd_iattr->ia_atime = sd_iattr->ia_mtime = sd_iattr->ia_ctime;
+	return sd_iattr;
+}
+
 int configfs_setattr(struct user_namespace *mnt_userns, struct dentry *dentry,
 		     struct iattr *iattr)
 {
@@ -47,15 +69,9 @@
 	sd_iattr = sd->s_iattr;
 	if (!sd_iattr) {
 		/* setting attributes for the first time, allocate now */
-		sd_iattr = kzalloc(sizeof(struct iattr), GFP_KERNEL);
+		sd_iattr = configfs_alloc_iattr(NULL, sd, inode->i_sb->s_time_gran);
 		if (!sd_iattr)
 			return -ENOMEM;
-		/* assign default attributes */
-		sd_iattr->ia_mode = sd->s_mode;
-		sd_iattr->ia_uid = GLOBAL_ROOT_UID;
-		sd_iattr->ia_gid = GLOBAL_ROOT_GID;
-		sd_iattr->ia_atime = sd_iattr->ia_mtime =
-			sd_iattr->ia_ctime = current_time(inode);
 		sd->s_iattr = sd_iattr;
 	}
 	/* attributes were changed atleast once in past */
@@ -159,6 +175,7 @@
 	struct inode *inode = NULL;
 	struct configfs_dirent *sd;
 	struct inode *p_inode;
+	struct dentry *parent;
 
 	if (!dentry)
 		return ERR_PTR(-ENOENT);
@@ -167,6 +184,14 @@
 		return ERR_PTR(-EEXIST);
 
 	sd = dentry->d_fsdata;
+	parent = dget_parent(dentry);
+	if (parent && !sd->s_iattr) {
+		sd->s_iattr = configfs_alloc_iattr(parent->d_fsdata, sd,
+						   parent->d_sb->s_time_gran);
+		if (!sd->s_iattr)
+			return ERR_PTR(-ENOMEM);
+	}
+	dput(parent);
 	inode = configfs_new_inode(mode, sd, dentry->d_sb);
 	if (!inode)
 		return ERR_PTR(-ENOMEM);
diff -ruN a/fs/ecryptfs/crypto.c b/fs/ecryptfs/crypto.c
--- a/fs/ecryptfs/crypto.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ecryptfs/crypto.c	2021-12-23 08:35:51.000000000 +0100
@@ -817,10 +817,10 @@
 	m_2 = get_unaligned_be32(data + 4);
 	if ((m_1 ^ MAGIC_ECRYPTFS_MARKER) == m_2)
 		return 0;
-	ecryptfs_printk(KERN_DEBUG, "m_1 = [0x%.8x]; m_2 = [0x%.8x]; "
+	ecryptfs_printk(KERN_WARNING, "m_1 = [0x%.8x]; m_2 = [0x%.8x]; "
 			"MAGIC_ECRYPTFS_MARKER = [0x%.8x]\n", m_1, m_2,
 			MAGIC_ECRYPTFS_MARKER);
-	ecryptfs_printk(KERN_DEBUG, "(m_1 ^ MAGIC_ECRYPTFS_MARKER) = "
+	ecryptfs_printk(KERN_WARNING, "(m_1 ^ MAGIC_ECRYPTFS_MARKER) = "
 			"[0x%.8x]\n", (m_1 ^ MAGIC_ECRYPTFS_MARKER));
 	return -EINVAL;
 }
@@ -1402,6 +1402,10 @@
 		rc = ecryptfs_read_headers_virt(page_virt, crypt_stat,
 						ecryptfs_dentry,
 						ECRYPTFS_VALIDATE_HEADER_SIZE);
+	else
+		ecryptfs_printk(KERN_WARNING, "ecryptfs_read_lower failed with "
+		       "rc=%d (extent_size = %zu)\n", rc,
+		       crypt_stat->extent_size);
 	if (rc) {
 		/* metadata is not in the file header, so try xattrs */
 		memset(page_virt, 0, PAGE_SIZE);
@@ -1973,16 +1977,6 @@
 	return rc;
 }
 
-static bool is_dot_dotdot(const char *name, size_t name_size)
-{
-	if (name_size == 1 && name[0] == '.')
-		return true;
-	else if (name_size == 2 && name[0] == '.' && name[1] == '.')
-		return true;
-
-	return false;
-}
-
 /**
  * ecryptfs_decode_and_decrypt_filename - converts the encoded cipher text name to decoded plaintext
  * @plaintext_name: The plaintext name
@@ -2007,21 +2001,13 @@
 	size_t packet_size;
 	int rc = 0;
 
-	if ((mount_crypt_stat->flags & ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES) &&
-	    !(mount_crypt_stat->flags & ECRYPTFS_ENCRYPTED_VIEW_ENABLED)) {
-		if (is_dot_dotdot(name, name_size)) {
-			rc = ecryptfs_copy_filename(plaintext_name,
-						    plaintext_name_size,
-						    name, name_size);
-			goto out;
-		}
-
-		if (name_size <= ECRYPTFS_FNEK_ENCRYPTED_FILENAME_PREFIX_SIZE ||
-		    strncmp(name, ECRYPTFS_FNEK_ENCRYPTED_FILENAME_PREFIX,
-			    ECRYPTFS_FNEK_ENCRYPTED_FILENAME_PREFIX_SIZE)) {
-			rc = -EINVAL;
-			goto out;
-		}
+	if ((mount_crypt_stat->flags & ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES)
+	    && !(mount_crypt_stat->flags & ECRYPTFS_ENCRYPTED_VIEW_ENABLED)
+	    && (name_size > ECRYPTFS_FNEK_ENCRYPTED_FILENAME_PREFIX_SIZE)
+	    && (strncmp(name, ECRYPTFS_FNEK_ENCRYPTED_FILENAME_PREFIX,
+			ECRYPTFS_FNEK_ENCRYPTED_FILENAME_PREFIX_SIZE) == 0)) {
+		const char *orig_name = name;
+		size_t orig_name_size = name_size;
 
 		name += ECRYPTFS_FNEK_ENCRYPTED_FILENAME_PREFIX_SIZE;
 		name_size -= ECRYPTFS_FNEK_ENCRYPTED_FILENAME_PREFIX_SIZE;
@@ -2041,9 +2027,12 @@
 						  decoded_name,
 						  decoded_name_size);
 		if (rc) {
-			ecryptfs_printk(KERN_DEBUG,
-					"%s: Could not parse tag 70 packet from filename\n",
-					__func__);
+			printk(KERN_INFO "%s: Could not parse tag 70 packet "
+			       "from filename; copying through filename "
+			       "as-is\n", __func__);
+			rc = ecryptfs_copy_filename(plaintext_name,
+						    plaintext_name_size,
+						    orig_name, orig_name_size);
 			goto out_free;
 		}
 	} else {
diff -ruN a/fs/ecryptfs/ecryptfs_kernel.h b/fs/ecryptfs/ecryptfs_kernel.h
--- a/fs/ecryptfs/ecryptfs_kernel.h	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ecryptfs/ecryptfs_kernel.h	2021-12-23 08:35:51.000000000 +0100
@@ -662,6 +662,7 @@
 				     pgoff_t page_index,
 				     size_t offset_in_page, size_t size,
 				     struct inode *ecryptfs_inode);
+int ecryptfs_fsync_lower(struct inode *ecryptfs_inode, int datasync);
 struct page *ecryptfs_get_locked_page(struct inode *inode, loff_t index);
 int ecryptfs_parse_packet_length(unsigned char *data, size_t *size,
 				 size_t *length_size);
diff -ruN a/fs/ecryptfs/file.c b/fs/ecryptfs/file.c
--- a/fs/ecryptfs/file.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ecryptfs/file.c	2021-12-23 08:35:51.000000000 +0100
@@ -68,28 +68,17 @@
 						  buf->sb, lower_name,
 						  lower_namelen);
 	if (rc) {
-		if (rc != -EINVAL) {
-			ecryptfs_printk(KERN_DEBUG,
-					"%s: Error attempting to decode and decrypt filename [%s]; rc = [%d]\n",
-					__func__, lower_name, rc);
-			return rc;
-		}
-
-		/* Mask -EINVAL errors as these are most likely due a plaintext
-		 * filename present in the lower filesystem despite filename
-		 * encryption being enabled. One unavoidable example would be
-		 * the "lost+found" dentry in the root directory of an Ext4
-		 * filesystem.
-		 */
-		return 0;
+		printk(KERN_ERR "%s: Error attempting to decode and decrypt "
+		       "filename [%s]; rc = [%d]\n", __func__, lower_name,
+		       rc);
+		goto out;
 	}
-
 	buf->caller->pos = buf->ctx.pos;
 	rc = !dir_emit(buf->caller, name, name_size, ino, d_type);
 	kfree(name);
 	if (!rc)
 		buf->entries_written++;
-
+out:
 	return rc;
 }
 
diff -ruN a/fs/ecryptfs/inode.c b/fs/ecryptfs/inode.c
--- a/fs/ecryptfs/inode.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ecryptfs/inode.c	2021-12-23 08:35:51.000000000 +0100
@@ -382,39 +382,54 @@
 				      unsigned int flags)
 {
 	char *encrypted_and_encoded_name = NULL;
-	struct ecryptfs_mount_crypt_stat *mount_crypt_stat;
+	size_t encrypted_and_encoded_name_size;
+	struct ecryptfs_mount_crypt_stat *mount_crypt_stat = NULL;
 	struct dentry *lower_dir_dentry, *lower_dentry;
-	const char *name = ecryptfs_dentry->d_name.name;
-	size_t len = ecryptfs_dentry->d_name.len;
 	struct dentry *res;
 	int rc = 0;
 
 	lower_dir_dentry = ecryptfs_dentry_to_lower(ecryptfs_dentry->d_parent);
-
+	lower_dentry = lookup_one_len_unlocked(ecryptfs_dentry->d_name.name,
+				      lower_dir_dentry,
+				      ecryptfs_dentry->d_name.len);
+	if (IS_ERR(lower_dentry)) {
+		ecryptfs_printk(KERN_DEBUG, "%s: lookup_one_len() returned "
+				"[%ld] on lower_dentry = [%pd]\n", __func__,
+				PTR_ERR(lower_dentry), ecryptfs_dentry);
+		res = ERR_CAST(lower_dentry);
+		goto out;
+	}
+	if (d_really_is_positive(lower_dentry))
+		goto interpose;
 	mount_crypt_stat = &ecryptfs_superblock_to_private(
 				ecryptfs_dentry->d_sb)->mount_crypt_stat;
-	if (mount_crypt_stat->flags & ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES) {
-		rc = ecryptfs_encrypt_and_encode_filename(
-			&encrypted_and_encoded_name, &len,
-			mount_crypt_stat, name, len);
-		if (rc) {
-			printk(KERN_ERR "%s: Error attempting to encrypt and encode "
-			       "filename; rc = [%d]\n", __func__, rc);
-			return ERR_PTR(rc);
-		}
-		name = encrypted_and_encoded_name;
+	if (!(mount_crypt_stat->flags & ECRYPTFS_GLOBAL_ENCRYPT_FILENAMES))
+		goto interpose;
+	dput(lower_dentry);
+	rc = ecryptfs_encrypt_and_encode_filename(
+		&encrypted_and_encoded_name, &encrypted_and_encoded_name_size,
+		mount_crypt_stat, ecryptfs_dentry->d_name.name,
+		ecryptfs_dentry->d_name.len);
+	if (rc) {
+		printk(KERN_ERR "%s: Error attempting to encrypt and encode "
+		       "filename; rc = [%d]\n", __func__, rc);
+		res = ERR_PTR(rc);
+		goto out;
 	}
-
-	lower_dentry = lookup_one_len_unlocked(name, lower_dir_dentry, len);
+	lower_dentry = lookup_one_len_unlocked(encrypted_and_encoded_name,
+				      lower_dir_dentry,
+				      encrypted_and_encoded_name_size);
 	if (IS_ERR(lower_dentry)) {
 		ecryptfs_printk(KERN_DEBUG, "%s: lookup_one_len() returned "
 				"[%ld] on lower_dentry = [%s]\n", __func__,
 				PTR_ERR(lower_dentry),
-				name);
+				encrypted_and_encoded_name);
 		res = ERR_CAST(lower_dentry);
-	} else {
-		res = ecryptfs_lookup_interpose(ecryptfs_dentry, lower_dentry);
+		goto out;
 	}
+interpose:
+	res = ecryptfs_lookup_interpose(ecryptfs_dentry, lower_dentry);
+out:
 	kfree(encrypted_and_encoded_name);
 	return res;
 }
@@ -790,6 +805,12 @@
 			       "rc = [%d]\n", rc);
 			goto out;
 		}
+		rc = ecryptfs_fsync_lower(inode, 1);
+		if (rc) {
+			printk(KERN_WARNING "Problem with ecryptfs_fsync_lower,"
+			       "continue without syncing; "
+			       "rc = [%d]\n", rc);
+		}
 		/* We are reducing the size of the ecryptfs file, and need to
 		 * know if we need to reduce the size of the lower file. */
 		lower_size_before_truncate =
@@ -1050,7 +1071,8 @@
 		goto out;
 	}
 	inode_lock(lower_inode);
-	rc = __vfs_getxattr(lower_dentry, lower_inode, name, value, size);
+	rc = __vfs_getxattr(&init_user_ns, lower_dentry, lower_inode, name,
+			    value, size, XATTR_NOSECURITY);
 	inode_unlock(lower_inode);
 out:
 	return rc;
@@ -1156,7 +1178,8 @@
 
 static int ecryptfs_xattr_get(const struct xattr_handler *handler,
 			      struct dentry *dentry, struct inode *inode,
-			      const char *name, void *buffer, size_t size)
+			      const char *name, void *buffer, size_t size,
+			      int flags)
 {
 	return ecryptfs_getxattr(dentry, inode, name, buffer, size);
 }
diff -ruN a/fs/ecryptfs/mmap.c b/fs/ecryptfs/mmap.c
--- a/fs/ecryptfs/mmap.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ecryptfs/mmap.c	2021-12-23 08:35:51.000000000 +0100
@@ -422,8 +422,9 @@
 		goto out;
 	}
 	inode_lock(lower_inode);
-	size = __vfs_getxattr(lower_dentry, lower_inode, ECRYPTFS_XATTR_NAME,
-			      xattr_virt, PAGE_SIZE);
+	size = __vfs_getxattr(&init_user_ns, lower_dentry, lower_inode,
+			      ECRYPTFS_XATTR_NAME, xattr_virt, PAGE_SIZE,
+			      XATTR_NOSECURITY);
 	if (size < 0)
 		size = 8;
 	put_unaligned_be64(i_size_read(ecryptfs_inode), xattr_virt);
diff -ruN a/fs/ecryptfs/read_write.c b/fs/ecryptfs/read_write.c
--- a/fs/ecryptfs/read_write.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ecryptfs/read_write.c	2021-12-23 08:35:51.000000000 +0100
@@ -261,3 +261,25 @@
 	flush_dcache_page(page_for_ecryptfs);
 	return rc;
 }
+
+/**
+ * ecryptfs_fsync_lower
+ * @ecryptfs_inode: The eCryptfs inode
+ * @datasync: Only perform a fdatasync operation
+ *
+ * Write back data and metadata for the lower file to disk.  If @datasync is
+ * set only metadata needed to access modified file data is written.
+ *
+ * Returns 0 on success; less than zero on error
+ */
+int ecryptfs_fsync_lower(struct inode *ecryptfs_inode, int datasync)
+{
+	struct file *lower_file;
+
+	lower_file = ecryptfs_inode_to_private(ecryptfs_inode)->lower_file;
+	if (!lower_file)
+		return -EIO;
+	if (!lower_file->f_op->fsync)
+		return 0;
+	return vfs_fsync(lower_file, datasync);
+}
diff -ruN a/fs/erofs/xattr.c b/fs/erofs/xattr.c
--- a/fs/erofs/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/erofs/xattr.c	2021-12-23 08:35:51.000000000 +0100
@@ -470,7 +470,8 @@
 
 static int erofs_xattr_generic_get(const struct xattr_handler *handler,
 				   struct dentry *unused, struct inode *inode,
-				   const char *name, void *buffer, size_t size)
+				   const char *name, void *buffer, size_t size,
+				   int flags)
 {
 	struct erofs_sb_info *const sbi = EROFS_I_SB(inode);
 
diff -ruN a/fs/esdfs/dentry.c b/fs/esdfs/dentry.c
--- a/fs/esdfs/dentry.c	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/dentry.c	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,158 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/ctype.h>
+#include "esdfs.h"
+
+/*
+ * returns: -ERRNO if error (returned to user)
+ *          0: tell VFS to invalidate dentry
+ *          1: dentry is valid
+ */
+static int esdfs_d_revalidate(struct dentry *dentry, unsigned int flags)
+{
+	struct path lower_path;
+	struct path lower_parent_path;
+	struct dentry *parent_dentry = NULL;
+	struct dentry *lower_dentry = NULL;
+	struct dentry *lower_parent_dentry = NULL;
+	int err = 1;
+
+	if (flags & LOOKUP_RCU)
+		return -ECHILD;
+
+	/* short-circuit if it's root */
+	spin_lock(&dentry->d_lock);
+	if (IS_ROOT(dentry)) {
+		spin_unlock(&dentry->d_lock);
+		return 1;
+	}
+	spin_unlock(&dentry->d_lock);
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	esdfs_get_lower_parent(dentry, lower_dentry, &lower_parent_dentry);
+
+	parent_dentry = dget_parent(dentry);
+	esdfs_get_lower_path(parent_dentry, &lower_parent_path);
+
+	if (lower_parent_path.dentry != lower_parent_dentry)
+		goto drop;
+
+	if (lower_dentry->d_flags & DCACHE_OP_REVALIDATE) {
+		err = lower_dentry->d_op->d_revalidate(lower_dentry, flags);
+		if (err == 0)
+			goto drop;
+	}
+
+	/* can't do strcmp if lower is hashed */
+	spin_lock(&lower_dentry->d_lock);
+	if (d_unhashed(lower_dentry)) {
+		spin_unlock(&lower_dentry->d_lock);
+		goto drop;
+	}
+
+	spin_lock_nested(&dentry->d_lock, DENTRY_D_LOCK_NESTED);
+
+	if (!qstr_case_eq(&lower_dentry->d_name, &dentry->d_name)) {
+		err = 0;
+		__d_drop(dentry);	/* already holding spin lock */
+	}
+
+	spin_unlock(&dentry->d_lock);
+	spin_unlock(&lower_dentry->d_lock);
+
+	esdfs_revalidate_perms(dentry);
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(dentry->d_sb)) &&
+	    esdfs_derived_revalidate(dentry, parent_dentry))
+		goto drop;
+
+	goto out;
+
+drop:
+	d_drop(dentry);
+	err = 0;
+out:
+	esdfs_put_lower_path(parent_dentry, &lower_parent_path);
+	dput(parent_dentry);
+	esdfs_put_lower_parent(dentry, &lower_parent_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	return err;
+}
+
+/* directly from fs/fat/namei_vfat.c */
+static unsigned int __vfat_striptail_len(unsigned int len, const char *name)
+{
+	while (len && name[len - 1] == '.')
+		len--;
+	return len;
+}
+
+static unsigned int vfat_striptail_len(const struct qstr *qstr)
+{
+	return __vfat_striptail_len(qstr->len, qstr->name);
+}
+
+
+/* based on vfat_hashi() in fs/fat/namei_vfat.c (no code pages) */
+static int esdfs_d_hash(const struct dentry *dentry, struct qstr *qstr)
+{
+	const unsigned char *name;
+	unsigned int len;
+	unsigned long hash;
+
+	name = qstr->name;
+	len = vfat_striptail_len(qstr);
+
+	hash = init_name_hash(dentry);
+	while (len--)
+		hash = partial_name_hash(tolower(*name++), hash);
+	qstr->hash = end_name_hash(hash);
+
+	return 0;
+}
+
+/* based on vfat_cmpi() in fs/fat/namei_vfat.c (no code pages) */
+static int esdfs_d_compare(const struct dentry *dentry, unsigned int len,
+			   const char *str, const struct qstr *name)
+{
+	unsigned int alen, blen;
+
+	/* A filename cannot end in '.' or we treat it like it has none */
+	alen = vfat_striptail_len(name);
+	blen = __vfat_striptail_len(len, str);
+	if (alen == blen) {
+		if (str_n_case_eq(name->name, str, alen))
+			return 0;
+	}
+	return 1;
+}
+
+static void esdfs_d_release(struct dentry *dentry)
+{
+	if (!dentry || !dentry->d_fsdata)
+		return;
+
+	/* release and reset the lower paths */
+	esdfs_put_reset_lower_paths(dentry);
+	esdfs_release_lower_parent(dentry);
+	esdfs_free_dentry_private_data(dentry);
+}
+
+const struct dentry_operations esdfs_dops = {
+	.d_revalidate	= esdfs_d_revalidate,
+	.d_delete	= always_delete_dentry,
+	.d_hash		= esdfs_d_hash,
+	.d_compare	= esdfs_d_compare,
+	.d_release	= esdfs_d_release,
+};
diff -ruN a/fs/esdfs/derive.c b/fs/esdfs/derive.c
--- a/fs/esdfs/derive.c	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/derive.c	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,608 @@
+/*
+ * Copyright (c) 2013-2014 Motorola Mobility LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 and
+ * only version 2 as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#include <linux/proc_fs.h>
+#include <linux/hashtable.h>
+#include <linux/syscalls.h>
+#include <linux/fcntl.h>
+#include <linux/ctype.h>
+#include <linux/vmalloc.h>
+#include <linux/security.h>
+#include <linux/uaccess.h>
+#include "esdfs.h"
+
+static struct qstr names_secure[] = {
+	QSTR_LITERAL("autorun.inf"),
+	QSTR_LITERAL(".android_secure"),
+	QSTR_LITERAL("android_secure"),
+	QSTR_LITERAL("")
+};
+
+/* special path name searches */
+static inline bool match_name(struct qstr *name, struct qstr names[])
+{
+	int i = 0;
+
+	BUG_ON(!name);
+	for (i = 0; *names[i].name; i++)
+		if (qstr_case_eq(name, &names[i]))
+			return true;
+
+	return false;
+}
+
+unsigned esdfs_package_list_version;
+
+static void fixup_perms_by_flag(int flags, const struct qstr *key,
+					uint32_t userid)
+{
+	esdfs_package_list_version++;
+}
+
+static struct pkg_list esdfs_pkg_list = {
+		.update = fixup_perms_by_flag,
+};
+
+int esdfs_init_package_list(void)
+{
+	pkglist_register_update_listener(&esdfs_pkg_list);
+	return 0;
+}
+
+void esdfs_destroy_package_list(void)
+{
+	pkglist_unregister_update_listener(&esdfs_pkg_list);
+}
+
+/*
+ * Derive an entry's premissions tree position based on its parent.
+ */
+void esdfs_derive_perms(struct dentry *dentry)
+{
+	struct esdfs_inode_info *inode_i = ESDFS_I(dentry->d_inode);
+	bool is_root;
+	int ret;
+	kuid_t appid;
+	struct qstr q_Download = QSTR_LITERAL("Download");
+	struct qstr q_Android = QSTR_LITERAL("Android");
+	struct qstr q_data = QSTR_LITERAL("data");
+	struct qstr q_obb = QSTR_LITERAL("obb");
+	struct qstr q_media = QSTR_LITERAL("media");
+	struct qstr q_cache = QSTR_LITERAL("cache");
+	struct qstr q_user = QSTR_LITERAL("user");
+	struct esdfs_inode_info *parent_i = ESDFS_I(dentry->d_parent->d_inode);
+
+	spin_lock(&dentry->d_lock);
+	is_root = IS_ROOT(dentry);
+	spin_unlock(&dentry->d_lock);
+	if (is_root)
+		return;
+
+	/* Inherit from the parent to start */
+	inode_i->tree = parent_i->tree;
+	inode_i->userid = parent_i->userid;
+	inode_i->appid = parent_i->appid;
+	inode_i->under_obb = parent_i->under_obb;
+
+	/*
+	 * ESDFS_TREE_MEDIA* are intentionally dead ends.
+	 */
+	switch (inode_i->tree) {
+	case ESDFS_TREE_ROOT_LEGACY:
+		inode_i->tree = ESDFS_TREE_ROOT;
+		ret = kstrtou32(dentry->d_name.name, 0, &inode_i->userid);
+		if (qstr_case_eq(&dentry->d_name, &q_obb))
+			inode_i->tree = ESDFS_TREE_ANDROID_OBB;
+		break;
+
+	case ESDFS_TREE_ROOT:
+		inode_i->tree = ESDFS_TREE_MEDIA;
+		if (qstr_case_eq(&dentry->d_name, &q_Download))
+			inode_i->tree = ESDFS_TREE_DOWNLOAD;
+		else if (qstr_case_eq(&dentry->d_name, &q_Android))
+			inode_i->tree = ESDFS_TREE_ANDROID;
+		break;
+
+	case ESDFS_TREE_ANDROID:
+		if (qstr_case_eq(&dentry->d_name, &q_data)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_DATA;
+		} else if (qstr_case_eq(&dentry->d_name, &q_obb)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_OBB;
+			inode_i->under_obb = true;
+		} else if (qstr_case_eq(&dentry->d_name, &q_media)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_MEDIA;
+		} else if (ESDFS_RESTRICT_PERMS(ESDFS_SB(dentry->d_sb)) &&
+			 qstr_case_eq(&dentry->d_name, &q_user)) {
+			inode_i->tree = ESDFS_TREE_ANDROID_USER;
+		}
+		break;
+
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_OBB:
+	case ESDFS_TREE_ANDROID_MEDIA:
+		appid = pkglist_get_allowed_appid(dentry->d_name.name,
+						inode_i->userid);
+		if (uid_valid(appid))
+			inode_i->appid = esdfs_from_kuid(
+					ESDFS_SB(dentry->d_sb), appid);
+		else
+			inode_i->appid = 0;
+		inode_i->tree = ESDFS_TREE_ANDROID_APP;
+		break;
+	case ESDFS_TREE_ANDROID_APP:
+		if (qstr_case_eq(&dentry->d_name, &q_cache))
+			inode_i->tree = ESDFS_TREE_ANDROID_APP_CACHE;
+		break;
+	case ESDFS_TREE_ANDROID_USER:
+		/* Another user, so start over */
+		inode_i->tree = ESDFS_TREE_ROOT;
+		ret = kstrtou32(dentry->d_name.name, 0, &inode_i->userid);
+		break;
+	}
+}
+
+/* Apply tree position-specific permissions */
+void esdfs_set_derived_perms(struct inode *inode)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(inode->i_sb);
+	struct esdfs_inode_info *inode_i = ESDFS_I(inode);
+	gid_t gid = sbi->upper_perms.gid;
+
+	esdfs_i_uid_write(inode, sbi->upper_perms.uid);
+	inode->i_mode &= S_IFMT;
+	if (ESDFS_RESTRICT_PERMS(sbi))
+		esdfs_i_gid_write(inode, gid);
+	else {
+		if (gid == AID_SDCARD_RW && !test_opt(sbi, DEFAULT_NORMAL))
+			esdfs_i_gid_write(inode, AID_SDCARD_RW);
+		else
+			esdfs_i_gid_write(inode, derive_uid(inode_i, gid));
+		inode->i_mode |= sbi->upper_perms.dmask;
+	}
+
+	switch (inode_i->tree) {
+	case ESDFS_TREE_ROOT_LEGACY:
+		if (ESDFS_RESTRICT_PERMS(sbi))
+			inode->i_mode |= sbi->upper_perms.dmask;
+		else if (test_opt(sbi, DERIVE_MULTI)) {
+			inode->i_mode &= S_IFMT;
+			inode->i_mode |= 0711;
+		}
+		break;
+
+	case ESDFS_TREE_NONE:
+	case ESDFS_TREE_ROOT:
+		if (ESDFS_RESTRICT_PERMS(sbi)) {
+			esdfs_i_gid_write(inode, AID_SDCARD_R);
+			inode->i_mode |= sbi->upper_perms.dmask;
+		} else if (test_opt(sbi, DERIVE_PUBLIC) &&
+			   test_opt(ESDFS_SB(inode->i_sb), DERIVE_CONFINE)) {
+			inode->i_mode &= S_IFMT;
+			inode->i_mode |= 0771;
+		}
+		break;
+
+	case ESDFS_TREE_MEDIA:
+		if (ESDFS_RESTRICT_PERMS(sbi)) {
+			esdfs_i_gid_write(inode, AID_SDCARD_R);
+			inode->i_mode |= 0770;
+		}
+		break;
+
+	case ESDFS_TREE_DOWNLOAD:
+	case ESDFS_TREE_ANDROID:
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_OBB:
+	case ESDFS_TREE_ANDROID_MEDIA:
+		if (ESDFS_RESTRICT_PERMS(sbi))
+			inode->i_mode |= 0771;
+		break;
+
+	case ESDFS_TREE_ANDROID_APP:
+	case ESDFS_TREE_ANDROID_APP_CACHE:
+		if (inode_i->appid)
+			esdfs_i_uid_write(inode, derive_uid(inode_i,
+							inode_i->appid));
+		if (ESDFS_RESTRICT_PERMS(sbi))
+			inode->i_mode |= 0770;
+		break;
+
+	case ESDFS_TREE_ANDROID_USER:
+		if (ESDFS_RESTRICT_PERMS(sbi)) {
+			esdfs_i_gid_write(inode, AID_SDCARD_ALL);
+			inode->i_mode |= 0770;
+		}
+		inode->i_mode |= 0770;
+		break;
+	}
+
+	/* strip execute bits from any non-directories */
+	if (!S_ISDIR(inode->i_mode))
+		inode->i_mode &= ~S_IXUGO;
+}
+
+/*
+ * Before rerouting a lookup to follow a pseudo hard link, make sure that
+ * a stub exists at the source.  Without it, readdir won't see an entry there
+ * resulting in a strange user experience.
+ */
+static int lookup_link_source(struct dentry *dentry, struct dentry *parent)
+{
+	struct path lower_parent_path, lower_path;
+	int err;
+
+	esdfs_get_lower_path(parent, &lower_parent_path);
+
+	/* Check if the stub user profile folder is there. */
+	err = esdfs_lookup_nocase(&lower_parent_path, &dentry->d_name,
+					&lower_path);
+	/* Remember it to handle renames and removal. */
+	if (!err)
+		esdfs_set_lower_stub_path(dentry, &lower_path);
+
+	esdfs_put_lower_path(parent, &lower_parent_path);
+
+	return err;
+}
+
+int esdfs_is_dl_lookup(struct dentry *dentry, struct dentry *parent)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(parent->d_sb);
+	struct esdfs_inode_info *parent_i = ESDFS_I(parent->d_inode);
+	/*
+	 * Return 1 if this is the Download directory:
+	 * The test for download checks:
+	 * 1. The parent is the mount root.
+	 * 2. The directory is named 'Download'.
+	 * 3. The stub for the directory exists.
+	 */
+	if (test_opt(sbi, SPECIAL_DOWNLOAD) &&
+			parent_i->tree == ESDFS_TREE_ROOT &&
+			ESDFS_DENTRY_NEEDS_DL_LINK(dentry) &&
+			lookup_link_source(dentry, parent) == 0) {
+		return 1;
+	}
+
+	return 0;
+}
+
+int esdfs_derived_lookup(struct dentry *dentry, struct dentry **parent)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB((*parent)->d_sb);
+	struct esdfs_inode_info *parent_i = ESDFS_I((*parent)->d_inode);
+	struct qstr q_Android = QSTR_LITERAL("Android");
+
+	/* Deny access to security-sensitive entries. */
+	if (ESDFS_I((*parent)->d_inode)->tree == ESDFS_TREE_ROOT &&
+	    match_name(&dentry->d_name, names_secure)) {
+		pr_debug("esdfs: denying access to: %s", dentry->d_name.name);
+		return -EACCES;
+	}
+
+	/* Pin the unified mode obb link parent as it flies by. */
+	if (!sbi->obb_parent &&
+	    test_opt(sbi, DERIVE_UNIFIED) &&
+	    parent_i->tree == ESDFS_TREE_ROOT &&
+	    parent_i->userid == 0 &&
+	    qstr_case_eq(&dentry->d_name, &q_Android))
+		sbi->obb_parent = dget(dentry);		/* keep it pinned */
+
+	/*
+	 * Handle obb directory "grafting" as a pseudo hard link by overriding
+	 * its parent to point to the target obb directory's parent.  The rest
+	 * of the lookup process will take care of setting up the bottom half
+	 * to point to the real obb directory.
+	 */
+	if (parent_i->tree == ESDFS_TREE_ANDROID &&
+	    ESDFS_DENTRY_NEEDS_LINK(dentry) &&
+	    lookup_link_source(dentry, *parent) == 0) {
+		BUG_ON(!sbi->obb_parent);
+		if (ESDFS_INODE_CAN_LINK((*parent)->d_inode))
+			*parent = dget(sbi->obb_parent);
+	}
+
+	return 0;
+}
+
+int esdfs_derived_revalidate(struct dentry *dentry, struct dentry *parent)
+{
+	/*
+	 * If obb is not linked yet, it means the dentry is pointing to the
+	 * stub.  Invalidate the dentry to force another lookup.
+	 */
+	if (ESDFS_I(parent->d_inode)->tree == ESDFS_TREE_ANDROID &&
+	    ESDFS_INODE_CAN_LINK(dentry->d_inode) &&
+	    ESDFS_DENTRY_NEEDS_LINK(dentry) &&
+	    !ESDFS_DENTRY_IS_LINKED(dentry))
+		return -ESTALE;
+	if (ESDFS_I(parent->d_inode)->tree == ESDFS_TREE_ROOT &&
+	    ESDFS_DENTRY_NEEDS_DL_LINK(dentry) &&
+	    !ESDFS_DENTRY_IS_LINKED(dentry))
+		return -ESTALE;
+	return 0;
+}
+
+/*
+ * Implement the extra checking that is done based on the caller's package
+ * list-based access rights.
+ */
+int esdfs_check_derived_permission(struct inode *inode, int mask)
+{
+	const struct cred *cred;
+	uid_t uid, appid;
+
+	/*
+	 * If we don't need to restrict access based on app GIDs and confine
+	 * writes to outside of the Android/... tree, we can skip all of this.
+	 */
+	if (!ESDFS_RESTRICT_PERMS(ESDFS_SB(inode->i_sb)) &&
+	    !test_opt(ESDFS_SB(inode->i_sb), DERIVE_CONFINE))
+			return 0;
+
+	cred = current_cred();
+	uid = from_kuid(&init_user_ns, cred->uid);
+	appid = uid % PKG_APPID_PER_USER;
+
+	/* Reads, owners, and root are always granted access */
+	if (!(mask & (MAY_WRITE | ESDFS_MAY_CREATE)) ||
+	    uid == 0 || uid_eq(cred->uid, inode->i_uid))
+		return 0;
+
+	/*
+	 * Grant access to sdcard_rw holders, unless we are in unified mode
+	 * and we are trying to write to the protected /Android tree or to
+	 * create files in the root (aka, "confined" access).
+	 */
+	if ((!test_opt(ESDFS_SB(inode->i_sb), DERIVE_UNIFIED) ||
+	     (ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_DOWNLOAD &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_DATA &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_OBB &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_MEDIA &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_APP &&
+	      ESDFS_I(inode)->tree != ESDFS_TREE_ANDROID_APP_CACHE &&
+	      (ESDFS_I(inode)->tree != ESDFS_TREE_ROOT ||
+	       !(mask & ESDFS_MAY_CREATE)))))
+		return 0;
+
+	pr_debug("esdfs: %s: denying access to appid: %u\n", __func__, appid);
+	return -EACCES;
+}
+
+static gid_t get_type(struct esdfs_sb_info *sbi, const char *name)
+{
+	const char *ext = strrchr(name, '.');
+	kgid_t id;
+
+	if (ext && ext[0]) {
+		ext = &ext[1];
+		id = pkglist_get_ext_gid(ext);
+		return gid_valid(id)?esdfs_from_kgid(sbi, id):AID_MEDIA_RW;
+	}
+	return AID_MEDIA_RW;
+}
+
+static kuid_t esdfs_get_derived_lower_uid(struct esdfs_sb_info *sbi,
+				struct esdfs_inode_info *info)
+{
+	uid_t uid = sbi->lower_perms.uid;
+	int perm;
+
+	perm = info->tree;
+	if (info->under_obb)
+		perm = ESDFS_TREE_ANDROID_OBB;
+
+	switch (perm) {
+	case ESDFS_TREE_DOWNLOAD:
+		if (test_opt(sbi, SPECIAL_DOWNLOAD))
+			return make_kuid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_uid);
+		fallthrough;
+	case ESDFS_TREE_ROOT:
+	case ESDFS_TREE_MEDIA:
+	case ESDFS_TREE_ANDROID:
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_MEDIA:
+	case ESDFS_TREE_ANDROID_APP:
+	case ESDFS_TREE_ANDROID_APP_CACHE:
+		uid = derive_uid(info, uid);
+		break;
+	case ESDFS_TREE_ANDROID_OBB:
+		uid = AID_MEDIA_OBB;
+		break;
+	case ESDFS_TREE_ROOT_LEGACY:
+	default:
+		break;
+	}
+	return esdfs_make_kuid(sbi, uid);
+}
+
+static kgid_t esdfs_get_derived_lower_gid(struct esdfs_sb_info *sbi,
+				struct esdfs_inode_info *info, const char *name)
+{
+	gid_t gid = sbi->lower_perms.gid;
+	uid_t upper_uid;
+	int perm;
+
+	upper_uid = esdfs_i_uid_read(&info->vfs_inode);
+	perm = info->tree;
+	if (info->under_obb)
+		perm = ESDFS_TREE_ANDROID_OBB;
+
+	switch (perm) {
+	case ESDFS_TREE_DOWNLOAD:
+		if (test_opt(sbi, SPECIAL_DOWNLOAD))
+			return make_kgid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_gid);
+		fallthrough;
+	case ESDFS_TREE_ROOT:
+	case ESDFS_TREE_MEDIA:
+	case ESDFS_TREE_ANDROID:
+	case ESDFS_TREE_ANDROID_DATA:
+	case ESDFS_TREE_ANDROID_MEDIA:
+		if (S_ISDIR(info->vfs_inode.i_mode))
+			gid = derive_uid(info, AID_MEDIA_RW);
+		else
+			gid = derive_uid(info, get_type(sbi, name));
+		break;
+	case ESDFS_TREE_ANDROID_OBB:
+		gid = AID_MEDIA_OBB;
+		break;
+	case ESDFS_TREE_ANDROID_APP:
+		if (uid_is_app(upper_uid))
+			gid = multiuser_get_ext_gid(upper_uid);
+		else
+			gid = derive_uid(info, AID_MEDIA_RW);
+		break;
+	case ESDFS_TREE_ANDROID_APP_CACHE:
+		if (uid_is_app(upper_uid))
+			gid = multiuser_get_ext_cache_gid(upper_uid);
+		else
+			gid = derive_uid(info, AID_MEDIA_RW);
+		break;
+	case ESDFS_TREE_ROOT_LEGACY:
+	default:
+		break;
+	}
+	return esdfs_make_kgid(sbi, gid);
+}
+
+void esdfs_derive_lower_ownership(struct dentry *dentry, const char *name)
+{
+	struct path path;
+	struct inode *inode;
+	struct inode *delegated_inode = NULL;
+	int error;
+	struct esdfs_sb_info *sbi = ESDFS_SB(dentry->d_sb);
+	struct esdfs_inode_info *info = ESDFS_I(dentry->d_inode);
+	kuid_t kuid;
+	kgid_t kgid;
+	struct iattr newattrs;
+
+	if (!test_opt(sbi, GID_DERIVATION))
+		return;
+
+	esdfs_get_lower_path(dentry, &path);
+	inode = path.dentry->d_inode;
+	kuid = esdfs_get_derived_lower_uid(sbi, info);
+	kgid = esdfs_get_derived_lower_gid(sbi, info, name);
+	if (!gid_eq(path.dentry->d_inode->i_gid, kgid)
+		|| !uid_eq(path.dentry->d_inode->i_uid, kuid)) {
+retry_deleg:
+		newattrs.ia_valid = ATTR_GID | ATTR_UID | ATTR_FORCE;
+		newattrs.ia_uid = kuid;
+		newattrs.ia_gid = kgid;
+		if (!S_ISDIR(inode->i_mode))
+			newattrs.ia_valid |= ATTR_KILL_SUID | ATTR_KILL_SGID
+						| ATTR_KILL_PRIV;
+		inode_lock(inode);
+		error = security_path_chown(&path, newattrs.ia_uid,
+						newattrs.ia_gid);
+		if (!error)
+			error = notify_change(&init_user_ns, path.dentry,
+						&newattrs, &delegated_inode);
+		inode_unlock(inode);
+		if (delegated_inode) {
+			error = break_deleg_wait(&delegated_inode);
+			if (!error)
+				goto retry_deleg;
+		}
+		if (error)
+			pr_debug("esdfs: Failed to touch up lower fs gid/uid for %s\n", name);
+	}
+	esdfs_put_lower_path(dentry, &path);
+}
+
+/*
+ * The sdcard service has a hack that creates .nomedia files along certain
+ * paths to stop MediaScanner.  Create those here.
+ */
+int esdfs_derive_mkdir_contents(struct dentry *dir_dentry)
+{
+	struct esdfs_inode_info *inode_i;
+	struct qstr nomedia;
+	struct dentry *lower_dentry;
+	struct path lower_dir_path, lower_path;
+	struct dentry *lower_parent_dentry = NULL;
+	umode_t mode;
+	int err = 0;
+	const struct cred *creds;
+	int mask = 0;
+
+	if (!dir_dentry->d_inode)
+		return 0;
+
+	inode_i = ESDFS_I(dir_dentry->d_inode);
+
+	/*
+	 * Only create .nomedia in Android/data and Android/obb, but never in
+	 * pseudo link stubs.
+	 */
+	if ((inode_i->tree != ESDFS_TREE_ANDROID_DATA &&
+	     inode_i->tree != ESDFS_TREE_ANDROID_OBB) ||
+	    (ESDFS_INODE_CAN_LINK(dir_dentry->d_inode) &&
+	     ESDFS_DENTRY_NEEDS_LINK(dir_dentry) &&
+	     !ESDFS_DENTRY_IS_LINKED(dir_dentry)))
+		return 0;
+
+	esdfs_get_lower_path(dir_dentry, &lower_dir_path);
+
+	nomedia.name = ".nomedia";
+	nomedia.len = strlen(nomedia.name);
+	nomedia.hash = full_name_hash(lower_dir_path.dentry, nomedia.name,
+				      nomedia.len);
+
+	/* check if lower has its own hash */
+	if (lower_dir_path.dentry->d_flags & DCACHE_OP_HASH)
+		lower_dir_path.dentry->d_op->d_hash(lower_dir_path.dentry,
+							&nomedia);
+
+	creds = esdfs_override_creds(ESDFS_SB(dir_dentry->d_sb),
+					inode_i, &mask);
+	/* See if the lower file is there already. */
+	err = vfs_path_lookup(lower_dir_path.dentry, lower_dir_path.mnt,
+			      nomedia.name, 0, &lower_path);
+	if (!err)
+		path_put(&lower_path);
+	/* If it's there or there was an error, we're done */
+	if (!err || err != -ENOENT)
+		goto out;
+
+	/* The lower file is not there.  See if the dentry is in the cache. */
+	lower_dentry = d_lookup(lower_dir_path.dentry, &nomedia);
+	if (!lower_dentry) {
+		/* It's not there, so create a negative lower dentry. */
+		lower_dentry = d_alloc(lower_dir_path.dentry, &nomedia);
+		if (!lower_dentry) {
+			err = -ENOMEM;
+			goto out;
+		}
+		d_add(lower_dentry, NULL);
+	}
+
+	/* Now create the lower file. */
+	mode = S_IFREG;
+	lower_parent_dentry = lock_parent(lower_dentry);
+	esdfs_set_lower_mode(ESDFS_SB(dir_dentry->d_sb), inode_i, &mode);
+	err = vfs_create(&init_user_ns, lower_dir_path.dentry->d_inode,
+			 lower_dentry, mode, true);
+	unlock_dir(lower_parent_dentry);
+	dput(lower_dentry);
+
+out:
+	esdfs_put_lower_path(dir_dentry, &lower_dir_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
diff -ruN a/fs/esdfs/esdfs.h b/fs/esdfs/esdfs.h
--- a/fs/esdfs/esdfs.h	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/esdfs.h	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,627 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef _ESDFS_H_
+#define _ESDFS_H_
+
+#include <linux/dcache.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/iversion.h>
+#include <linux/aio.h>
+#include <linux/mm.h>
+#include <linux/mount.h>
+#include <uapi/linux/mount.h>
+#include <linux/namei.h>
+#include <linux/seq_file.h>
+#include <linux/statfs.h>
+#include <linux/fs_stack.h>
+#include <linux/magic.h>
+#include <linux/uaccess.h>
+#include <linux/slab.h>
+#include <linux/sched.h>
+#include <linux/fs_struct.h>
+#include <linux/uidgid.h>
+#include <linux/user_namespace.h>
+#include <linux/pkglist.h>
+
+#include "../internal.h"
+
+/* the file system name */
+#define ESDFS_NAME "esdfs"
+
+/* ioctl command */
+#define ESDFS_IOCTL_MAGIC	'e'
+#define ESDFS_IOC_DIS_ACCESS	_IO(ESDFS_IOCTL_MAGIC, 1)
+
+/* esdfs root inode number */
+#define ESDFS_ROOT_INO     1
+
+/* useful for tracking code reachability */
+#define UDBG printk(KERN_DEFAULT "DBG:%s:%s:%d\n", __FILE__, __func__, __LINE__)
+
+/* mount options */
+#define ESDFS_MOUNT_DERIVE_LEGACY	0x00000001
+#define ESDFS_MOUNT_DERIVE_UNIFIED	0x00000002
+#define ESDFS_MOUNT_DERIVE_MULTI	0x00000004
+#define ESDFS_MOUNT_DERIVE_PUBLIC	0x00000008
+#define ESDFS_MOUNT_DERIVE_CONFINE	0x00000010
+#define ESDFS_MOUNT_ACCESS_DISABLE	0x00000020
+#define ESDFS_MOUNT_GID_DERIVATION	0x00000040
+#define ESDFS_MOUNT_DEFAULT_NORMAL	0x00000080
+#define ESDFS_MOUNT_SPECIAL_DOWNLOAD	0x00000100
+
+#define clear_opt(sbi, option)	(sbi->options &= ~ESDFS_MOUNT_##option)
+#define set_opt(sbi, option)	(sbi->options |= ESDFS_MOUNT_##option)
+#define test_opt(sbi, option)	(sbi->options & ESDFS_MOUNT_##option)
+
+#define ESDFS_DERIVE_PERMS(sbi)	(test_opt(sbi, DERIVE_UNIFIED) || \
+				 test_opt(sbi, DERIVE_LEGACY))
+#define ESDFS_RESTRICT_PERMS(sbi) (ESDFS_DERIVE_PERMS(sbi) && \
+				   !test_opt(sbi, DERIVE_PUBLIC) && \
+				   !test_opt(sbi, DERIVE_MULTI))
+
+/* from android_filesystem_config.h */
+#define AID_ROOT             0
+#define AID_SDCARD_RW     1015
+#define AID_MEDIA_RW      1023
+#define AID_SDCARD_R      1028
+#define AID_SDCARD_PICS   1033
+#define AID_SDCARD_AV     1034
+#define AID_SDCARD_ALL    1035
+#define AID_MEDIA_OBB     1059
+
+/* used in extra persmission check during file creation */
+#define ESDFS_MAY_CREATE	0x00001000
+
+/* derived permissions model based on tree location */
+enum {
+	ESDFS_TREE_NONE = 0,		/* permissions not derived */
+	ESDFS_TREE_ROOT_LEGACY,		/* root for legacy emulated storage */
+	ESDFS_TREE_ROOT,		/* root for a user */
+	ESDFS_TREE_MEDIA,		/* per-user basic permissions */
+	ESDFS_TREE_DOWNLOAD,		/* .../Download */
+	ESDFS_TREE_ANDROID,		/* .../Android */
+	ESDFS_TREE_ANDROID_DATA,	/* .../Android/data */
+	ESDFS_TREE_ANDROID_OBB,		/* .../Android/obb */
+	ESDFS_TREE_ANDROID_MEDIA,	/* .../Android/media */
+	ESDFS_TREE_ANDROID_APP,		/* .../Android/data|obb|media/... */
+	ESDFS_TREE_ANDROID_APP_CACHE,	/* .../Android/data|obb|media/.../cache */
+	ESDFS_TREE_ANDROID_USER,	/* .../Android/user */
+};
+
+/* for permissions table lookups */
+enum {
+	ESDFS_PERMS_LOWER_DEFAULT = 0,
+	ESDFS_PERMS_UPPER_LEGACY,
+	ESDFS_PERMS_UPPER_DERIVED,
+	ESDFS_PERMS_LOWER_DOWNLOAD,
+	ESDFS_PERMS_TABLE_SIZE
+
+};
+
+#define PKG_NAME_MAX		128
+#define PKG_APPID_PER_USER	100000
+#define AID_APP_START		10000 /* first app user */
+#define AID_APP_END		19999 /* last app user */
+#define AID_CACHE_GID_START	20000 /* start of gids for apps to mark cached data */
+#define AID_EXT_GID_START	30000 /* start of gids for apps to mark external data */
+#define AID_EXT_CACHE_GID_START	40000 /* start of gids for apps to mark external cached data */
+#define AID_EXT_CACHE_GID_END	49999 /* end of gids for apps to mark external cached data */
+#define AID_SHARED_GID_START	50000 /* start of gids for apps in each user to share */
+#define PKG_APPID_MIN		1000
+#define PKG_APPID_MAX		(PKG_APPID_PER_USER - 1)
+
+/* operations vectors defined in specific files */
+extern const struct file_operations esdfs_main_fops;
+extern const struct file_operations esdfs_dir_fops;
+extern const struct inode_operations esdfs_main_iops;
+extern const struct inode_operations esdfs_dir_iops;
+extern const struct inode_operations esdfs_symlink_iops;
+extern const struct super_operations esdfs_sops;
+extern const struct dentry_operations esdfs_dops;
+extern const struct address_space_operations esdfs_aops, esdfs_dummy_aops;
+extern const struct vm_operations_struct esdfs_vm_ops;
+
+extern void esdfs_msg(struct super_block *, const char *, const char *, ...);
+extern int esdfs_init_inode_cache(void);
+extern void esdfs_destroy_inode_cache(void);
+extern int esdfs_init_dentry_cache(void);
+extern void esdfs_destroy_dentry_cache(void);
+extern int esdfs_new_dentry_private_data(struct dentry *dentry);
+extern void esdfs_free_dentry_private_data(struct dentry *dentry);
+extern struct dentry *esdfs_lookup(struct inode *dir, struct dentry *dentry,
+				   unsigned int flags);
+extern struct inode *esdfs_iget(struct super_block *sb,
+				struct inode *lower_inode,
+				uint32_t id);
+extern int esdfs_interpose(struct dentry *dentry, struct super_block *sb,
+			   struct path *lower_path, uint32_t id);
+extern int esdfs_init_package_list(void);
+extern void esdfs_destroy_package_list(void);
+extern void esdfs_derive_perms(struct dentry *dentry);
+extern void esdfs_set_derived_perms(struct inode *inode);
+extern int esdfs_is_dl_lookup(struct dentry *dentry, struct dentry *parent);
+extern int esdfs_derived_lookup(struct dentry *dentry, struct dentry **parent);
+extern int esdfs_derived_revalidate(struct dentry *dentry,
+				    struct dentry *parent);
+extern int esdfs_check_derived_permission(struct inode *inode, int mask);
+extern int esdfs_derive_mkdir_contents(struct dentry *dentry);
+extern int esdfs_lookup_nocase(struct path *lower_parent_path,
+		const struct qstr *name, struct path *lower_path);
+
+/* file private data */
+struct esdfs_file_info {
+	struct file *lower_file;
+	const struct vm_operations_struct *lower_vm_ops;
+};
+
+struct esdfs_perms {
+	uid_t raw_uid;
+	uid_t raw_gid;
+	uid_t uid;
+	gid_t gid;
+	unsigned short fmask;
+	unsigned short dmask;
+};
+
+/* esdfs inode data in memory */
+struct esdfs_inode_info {
+	struct inode *lower_inode;
+	struct inode vfs_inode;
+	unsigned version;	/* package list version this was derived from */
+	int tree;		/* storage tree location */
+	uint32_t userid;	/* Android User ID (not Linux UID) */
+	uid_t appid;		/* Linux UID for this app/user combo */
+	bool under_obb;
+};
+
+/* esdfs dentry data in memory */
+struct esdfs_dentry_info {
+	spinlock_t lock;	/* protects lower_path and lower_stub_path */
+	struct path lower_path;
+	struct path lower_stub_path;
+	struct dentry *real_parent;
+};
+
+/* esdfs super-block data in memory */
+struct esdfs_sb_info {
+	struct super_block *lower_sb;
+	struct super_block *s_sb;
+	struct user_namespace *base_ns;
+	struct list_head s_list;
+	struct esdfs_perms lower_perms;
+	struct esdfs_perms upper_perms;	   /* root in derived mode */
+	struct dentry *obb_parent;	   /* pinned dentry for obb link parent */
+	struct path dl_path;		   /* path of lower downloads folder */
+	struct qstr dl_name;		   /* name of lower downloads folder */
+	const char *dl_loc;		   /* location of dl folder */
+	struct esdfs_perms lower_dl_perms; /* permissions for lower downloads folder */
+	struct user_namespace *dl_ns;	   /* lower downloads namespace */
+	int ns_fd;
+	unsigned int options;
+};
+
+extern struct esdfs_perms esdfs_perms_table[ESDFS_PERMS_TABLE_SIZE];
+extern unsigned esdfs_package_list_version;
+
+void esdfs_add_super(struct esdfs_sb_info *, struct super_block *);
+void esdfs_truncate_share(struct super_block *, struct inode *, loff_t newsize);
+
+void esdfs_derive_lower_ownership(struct dentry *dentry, const char *name);
+
+static inline bool is_obb(struct qstr *name)
+{
+	struct qstr q_obb = QSTR_LITERAL("obb");
+	return qstr_case_eq(name, &q_obb);
+}
+
+static inline bool is_dl(struct qstr *name)
+{
+	struct qstr q_dl = QSTR_LITERAL("Download");
+
+	return qstr_case_eq(name, &q_dl);
+}
+
+#define ESDFS_INODE_IS_STALE(i) ((i)->version != esdfs_package_list_version)
+#define ESDFS_INODE_CAN_LINK(i) (test_opt(ESDFS_SB((i)->i_sb), \
+					  DERIVE_LEGACY) || \
+				 (test_opt(ESDFS_SB((i)->i_sb), \
+					   DERIVE_UNIFIED) && \
+				  ESDFS_I(i)->userid > 0))
+#define ESDFS_DENTRY_NEEDS_LINK(d) (is_obb(&(d)->d_name))
+#define ESDFS_DENTRY_NEEDS_DL_LINK(d) (is_dl(&(d)->d_name))
+#define ESDFS_DENTRY_IS_LINKED(d) (ESDFS_D(d)->real_parent)
+#define ESDFS_DENTRY_HAS_STUB(d) (ESDFS_D(d)->lower_stub_path.dentry)
+
+/*
+ * inode to private data
+ *
+ * Since we use containers and the struct inode is _inside_ the
+ * esdfs_inode_info structure, ESDFS_I will always (given a non-NULL
+ * inode pointer), return a valid non-NULL pointer.
+ */
+static inline struct esdfs_inode_info *ESDFS_I(const struct inode *inode)
+{
+	return container_of(inode, struct esdfs_inode_info, vfs_inode);
+}
+
+/* dentry to private data */
+#define ESDFS_D(dent) ((struct esdfs_dentry_info *)(dent)->d_fsdata)
+
+/* superblock to private data */
+#define ESDFS_SB(super) ((struct esdfs_sb_info *)(super)->s_fs_info)
+
+/* file to private Data */
+#define ESDFS_F(file) ((struct esdfs_file_info *)((file)->private_data))
+
+/* file to lower file */
+static inline struct file *esdfs_lower_file(const struct file *f)
+{
+	return ESDFS_F(f)->lower_file;
+}
+
+static inline void esdfs_set_lower_file(struct file *f, struct file *val)
+{
+	ESDFS_F(f)->lower_file = val;
+}
+
+/* inode to lower inode. */
+static inline struct inode *esdfs_lower_inode(const struct inode *i)
+{
+	return ESDFS_I(i)->lower_inode;
+}
+
+static inline void esdfs_set_lower_inode(struct inode *i, struct inode *val)
+{
+	ESDFS_I(i)->lower_inode = val;
+}
+
+/* superblock to lower superblock */
+static inline struct super_block *esdfs_lower_super(
+	const struct super_block *sb)
+{
+	return ESDFS_SB(sb)->lower_sb;
+}
+
+static inline void esdfs_set_lower_super(struct super_block *sb,
+					  struct super_block *val)
+{
+	ESDFS_SB(sb)->lower_sb = val;
+}
+
+/* path based (dentry/mnt) macros */
+static inline void pathcpy(struct path *dst, const struct path *src)
+{
+	dst->dentry = src->dentry;
+	dst->mnt = src->mnt;
+}
+/* Returns struct path.  Caller must path_put it. */
+static inline void esdfs_get_lower_path(const struct dentry *dent,
+					 struct path *lower_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(lower_path, &ESDFS_D(dent)->lower_path);
+	path_get(lower_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_get_lower_stub_path(const struct dentry *dent,
+					     struct path *lower_stub_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(lower_stub_path, &ESDFS_D(dent)->lower_stub_path);
+	path_get(lower_stub_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_put_lower_path(const struct dentry *dent,
+					 struct path *lower_path)
+{
+	path_put(lower_path);
+}
+static inline void esdfs_set_lower_path(const struct dentry *dent,
+					 struct path *lower_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(&ESDFS_D(dent)->lower_path, lower_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_set_lower_stub_path(const struct dentry *dent,
+					     struct path *lower_stub_path)
+{
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(&ESDFS_D(dent)->lower_stub_path, lower_stub_path);
+	spin_unlock(&ESDFS_D(dent)->lock);
+}
+static inline void esdfs_put_reset_lower_paths(const struct dentry *dent)
+{
+	struct path lower_path;
+	struct path lower_stub_path = {
+		.mnt = NULL,
+		.dentry = NULL,
+	};
+
+	spin_lock(&ESDFS_D(dent)->lock);
+	pathcpy(&lower_path, &ESDFS_D(dent)->lower_path);
+	ESDFS_D(dent)->lower_path.dentry = NULL;
+	ESDFS_D(dent)->lower_path.mnt = NULL;
+	if (ESDFS_DENTRY_HAS_STUB(dent)) {
+		pathcpy(&lower_stub_path, &ESDFS_D(dent)->lower_stub_path);
+		ESDFS_D(dent)->lower_stub_path.dentry = NULL;
+		ESDFS_D(dent)->lower_stub_path.mnt = NULL;
+	}
+	spin_unlock(&ESDFS_D(dent)->lock);
+
+	path_put(&lower_path);
+	if (lower_stub_path.dentry)
+		path_put(&lower_stub_path);
+}
+static inline void esdfs_get_lower_parent(const struct dentry *dent,
+					  struct dentry *lower_dentry,
+					  struct dentry **lower_parent)
+{
+	*lower_parent = NULL;
+	spin_lock(&ESDFS_D(dent)->lock);
+	if (ESDFS_DENTRY_IS_LINKED(dent)) {
+		*lower_parent = ESDFS_D(dent)->real_parent;
+		dget(*lower_parent);
+	}
+	spin_unlock(&ESDFS_D(dent)->lock);
+	if (!*lower_parent)
+		*lower_parent = dget_parent(lower_dentry);
+}
+static inline void esdfs_put_lower_parent(const struct dentry *dent,
+					  struct dentry **lower_parent)
+{
+	dput(*lower_parent);
+}
+static inline void esdfs_set_lower_parent(const struct dentry *dent,
+					  struct dentry *parent)
+{
+	struct dentry *old_parent = NULL;
+
+	spin_lock(&ESDFS_D(dent)->lock);
+	if (ESDFS_DENTRY_IS_LINKED(dent))
+		old_parent = ESDFS_D(dent)->real_parent;
+	ESDFS_D(dent)->real_parent = parent;
+	dget(parent);	/* pin the lower parent */
+	spin_unlock(&ESDFS_D(dent)->lock);
+	if (old_parent)
+		dput(old_parent);
+}
+static inline void esdfs_release_lower_parent(const struct dentry *dent)
+{
+	struct dentry *real_parent = NULL;
+
+	spin_lock(&ESDFS_D(dent)->lock);
+	if (ESDFS_DENTRY_IS_LINKED(dent)) {
+		real_parent = ESDFS_D(dent)->real_parent;
+		ESDFS_D(dent)->real_parent = NULL;
+	}
+	spin_unlock(&ESDFS_D(dent)->lock);
+	if (real_parent)
+		dput(real_parent);
+}
+
+/* locking helpers */
+static inline struct dentry *lock_parent(struct dentry *dentry)
+{
+	struct dentry *dir = dget_parent(dentry);
+
+	inode_lock_nested(dir->d_inode, I_MUTEX_PARENT);
+	return dir;
+}
+
+static inline void unlock_dir(struct dentry *dir)
+{
+	inode_unlock(dir->d_inode);
+	dput(dir);
+}
+
+static inline void esdfs_set_lower_mode(struct esdfs_sb_info *sbi,
+		struct esdfs_inode_info *inode_i, umode_t *mode)
+{
+	struct esdfs_perms *perms = &sbi->lower_perms;
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD) &&
+			inode_i->tree == ESDFS_TREE_DOWNLOAD)
+		perms = &sbi->lower_dl_perms;
+
+	if (S_ISDIR(*mode))
+		*mode = (*mode & S_IFMT) | perms->dmask;
+	else
+		*mode = (*mode & S_IFMT) | perms->fmask;
+}
+
+static inline void esdfs_set_perms(struct inode *inode)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(inode->i_sb);
+
+	if (ESDFS_DERIVE_PERMS(sbi)) {
+		esdfs_set_derived_perms(inode);
+		return;
+	}
+	i_uid_write(inode, sbi->upper_perms.uid);
+	i_gid_write(inode, sbi->upper_perms.gid);
+	if (S_ISDIR(inode->i_mode))
+		inode->i_mode = (inode->i_mode & S_IFMT) |
+				sbi->upper_perms.dmask;
+	else
+		inode->i_mode = (inode->i_mode & S_IFMT) |
+				sbi->upper_perms.fmask;
+}
+
+static inline void esdfs_revalidate_perms(struct dentry *dentry)
+{
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(dentry->d_sb)) &&
+	    dentry->d_inode &&
+	    ESDFS_INODE_IS_STALE(ESDFS_I(dentry->d_inode))) {
+		esdfs_derive_perms(dentry);
+		esdfs_set_perms(dentry->d_inode);
+	}
+}
+
+static inline uid_t derive_uid(struct esdfs_inode_info *inode_i, uid_t uid)
+{
+	return inode_i->userid * PKG_APPID_PER_USER +
+	       (uid % PKG_APPID_PER_USER);
+}
+
+static inline bool uid_is_app(uid_t uid)
+{
+	uid_t appid = uid % PKG_APPID_PER_USER;
+
+	return appid >= AID_APP_START && appid <= AID_APP_END;
+}
+
+static inline gid_t multiuser_get_ext_cache_gid(uid_t uid)
+{
+	return uid - AID_APP_START + AID_EXT_CACHE_GID_START;
+}
+
+static inline gid_t multiuser_get_ext_gid(uid_t uid)
+{
+	return uid - AID_APP_START + AID_EXT_GID_START;
+}
+
+/* file attribute helpers */
+static inline void esdfs_copy_lower_attr(struct inode *dest,
+					 const struct inode *src)
+{
+	dest->i_mode = src->i_mode & S_IFMT;
+	dest->i_rdev = src->i_rdev;
+	dest->i_atime = src->i_atime;
+	dest->i_mtime = src->i_mtime;
+	dest->i_ctime = src->i_ctime;
+	dest->i_blkbits = src->i_blkbits;
+	dest->i_flags = src->i_flags;
+	set_nlink(dest, src->i_nlink);
+}
+
+static inline void esdfs_copy_attr(struct inode *dest, const struct inode *src)
+{
+	esdfs_copy_lower_attr(dest, src);
+	esdfs_set_perms(dest);
+}
+
+static inline uid_t esdfs_from_local_uid(struct esdfs_sb_info *sbi, uid_t uid)
+{
+	return from_kuid(sbi->base_ns, make_kuid(current_user_ns(), uid));
+}
+
+static inline gid_t esdfs_from_local_gid(struct esdfs_sb_info *sbi, gid_t gid)
+{
+	return from_kgid(sbi->base_ns, make_kgid(current_user_ns(), gid));
+}
+
+static inline uid_t esdfs_from_kuid(struct esdfs_sb_info *sbi, kuid_t uid)
+{
+	return from_kuid(sbi->base_ns, uid);
+}
+
+static inline gid_t esdfs_from_kgid(struct esdfs_sb_info *sbi, kgid_t gid)
+{
+	return from_kgid(sbi->base_ns, gid);
+}
+
+static inline kuid_t esdfs_make_kuid(struct esdfs_sb_info *sbi, uid_t uid)
+{
+	return make_kuid(sbi->base_ns, uid);
+}
+
+static inline kgid_t esdfs_make_kgid(struct esdfs_sb_info *sbi, gid_t gid)
+{
+	return make_kgid(sbi->base_ns, gid);
+}
+
+/* Helper functions to read and write to inode uid/gids without
+ * having to worry about translating into/out of esdfs's preferred
+ * base user namespace.
+ */
+static inline uid_t esdfs_i_uid_read(const struct inode *inode)
+{
+	return esdfs_from_kuid(ESDFS_SB(inode->i_sb), inode->i_uid);
+}
+
+static inline gid_t esdfs_i_gid_read(const struct inode *inode)
+{
+	return esdfs_from_kgid(ESDFS_SB(inode->i_sb), inode->i_gid);
+}
+
+static inline void esdfs_i_uid_write(struct inode *inode, uid_t uid)
+{
+	inode->i_uid = esdfs_make_kuid(ESDFS_SB(inode->i_sb), uid);
+}
+
+static inline void esdfs_i_gid_write(struct inode *inode, gid_t gid)
+{
+	inode->i_gid = esdfs_make_kgid(ESDFS_SB(inode->i_sb), gid);
+}
+
+/*
+ * Based on nfs4_save_creds() and nfs4_reset_creds() in nfsd/nfs4recover.c.
+ * Returns NULL if prepare_creds() could not allocate heap, otherwise
+ */
+static inline const struct cred *esdfs_override_creds(
+		struct esdfs_sb_info *sbi,
+		struct esdfs_inode_info *info, int *mask)
+{
+	struct cred *creds = prepare_creds();
+	uid_t uid;
+	gid_t gid = sbi->lower_perms.gid;
+
+	if (!creds)
+		return NULL;
+
+	/* clear the umask so that the lower mode works for create cases */
+	if (mask) {
+		*mask = 0;
+		*mask = xchg(&current->fs->umask, *mask & S_IRWXUGO);
+	}
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD) &&
+			info->tree == ESDFS_TREE_DOWNLOAD) {
+		creds->fsuid = make_kuid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_uid);
+		creds->fsgid = make_kgid(sbi->dl_ns,
+					 sbi->lower_dl_perms.raw_gid);
+	} else {
+		if (test_opt(sbi, GID_DERIVATION)) {
+			if (info->under_obb)
+				uid = AID_MEDIA_OBB;
+			else
+				uid = derive_uid(info, sbi->lower_perms.uid);
+		} else {
+			uid = sbi->lower_perms.uid;
+		}
+		creds->fsuid = esdfs_make_kuid(sbi, uid);
+		creds->fsgid = esdfs_make_kgid(sbi, gid);
+	}
+
+	/* this installs the new creds into current, which we must destroy */
+	return override_creds(creds);
+}
+
+static inline void esdfs_revert_creds(const struct cred *creds, int *mask)
+{
+	const struct cred *current_creds = current->cred;
+
+	/* restore the old umask */
+	if (mask)
+		*mask = xchg(&current->fs->umask, *mask & S_IRWXUGO);
+
+	/* restore the old creds into current */
+	revert_creds(creds);
+	put_cred(current_creds);	/* destroy the old temporary creds */
+}
+
+#endif	/* not _ESDFS_H_ */
diff -ruN a/fs/esdfs/file.c b/fs/esdfs/file.c
--- a/fs/esdfs/file.c	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/file.c	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,471 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014, 2016 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+static ssize_t esdfs_read(struct file *file, char __user *buf,
+			   size_t count, loff_t *ppos)
+{
+	int err;
+	struct file *lower_file;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	err = vfs_read(lower_file, buf, count, ppos);
+	/* update our inode atime upon a successful lower read */
+	if (err >= 0)
+		fsstack_copy_attr_atime(dentry->d_inode,
+					file_inode(lower_file));
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static ssize_t esdfs_write(struct file *file, const char __user *buf,
+			    size_t count, loff_t *ppos)
+{
+	int err;
+
+	struct file *lower_file;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	err = vfs_write(lower_file, buf, count, ppos);
+	/* update our inode times+sizes upon a successful lower write */
+	if (err >= 0) {
+		fsstack_copy_inode_size(dentry->d_inode,
+					file_inode(lower_file));
+		esdfs_copy_attr(dentry->d_inode,
+				file_inode(lower_file));
+	}
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_readdir(struct file *file, struct dir_context *ctx)
+{
+	int err;
+	struct file *lower_file = NULL;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	err = iterate_dir(lower_file, ctx);
+	file->f_pos = lower_file->f_pos;
+	if (err >= 0)		/* copy the atime */
+		fsstack_copy_attr_atime(dentry->d_inode,
+					file_inode(lower_file));
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static long esdfs_unlocked_ioctl(struct file *file, unsigned int cmd,
+				  unsigned long arg)
+{
+	long err = -ENOTTY;
+	struct file *lower_file;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (cmd == ESDFS_IOC_DIS_ACCESS) {
+		if (!capable(CAP_SYS_ADMIN)) {
+			err = -EPERM;
+			goto out;
+		}
+		set_opt(sbi, ACCESS_DISABLE);
+		err = 0;
+		goto out;
+	}
+
+	lower_file = esdfs_lower_file(file);
+
+	/* XXX: use vfs_ioctl if/when VFS exports it */
+	if (!lower_file || !lower_file->f_op)
+		goto out;
+	if (lower_file->f_op->unlocked_ioctl)
+		err = lower_file->f_op->unlocked_ioctl(lower_file, cmd, arg);
+
+	/* some ioctls can change inode attributes (EXT2_IOC_SETFLAGS) */
+	if (!err)
+		esdfs_copy_attr(file->f_path.dentry->d_inode,
+				file_inode(lower_file));
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+#ifdef CONFIG_COMPAT
+static long esdfs_compat_ioctl(struct file *file, unsigned int cmd,
+				unsigned long arg)
+{
+	long err = -ENOTTY;
+	struct file *lower_file;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+
+	/* XXX: use vfs_ioctl if/when VFS exports it */
+	if (!lower_file || !lower_file->f_op)
+		goto out;
+	if (lower_file->f_op->compat_ioctl)
+		err = lower_file->f_op->compat_ioctl(lower_file, cmd, arg);
+
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+#endif
+
+static int esdfs_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	int err = 0;
+	bool willwrite;
+	struct file *lower_file;
+	const struct vm_operations_struct *saved_vm_ops = NULL;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	/* this might be deferred to mmap's writepage */
+	willwrite = ((vma->vm_flags | VM_SHARED | VM_WRITE) == vma->vm_flags);
+
+	/*
+	 * File systems which do not implement ->writepage may use
+	 * generic_file_readonly_mmap as their ->mmap op.  If you call
+	 * generic_file_readonly_mmap with VM_WRITE, you'd get an -EINVAL.
+	 * But we cannot call the lower ->mmap op, so we can't tell that
+	 * writeable mappings won't work.  Therefore, our only choice is to
+	 * check if the lower file system supports the ->writepage, and if
+	 * not, return EINVAL (the same error that
+	 * generic_file_readonly_mmap returns in that case).
+	 */
+	lower_file = esdfs_lower_file(file);
+	if (willwrite && !lower_file->f_mapping->a_ops->writepage) {
+		err = -EINVAL;
+		esdfs_msg(file->f_mapping->host->i_sb, KERN_INFO,
+			"lower file system does not support writeable mmap\n");
+		goto out;
+	}
+
+	/*
+	 * find and save lower vm_ops.
+	 *
+	 * XXX: the VFS should have a cleaner way of finding the lower vm_ops
+	 */
+	if (!ESDFS_F(file)->lower_vm_ops) {
+		err = lower_file->f_op->mmap(lower_file, vma);
+		if (err) {
+			esdfs_msg(file->f_mapping->host->i_sb, KERN_ERR,
+				"lower mmap failed %d\n", err);
+			goto out;
+		}
+		saved_vm_ops = vma->vm_ops; /* save: came from lower ->mmap */
+	}
+
+	/*
+	 * Next 3 lines are all I need from generic_file_mmap.  I definitely
+	 * don't want its test for ->readpage which returns -ENOEXEC.
+	 */
+	file_accessed(file);
+	vma->vm_ops = &esdfs_vm_ops;
+
+	file->f_mapping->a_ops = &esdfs_aops; /* set our aops */
+	if (!ESDFS_F(file)->lower_vm_ops) /* save for our ->fault */
+		ESDFS_F(file)->lower_vm_ops = saved_vm_ops;
+
+	vma->vm_private_data = file;
+	get_file(lower_file);
+	vma->vm_file = lower_file;
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_open(struct inode *inode, struct file *file)
+{
+	int err = 0;
+	struct file *lower_file = NULL;
+	struct path lower_path;
+	struct esdfs_sb_info *sbi = ESDFS_SB(inode->i_sb);
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(inode->i_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(sbi, ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	/* don't open unhashed/deleted files */
+	if (d_unhashed(file->f_path.dentry)) {
+		err = -ENOENT;
+		goto out_err;
+	}
+
+	file->private_data =
+		kzalloc(sizeof(struct esdfs_file_info), GFP_KERNEL);
+	if (!ESDFS_F(file)) {
+		err = -ENOMEM;
+		goto out_err;
+	}
+
+	/* open lower object and link esdfs's file struct to lower's */
+	esdfs_get_lower_path(file->f_path.dentry, &lower_path);
+	lower_file = dentry_open(&lower_path, file->f_flags, current_cred());
+	path_put(&lower_path);
+	if (IS_ERR(lower_file)) {
+		err = PTR_ERR(lower_file);
+		lower_file = esdfs_lower_file(file);
+		if (lower_file) {
+			esdfs_set_lower_file(file, NULL);
+			fput(lower_file); /* fput calls dput for lower_dentry */
+		}
+	} else {
+		esdfs_set_lower_file(file, lower_file);
+	}
+
+	if (err)
+		kfree(ESDFS_F(file));
+	else
+		esdfs_copy_attr(inode, esdfs_lower_inode(inode));
+out_err:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_flush(struct file *file, fl_owner_t id)
+{
+	int err = 0;
+	struct file *lower_file = NULL;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	if (lower_file && lower_file->f_op && lower_file->f_op->flush) {
+		filemap_write_and_wait(file->f_mapping);
+		err = lower_file->f_op->flush(lower_file, id);
+	}
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/* release all lower object references & free the file info structure */
+static int esdfs_file_release(struct inode *inode, struct file *file)
+{
+	struct file *lower_file;
+
+	lower_file = esdfs_lower_file(file);
+	if (lower_file) {
+		esdfs_set_lower_file(file, NULL);
+		fput(lower_file);
+	}
+
+	kfree(ESDFS_F(file));
+	return 0;
+}
+
+static int esdfs_fsync(struct file *file, loff_t start, loff_t end,
+			int datasync)
+{
+	int err;
+	struct file *lower_file;
+	struct path lower_path;
+	struct dentry *dentry = file->f_path.dentry;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dentry->d_sb),
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	err = __generic_file_fsync(file, start, end, datasync);
+	if (err)
+		goto out;
+	lower_file = esdfs_lower_file(file);
+	esdfs_get_lower_path(dentry, &lower_path);
+	err = vfs_fsync_range(lower_file, start, end, datasync);
+	esdfs_put_lower_path(dentry, &lower_path);
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_fasync(int fd, struct file *file, int flag)
+{
+	int err = 0;
+	struct file *lower_file = NULL;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+					ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	lower_file = esdfs_lower_file(file);
+	if (lower_file->f_op && lower_file->f_op->fasync)
+		err = lower_file->f_op->fasync(fd, lower_file, flag);
+
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/*
+ * Wrapfs cannot use generic_file_llseek as ->llseek, because it would
+ * only set the offset of the upper file.  So we have to implement our
+ * own method to set both the upper and lower file offsets
+ * consistently.
+ */
+static loff_t esdfs_file_llseek(struct file *file, loff_t offset, int whence)
+{
+	int err;
+	struct file *lower_file;
+	struct esdfs_sb_info *sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	const struct cred *creds = esdfs_override_creds(sbi,
+				ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	err = generic_file_llseek(file, offset, whence);
+	if (err < 0)
+		goto out;
+
+	lower_file = esdfs_lower_file(file);
+	err = generic_file_llseek(lower_file, offset, whence);
+
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/*
+ * Wrapfs read_iter, redirect modified iocb to lower read_iter
+ */
+ssize_t
+esdfs_read_iter(struct kiocb *iocb, struct iov_iter *iter)
+{
+	int err;
+	struct file *file = iocb->ki_filp, *lower_file;
+
+	lower_file = esdfs_lower_file(file);
+	if (!lower_file->f_op->read_iter) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	get_file(lower_file); /* prevent lower_file from being released */
+	iocb->ki_filp = lower_file;
+	err = lower_file->f_op->read_iter(iocb, iter);
+	iocb->ki_filp = file;
+	fput(lower_file);
+	/* update upper inode atime as needed */
+	if (err >= 0 || err == -EIOCBQUEUED)
+		fsstack_copy_attr_atime(file->f_path.dentry->d_inode,
+					file_inode(lower_file));
+out:
+	return err;
+}
+
+/*
+ * Wrapfs write_iter, redirect modified iocb to lower write_iter
+ */
+ssize_t
+esdfs_write_iter(struct kiocb *iocb, struct iov_iter *iter)
+{
+	int err;
+	struct file *file = iocb->ki_filp, *lower_file;
+
+	lower_file = esdfs_lower_file(file);
+	if (!lower_file->f_op->write_iter) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	get_file(lower_file); /* prevent lower_file from being released */
+	iocb->ki_filp = lower_file;
+	err = lower_file->f_op->write_iter(iocb, iter);
+	iocb->ki_filp = file;
+	fput(lower_file);
+	/* update upper inode times/sizes as needed */
+	if (err >= 0 || err == -EIOCBQUEUED) {
+		fsstack_copy_inode_size(file->f_path.dentry->d_inode,
+					file_inode(lower_file));
+		fsstack_copy_attr_times(file->f_path.dentry->d_inode,
+					file_inode(lower_file));
+	}
+out:
+	return err;
+}
+
+const struct file_operations esdfs_main_fops = {
+	.llseek		= generic_file_llseek,
+	.read		= esdfs_read,
+	.write		= esdfs_write,
+	.unlocked_ioctl	= esdfs_unlocked_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl	= esdfs_compat_ioctl,
+#endif
+	.mmap		= esdfs_mmap,
+	.open		= esdfs_open,
+	.flush		= esdfs_flush,
+	.release	= esdfs_file_release,
+	.fsync		= esdfs_fsync,
+	.fasync		= esdfs_fasync,
+	.read_iter	= esdfs_read_iter,
+	.write_iter	= esdfs_write_iter,
+	.splice_read    = generic_file_splice_read,
+	.splice_write   = iter_file_splice_write,
+};
+
+/* trimmed directory options */
+const struct file_operations esdfs_dir_fops = {
+	.llseek		= esdfs_file_llseek,
+	.read		= generic_read_dir,
+	.iterate	= esdfs_readdir,
+	.unlocked_ioctl	= esdfs_unlocked_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl	= esdfs_compat_ioctl,
+#endif
+	.open		= esdfs_open,
+	.release	= esdfs_file_release,
+	.flush		= esdfs_flush,
+	.fsync		= esdfs_fsync,
+	.fasync		= esdfs_fasync,
+};
diff -ruN a/fs/esdfs/inode.c b/fs/esdfs/inode.c
--- a/fs/esdfs/inode.c	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/inode.c	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,550 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+#include <linux/fsnotify.h>
+
+static int esdfs_create(struct user_namespace *mnt_userns, struct inode *dir,
+			struct dentry *dentry, umode_t mode, bool want_excl)
+{
+	int err;
+	struct dentry *lower_dentry;
+	struct dentry *lower_parent_dentry = NULL;
+	struct path lower_path;
+	struct inode *lower_inode;
+	int mask;
+	const struct cred *creds;
+
+	/*
+	 * Need to recheck derived permissions unified mode to prevent certain
+	 * applications from creating files at the root.
+	 */
+	if (test_opt(ESDFS_SB(dir->i_sb), DERIVE_UNIFIED) &&
+	    esdfs_check_derived_permission(dir, ESDFS_MAY_CREATE) != 0)
+		return -EACCES;
+
+	if (test_opt(ESDFS_SB(dir->i_sb), ACCESS_DISABLE))
+		return -ENOENT;
+
+	creds = esdfs_override_creds(ESDFS_SB(dir->i_sb), ESDFS_I(dir), &mask);
+	if (!creds)
+		return -ENOMEM;
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	lower_parent_dentry = lock_parent(lower_dentry);
+
+	esdfs_set_lower_mode(ESDFS_SB(dir->i_sb), ESDFS_I(dir), &mode);
+
+	lower_inode = esdfs_lower_inode(dir);
+	err = vfs_create(mnt_userns, lower_inode, lower_dentry, mode, 
+			 want_excl);
+	if (err)
+		goto out;
+
+	err = esdfs_interpose(dentry, dir->i_sb, &lower_path,
+				ESDFS_I(dir)->userid);
+	if (err)
+		goto out;
+	fsstack_copy_attr_times(dir, esdfs_lower_inode(dir));
+	fsstack_copy_inode_size(dir, lower_parent_dentry->d_inode);
+	esdfs_derive_lower_ownership(dentry, dentry->d_name.name);
+
+out:
+	unlock_dir(lower_parent_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
+
+static int esdfs_unlink(struct inode *dir, struct dentry *dentry)
+{
+	int err;
+	struct dentry *lower_dentry;
+	struct inode *lower_dir_inode;
+	struct dentry *lower_dir_dentry;
+	struct path lower_path;
+	const struct cred *creds;
+
+	creds = esdfs_override_creds(ESDFS_SB(dir->i_sb), ESDFS_I(dir), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(dir->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	dget(lower_dentry);
+
+	lower_dir_dentry = lock_parent(lower_dentry);
+
+	/* d_parent might be changed in vfs_rename */
+	if (lower_dir_dentry != lower_dentry->d_parent) {
+		err = -ENOENT;
+		goto out;
+	}
+
+	/* lower_dir_inode might be changed as well
+	 * get the new inode with new lower dir dentry
+	 */
+	lower_dir_inode = lower_dir_dentry->d_inode;
+
+	err = vfs_unlink(&init_user_ns, lower_dir_inode, lower_dentry, NULL);
+
+	/*
+	 * Note: unlinking on top of NFS can cause silly-renamed files.
+	 * Trying to delete such files results in EBUSY from NFS
+	 * below.  Silly-renamed files will get deleted by NFS later on, so
+	 * we just need to detect them here and treat such EBUSY errors as
+	 * if the upper file was successfully deleted.
+	 */
+	if (err == -EBUSY && lower_dentry->d_flags & DCACHE_NFSFS_RENAMED)
+		err = 0;
+	if (err)
+		goto out;
+	fsstack_copy_attr_times(dir, lower_dir_inode);
+	fsstack_copy_inode_size(dir, lower_dir_inode);
+	set_nlink(dentry->d_inode,
+		  esdfs_lower_inode(dentry->d_inode)->i_nlink);
+	dentry->d_inode->i_ctime = dir->i_ctime;
+	d_drop(dentry); /* this is needed, else LTP fails (VFS won't do it) */
+out:
+	unlock_dir(lower_dir_dentry);
+	dput(lower_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_mkdir(struct user_namespace *mnt_userns, struct inode *dir,
+		       struct dentry *dentry, umode_t mode)
+{
+	int err;
+	struct dentry *lower_dentry;
+	struct dentry *lower_parent_dentry = NULL;
+	struct path lower_path;
+	int mask;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dir->i_sb),
+					ESDFS_I(dir), &mask);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(dir->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	lower_parent_dentry = lock_parent(lower_dentry);
+
+	mode |= S_IFDIR;
+	esdfs_set_lower_mode(ESDFS_SB(dir->i_sb), ESDFS_I(dir), &mode);
+	err = vfs_mkdir(mnt_userns, lower_parent_dentry->d_inode, lower_dentry,
+			mode);
+	if (err)
+		goto unlock_lower_parent;
+
+	err = esdfs_interpose(dentry, dir->i_sb, &lower_path,
+				ESDFS_I(dir)->userid);
+	if (err)
+		goto unlock_lower_parent;
+
+	fsstack_copy_attr_times(dir, esdfs_lower_inode(dir));
+	fsstack_copy_inode_size(dir, lower_parent_dentry->d_inode);
+	/* update number of links on parent directory */
+	set_nlink(dir, esdfs_lower_inode(dir)->i_nlink);
+	esdfs_derive_lower_ownership(dentry, dentry->d_name.name);
+
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(dir->i_sb))) {
+		unlock_dir(lower_parent_dentry);
+		err = esdfs_derive_mkdir_contents(dentry);
+		goto out;
+	}
+
+unlock_lower_parent:
+	unlock_dir(lower_parent_dentry);
+out:
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
+
+static int esdfs_rmdir(struct inode *dir, struct dentry *dentry)
+{
+	struct dentry *lower_dentry;
+	struct dentry *lower_dir_dentry;
+	int err;
+	struct path lower_path;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(dir->i_sb),
+					ESDFS_I(dir), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	/* Never remove a pseudo link target.  Only the source. */
+	if (ESDFS_DENTRY_HAS_STUB(dentry))
+		esdfs_get_lower_stub_path(dentry, &lower_path);
+	else
+		esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+
+	lower_dir_dentry = lock_parent(lower_dentry);
+
+	/* d_parent might be changed in vfs_rename */
+	if (lower_dir_dentry != lower_dentry->d_parent) {
+		err = -ENOENT;
+		goto out;
+	}
+
+	err = vfs_rmdir(&init_user_ns, lower_dir_dentry->d_inode, lower_dentry);
+	if (err)
+		goto out;
+
+	d_drop(dentry);	/* drop our dentry on success (why not VFS's job?) */
+	if (dentry->d_inode)
+		clear_nlink(dentry->d_inode);
+	fsstack_copy_attr_times(dir, lower_dir_dentry->d_inode);
+	fsstack_copy_inode_size(dir, lower_dir_dentry->d_inode);
+	set_nlink(dir, lower_dir_dentry->d_inode->i_nlink);
+
+out:
+	unlock_dir(lower_dir_dentry);
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+/*
+ * The locking rules in esdfs_rename are complex.  We could use a simpler
+ * superblock-level name-space lock for renames and copy-ups.
+ */
+static int esdfs_rename(struct user_namespace *mnt_userns, 
+			struct inode *old_dir, struct dentry *old_dentry,
+			struct inode *new_dir, struct dentry *new_dentry,
+			unsigned int flags)
+{
+	int err = 0;
+	struct esdfs_sb_info *sbi = ESDFS_SB(old_dir->i_sb);
+	struct dentry *lower_old_dentry = NULL;
+	struct dentry *lower_new_dentry = NULL;
+	struct dentry *lower_old_dir_dentry = NULL;
+	struct dentry *lower_new_dir_dentry = NULL;
+	struct dentry *trap = NULL;
+	struct path lower_old_path, lower_new_path;
+	int mask;
+	const struct cred *creds;
+	struct renamedata rd;
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD)) {
+		if ((ESDFS_I(old_dir)->tree == ESDFS_TREE_DOWNLOAD
+			|| ESDFS_I(new_dir)->tree == ESDFS_TREE_DOWNLOAD)
+			&& ESDFS_I(old_dir)->tree != ESDFS_I(new_dir)->tree)
+			return -EXDEV;
+	}
+
+	if (test_opt(sbi, GID_DERIVATION)) {
+		if (ESDFS_I(old_dir)->userid != ESDFS_I(new_dir)->userid
+			|| ((ESDFS_I(old_dir)->under_obb
+			|| ESDFS_I(new_dir)->under_obb)
+			&& ESDFS_I(old_dir)->under_obb
+				!= ESDFS_I(new_dir)->under_obb))
+			return -EXDEV;
+	}
+	creds = esdfs_override_creds(sbi, ESDFS_I(new_dir), &mask);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(old_dir->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	/* Never rename to or from a pseudo hard link target. */
+	if (ESDFS_DENTRY_HAS_STUB(old_dentry))
+		esdfs_get_lower_stub_path(old_dentry, &lower_old_path);
+	else
+		esdfs_get_lower_path(old_dentry, &lower_old_path);
+	if (ESDFS_DENTRY_HAS_STUB(new_dentry))
+		esdfs_get_lower_stub_path(new_dentry, &lower_new_path);
+	else
+		esdfs_get_lower_path(new_dentry, &lower_new_path);
+	lower_old_dentry = lower_old_path.dentry;
+	lower_new_dentry = lower_new_path.dentry;
+	esdfs_get_lower_parent(old_dentry, lower_old_dentry,
+			       &lower_old_dir_dentry);
+	esdfs_get_lower_parent(new_dentry, lower_new_dentry,
+			       &lower_new_dir_dentry);
+
+	trap = lock_rename(lower_old_dir_dentry, lower_new_dir_dentry);
+	/* source should not be ancestor of target */
+	if (trap == lower_old_dentry) {
+		err = -EINVAL;
+		goto out;
+	}
+	/* target should not be ancestor of source */
+	if (trap == lower_new_dentry) {
+		err = -ENOTEMPTY;
+		goto out;
+	}
+
+	rd.old_mnt_userns = mnt_userns;
+	rd.old_dir = lower_old_dir_dentry->d_inode;
+	rd.old_dentry = lower_old_dentry;
+	rd.new_mnt_userns = mnt_userns;
+	rd.new_dir = lower_new_dir_dentry->d_inode;
+	rd.new_dentry = lower_new_dentry;
+	rd.flags = flags;
+ 
+	err = vfs_rename(&rd);
+	if (err)
+		goto out;
+
+	esdfs_copy_attr(new_dir, lower_new_dir_dentry->d_inode);
+	fsstack_copy_inode_size(new_dir, lower_new_dir_dentry->d_inode);
+	if (new_dir != old_dir) {
+		esdfs_copy_attr(old_dir,
+				      lower_old_dir_dentry->d_inode);
+		fsstack_copy_inode_size(old_dir,
+					lower_old_dir_dentry->d_inode);
+	}
+
+	/* Drop any old links */
+	if (ESDFS_DENTRY_HAS_STUB(old_dentry))
+		d_drop(old_dentry);
+	if (ESDFS_DENTRY_HAS_STUB(new_dentry))
+		d_drop(new_dentry);
+	esdfs_derive_lower_ownership(old_dentry, new_dentry->d_name.name);
+out:
+	unlock_rename(lower_old_dir_dentry, lower_new_dir_dentry);
+	esdfs_put_lower_parent(old_dentry, &lower_old_dir_dentry);
+	esdfs_put_lower_parent(new_dentry, &lower_new_dir_dentry);
+	esdfs_put_lower_path(old_dentry, &lower_old_path);
+	esdfs_put_lower_path(new_dentry, &lower_new_path);
+	esdfs_revert_creds(creds, &mask);
+	return err;
+}
+
+static int esdfs_permission(struct user_namespace *mnt_userns,
+			    struct inode *inode, int mask)
+{
+	struct inode *lower_inode;
+	int err;
+
+	/* First, check the upper permissions */
+	err = generic_permission(mnt_userns, inode, mask);
+
+	/* Basic checking of the lower inode (can't override creds here) */
+	lower_inode = esdfs_lower_inode(inode);
+	if (S_ISSOCK(lower_inode->i_mode) ||
+	    S_ISLNK(lower_inode->i_mode) ||
+	    S_ISBLK(lower_inode->i_mode) ||
+	    S_ISCHR(lower_inode->i_mode) ||
+	    S_ISFIFO(lower_inode->i_mode))
+		err = -EACCES;
+
+	/* Finally, check the derived permissions */
+	if (!err && ESDFS_DERIVE_PERMS(ESDFS_SB(inode->i_sb)))
+		err = esdfs_check_derived_permission(inode, mask);
+
+	return err;
+}
+
+static int esdfs_setattr(struct user_namespace *mnt_userns,
+			 struct dentry *dentry, struct iattr *ia)
+{
+	int err;
+	loff_t oldsize;
+	loff_t newsize;
+	struct dentry *lower_dentry;
+	struct inode *inode;
+	struct inode *lower_inode;
+	struct path lower_path;
+	struct iattr lower_ia;
+	const struct cred *creds;
+
+	/* We don't allow chmod or chown, so skip those */
+	ia->ia_valid &= ~(ATTR_UID | ATTR_GID | ATTR_MODE);
+	if (!ia->ia_valid)
+		return 0;
+	/* Allow touch updating timestamps. A previous permission check ensures
+	 * we have write access. Changes to mode, owner, and group are ignored
+	 */
+	ia->ia_valid |= ATTR_FORCE;
+
+	inode = dentry->d_inode;
+
+	if (test_opt(ESDFS_SB(inode->i_sb), ACCESS_DISABLE))
+		return -ENOENT;
+
+	/*
+	 * Check if user has permission to change inode.  We don't check if
+	 * this user can change the lower inode: that should happen when
+	 * calling notify_change on the lower inode.
+	 */
+	err = setattr_prepare(mnt_userns, dentry, ia);
+	if (err)
+		return err;
+
+	creds = esdfs_override_creds(ESDFS_SB(dentry->d_inode->i_sb),
+				ESDFS_I(inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	lower_dentry = lower_path.dentry;
+	lower_inode = esdfs_lower_inode(inode);
+
+	/* prepare our own lower struct iattr (with the lower file) */
+	memcpy(&lower_ia, ia, sizeof(lower_ia));
+	if (ia->ia_valid & ATTR_FILE)
+		lower_ia.ia_file = esdfs_lower_file(ia->ia_file);
+
+	/*
+	 * If shrinking, first truncate upper level to cancel writing dirty
+	 * pages beyond the new eof; and also if its' maxbytes is more
+	 * limiting (fail with -EFBIG before making any change to the lower
+	 * level).  There is no need to vmtruncate the upper level
+	 * afterwards in the other cases: we fsstack_copy_inode_size from
+	 * the lower level.
+	 */
+	if (ia->ia_valid & ATTR_SIZE) {
+		err = inode_newsize_ok(inode, ia->ia_size);
+		if (err)
+			goto out;
+		/*
+		 * i_size_write needs locking around it
+		 * otherwise i_size_read() may spin forever
+		 * (see include/linux/fs.h).
+		 * similar to function fsstack_copy_inode_size
+		 */
+		oldsize = i_size_read(inode);
+		newsize = ia->ia_size;
+
+#if BITS_PER_LONG == 32 && defined(CONFIG_SMP)
+		spin_lock(&inode->i_lock);
+#endif
+		i_size_write(inode, newsize);
+#if BITS_PER_LONG == 32 && defined(CONFIG_SMP)
+		spin_unlock(&inode->i_lock);
+#endif
+		if (newsize > oldsize)
+			pagecache_isize_extended(inode, oldsize, newsize);
+		truncate_pagecache(inode, newsize);
+		esdfs_truncate_share(inode->i_sb, lower_dentry->d_inode,
+					ia->ia_size);
+	}
+
+	/*
+	 * mode change is for clearing setuid/setgid bits. Allow lower fs
+	 * to interpret this in its own way.
+	 */
+	if (lower_ia.ia_valid & (ATTR_KILL_SUID | ATTR_KILL_SGID))
+		lower_ia.ia_valid &= ~ATTR_MODE;
+
+	/* notify the (possibly copied-up) lower inode */
+	/*
+	 * Note: we use lower_dentry->d_inode, because lower_inode may be
+	 * unlinked (no inode->i_sb and i_ino==0.  This happens if someone
+	 * tries to open(), unlink(), then ftruncate() a file.
+	 */
+	inode_lock(lower_dentry->d_inode);
+	err = notify_change(mnt_userns, lower_dentry,
+			    &lower_ia, /* note: lower_ia */
+			    NULL);
+	inode_unlock(lower_dentry->d_inode);
+	if (err)
+		goto out;
+
+	/* get attributes from the lower inode */
+	esdfs_copy_attr(inode, lower_inode);
+	/*
+	 * Not running fsstack_copy_inode_size(inode, lower_inode), because
+	 * VFS should update our inode size, and notify_change on
+	 * lower_inode should update its size.
+	 */
+
+out:
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static int esdfs_getattr(struct user_namespace *mnt_userns, 
+			 const struct path *path, struct kstat *stat,
+			 u32 request_mask, unsigned int flags)
+{
+	int err;
+	struct dentry *dentry = path->dentry;
+	struct path lower_path;
+	struct kstat lower_stat;
+	struct inode *lower_inode;
+	struct inode *inode = dentry->d_inode;
+	const struct cred *creds =
+			esdfs_override_creds(ESDFS_SB(inode->i_sb),
+						ESDFS_I(inode), NULL);
+	if (!creds)
+		return -ENOMEM;
+
+	if (test_opt(ESDFS_SB(inode->i_sb), ACCESS_DISABLE)) {
+		esdfs_revert_creds(creds, NULL);
+		return -ENOENT;
+	}
+
+	esdfs_get_lower_path(dentry, &lower_path);
+
+	/* We need the lower getattr to calculate stat->blocks for us. */
+	err = vfs_getattr(&lower_path, &lower_stat, request_mask, flags);
+	if (err)
+		goto out;
+
+	lower_inode = esdfs_lower_inode(inode);
+	esdfs_copy_attr(inode, lower_inode);
+	fsstack_copy_inode_size(inode, lower_inode);
+	generic_fillattr(mnt_userns, inode, stat);
+
+	stat->blocks = lower_stat.blocks;
+
+out:
+	esdfs_put_lower_path(dentry, &lower_path);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+const struct inode_operations esdfs_symlink_iops = {
+	.permission     = esdfs_permission,
+	.setattr	= esdfs_setattr,
+	.getattr	= esdfs_getattr,
+};
+
+const struct inode_operations esdfs_dir_iops = {
+	.create		= esdfs_create,
+	.lookup		= esdfs_lookup,
+	.unlink		= esdfs_unlink,
+	.mkdir		= esdfs_mkdir,
+	.rmdir		= esdfs_rmdir,
+	.rename		= esdfs_rename,
+	.permission     = esdfs_permission,
+	.setattr	= esdfs_setattr,
+	.getattr	= esdfs_getattr,
+};
+
+const struct inode_operations esdfs_main_iops = {
+	.permission     = esdfs_permission,
+	.setattr	= esdfs_setattr,
+	.getattr	= esdfs_getattr,
+};
diff -ruN a/fs/esdfs/Kconfig b/fs/esdfs/Kconfig
--- a/fs/esdfs/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/Kconfig	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,7 @@
+config ESD_FS
+	tristate "Emulated 'SD card' file system for Android (EXPERIMENTAL)"
+	depends on PKGLIST
+	depends on USER_NS
+	help
+	  Esdfs is a wrapfs-based file system, designed to implement the
+	  Android "sdcard" FUSE-backed file system from within the kernel.
diff -ruN a/fs/esdfs/lookup.c b/fs/esdfs/lookup.c
--- a/fs/esdfs/lookup.c	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/lookup.c	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,474 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+struct esdfs_name_data {
+	struct dir_context ctx;
+	const struct qstr *to_find;
+	char match_name[NAME_MAX+1];
+	bool found;
+};
+
+static int esdfs_name_match(struct dir_context *ctx, const char *name, int namelen,
+		loff_t offset, u64 ino, unsigned int d_type)
+{
+	struct esdfs_name_data *buf = container_of(ctx, struct esdfs_name_data, ctx);
+	struct qstr candidate = QSTR_INIT(name, namelen);
+
+	if (qstr_case_eq(buf->to_find, &candidate)) {
+		memcpy(buf->match_name, name, namelen);
+		buf->match_name[namelen] = 0;
+		buf->found = true;
+		return 1;
+	}
+	return 0;
+}
+
+int esdfs_lookup_nocase(struct path *parent,
+		const struct qstr *name,
+		struct path *path) {
+	int err = 0;
+	/* Use vfs_path_lookup to check if the dentry exists or not */
+	err = vfs_path_lookup(parent->dentry, parent->mnt, name->name, 0, path);
+	/* check for other cases */
+	if (err == -ENOENT) {
+		struct file *file;
+		const struct cred *cred = current_cred();
+
+		struct esdfs_name_data buffer = {
+			.ctx.actor = esdfs_name_match,
+			.to_find = name,
+			.found = false,
+		};
+
+		file = dentry_open(parent, O_RDONLY | O_DIRECTORY, cred);
+		if (IS_ERR(file))
+			return PTR_ERR(file);
+		err = iterate_dir(file, &buffer.ctx);
+		fput(file);
+		if (err)
+			return err;
+
+		if (buffer.found)
+			err = vfs_path_lookup(parent->dentry, parent->mnt,
+						buffer.match_name, 0, path);
+		else
+			err = -ENOENT;
+	}
+	return err;
+}
+
+struct esdfs_ci_getdents_callback {
+	struct dir_context ctx;
+	const char *name;
+	char match_name[NAME_MAX+1];
+	int found; /*-1: not found, 0: found*/
+	int count;
+};
+
+/* The dentry cache is just so we have properly sized dentries */
+static struct kmem_cache *esdfs_dentry_cachep;
+
+int esdfs_init_dentry_cache(void)
+{
+	esdfs_dentry_cachep =
+		kmem_cache_create("esdfs_dentry",
+				  sizeof(struct esdfs_dentry_info),
+				  0, SLAB_RECLAIM_ACCOUNT, NULL);
+
+	return esdfs_dentry_cachep ? 0 : -ENOMEM;
+}
+
+void esdfs_destroy_dentry_cache(void)
+{
+	if (esdfs_dentry_cachep)
+		kmem_cache_destroy(esdfs_dentry_cachep);
+}
+
+void esdfs_free_dentry_private_data(struct dentry *dentry)
+{
+	kmem_cache_free(esdfs_dentry_cachep, dentry->d_fsdata);
+	dentry->d_fsdata = NULL;
+}
+
+/* allocate new dentry private data */
+int esdfs_new_dentry_private_data(struct dentry *dentry)
+{
+	struct esdfs_dentry_info *info = ESDFS_D(dentry);
+
+	/* use zalloc to init dentry_info.lower_path */
+	info = kmem_cache_zalloc(esdfs_dentry_cachep, GFP_ATOMIC);
+	if (!info)
+		return -ENOMEM;
+
+	spin_lock_init(&info->lock);
+	dentry->d_fsdata = info;
+
+	return 0;
+}
+
+struct inode_data {
+	struct inode *lower_inode;
+	uint32_t id;
+};
+
+/* Multiple obb files can point to the same lower file */
+static int esdfs_inode_test(struct inode *inode, void *candidate_data)
+{
+	struct inode *current_lower_inode = esdfs_lower_inode(inode);
+	uint32_t current_userid = ESDFS_I(inode)->userid;
+	struct inode_data *data = (struct inode_data *)candidate_data;
+
+	if (current_lower_inode == data->lower_inode
+			&& current_userid == data->id)
+		return 1; /* found a match */
+	else
+		return 0; /* no match */
+}
+
+static int esdfs_inode_set(struct inode *inode, void *lower_inode)
+{
+	/* we do actual inode initialization in esdfs_iget */
+	return 0;
+}
+
+struct inode *esdfs_iget(struct super_block *sb, struct inode *lower_inode,
+						uint32_t id)
+{
+	struct esdfs_inode_info *info;
+	struct inode_data data;
+	struct inode *inode; /* the new inode to return */
+
+	if (!igrab(lower_inode))
+		return ERR_PTR(-ESTALE);
+	data.id = id;
+	data.lower_inode = lower_inode;
+	inode = iget5_locked(sb, /* our superblock */
+			     /*
+			      * hashval: we use inode number, but we can
+			      * also use "(unsigned long)lower_inode"
+			      * instead.
+			      */
+			     lower_inode->i_ino, /* hashval */
+			     esdfs_inode_test,	/* inode comparison function */
+			     esdfs_inode_set, /* inode init function */
+			     &data); /* data passed to test+set fxns */
+	if (!inode) {
+		iput(lower_inode);
+		return ERR_PTR(-ENOMEM);
+	}
+	/* if found a cached inode, then just return it (after iput) */
+	if (!(inode->i_state & I_NEW)) {
+		iput(lower_inode);
+		return inode;
+	}
+
+	/* initialize new inode */
+	info = ESDFS_I(inode);
+	info->tree = ESDFS_TREE_NONE;
+	info->userid = 0;
+	info->appid = 0;
+	info->under_obb = false;
+
+	inode->i_ino = lower_inode->i_ino;
+	esdfs_set_lower_inode(inode, lower_inode);
+
+	inode_inc_iversion(inode);
+
+	/* use different set of inode ops for symlinks & directories */
+	if (S_ISDIR(lower_inode->i_mode))
+		inode->i_op = &esdfs_dir_iops;
+	else if (S_ISLNK(lower_inode->i_mode))
+		inode->i_op = &esdfs_symlink_iops;
+	else
+		inode->i_op = &esdfs_main_iops;
+
+	/* use different set of file ops for directories */
+	if (S_ISDIR(lower_inode->i_mode))
+		inode->i_fop = &esdfs_dir_fops;
+	else
+		inode->i_fop = &esdfs_main_fops;
+
+	inode->i_mapping->a_ops = &esdfs_aops;
+
+	inode->i_atime.tv_sec = 0;
+	inode->i_atime.tv_nsec = 0;
+	inode->i_mtime.tv_sec = 0;
+	inode->i_mtime.tv_nsec = 0;
+	inode->i_ctime.tv_sec = 0;
+	inode->i_ctime.tv_nsec = 0;
+
+	/* properly initialize special inodes */
+	if (S_ISBLK(lower_inode->i_mode) || S_ISCHR(lower_inode->i_mode) ||
+	    S_ISFIFO(lower_inode->i_mode) || S_ISSOCK(lower_inode->i_mode))
+		init_special_inode(inode, lower_inode->i_mode,
+				   lower_inode->i_rdev);
+
+	/* all well, copy inode attributes */
+	esdfs_copy_lower_attr(inode, lower_inode);
+	fsstack_copy_inode_size(inode, lower_inode);
+
+	unlock_new_inode(inode);
+	return inode;
+}
+
+/*
+ * Helper interpose routine, called directly by ->lookup to handle
+ * spliced dentries
+ */
+static struct dentry *__esdfs_interpose(struct dentry *dentry,
+					struct super_block *sb,
+					struct path *lower_path,
+					uint32_t id)
+{
+	struct inode *inode;
+	struct inode *lower_inode;
+	struct super_block *lower_sb;
+	struct dentry *ret_dentry;
+
+	lower_inode = lower_path->dentry->d_inode;
+	lower_sb = esdfs_lower_super(sb);
+
+	/* check that the lower file system didn't cross a mount point */
+	if (lower_inode->i_sb != lower_sb) {
+		ret_dentry = ERR_PTR(-EXDEV);
+		goto out;
+	}
+
+	/*
+	 * We allocate our new inode below by calling esdfs_iget,
+	 * which will initialize some of the new inode's fields
+	 */
+
+	/* inherit lower inode number for esdfs's inode */
+	inode = esdfs_iget(sb, lower_inode, id);
+	if (IS_ERR(inode)) {
+		ret_dentry = ERR_CAST(inode);
+		goto out;
+	}
+
+	ret_dentry = d_splice_alias(inode, dentry);
+	dentry = ret_dentry ?: dentry;
+	if (IS_ERR(dentry))
+		goto out;
+
+	if (ESDFS_DERIVE_PERMS(ESDFS_SB(sb)))
+		esdfs_derive_perms(dentry);
+	esdfs_set_perms(inode);
+out:
+	return ret_dentry;
+}
+
+/*
+ * Connect an esdfs inode dentry/inode with several lower ones.  This is
+ * the classic stackable file system "vnode interposition" action.
+ *
+ * @dentry: esdfs's dentry which interposes on lower one
+ * @sb: esdfs's super_block
+ * @lower_path: the lower path (caller does path_get/put)
+ */
+int esdfs_interpose(struct dentry *dentry, struct super_block *sb,
+		     struct path *lower_path, uint32_t id)
+{
+	struct dentry *ret_dentry;
+
+	ret_dentry = __esdfs_interpose(dentry, sb, lower_path, id);
+	return PTR_ERR(ret_dentry);
+}
+
+/*
+ * Main driver function for esdfs's lookup.
+ *
+ * Returns: NULL (ok), ERR_PTR if an error occurred.
+ * Fills in lower_parent_path with <dentry,mnt> on success.
+ */
+static struct dentry *__esdfs_lookup(struct dentry *dentry,
+				     unsigned int flags,
+				     struct path *lower_parent_path,
+				     uint32_t id, bool use_dl)
+{
+	int err = 0;
+	struct vfsmount *lower_dir_mnt;
+	struct dentry *lower_dir_dentry = NULL;
+	struct dentry *lower_dentry;
+	const char *name;
+	struct path lower_path;
+	struct qstr dname;
+	struct dentry *ret_dentry = NULL;
+
+	/* must initialize dentry operations */
+	d_set_d_op(dentry, &esdfs_dops);
+
+	if (IS_ROOT(dentry))
+		goto out;
+
+	if (use_dl)
+		name = ESDFS_SB(dentry->d_sb)->dl_name.name;
+	else
+		name = dentry->d_name.name;
+
+	dname.name = name;
+	dname.len = strlen(name);
+
+	/* now start the actual lookup procedure */
+	lower_dir_dentry = lower_parent_path->dentry;
+	lower_dir_mnt = lower_parent_path->mnt;
+
+	/* if the access is to the Download directory, redirect
+	 * to lower path.
+	 */
+	if (use_dl) {
+		pathcpy(&lower_path, &ESDFS_SB(dentry->d_sb)->dl_path);
+		path_get(&ESDFS_SB(dentry->d_sb)->dl_path);
+	} else {
+		err = esdfs_lookup_nocase(lower_parent_path, &dname,
+					  &lower_path);
+	}
+
+	/* no error: handle positive dentries */
+	if (!err) {
+		esdfs_set_lower_path(dentry, &lower_path);
+		ret_dentry =
+			__esdfs_interpose(dentry, dentry->d_sb,
+						&lower_path, id);
+		if (IS_ERR(ret_dentry)) {
+			err = PTR_ERR(ret_dentry);
+			/* path_put underlying underlying path on error */
+			esdfs_put_reset_lower_paths(dentry);
+		}
+		goto out;
+	}
+
+	/*
+	 * We don't consider ENOENT an error, and we want to return a
+	 * negative dentry.
+	 */
+	if (err && err != -ENOENT)
+		goto out;
+
+	/* instatiate a new negative dentry */
+	/* See if the low-level filesystem might want
+	 * to use its own hash */
+	lower_dentry = d_hash_and_lookup(lower_dir_dentry, &dname);
+	if (IS_ERR(lower_dentry))
+		return lower_dentry;
+
+	if (!lower_dentry) {
+		/* We called vfs_path_lookup earlier, and did not get a negative
+		 * dentry then. Don't confuse the lower filesystem by forcing
+		 * one on it now...
+		 */
+		err = -ENOENT;
+		goto out;
+	}
+
+	lower_path.dentry = lower_dentry;
+	lower_path.mnt = mntget(lower_dir_mnt);
+	esdfs_set_lower_path(dentry, &lower_path);
+
+	/*
+	 * If the intent is to create a file, then don't return an error, so
+	 * the VFS will continue the process of making this negative dentry
+	 * into a positive one.
+	 */
+	if (flags & (LOOKUP_CREATE|LOOKUP_RENAME_TARGET))
+		err = 0;
+
+out:
+	if (err)
+		return ERR_PTR(err);
+	return ret_dentry;
+}
+
+struct dentry *esdfs_lookup(struct inode *dir, struct dentry *dentry,
+			    unsigned int flags)
+{
+	int err;
+	struct dentry *ret, *real_parent, *parent;
+	struct path lower_parent_path, old_lower_parent_path;
+	const struct cred *creds;
+	struct esdfs_sb_info *sbi = ESDFS_SB(dir->i_sb);
+	int use_dl;
+
+	parent = real_parent = dget_parent(dentry);
+
+	/* allocate dentry private data.  We free it in ->d_release */
+	err = esdfs_new_dentry_private_data(dentry);
+	if (err) {
+		ret = ERR_PTR(err);
+		goto out;
+	}
+
+	if (ESDFS_DERIVE_PERMS(sbi)) {
+		err = esdfs_derived_lookup(dentry, &parent);
+		if (err) {
+			ret = ERR_PTR(err);
+			goto out;
+		}
+	}
+
+	esdfs_get_lower_path(parent, &lower_parent_path);
+
+	creds =	esdfs_override_creds(ESDFS_SB(dir->i_sb),
+			ESDFS_I(d_inode(parent)), NULL);
+	if (!creds) {
+		ret = ERR_PTR(-EINVAL);
+		goto out_put;
+	}
+
+	/* Check if the lookup corresponds to the Download directory */
+	use_dl = esdfs_is_dl_lookup(dentry, parent);
+
+	ret = __esdfs_lookup(dentry, flags, &lower_parent_path,
+					ESDFS_I(dir)->userid,
+					use_dl);
+	if (IS_ERR(ret))
+		goto out_cred;
+	if (ret)
+		dentry = ret;
+	if (dentry->d_inode) {
+		fsstack_copy_attr_times(dentry->d_inode,
+					esdfs_lower_inode(dentry->d_inode));
+		/*
+		 * Do not modify the ownership of the lower directory if it
+		 * is the Download directory
+		 */
+		if (!use_dl)
+			esdfs_derive_lower_ownership(dentry,
+						     dentry->d_name.name);
+	}
+	/* update parent directory's atime */
+	fsstack_copy_attr_atime(parent->d_inode,
+				esdfs_lower_inode(parent->d_inode));
+
+	/*
+	 * If this is a pseudo hard link, store the real parent and ensure
+	 * that the link target directory contains any derived contents.
+	 */
+	if (parent != real_parent) {
+		esdfs_get_lower_path(real_parent, &old_lower_parent_path);
+		esdfs_set_lower_parent(dentry, old_lower_parent_path.dentry);
+		esdfs_put_lower_path(real_parent, &old_lower_parent_path);
+		esdfs_derive_mkdir_contents(dentry);
+	}
+out_cred:
+	esdfs_revert_creds(creds, NULL);
+out_put:
+	esdfs_put_lower_path(parent, &lower_parent_path);
+out:
+	dput(parent);
+	if (parent != real_parent)
+		dput(real_parent);
+	return ret;
+}
diff -ruN a/fs/esdfs/main.c b/fs/esdfs/main.c
--- a/fs/esdfs/main.c	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/main.c	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,725 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+#include <linux/module.h>
+#include <linux/parser.h>
+#include <linux/security.h>
+#include <linux/proc_ns.h>
+
+/*
+ * Derived from first generation "ANDROID_EMU" glue in modifed F2FS driver.
+ */
+enum {
+	Opt_lower_perms,
+	Opt_upper_perms,
+	Opt_derive_none,
+	Opt_derive_legacy,
+	Opt_derive_unified,
+	Opt_derive_multi,
+	Opt_derive_public,
+	Opt_confine,
+	Opt_noconfine,
+	Opt_gid_derivation,
+	Opt_default_normal,
+	Opt_dl_loc,
+	Opt_dl_uid,
+	Opt_dl_gid,
+	Opt_ns_fd,
+
+	/* From sdcardfs */
+	Opt_fsuid,
+	Opt_fsgid,
+	Opt_gid,
+	Opt_debug,
+	Opt_mask,
+	Opt_multiuser,
+	Opt_userid,
+
+	Opt_err,
+};
+
+static match_table_t esdfs_tokens = {
+	{Opt_lower_perms, "lower=%s"},
+	{Opt_upper_perms, "upper=%s"},
+	{Opt_derive_none, "derive=none"},
+	{Opt_derive_legacy, "derive=legacy"},
+	{Opt_derive_unified, "derive=unified"},
+	{Opt_derive_multi, "derive=multi"},
+	{Opt_derive_public, "derive=public"},
+	{Opt_confine, "confine"},
+	{Opt_noconfine, "noconfine"},
+	{Opt_gid_derivation, "derive_gid"},
+	{Opt_default_normal, "default_normal"},
+	{Opt_dl_loc, "dl_loc=%s"},
+	{Opt_dl_uid, "dl_uid=%u"},
+	{Opt_dl_gid, "dl_gid=%u"},
+	{Opt_ns_fd, "ns_fd=%d"},
+	/* compatibility with sdcardfs options */
+	{Opt_fsuid, "fsuid=%u"},
+	{Opt_fsgid, "fsgid=%u"},
+	{Opt_gid, "gid=%u"},
+	{Opt_mask, "mask=%u"},
+	{Opt_userid, "userid=%d"},
+	{Opt_multiuser, "multiuser"},
+	{Opt_gid_derivation, "derive_gid"},
+	{Opt_err, NULL},
+};
+
+struct esdfs_perms esdfs_perms_table[ESDFS_PERMS_TABLE_SIZE] = {
+	/* ESDFS_PERMS_LOWER_DEFAULT */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = AID_MEDIA_RW,
+	  .gid   = AID_MEDIA_RW,
+	  .fmask = 0664,
+	  .dmask = 0775 },
+	/* ESDFS_PERMS_UPPER_LEGACY */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = AID_ROOT,
+	  .gid   = AID_SDCARD_RW,
+	  .fmask = 0664,
+	  .dmask = 0775 },
+	/* ESDFS_PERMS_UPPER_DERIVED */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = AID_ROOT,
+	  .gid   = AID_SDCARD_R,
+	  .fmask = 0660,
+	  .dmask = 0771 },
+	/* ESDFS_PERMS_LOWER_DOWNLOAD */
+	{ .raw_uid = -1,
+	  .raw_gid = -1,
+	  .uid   = -1,
+	  .gid   = -1,
+	  .fmask = 0644,
+	  .dmask = 0711 },
+};
+
+static int parse_perms(struct esdfs_perms *perms, char *args)
+{
+	char *sep = args;
+	char *sepres;
+	int ret;
+
+	if (!sep)
+		return -EINVAL;
+
+	sepres = strsep(&sep, ":");
+	if (!sep)
+		return -EINVAL;
+	ret = kstrtou32(sepres, 0, &perms->uid);
+	if (ret)
+		return ret;
+
+	sepres = strsep(&sep, ":");
+	if (!sep)
+		return -EINVAL;
+	ret = kstrtou32(sepres, 0, &perms->gid);
+	if (ret)
+		return ret;
+
+	sepres = strsep(&sep, ":");
+	if (!sep)
+		return -EINVAL;
+	ret = kstrtou16(sepres, 8, &perms->fmask);
+	if (ret)
+		return ret;
+
+	sepres = strsep(&sep, ":");
+	ret = kstrtou16(sepres, 8, &perms->dmask);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static inline struct user_namespace *to_user_ns(struct ns_common *ns)
+{
+	return container_of(ns, struct user_namespace, ns);
+}
+
+static struct user_namespace *get_ns_from_fd(int fd)
+{
+	struct file *file;
+	struct ns_common *ns;
+	struct user_namespace *user_ns = ERR_PTR(-EINVAL);
+
+	file = proc_ns_fget(fd);
+	if (IS_ERR(file))
+		return ERR_CAST(file);
+
+	ns = get_proc_ns(file_inode(file));
+#ifdef CONFIG_USER_NS
+	if (ns->ops == &userns_operations)
+		user_ns = to_user_ns(ns);
+#endif
+	fput(file);
+	return user_ns;
+}
+
+static int parse_options(struct super_block *sb, char *options)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(sb);
+	substring_t args[MAX_OPT_ARGS];
+	char *p;
+	int option;
+
+	if (!options)
+		return 0;
+
+	while ((p = strsep(&options, ",")) != NULL) {
+		int token;
+
+		if (!*p)
+			continue;
+		/*
+		 * Initialize args struct so we know whether arg was
+		 * found; some options take optional arguments.
+		 */
+		args[0].to = args[0].from = NULL;
+		token = match_token(p, esdfs_tokens, args);
+
+		switch (token) {
+		case Opt_lower_perms:
+			if (args->from) {
+				int ret;
+				char *perms = match_strdup(args);
+
+				ret = parse_perms(&sbi->lower_perms, perms);
+				kfree(perms);
+
+				if (ret)
+					return -EINVAL;
+			} else
+				return -EINVAL;
+			break;
+		case Opt_upper_perms:
+			if (args->from) {
+				int ret;
+				char *perms = match_strdup(args);
+
+				ret = parse_perms(&sbi->upper_perms, perms);
+				kfree(perms);
+
+				if (ret)
+					return -EINVAL;
+			} else
+				return -EINVAL;
+			break;
+		case Opt_derive_none:
+			clear_opt(sbi, DERIVE_LEGACY);
+			clear_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_derive_legacy:
+			set_opt(sbi, DERIVE_LEGACY);
+			clear_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_derive_unified:
+			clear_opt(sbi, DERIVE_LEGACY);
+			set_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			set_opt(sbi, DERIVE_CONFINE);	/* confine by default */
+			break;
+		case Opt_derive_multi:
+		case Opt_multiuser:
+			set_opt(sbi, DERIVE_LEGACY);
+			clear_opt(sbi, DERIVE_UNIFIED);
+			set_opt(sbi, DERIVE_MULTI);
+			clear_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_derive_public:
+			clear_opt(sbi, DERIVE_LEGACY);
+			set_opt(sbi, DERIVE_UNIFIED);
+			clear_opt(sbi, DERIVE_MULTI);
+			set_opt(sbi, DERIVE_PUBLIC);
+			break;
+		case Opt_confine:
+			set_opt(sbi, DERIVE_CONFINE);
+			break;
+		case Opt_noconfine:
+			clear_opt(sbi, DERIVE_CONFINE);
+			break;
+		/* for compatibility with sdcardfs options */
+		case Opt_gid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->upper_perms.raw_gid = option;
+			break;
+		case Opt_userid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->upper_perms.raw_uid = option;
+			break;
+		case Opt_mask:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->upper_perms.dmask = 0775 & ~option;
+			sbi->upper_perms.fmask = 0775 & ~option;
+			break;
+		case Opt_fsuid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_perms.raw_uid = option;
+			break;
+		case Opt_fsgid:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_perms.raw_gid = option;
+			break;
+		case Opt_gid_derivation:
+			set_opt(sbi, GID_DERIVATION);
+			break;
+		case Opt_default_normal:
+			set_opt(sbi, DEFAULT_NORMAL);
+			break;
+		case Opt_dl_loc:
+			set_opt(sbi, SPECIAL_DOWNLOAD);
+			sbi->dl_loc = match_strdup(args);
+			break;
+		case Opt_dl_uid:
+			set_opt(sbi, SPECIAL_DOWNLOAD);
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_dl_perms.raw_uid = option;
+			break;
+		case Opt_dl_gid:
+			set_opt(sbi, SPECIAL_DOWNLOAD);
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->lower_dl_perms.raw_gid = option;
+			break;
+		case Opt_ns_fd:
+			if (match_int(&args[0], &option))
+				return -EINVAL;
+			sbi->ns_fd = option;
+			break;
+		default:
+			esdfs_msg(sb, KERN_ERR,
+			  "unrecognized mount option \"%s\" or missing value\n",
+			  p);
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
+
+static int interpret_perms(struct esdfs_sb_info *sbi, struct esdfs_perms *perms)
+{
+	if (perms->raw_uid == -1) {
+		perms->raw_uid = perms->uid;
+	} else {
+		perms->uid = esdfs_from_local_uid(sbi, perms->raw_uid);
+		if (perms->uid == -1)
+			return -EINVAL;
+	}
+
+	if (perms->raw_gid == -1) {
+		perms->raw_gid = perms->gid;
+	} else {
+		perms->gid = esdfs_from_local_gid(sbi, perms->raw_gid);
+		if (perms->gid == -1)
+			return -EINVAL;
+	}
+	return 0;
+}
+
+/*
+ * There is no need to lock the esdfs_super_info's rwsem as there is no
+ * way anyone can have a reference to the superblock at this point in time.
+ */
+static int esdfs_read_super(struct super_block *sb, const char *dev_name,
+		void *raw_data, int silent)
+{
+	int err = 0;
+	struct super_block *lower_sb;
+	struct path lower_path;
+	struct esdfs_sb_info *sbi;
+	struct inode *inode;
+	struct dentry *lower_dl_dentry;
+	struct user_namespace *user_ns;
+	kuid_t dl_kuid = INVALID_UID;
+	kgid_t dl_kgid = INVALID_GID;
+
+	if (!dev_name) {
+		esdfs_msg(sb, KERN_ERR, "missing dev_name argument\n");
+		err = -EINVAL;
+		goto out;
+	}
+
+	/* parse lower path */
+	err = kern_path(dev_name, LOOKUP_FOLLOW | LOOKUP_DIRECTORY,
+			&lower_path);
+	if (err) {
+		esdfs_msg(sb, KERN_ERR,
+			"error accessing lower directory '%s'\n", dev_name);
+		goto out;
+	}
+
+	/* allocate superblock private data */
+	sb->s_fs_info = kzalloc(sizeof(struct esdfs_sb_info), GFP_KERNEL);
+	sbi = ESDFS_SB(sb);
+	if (!sbi) {
+		esdfs_msg(sb, KERN_CRIT, "read_super: out of memory\n");
+		err = -ENOMEM;
+		goto out_pput;
+	}
+	INIT_LIST_HEAD(&sbi->s_list);
+
+	/* set defaults and then parse the mount options */
+
+	sbi->ns_fd = -1;
+
+	/* make public default */
+	clear_opt(sbi, DERIVE_LEGACY);
+	set_opt(sbi, DERIVE_UNIFIED);
+	clear_opt(sbi, DERIVE_MULTI);
+	set_opt(sbi, DERIVE_PUBLIC);
+
+	memcpy(&sbi->lower_perms,
+	       &esdfs_perms_table[ESDFS_PERMS_LOWER_DEFAULT],
+	       sizeof(struct esdfs_perms));
+	if (ESDFS_DERIVE_PERMS(sbi))
+		memcpy(&sbi->upper_perms,
+		       &esdfs_perms_table[ESDFS_PERMS_UPPER_DERIVED],
+		       sizeof(struct esdfs_perms));
+	else
+		memcpy(&sbi->upper_perms,
+		       &esdfs_perms_table[ESDFS_PERMS_UPPER_LEGACY],
+		       sizeof(struct esdfs_perms));
+
+	memcpy(&sbi->lower_dl_perms,
+	       &esdfs_perms_table[ESDFS_PERMS_LOWER_DOWNLOAD],
+	       sizeof(struct esdfs_perms));
+
+	err = parse_options(sb, (char *)raw_data);
+	if (err)
+		goto out_free;
+
+	/* Initialize special namespace for lower Downloads directory */
+	sbi->dl_ns = get_user_ns(current_user_ns());
+
+	if (sbi->ns_fd == -1) {
+		sbi->base_ns = get_user_ns(current_user_ns());
+	} else {
+		user_ns = get_ns_from_fd(sbi->ns_fd);
+		if (IS_ERR(user_ns)) {
+			err = PTR_ERR(user_ns);
+			goto out_free;
+		}
+		sbi->base_ns = get_user_ns(user_ns);
+	}
+	/* interpret all parameters in given namespace */
+	err = interpret_perms(sbi, &sbi->lower_perms);
+	if (err) {
+		pr_err("esdfs: Invalid permissions for lower layer\n");
+		goto out_free;
+	}
+	err = interpret_perms(sbi, &sbi->upper_perms);
+	if (err) {
+		pr_err("esdfs: Invalid permissions for upper layer\n");
+		goto out_free;
+	}
+
+	/* Check if the downloads uid maps into a valid kuid from
+	 * the namespace of the mounting process
+	 */
+	if (sbi->lower_dl_perms.raw_uid != -1) {
+		dl_kuid = make_kuid(sbi->dl_ns,
+				    sbi->lower_dl_perms.raw_uid);
+		if (!uid_valid(dl_kuid)) {
+			pr_err("esdfs: Invalid permissions for dl_uid");
+			err = -EINVAL;
+			goto out_free;
+		}
+	}
+	if (sbi->lower_dl_perms.raw_gid != -1) {
+		dl_kgid = make_kgid(sbi->dl_ns,
+				    sbi->lower_dl_perms.raw_gid);
+		if (!gid_valid(dl_kgid)) {
+			pr_err("esdfs: Invalid permissions for dl_gid");
+			err = -EINVAL;
+			goto out_free;
+		}
+	}
+
+	/* set the lower superblock field of upper superblock */
+	lower_sb = lower_path.dentry->d_sb;
+	atomic_inc(&lower_sb->s_active);
+	esdfs_set_lower_super(sb, lower_sb);
+
+	sb->s_stack_depth = lower_sb->s_stack_depth + 1;
+	if (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {
+		pr_err("esdfs: maximum fs stacking depth exceeded\n");
+		err = -EINVAL;
+		goto out_sput;
+	}
+
+	/* inherit maxbytes from lower file system */
+	sb->s_maxbytes = lower_sb->s_maxbytes;
+
+	/*
+	 * Our c/m/atime granularity is 1 ns because we may stack on file
+	 * systems whose granularity is as good.
+	 */
+	sb->s_time_gran = 1;
+
+	sb->s_op = &esdfs_sops;
+
+	/* get a new inode and allocate our root dentry */
+	inode = esdfs_iget(sb, lower_path.dentry->d_inode, 0);
+	if (IS_ERR(inode)) {
+		err = PTR_ERR(inode);
+		goto out_sput;
+	}
+	sb->s_root = d_make_root(inode);
+	if (!sb->s_root) {
+		err = -ENOMEM;
+		goto out_sput;
+	}
+	d_set_d_op(sb->s_root, &esdfs_dops);
+
+	/* link the upper and lower dentries */
+	sb->s_root->d_fsdata = NULL;
+	err = esdfs_new_dentry_private_data(sb->s_root);
+	if (err)
+		goto out_freeroot;
+
+	if (test_opt(sbi, SPECIAL_DOWNLOAD)) {
+		/* parse lower path */
+		err = kern_path(sbi->dl_loc, LOOKUP_FOLLOW | LOOKUP_DIRECTORY,
+				&sbi->dl_path);
+		if (err) {
+			esdfs_msg(sb, KERN_ERR,
+				"error accessing download directory '%s'\n",
+				sbi->dl_loc);
+			goto out_freeroot;
+		}
+
+		lower_dl_dentry = sbi->dl_path.dentry;
+
+		if (!S_ISDIR(lower_dl_dentry->d_inode->i_mode)) {
+			err = -EINVAL;
+			esdfs_msg(sb, KERN_ERR,
+				"dl_loc must be a directory '%s'\n",
+				sbi->dl_loc);
+			goto out_dlput;
+		}
+
+		if (lower_dl_dentry->d_sb != lower_sb) {
+			esdfs_msg(sb, KERN_ERR,
+				"dl_loc must be in the same filesystem '%s'\n",
+				sbi->dl_loc);
+			goto out_dlput;
+		}
+
+		if (!uid_valid(dl_kuid)) {
+			dl_kuid = esdfs_make_kuid(sbi, sbi->lower_perms.uid);
+			sbi->lower_dl_perms.raw_uid = from_kuid(sbi->dl_ns,
+								dl_kuid);
+		}
+		if (!gid_valid(dl_kgid)) {
+			dl_kgid = esdfs_make_kgid(sbi, sbi->lower_perms.gid);
+			sbi->lower_dl_perms.raw_gid = from_kgid(sbi->dl_ns,
+								dl_kgid);
+		}
+		spin_lock(&lower_dl_dentry->d_lock);
+		sbi->dl_name.name = kstrndup(lower_dl_dentry->d_name.name,
+				lower_dl_dentry->d_name.len, GFP_ATOMIC);
+		sbi->dl_name.len = lower_dl_dentry->d_name.len;
+		spin_unlock(&lower_dl_dentry->d_lock);
+	}
+	/* if get here: cannot have error */
+
+	/* set the lower dentries for s_root */
+	esdfs_set_lower_path(sb->s_root, &lower_path);
+
+	/*
+	 * No need to call interpose because we already have a positive
+	 * dentry, which was instantiated by d_make_root.  Just need to
+	 * d_rehash it.
+	 */
+	d_rehash(sb->s_root);
+	if (!silent)
+		esdfs_msg(sb, KERN_INFO, "mounted on top of %s type %s\n",
+			dev_name, lower_sb->s_type->name);
+
+	if (!ESDFS_DERIVE_PERMS(sbi))
+		goto out;
+
+	/* let user know that we ignore this option in older derived modes */
+	if (ESDFS_RESTRICT_PERMS(sbi) &&
+	    memcmp(&sbi->upper_perms,
+		   &esdfs_perms_table[ESDFS_PERMS_UPPER_DERIVED],
+		   sizeof(struct esdfs_perms)))
+		esdfs_msg(sb, KERN_WARNING,
+			"'upper' mount option ignored in this derived mode\n");
+
+	/*
+	 * In Android 3.0 all user conent in the emulated storage tree was
+	 * stored in /data/media.  Android 4.2 introduced multi-user support,
+	 * which required that the primary user's content be migrated from
+	 * /data/media to /data/media/0.  The framework then uses bind mounts
+	 * to create per-process namespaces to isolate each user's tree at
+	 * /data/media/N.  This approach of having each user in a common root
+	 * is now considered "legacy" by the sdcard service.
+	 */
+	if (test_opt(sbi, DERIVE_LEGACY)) {
+		ESDFS_I(inode)->tree = ESDFS_TREE_ROOT_LEGACY;
+		sbi->obb_parent = dget(sb->s_root);
+	/*
+	 * Android 4.4 reorganized this sturcture yet again, so that the
+	 * primary user's content was again at the root.  Secondary users'
+	 * content is found in Android/user/N.  Emulated internal storage still
+	 * seems to use the legacy tree, but secondary external storage uses
+	 * this method.
+	 */
+	} else if (test_opt(sbi, DERIVE_UNIFIED))
+		ESDFS_I(inode)->tree = ESDFS_TREE_ROOT;
+	/*
+	 * Later versions of Android organize user content using quantum
+	 * entanglement, which has a low probability of being supported by
+	 * this driver.
+	 */
+	else
+		esdfs_msg(sb, KERN_WARNING,
+				"unsupported derived permissions mode\n");
+
+	/* initialize root inode */
+	esdfs_derive_perms(sb->s_root);
+	esdfs_set_perms(inode);
+
+	esdfs_add_super(sbi, sb);
+
+	goto out;
+
+out_dlput:
+	path_put(&sbi->dl_path);
+	sbi->dl_path.dentry = NULL;
+	sbi->dl_path.mnt = NULL;
+out_freeroot:
+	dput(sb->s_root);
+	sb->s_root = NULL;
+out_sput:
+	/* drop refs we took earlier */
+	atomic_dec(&lower_sb->s_active);
+out_free:
+	if (sbi->dl_ns)
+		put_user_ns(sbi->dl_ns);
+	if (sbi->base_ns)
+		put_user_ns(sbi->base_ns);
+	kfree(sbi->dl_loc);
+	kfree(ESDFS_SB(sb));
+	sb->s_fs_info = NULL;
+out_pput:
+	path_put(&lower_path);
+
+out:
+	return err;
+}
+
+struct esdfs_mount_private {
+	const char *dev_name;
+	void *raw_data;
+};
+
+static int __esdfs_fill_super(struct super_block *sb, void *_priv, int silent)
+{
+	struct esdfs_mount_private *priv = _priv;
+
+	return esdfs_read_super(sb, priv->dev_name, priv->raw_data, silent);
+}
+
+static struct dentry *esdfs_mount(struct file_system_type *fs_type, int flags,
+				const char *dev_name, void *raw_data)
+{
+	struct esdfs_mount_private priv = {
+		.dev_name = dev_name,
+		.raw_data = raw_data,
+	};
+
+	return mount_nodev(fs_type, flags, &priv, __esdfs_fill_super);
+}
+
+static void esdfs_kill_sb(struct super_block *sb)
+{
+	if (sb->s_fs_info && ESDFS_SB(sb)->obb_parent)
+		dput(ESDFS_SB(sb)->obb_parent);
+	if (sb->s_fs_info && ESDFS_SB(sb)->dl_ns)
+		put_user_ns(ESDFS_SB(sb)->dl_ns);
+	if (sb->s_fs_info && ESDFS_SB(sb)->base_ns)
+		put_user_ns(ESDFS_SB(sb)->base_ns);
+	if (sb->s_fs_info) {
+		kfree(ESDFS_SB(sb)->dl_loc);
+		kfree(ESDFS_SB(sb)->dl_name.name);
+		path_put(&ESDFS_SB(sb)->dl_path);
+	}
+
+	kill_anon_super(sb);
+}
+
+static struct file_system_type esdfs_fs_type = {
+	.owner		= THIS_MODULE,
+	.name		= ESDFS_NAME,
+	.mount		= esdfs_mount,
+	.kill_sb	= esdfs_kill_sb,
+	.fs_flags	= 0,
+};
+MODULE_ALIAS_FS(ESDFS_NAME);
+
+static int __init init_esdfs_fs(void)
+{
+	int err;
+
+	pr_info("Registering esdfs " ESDFS_VERSION "\n");
+
+	esdfs_init_package_list();
+
+	err = esdfs_init_inode_cache();
+	if (err)
+		goto out;
+	err = esdfs_init_dentry_cache();
+	if (err)
+		goto out;
+	err = register_filesystem(&esdfs_fs_type);
+out:
+	if (err) {
+		esdfs_destroy_inode_cache();
+		esdfs_destroy_dentry_cache();
+		esdfs_destroy_package_list();
+	}
+	return err;
+}
+
+static void __exit exit_esdfs_fs(void)
+{
+	esdfs_destroy_inode_cache();
+	esdfs_destroy_dentry_cache();
+	esdfs_destroy_package_list();
+	unregister_filesystem(&esdfs_fs_type);
+	pr_info("Completed esdfs module unload\n");
+}
+
+MODULE_AUTHOR("Erez Zadok, Filesystems and Storage Lab, Stony Brook University"
+	      " (http://www.fsl.cs.sunysb.edu/)");
+MODULE_DESCRIPTION("esdfs " ESDFS_VERSION);
+MODULE_LICENSE("GPL");
+
+module_init(init_esdfs_fs);
+module_exit(exit_esdfs_fs);
diff -ruN a/fs/esdfs/Makefile b/fs/esdfs/Makefile
--- a/fs/esdfs/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/Makefile	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,7 @@
+ESDFS_VERSION="0.2"
+
+EXTRA_CFLAGS += -DESDFS_VERSION=\"$(ESDFS_VERSION)\"
+
+obj-$(CONFIG_ESD_FS) += esdfs.o
+
+esdfs-y := dentry.o file.o inode.o main.o super.o lookup.o mmap.o derive.o
diff -ruN a/fs/esdfs/mmap.c b/fs/esdfs/mmap.c
--- a/fs/esdfs/mmap.c	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/mmap.c	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,98 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014, 2016 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+static vm_fault_t esdfs_fault(struct vm_fault *vmf)
+{
+	vm_fault_t err;
+	struct file *file;
+	const struct vm_operations_struct *lower_vm_ops;
+	struct esdfs_sb_info *sbi;
+	const struct cred *creds;
+	const struct vm_area_struct *vma = vmf->vma;
+
+	file = (struct file *)vma->vm_private_data;
+	sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	creds = esdfs_override_creds(sbi, ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return VM_FAULT_OOM;
+
+	lower_vm_ops = ESDFS_F(file)->lower_vm_ops;
+	BUG_ON(!lower_vm_ops);
+	err = lower_vm_ops->fault(vmf);
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static void esdfs_vm_open(struct vm_area_struct *vma)
+{
+	struct file *file = (struct file *)vma->vm_private_data;
+
+	get_file(file);
+}
+
+static void esdfs_vm_close(struct vm_area_struct *vma)
+{
+	struct file *file = (struct file *)vma->vm_private_data;
+
+	fput(file);
+}
+
+static vm_fault_t esdfs_page_mkwrite(struct vm_fault *vmf)
+{
+	vm_fault_t err = 0;
+	struct file *file;
+	const struct vm_operations_struct *lower_vm_ops;
+	struct esdfs_sb_info *sbi;
+	const struct cred *creds;
+	const struct vm_area_struct *vma = vmf->vma;
+
+	file = (struct file *)vma->vm_private_data;
+	sbi = ESDFS_SB(file->f_path.dentry->d_sb);
+	creds = esdfs_override_creds(sbi, ESDFS_I(file->f_inode), NULL);
+	if (!creds)
+		return VM_FAULT_OOM;
+
+	lower_vm_ops = ESDFS_F(file)->lower_vm_ops;
+	BUG_ON(!lower_vm_ops);
+	if (!lower_vm_ops->page_mkwrite)
+		goto out;
+
+	err = lower_vm_ops->page_mkwrite(vmf);
+out:
+	esdfs_revert_creds(creds, NULL);
+	return err;
+}
+
+static ssize_t esdfs_direct_IO(struct kiocb *iocb,
+				struct iov_iter *iter)
+{
+	/*
+	 * This function should never be called directly.  We need it
+	 * to exist, to get past a check in open_check_o_direct(),
+	 * which is called from do_last().
+	 */
+	return -EINVAL;
+}
+
+const struct address_space_operations esdfs_aops = {
+	.direct_IO = esdfs_direct_IO,
+};
+
+const struct vm_operations_struct esdfs_vm_ops = {
+	.fault		= esdfs_fault,
+	.page_mkwrite	= esdfs_page_mkwrite,
+	.open		= esdfs_vm_open,
+	.close		= esdfs_vm_close,
+};
diff -ruN a/fs/esdfs/super.c b/fs/esdfs/super.c
--- a/fs/esdfs/super.c	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/esdfs/super.c	2021-12-23 08:35:51.000000000 +0100
@@ -0,0 +1,290 @@
+/*
+ * Copyright (c) 1998-2014 Erez Zadok
+ * Copyright (c) 2009	   Shrikar Archak
+ * Copyright (c) 2003-2014 Stony Brook University
+ * Copyright (c) 2003-2014 The Research Foundation of SUNY
+ * Copyright (C) 2013-2014 Motorola Mobility, LLC
+ * Copyright (C) 2017      Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include "esdfs.h"
+
+/*
+ * The inode cache is used with alloc_inode for both our inode info and the
+ * vfs inode.
+ */
+static struct kmem_cache *esdfs_inode_cachep;
+static LIST_HEAD(esdfs_list);
+static DEFINE_SPINLOCK(esdfs_list_lock);
+
+void esdfs_msg(struct super_block *sb, const char *level, const char *fmt, ...)
+{
+	struct va_format vaf;
+	va_list args;
+
+	va_start(args, fmt);
+	vaf.fmt = fmt;
+	vaf.va = &args;
+	printk("%sESDFS-fs (%s): %pV", level, sb->s_id, &vaf);
+	va_end(args);
+}
+
+void esdfs_add_super(struct esdfs_sb_info *sbi, struct super_block *sb)
+{
+	sbi->s_sb = sb;
+
+	spin_lock(&esdfs_list_lock);
+	list_add_tail(&sbi->s_list, &esdfs_list);
+	spin_unlock(&esdfs_list_lock);
+}
+
+static void esdfs_remove_super(struct esdfs_sb_info *sbi)
+{
+	spin_lock(&esdfs_list_lock);
+	list_del(&sbi->s_list);
+	spin_unlock(&esdfs_list_lock);
+}
+
+void esdfs_truncate_share(struct super_block *sb, struct inode *lower_inode,
+				loff_t newsize)
+{
+	struct list_head *p;
+	struct esdfs_sb_info *sbi;
+	struct super_block *lower_sb = lower_inode->i_sb;
+	struct inode *inode;
+
+	spin_lock(&esdfs_list_lock);
+	p = esdfs_list.next;
+	while (p != &esdfs_list) {
+		sbi = list_entry(p, struct esdfs_sb_info, s_list);
+		if (sbi->s_sb == sb || sbi->lower_sb != lower_sb) {
+			p = p->next;
+			continue;
+		}
+		spin_unlock(&esdfs_list_lock);
+		inode = ilookup(sbi->s_sb, lower_inode->i_ino);
+		if (inode) {
+			truncate_setsize(inode, newsize);
+			iput(inode);
+		}
+		spin_lock(&esdfs_list_lock);
+		p = p->next;
+	}
+	spin_unlock(&esdfs_list_lock);
+}
+
+/* final actions when unmounting a file system */
+static void esdfs_put_super(struct super_block *sb)
+{
+	struct esdfs_sb_info *spd;
+	struct super_block *s;
+
+	spd = ESDFS_SB(sb);
+	if (!spd)
+		return;
+
+	/* decrement lower super references */
+	s = esdfs_lower_super(sb);
+	esdfs_set_lower_super(sb, NULL);
+	atomic_dec(&s->s_active);
+
+	esdfs_remove_super(spd);
+
+	kfree(spd);
+	sb->s_fs_info = NULL;
+}
+
+static int esdfs_statfs(struct dentry *dentry, struct kstatfs *buf)
+{
+	int err;
+	struct path lower_path;
+	struct inode *inode = dentry->d_inode;
+
+	if (test_opt(ESDFS_SB(inode->i_sb), ACCESS_DISABLE))
+		return -ENOENT;
+
+	esdfs_get_lower_path(dentry, &lower_path);
+	err = vfs_statfs(&lower_path, buf);
+	esdfs_put_lower_path(dentry, &lower_path);
+
+	/* set return buf to our f/s to avoid confusing user-level utils */
+	buf->f_type = ESDFS_SUPER_MAGIC;
+
+	return err;
+}
+
+/*
+ * @flags: numeric mount options
+ * @options: mount options string
+ */
+static int esdfs_remount_fs(struct super_block *sb, int *flags, char *options)
+{
+	int err = 0;
+
+	/*
+	 * The VFS will take care of "ro" and "rw" flags among others.  We
+	 * can safely accept a few flags (RDONLY, MANDLOCK), and honor
+	 * SILENT, but anything else left over is an error.
+	 */
+	if ((*flags & ~(MS_RDONLY | MS_MANDLOCK | MS_SILENT)) != 0) {
+		esdfs_msg(sb, KERN_ERR, "remount flags 0x%x unsupported\n",
+			*flags);
+		err = -EINVAL;
+	}
+
+	return err;
+}
+
+/*
+ * Called by iput() when the inode reference count reached zero
+ * and the inode is not hashed anywhere.  Used to clear anything
+ * that needs to be, before the inode is completely destroyed and put
+ * on the inode free list.
+ */
+static void esdfs_evict_inode(struct inode *inode)
+{
+	struct inode *lower_inode;
+
+	truncate_inode_pages(&inode->i_data, 0);
+	clear_inode(inode);
+	/*
+	 * Decrement a reference to a lower_inode, which was incremented
+	 * by our read_inode when it was created initially.
+	 */
+	lower_inode = esdfs_lower_inode(inode);
+	esdfs_set_lower_inode(inode, NULL);
+	iput(lower_inode);
+}
+
+static struct inode *esdfs_alloc_inode(struct super_block *sb)
+{
+	struct esdfs_inode_info *i;
+
+	i = kmem_cache_alloc(esdfs_inode_cachep, GFP_KERNEL);
+	if (!i)
+		return NULL;
+
+	/* memset everything up to the inode to 0 */
+	memset(i, 0, offsetof(struct esdfs_inode_info, vfs_inode));
+
+	inode_set_iversion(&i->vfs_inode, 1);
+	return &i->vfs_inode;
+}
+
+static void i_callback(struct rcu_head *head)
+{
+	struct inode *inode = container_of(head, struct inode, i_rcu);
+
+	kmem_cache_free(esdfs_inode_cachep, ESDFS_I(inode));
+}
+
+static void esdfs_destroy_inode(struct inode *inode)
+{
+	call_rcu(&inode->i_rcu, i_callback);
+}
+
+/* esdfs inode cache constructor */
+static void init_once(void *obj)
+{
+	struct esdfs_inode_info *i = obj;
+
+	inode_init_once(&i->vfs_inode);
+}
+
+int esdfs_init_inode_cache(void)
+{
+	int err = 0;
+
+	esdfs_inode_cachep =
+		kmem_cache_create("esdfs_inode_cache",
+				  sizeof(struct esdfs_inode_info), 0,
+				  SLAB_RECLAIM_ACCOUNT, init_once);
+	if (!esdfs_inode_cachep)
+		err = -ENOMEM;
+	return err;
+}
+
+/* esdfs inode cache destructor */
+void esdfs_destroy_inode_cache(void)
+{
+	if (esdfs_inode_cachep)
+		kmem_cache_destroy(esdfs_inode_cachep);
+}
+
+/*
+ * Used only in nfs, to kill any pending RPC tasks, so that subsequent
+ * code can actually succeed and won't leave tasks that need handling.
+ */
+static void esdfs_umount_begin(struct super_block *sb)
+{
+	struct super_block *lower_sb;
+
+	lower_sb = esdfs_lower_super(sb);
+	if (lower_sb && lower_sb->s_op && lower_sb->s_op->umount_begin)
+		lower_sb->s_op->umount_begin(lower_sb);
+}
+
+static int esdfs_show_options(struct seq_file *seq, struct dentry *root)
+{
+	struct esdfs_sb_info *sbi = ESDFS_SB(root->d_sb);
+
+	if (memcmp(&sbi->lower_perms,
+		   &esdfs_perms_table[ESDFS_PERMS_LOWER_DEFAULT],
+		   sizeof(struct esdfs_perms)))
+		seq_printf(seq, ",lower=%u:%u:%ho:%ho",
+				sbi->lower_perms.raw_uid,
+				sbi->lower_perms.raw_gid,
+				sbi->lower_perms.fmask,
+				sbi->lower_perms.dmask);
+
+	if (memcmp(&sbi->upper_perms,
+		   &esdfs_perms_table[ESDFS_PERMS_UPPER_LEGACY],
+		   sizeof(struct esdfs_perms)))
+		seq_printf(seq, ",upper=%u:%u:%ho:%ho",
+				sbi->upper_perms.raw_uid,
+				sbi->upper_perms.raw_gid,
+				sbi->upper_perms.fmask,
+				sbi->upper_perms.dmask);
+
+	if (test_opt(sbi, DERIVE_PUBLIC))
+		seq_puts(seq, ",derive=public");
+	else if (test_opt(sbi, DERIVE_MULTI))
+		seq_puts(seq, ",derive=multi");
+	else if (test_opt(sbi, DERIVE_UNIFIED))
+		seq_puts(seq, ",derive=unified");
+	else if (test_opt(sbi, DERIVE_LEGACY))
+		seq_puts(seq, ",derive=legacy");
+	else
+		seq_puts(seq, ",derive=none");
+
+	if (test_opt(sbi, DERIVE_CONFINE))
+		seq_puts(seq, ",confine");
+	else
+		seq_puts(seq, ",noconfine");
+	if (test_opt(sbi, GID_DERIVATION))
+		seq_puts(seq, ",derive_gid");
+	if (test_opt(sbi, DEFAULT_NORMAL))
+		seq_puts(seq, ",default_normal");
+	if (test_opt(sbi, SPECIAL_DOWNLOAD)) {
+		seq_printf(seq, ",dl_loc=%s", sbi->dl_loc);
+		seq_printf(seq, ",dl_uid=%d", sbi->lower_dl_perms.raw_uid);
+		seq_printf(seq, ",dl_gid=%d", sbi->lower_dl_perms.raw_gid);
+	}
+	return 0;
+}
+
+const struct super_operations esdfs_sops = {
+	.put_super	= esdfs_put_super,
+	.statfs		= esdfs_statfs,
+	.remount_fs	= esdfs_remount_fs,
+	.evict_inode	= esdfs_evict_inode,
+	.umount_begin	= esdfs_umount_begin,
+	.show_options	= esdfs_show_options,
+	.alloc_inode	= esdfs_alloc_inode,
+	.destroy_inode	= esdfs_destroy_inode,
+	.drop_inode	= generic_delete_inode,
+};
diff -ruN a/fs/eventfd.c b/fs/eventfd.c
--- a/fs/eventfd.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/eventfd.c	2021-12-23 08:35:51.000000000 +0100
@@ -170,8 +170,21 @@
 	 */
 	count = READ_ONCE(ctx->count);
 
-	if (count > 0)
+	if (count > 0) {
+		if ((ctx->flags & EFD_ZERO_ON_WAKE) &&
+				(poll_requested_events(wait) & EPOLLIN)) {
+			/*
+			 * We're going to cause a wake on EPOLLIN, we need to zero the count.
+			 * We validate that EPOLLIN is a requested event because if the user
+			 * did something odd like POLLPRI we wouldn't want to zero the count
+			 * if no wake happens.
+			 */
+			spin_lock_irq(&ctx->wqh.lock);
+			ctx->count = 0;
+			spin_unlock_irq(&ctx->wqh.lock);
+		}
 		events |= EPOLLIN;
+	}
 	if (count == ULLONG_MAX)
 		events |= EPOLLERR;
 	if (ULLONG_MAX - 1 > count)
@@ -237,8 +250,11 @@
 		__add_wait_queue(&ctx->wqh, &wait);
 		for (;;) {
 			set_current_state(TASK_INTERRUPTIBLE);
-			if (ctx->count)
+			if (ctx->count) {
+				if (ctx->flags & EFD_ZERO_ON_WAKE)
+					ctx->count = 0;
 				break;
+			}
 			if (signal_pending(current)) {
 				__remove_wait_queue(&ctx->wqh, &wait);
 				__set_current_state(TASK_RUNNING);
@@ -278,6 +294,18 @@
 		return -EINVAL;
 	spin_lock_irq(&ctx->wqh.lock);
 	res = -EAGAIN;
+
+	/*
+	 * In the case of EFD_ZERO_ON_WAKE the actual count is never needed, for this
+	 * reason we only adjust it to set it from 0 to 1 or 1 to 0. This means that
+	 * write will never return EWOULDBLOCK or block, because there is always
+	 * going to be enough space to write as the amount we will increment could
+	 * be at most 1 as it's clamped below. Additionally, we know that POLLERR
+	 * cannot be returned when EFD_ZERO_ON_WAKE is used for the same reason.
+	 */
+	if (ctx->flags & EFD_ZERO_ON_WAKE)
+		ucnt = (ctx->count == 0) ? 1 : 0;
+
 	if (ULLONG_MAX - ctx->count > ucnt)
 		res = sizeof(ucnt);
 	else if (!(file->f_flags & O_NONBLOCK)) {
@@ -412,9 +440,16 @@
 	BUILD_BUG_ON(EFD_CLOEXEC != O_CLOEXEC);
 	BUILD_BUG_ON(EFD_NONBLOCK != O_NONBLOCK);
 
+	/* O_NOFOLLOW has been repurposed as EFD_ZERO_ON_WAKE */
+	BUILD_BUG_ON(EFD_ZERO_ON_WAKE != O_NOFOLLOW);
+
 	if (flags & ~EFD_FLAGS_SET)
 		return -EINVAL;
 
+	/* The semaphore semantics would be lost if using EFD_ZERO_ON_WAKE */
+	if ((flags & EFD_ZERO_ON_WAKE) && (flags & EFD_SEMAPHORE))
+		return -EINVAL;
+
 	ctx = kmalloc(sizeof(*ctx), GFP_KERNEL);
 	if (!ctx)
 		return -ENOMEM;
diff -ruN a/fs/eventpoll.c b/fs/eventpoll.c
--- a/fs/eventpoll.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/eventpoll.c	2021-12-23 08:35:51.000000000 +0100
@@ -29,6 +29,7 @@
 #include <linux/mutex.h>
 #include <linux/anon_inodes.h>
 #include <linux/device.h>
+#include <linux/freezer.h>
 #include <linux/uaccess.h>
 #include <asm/io.h>
 #include <asm/mman.h>
@@ -1912,8 +1913,8 @@
 		write_unlock_irq(&ep->lock);
 
 		if (!eavail)
-			timed_out = !schedule_hrtimeout_range(to, slack,
-							      HRTIMER_MODE_ABS);
+			timed_out = !freezable_schedule_hrtimeout_range(to, slack,
+							      HRTIMER_MODE_ABS);
 		__set_current_state(TASK_RUNNING);
 
 		/*
diff -ruN a/fs/exec.c b/fs/exec.c
--- a/fs/exec.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/exec.c	2021-12-23 08:35:51.000000000 +0100
@@ -70,6 +70,7 @@
 #include <asm/mmu_context.h>
 #include <asm/tlb.h>
 
+#include <trace/events/fs.h>
 #include <trace/events/task.h>
 #include "internal.h"
 
@@ -927,9 +928,12 @@
 	if (err)
 		goto exit;
 
-	if (name->name[0] != '\0')
+	if (name->name[0] != '\0') {
 		fsnotify_open(file);
 
+		trace_open_exec(name->name);
+	}
+
 out:
 	return file;
 
diff -ruN a/fs/ext2/xattr_security.c b/fs/ext2/xattr_security.c
--- a/fs/ext2/xattr_security.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ext2/xattr_security.c	2021-12-23 08:35:51.000000000 +0100
@@ -11,7 +11,7 @@
 static int
 ext2_xattr_security_get(const struct xattr_handler *handler,
 			struct dentry *unused, struct inode *inode,
-			const char *name, void *buffer, size_t size)
+			const char *name, void *buffer, size_t size, int flags)
 {
 	return ext2_xattr_get(inode, EXT2_XATTR_INDEX_SECURITY, name,
 			      buffer, size);
diff -ruN a/fs/ext2/xattr_trusted.c b/fs/ext2/xattr_trusted.c
--- a/fs/ext2/xattr_trusted.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ext2/xattr_trusted.c	2021-12-23 08:35:51.000000000 +0100
@@ -18,7 +18,7 @@
 static int
 ext2_xattr_trusted_get(const struct xattr_handler *handler,
 		       struct dentry *unused, struct inode *inode,
-		       const char *name, void *buffer, size_t size)
+		       const char *name, void *buffer, size_t size, int flags)
 {
 	return ext2_xattr_get(inode, EXT2_XATTR_INDEX_TRUSTED, name,
 			      buffer, size);
diff -ruN a/fs/ext2/xattr_user.c b/fs/ext2/xattr_user.c
--- a/fs/ext2/xattr_user.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ext2/xattr_user.c	2021-12-23 08:35:51.000000000 +0100
@@ -20,7 +20,7 @@
 static int
 ext2_xattr_user_get(const struct xattr_handler *handler,
 		    struct dentry *unused, struct inode *inode,
-		    const char *name, void *buffer, size_t size)
+		    const char *name, void *buffer, size_t size, int flags)
 {
 	if (!test_opt(inode->i_sb, XATTR_USER))
 		return -EOPNOTSUPP;
diff -ruN a/fs/ext4/xattr_hurd.c b/fs/ext4/xattr_hurd.c
--- a/fs/ext4/xattr_hurd.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ext4/xattr_hurd.c	2021-12-23 08:35:51.000000000 +0100
@@ -21,7 +21,8 @@
 static int
 ext4_xattr_hurd_get(const struct xattr_handler *handler,
 		    struct dentry *unused, struct inode *inode,
-		    const char *name, void *buffer, size_t size)
+		    const char *name, void *buffer, size_t size,
+		    int flags)
 {
 	if (!test_opt(inode->i_sb, XATTR_USER))
 		return -EOPNOTSUPP;
diff -ruN a/fs/ext4/xattr_security.c b/fs/ext4/xattr_security.c
--- a/fs/ext4/xattr_security.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ext4/xattr_security.c	2021-12-23 08:35:51.000000000 +0100
@@ -15,7 +15,7 @@
 static int
 ext4_xattr_security_get(const struct xattr_handler *handler,
 			struct dentry *unused, struct inode *inode,
-			const char *name, void *buffer, size_t size)
+			const char *name, void *buffer, size_t size, int flags)
 {
 	return ext4_xattr_get(inode, EXT4_XATTR_INDEX_SECURITY,
 			      name, buffer, size);
diff -ruN a/fs/ext4/xattr_trusted.c b/fs/ext4/xattr_trusted.c
--- a/fs/ext4/xattr_trusted.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ext4/xattr_trusted.c	2021-12-23 08:35:51.000000000 +0100
@@ -22,7 +22,7 @@
 static int
 ext4_xattr_trusted_get(const struct xattr_handler *handler,
 		       struct dentry *unused, struct inode *inode,
-		       const char *name, void *buffer, size_t size)
+		       const char *name, void *buffer, size_t size, int flags)
 {
 	return ext4_xattr_get(inode, EXT4_XATTR_INDEX_TRUSTED,
 			      name, buffer, size);
diff -ruN a/fs/ext4/xattr_user.c b/fs/ext4/xattr_user.c
--- a/fs/ext4/xattr_user.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ext4/xattr_user.c	2021-12-23 08:35:51.000000000 +0100
@@ -21,7 +21,7 @@
 static int
 ext4_xattr_user_get(const struct xattr_handler *handler,
 		    struct dentry *unused, struct inode *inode,
-		    const char *name, void *buffer, size_t size)
+		    const char *name, void *buffer, size_t size, int flags)
 {
 	if (!test_opt(inode->i_sb, XATTR_USER))
 		return -EOPNOTSUPP;
diff -ruN a/fs/f2fs/xattr.c b/fs/f2fs/xattr.c
--- a/fs/f2fs/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/f2fs/xattr.c	2021-12-23 08:35:52.000000000 +0100
@@ -45,7 +45,7 @@
 
 static int f2fs_xattr_generic_get(const struct xattr_handler *handler,
 		struct dentry *unused, struct inode *inode,
-		const char *name, void *buffer, size_t size)
+		const char *name, void *buffer, size_t size, int flags)
 {
 	struct f2fs_sb_info *sbi = F2FS_SB(inode->i_sb);
 
@@ -101,7 +101,7 @@
 
 static int f2fs_xattr_advise_get(const struct xattr_handler *handler,
 		struct dentry *unused, struct inode *inode,
-		const char *name, void *buffer, size_t size)
+		const char *name, void *buffer, size_t size, int flags)
 {
 	if (buffer)
 		*((char *)buffer) = F2FS_I(inode)->i_advise;
diff -ruN a/fs/fuse/control.c b/fs/fuse/control.c
--- a/fs/fuse/control.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/fuse/control.c	2021-12-23 08:35:52.000000000 +0100
@@ -64,6 +64,33 @@
 	return simple_read_from_buffer(buf, len, ppos, tmp, size);
 }
 
+static ssize_t fuse_conn_file_system_read(struct file *file, char __user *buf,
+					  size_t len, loff_t *ppos)
+{
+	char tmp[32];
+	size_t size;
+
+	if (!*ppos) {
+		struct fuse_conn *fc = fuse_ctl_file_conn_get(file);
+
+		if (!fc)
+			return 0;
+		down_read(&fc->killsb);
+		if (!list_empty(&fc->mounts)) {
+			struct fuse_mount *fm;
+
+			fm = list_first_entry(&fc->mounts, struct fuse_mount, fc_entry);
+			file->private_data = (void *)fm->sb->s_type->name;
+		} else {
+			file->private_data = "(NULL)";
+		}
+		up_read(&fc->killsb);
+		fuse_conn_put(fc);
+	}
+	size = sprintf(tmp, "%.30s\n", (char *)file->private_data);
+	return simple_read_from_buffer(buf, len, ppos, tmp, size);
+}
+
 static ssize_t fuse_conn_limit_read(struct file *file, char __user *buf,
 				    size_t len, loff_t *ppos, unsigned val)
 {
@@ -227,6 +254,12 @@
 	.llseek = no_llseek,
 };
 
+static const struct file_operations fuse_conn_file_system_ops = {
+	.open = nonseekable_open,
+	.read = fuse_conn_file_system_read,
+	.llseek = no_llseek,
+};
+
 static struct dentry *fuse_ctl_add_dentry(struct dentry *parent,
 					  struct fuse_conn *fc,
 					  const char *name,
@@ -295,7 +328,9 @@
 				 1, NULL, &fuse_conn_max_background_ops) ||
 	    !fuse_ctl_add_dentry(parent, fc, "congestion_threshold",
 				 S_IFREG | 0600, 1, NULL,
-				 &fuse_conn_congestion_threshold_ops))
+				 &fuse_conn_congestion_threshold_ops) ||
+	    !fuse_ctl_add_dentry(parent, fc, "filesystem", S_IFREG | 0400, 1,
+				 NULL, &fuse_conn_file_system_ops))
 		goto err;
 
 	return 0;
diff -ruN a/fs/fuse/dev.c b/fs/fuse/dev.c
--- a/fs/fuse/dev.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/fuse/dev.c	2021-12-23 08:35:52.000000000 +0100
@@ -2266,7 +2266,8 @@
 				 * uses the same ioctl handler.
 				 */
 				if (old->f_op == file->f_op &&
-				    old->f_cred->user_ns == file->f_cred->user_ns)
+				    old->f_cred->user_ns ==
+					    file->f_cred->user_ns)
 					fud = fuse_get_dev(old);
 
 				if (fud) {
diff -ruN a/fs/fuse/dir.c b/fs/fuse/dir.c
--- a/fs/fuse/dir.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/fuse/dir.c	2021-12-23 08:35:52.000000000 +0100
@@ -468,6 +468,7 @@
 {
 	int err;
 	struct inode *inode;
+	struct fuse_conn *fc = get_fuse_conn(dir);
 	struct fuse_mount *fm = get_fuse_mount(dir);
 	FUSE_ARGS(args);
 	struct fuse_forget_link *forget;
@@ -529,6 +530,7 @@
 	ff->fh = outopen.fh;
 	ff->nodeid = outentry.nodeid;
 	ff->open_flags = outopen.open_flags;
+	fuse_passthrough_setup(fc, ff, &outopen);
 	inode = fuse_iget(dir->i_sb, outentry.nodeid, outentry.generation,
 			  &outentry.attr, entry_attr_timeout(&outentry), 0);
 	if (!inode) {
@@ -722,6 +724,27 @@
 	return create_new_entry(fm, &args, dir, entry, S_IFDIR);
 }
 
+static int fuse_chromeos_tmpfile(struct user_namespace *mnt_userns, struct inode *dir,
+				 struct dentry *entry, umode_t mode)
+{
+	struct fuse_chromeos_tmpfile_in inarg;
+	struct fuse_mount *fm = get_fuse_mount(dir);
+	FUSE_ARGS(args);
+
+	if (!fm->fc->dont_mask)
+		mode &= ~current_umask();
+
+	memset(&inarg, 0, sizeof(inarg));
+	inarg.mode = mode;
+	inarg.umask = current_umask();
+	args.opcode = FUSE_CHROMEOS_TMPFILE;
+	args.in_numargs = 1;
+	args.in_args[0].size = sizeof(inarg);
+	args.in_args[0].value = &inarg;
+
+	return create_new_entry(fm, &args, dir, entry, S_IFREG);
+}
+
 static int fuse_symlink(struct user_namespace *mnt_userns, struct inode *dir,
 			struct dentry *entry, const char *link)
 {
@@ -1824,6 +1847,7 @@
 	.set_acl	= fuse_set_acl,
 	.fileattr_get	= fuse_fileattr_get,
 	.fileattr_set	= fuse_fileattr_set,
+	.tmpfile	= fuse_chromeos_tmpfile,
 };
 
 static const struct file_operations fuse_dir_operations = {
diff -ruN a/fs/fuse/file.c b/fs/fuse/file.c
--- a/fs/fuse/file.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/fuse/file.c	2021-12-23 08:35:52.000000000 +0100
@@ -27,7 +27,7 @@
 	FUSE_ARGS(args);
 
 	memset(&inarg, 0, sizeof(inarg));
-	inarg.flags = open_flags & ~(O_CREAT | O_EXCL | O_NOCTTY);
+	inarg.flags = open_flags & ~(O_CREAT | O_EXCL | O_NOCTTY | O_TMPFILE);
 	if (!fm->fc->atomic_o_trunc)
 		inarg.flags &= ~O_TRUNC;
 
@@ -146,7 +146,7 @@
 		if (!err) {
 			ff->fh = outarg.fh;
 			ff->open_flags = outarg.open_flags;
-
+			fuse_passthrough_setup(fc, ff, &outarg);
 		} else if (err != -ENOSYS) {
 			fuse_file_free(ff);
 			return ERR_PTR(err);
@@ -305,6 +305,8 @@
 	struct fuse_release_args *ra = ff->release_args;
 	int opcode = isdir ? FUSE_RELEASEDIR : FUSE_RELEASE;
 
+	fuse_passthrough_release(&ff->passthrough);
+
 	fuse_prepare_release(fi, ff, open_flags, opcode);
 
 	if (ff->flock) {
@@ -1583,7 +1585,9 @@
 	if (FUSE_IS_DAX(inode))
 		return fuse_dax_read_iter(iocb, to);
 
-	if (!(ff->open_flags & FOPEN_DIRECT_IO))
+	if (ff->passthrough.filp)
+		return fuse_passthrough_read_iter(iocb, to);
+	else if (!(ff->open_flags & FOPEN_DIRECT_IO))
 		return fuse_cache_read_iter(iocb, to);
 	else
 		return fuse_direct_read_iter(iocb, to);
@@ -1601,7 +1605,9 @@
 	if (FUSE_IS_DAX(inode))
 		return fuse_dax_write_iter(iocb, from);
 
-	if (!(ff->open_flags & FOPEN_DIRECT_IO))
+	if (ff->passthrough.filp)
+		return fuse_passthrough_write_iter(iocb, from);
+	else if (!(ff->open_flags & FOPEN_DIRECT_IO))
 		return fuse_cache_write_iter(iocb, from);
 	else
 		return fuse_direct_write_iter(iocb, from);
@@ -2394,6 +2400,9 @@
 	if (FUSE_IS_DAX(file_inode(file)))
 		return fuse_dax_mmap(file, vma);
 
+	if (ff->passthrough.filp)
+		return fuse_passthrough_mmap(file, vma);
+
 	if (ff->open_flags & FOPEN_DIRECT_IO) {
 		/* Can't provide the coherency needed for MAP_SHARED */
 		if (vma->vm_flags & VM_MAYSHARE)
diff -ruN a/fs/fuse/fuse_i.h b/fs/fuse/fuse_i.h
--- a/fs/fuse/fuse_i.h	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/fuse/fuse_i.h	2021-12-23 08:35:52.000000000 +0100
@@ -45,7 +45,7 @@
 #define FUSE_NAME_MAX 1024
 
 /** Number of dentries for each connection in the control filesystem */
-#define FUSE_CTL_NUM_DENTRIES 5
+#define FUSE_CTL_NUM_DENTRIES 6
 
 /** List of active connections */
 extern struct list_head fuse_conn_list;
@@ -173,6 +173,17 @@
 struct fuse_mount;
 struct fuse_release_args;
 
+/**
+ * Reference to lower filesystem file for read/write operations handled in
+ * passthrough mode.
+ * This struct also tracks the credentials to be used for handling read/write
+ * operations.
+ */
+struct fuse_passthrough {
+	struct file *filp;
+	struct cred *cred;
+};
+
 /** FUSE specific file data */
 struct fuse_file {
 	/** Fuse connection for this file */
@@ -218,6 +229,9 @@
 
 	} readdir;
 
+	/** Container for data related to the passthrough functionality */
+	struct fuse_passthrough passthrough;
+
 	/** RB node to be linked on fuse_conn->polled_files */
 	struct rb_node polled_node;
 
@@ -762,6 +776,9 @@
 	/* Auto-mount submounts announced by the server */
 	unsigned int auto_submounts:1;
 
+	/** Passthrough mode for read/write IO */
+	unsigned int passthrough:1;
+
 	/* Propagate syncfs() to server */
 	unsigned int sync_fs:1;
 
@@ -811,6 +828,12 @@
 
 	/* New writepages go into this bucket */
 	struct fuse_sync_bucket __rcu *curr_bucket;
+
+	/** IDR for passthrough requests */
+	struct idr passthrough_req;
+
+	/** Protects passthrough_req */
+	spinlock_t passthrough_req_lock;
 };
 
 /*
@@ -1281,4 +1304,14 @@
 void fuse_file_release(struct inode *inode, struct fuse_file *ff,
 		       unsigned int open_flags, fl_owner_t id, bool isdir);
 
+/* passthrough.c */
+int fuse_passthrough_open(struct fuse_dev *fud,
+			  struct fuse_passthrough_out *pto);
+int fuse_passthrough_setup(struct fuse_conn *fc, struct fuse_file *ff,
+			   struct fuse_open_out *openarg);
+void fuse_passthrough_release(struct fuse_passthrough *passthrough);
+ssize_t fuse_passthrough_read_iter(struct kiocb *iocb, struct iov_iter *to);
+ssize_t fuse_passthrough_write_iter(struct kiocb *iocb, struct iov_iter *from);
+ssize_t fuse_passthrough_mmap(struct file *file, struct vm_area_struct *vma);
+
 #endif /* _FS_FUSE_I_H */
diff -ruN a/fs/fuse/inode.c b/fs/fuse/inode.c
--- a/fs/fuse/inode.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/fuse/inode.c	2021-12-23 08:35:52.000000000 +0100
@@ -774,6 +774,7 @@
 	memset(fc, 0, sizeof(*fc));
 	spin_lock_init(&fc->lock);
 	spin_lock_init(&fc->bg_lock);
+	spin_lock_init(&fc->passthrough_req_lock);
 	init_rwsem(&fc->killsb);
 	refcount_set(&fc->count, 1);
 	atomic_set(&fc->dev_count, 1);
@@ -782,6 +783,7 @@
 	INIT_LIST_HEAD(&fc->bg_queue);
 	INIT_LIST_HEAD(&fc->entry);
 	INIT_LIST_HEAD(&fc->devices);
+	idr_init(&fc->passthrough_req);
 	atomic_set(&fc->num_waiting, 0);
 	fc->max_background = FUSE_DEFAULT_MAX_BACKGROUND;
 	fc->congestion_threshold = FUSE_DEFAULT_CONGESTION_THRESHOLD;
@@ -1141,6 +1143,12 @@
 				fc->handle_killpriv_v2 = 1;
 				fm->sb->s_flags |= SB_NOSEC;
 			}
+			if (arg->flags & FUSE_PASSTHROUGH) {
+				fc->passthrough = 1;
+				/* Prevent further stacking */
+				fm->sb->s_stack_depth =
+					FILESYSTEM_MAX_STACK_DEPTH;
+			}
 			if (arg->flags & FUSE_SETXATTR_EXT)
 				fc->setxattr_ext = 1;
 		} else {
@@ -1186,6 +1194,7 @@
 		FUSE_PARALLEL_DIROPS | FUSE_HANDLE_KILLPRIV | FUSE_POSIX_ACL |
 		FUSE_ABORT_ERROR | FUSE_MAX_PAGES | FUSE_CACHE_SYMLINKS |
 		FUSE_NO_OPENDIR_SUPPORT | FUSE_EXPLICIT_INVAL_DATA |
+		FUSE_PASSTHROUGH |
 		FUSE_HANDLE_KILLPRIV_V2 | FUSE_SETXATTR_EXT;
 #ifdef CONFIG_FUSE_DAX
 	if (fm->fc->dax)
@@ -1214,9 +1223,21 @@
 }
 EXPORT_SYMBOL_GPL(fuse_send_init);
 
+static int free_fuse_passthrough(int id, void *p, void *data)
+{
+	struct fuse_passthrough *passthrough = (struct fuse_passthrough *)p;
+
+	fuse_passthrough_release(passthrough);
+	kfree(p);
+
+	return 0;
+}
+
 void fuse_free_conn(struct fuse_conn *fc)
 {
 	WARN_ON(!list_empty(&fc->devices));
+	idr_for_each(&fc->passthrough_req, free_fuse_passthrough, NULL);
+	idr_destroy(&fc->passthrough_req);
 	kfree_rcu(fc, rcu);
 }
 EXPORT_SYMBOL_GPL(fuse_free_conn);
diff -ruN a/fs/fuse/Makefile b/fs/fuse/Makefile
--- a/fs/fuse/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/fuse/Makefile	2021-12-23 08:35:52.000000000 +0100
@@ -8,6 +8,7 @@
 obj-$(CONFIG_VIRTIO_FS) += virtiofs.o
 
 fuse-y := dev.o dir.o file.o inode.o control.o xattr.o acl.o readdir.o ioctl.o
+fuse-y += passthrough.o
 fuse-$(CONFIG_FUSE_DAX) += dax.o
 
 virtiofs-y := virtio_fs.o
diff -ruN a/fs/fuse/passthrough.c b/fs/fuse/passthrough.c
--- a/fs/fuse/passthrough.c	1970-01-01 01:00:00.000000000 +0100
+++ b/fs/fuse/passthrough.c	2021-12-23 08:35:52.000000000 +0100
@@ -0,0 +1,283 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#include "fuse_i.h"
+
+#include <linux/fuse.h>
+#include <linux/idr.h>
+#include <linux/uio.h>
+
+#define PASSTHROUGH_IOCB_MASK                                                  \
+	(IOCB_APPEND | IOCB_DSYNC | IOCB_HIPRI | IOCB_NOWAIT | IOCB_SYNC)
+
+struct fuse_aio_req {
+	struct kiocb iocb;
+	struct kiocb *iocb_fuse;
+};
+
+static void fuse_copyattr(struct file *dst_file, struct file *src_file)
+{
+	struct inode *dst = file_inode(dst_file);
+	struct inode *src = file_inode(src_file);
+
+	i_size_write(dst, i_size_read(src));
+}
+
+static void fuse_aio_cleanup_handler(struct fuse_aio_req *aio_req)
+{
+	struct kiocb *iocb = &aio_req->iocb;
+	struct kiocb *iocb_fuse = aio_req->iocb_fuse;
+
+	if (iocb->ki_flags & IOCB_WRITE) {
+		__sb_writers_acquired(file_inode(iocb->ki_filp)->i_sb,
+				      SB_FREEZE_WRITE);
+		file_end_write(iocb->ki_filp);
+		fuse_copyattr(iocb_fuse->ki_filp, iocb->ki_filp);
+	}
+
+	iocb_fuse->ki_pos = iocb->ki_pos;
+	kfree(aio_req);
+}
+
+static void fuse_aio_rw_complete(struct kiocb *iocb, long res, long res2)
+{
+	struct fuse_aio_req *aio_req =
+		container_of(iocb, struct fuse_aio_req, iocb);
+	struct kiocb *iocb_fuse = aio_req->iocb_fuse;
+
+	fuse_aio_cleanup_handler(aio_req);
+	iocb_fuse->ki_complete(iocb_fuse, res, res2);
+}
+
+ssize_t fuse_passthrough_read_iter(struct kiocb *iocb_fuse,
+				   struct iov_iter *iter)
+{
+	ssize_t ret;
+	const struct cred *old_cred;
+	struct file *fuse_filp = iocb_fuse->ki_filp;
+	struct fuse_file *ff = fuse_filp->private_data;
+	struct file *passthrough_filp = ff->passthrough.filp;
+
+	if (!iov_iter_count(iter))
+		return 0;
+
+	old_cred = override_creds(ff->passthrough.cred);
+	if (is_sync_kiocb(iocb_fuse)) {
+		ret = vfs_iter_read(passthrough_filp, iter, &iocb_fuse->ki_pos,
+				    iocb_to_rw_flags(iocb_fuse->ki_flags,
+						     PASSTHROUGH_IOCB_MASK));
+	} else {
+		struct fuse_aio_req *aio_req;
+
+		aio_req = kmalloc(sizeof(struct fuse_aio_req), GFP_KERNEL);
+		if (!aio_req) {
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		aio_req->iocb_fuse = iocb_fuse;
+		kiocb_clone(&aio_req->iocb, iocb_fuse, passthrough_filp);
+		aio_req->iocb.ki_complete = fuse_aio_rw_complete;
+		ret = call_read_iter(passthrough_filp, &aio_req->iocb, iter);
+		if (ret != -EIOCBQUEUED)
+			fuse_aio_cleanup_handler(aio_req);
+	}
+out:
+	revert_creds(old_cred);
+
+	return ret;
+}
+
+ssize_t fuse_passthrough_write_iter(struct kiocb *iocb_fuse,
+				    struct iov_iter *iter)
+{
+	ssize_t ret;
+	const struct cred *old_cred;
+	struct file *fuse_filp = iocb_fuse->ki_filp;
+	struct fuse_file *ff = fuse_filp->private_data;
+	struct inode *fuse_inode = file_inode(fuse_filp);
+	struct file *passthrough_filp = ff->passthrough.filp;
+	struct inode *passthrough_inode = file_inode(passthrough_filp);
+
+	if (!iov_iter_count(iter))
+		return 0;
+
+	inode_lock(fuse_inode);
+
+	old_cred = override_creds(ff->passthrough.cred);
+	if (is_sync_kiocb(iocb_fuse)) {
+		file_start_write(passthrough_filp);
+		ret = vfs_iter_write(passthrough_filp, iter, &iocb_fuse->ki_pos,
+				     iocb_to_rw_flags(iocb_fuse->ki_flags,
+						      PASSTHROUGH_IOCB_MASK));
+		file_end_write(passthrough_filp);
+		if (ret > 0)
+			fuse_copyattr(fuse_filp, passthrough_filp);
+	} else {
+		struct fuse_aio_req *aio_req;
+
+		aio_req = kmalloc(sizeof(struct fuse_aio_req), GFP_KERNEL);
+		if (!aio_req) {
+			ret = -ENOMEM;
+			goto out;
+		}
+
+		file_start_write(passthrough_filp);
+		__sb_writers_release(passthrough_inode->i_sb, SB_FREEZE_WRITE);
+
+		aio_req->iocb_fuse = iocb_fuse;
+		kiocb_clone(&aio_req->iocb, iocb_fuse, passthrough_filp);
+		aio_req->iocb.ki_complete = fuse_aio_rw_complete;
+		ret = call_write_iter(passthrough_filp, &aio_req->iocb, iter);
+		if (ret != -EIOCBQUEUED)
+			fuse_aio_cleanup_handler(aio_req);
+	}
+out:
+	revert_creds(old_cred);
+	inode_unlock(fuse_inode);
+
+	return ret;
+}
+
+ssize_t fuse_passthrough_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	int ret;
+	const struct cred *old_cred;
+	struct fuse_file *ff = file->private_data;
+	struct inode *fuse_inode = file_inode(file);
+	struct file *passthrough_filp = ff->passthrough.filp;
+	struct inode *passthrough_inode = file_inode(passthrough_filp);
+
+	if (!passthrough_filp->f_op->mmap)
+		return -ENODEV;
+
+	if (WARN_ON(file != vma->vm_file))
+		return -EIO;
+
+	vma->vm_file = get_file(passthrough_filp);
+
+	old_cred = override_creds(ff->passthrough.cred);
+	ret = call_mmap(vma->vm_file, vma);
+	revert_creds(old_cred);
+
+	if (ret)
+		fput(passthrough_filp);
+	else
+		fput(file);
+
+	if (file->f_flags & O_NOATIME)
+		return ret;
+
+	if ((!timespec64_equal(&fuse_inode->i_mtime,
+			       &passthrough_inode->i_mtime) ||
+	     !timespec64_equal(&fuse_inode->i_ctime,
+			       &passthrough_inode->i_ctime))) {
+		fuse_inode->i_mtime = passthrough_inode->i_mtime;
+		fuse_inode->i_ctime = passthrough_inode->i_ctime;
+	}
+	touch_atime(&file->f_path);
+
+	return ret;
+}
+
+int fuse_passthrough_open(struct fuse_dev *fud,
+			  struct fuse_passthrough_out *pto)
+{
+	int res;
+	struct file *passthrough_filp;
+	struct fuse_conn *fc = fud->fc;
+	struct inode *passthrough_inode;
+	struct super_block *passthrough_sb;
+	struct fuse_passthrough *passthrough;
+
+	if (!fc->passthrough)
+		return -EPERM;
+
+	/* This field is reserved for future implementation */
+	if (pto->len != 0)
+		return -EINVAL;
+
+	passthrough_filp = fget(pto->fd);
+	if (!passthrough_filp) {
+		pr_err("FUSE: invalid file descriptor for passthrough.\n");
+		return -EBADF;
+	}
+
+	if (!passthrough_filp->f_op->read_iter ||
+	    !passthrough_filp->f_op->write_iter) {
+		pr_err("FUSE: passthrough file misses file operations.\n");
+		res = -EBADF;
+		goto err_free_file;
+	}
+
+	passthrough_inode = file_inode(passthrough_filp);
+	passthrough_sb = passthrough_inode->i_sb;
+	if (passthrough_sb->s_stack_depth >= FILESYSTEM_MAX_STACK_DEPTH) {
+		pr_err("FUSE: fs stacking depth exceeded for passthrough\n");
+		res = -EINVAL;
+		goto err_free_file;
+	}
+
+	passthrough = kmalloc(sizeof(struct fuse_passthrough), GFP_KERNEL);
+	if (!passthrough) {
+		res = -ENOMEM;
+		goto err_free_file;
+	}
+
+	passthrough->filp = passthrough_filp;
+	passthrough->cred = prepare_creds();
+
+	idr_preload(GFP_KERNEL);
+	spin_lock(&fc->passthrough_req_lock);
+	res = idr_alloc(&fc->passthrough_req, passthrough, 1, 0, GFP_ATOMIC);
+	spin_unlock(&fc->passthrough_req_lock);
+	idr_preload_end();
+
+	if (res > 0)
+		return res;
+
+	fuse_passthrough_release(passthrough);
+	kfree(passthrough);
+
+err_free_file:
+	fput(passthrough_filp);
+
+	return res;
+}
+
+int fuse_passthrough_setup(struct fuse_conn *fc, struct fuse_file *ff,
+			   struct fuse_open_out *openarg)
+{
+	struct fuse_passthrough *passthrough;
+	int passthrough_fh = openarg->passthrough_fh;
+
+	if (!fc->passthrough)
+		return -EPERM;
+
+	/* Default case, passthrough is not requested */
+	if (passthrough_fh <= 0)
+		return -EINVAL;
+
+	spin_lock(&fc->passthrough_req_lock);
+	passthrough = idr_remove(&fc->passthrough_req, passthrough_fh);
+	spin_unlock(&fc->passthrough_req_lock);
+
+	if (!passthrough)
+		return -EINVAL;
+
+	ff->passthrough = *passthrough;
+	kfree(passthrough);
+
+	return 0;
+}
+
+void fuse_passthrough_release(struct fuse_passthrough *passthrough)
+{
+	if (passthrough->filp) {
+		fput(passthrough->filp);
+		passthrough->filp = NULL;
+	}
+	if (passthrough->cred) {
+		put_cred(passthrough->cred);
+		passthrough->cred = NULL;
+	}
+}
diff -ruN a/fs/fuse/xattr.c b/fs/fuse/xattr.c
--- a/fs/fuse/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/fuse/xattr.c	2021-12-23 08:35:52.000000000 +0100
@@ -182,7 +182,7 @@
 
 static int fuse_xattr_get(const struct xattr_handler *handler,
 			 struct dentry *dentry, struct inode *inode,
-			 const char *name, void *value, size_t size)
+			 const char *name, void *value, size_t size, int flags)
 {
 	if (fuse_is_bad(inode))
 		return -EIO;
@@ -212,7 +212,7 @@
 
 static int no_xattr_get(const struct xattr_handler *handler,
 			struct dentry *dentry, struct inode *inode,
-			const char *name, void *value, size_t size)
+			const char *name, void *value, size_t size, int flags)
 {
 	return -EOPNOTSUPP;
 }
diff -ruN a/fs/gfs2/xattr.c b/fs/gfs2/xattr.c
--- a/fs/gfs2/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/gfs2/xattr.c	2021-12-23 08:35:52.000000000 +0100
@@ -602,7 +602,8 @@
 
 static int gfs2_xattr_get(const struct xattr_handler *handler,
 			  struct dentry *unused, struct inode *inode,
-			  const char *name, void *buffer, size_t size)
+			  const char *name, void *buffer, size_t size,
+			  int flags)
 {
 	struct gfs2_inode *ip = GFS2_I(inode);
 	struct gfs2_holder gh;
diff -ruN a/fs/hfs/attr.c b/fs/hfs/attr.c
--- a/fs/hfs/attr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/hfs/attr.c	2021-12-23 08:35:52.000000000 +0100
@@ -115,7 +115,7 @@
 
 static int hfs_xattr_get(const struct xattr_handler *handler,
 			 struct dentry *unused, struct inode *inode,
-			 const char *name, void *value, size_t size)
+			 const char *name, void *value, size_t size, int flags)
 {
 	return __hfs_getxattr(inode, handler->flags, value, size);
 }
diff -ruN a/fs/hfsplus/xattr.c b/fs/hfsplus/xattr.c
--- a/fs/hfsplus/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/hfsplus/xattr.c	2021-12-23 08:35:52.000000000 +0100
@@ -838,7 +838,8 @@
 
 static int hfsplus_osx_getxattr(const struct xattr_handler *handler,
 				struct dentry *unused, struct inode *inode,
-				const char *name, void *buffer, size_t size)
+				const char *name, void *buffer, size_t size,
+				int flags)
 {
 	/*
 	 * Don't allow retrieving properly prefixed attributes
diff -ruN a/fs/hfsplus/xattr_security.c b/fs/hfsplus/xattr_security.c
--- a/fs/hfsplus/xattr_security.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/hfsplus/xattr_security.c	2021-12-23 08:35:52.000000000 +0100
@@ -15,7 +15,8 @@
 
 static int hfsplus_security_getxattr(const struct xattr_handler *handler,
 				     struct dentry *unused, struct inode *inode,
-				     const char *name, void *buffer, size_t size)
+				     const char *name, void *buffer,
+				     size_t size, int flags)
 {
 	return hfsplus_getxattr(inode, name, buffer, size,
 				XATTR_SECURITY_PREFIX,
diff -ruN a/fs/hfsplus/xattr_trusted.c b/fs/hfsplus/xattr_trusted.c
--- a/fs/hfsplus/xattr_trusted.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/hfsplus/xattr_trusted.c	2021-12-23 08:35:52.000000000 +0100
@@ -14,7 +14,8 @@
 
 static int hfsplus_trusted_getxattr(const struct xattr_handler *handler,
 				    struct dentry *unused, struct inode *inode,
-				    const char *name, void *buffer, size_t size)
+				    const char *name, void *buffer,
+				    size_t size, int flags)
 {
 	return hfsplus_getxattr(inode, name, buffer, size,
 				XATTR_TRUSTED_PREFIX,
diff -ruN a/fs/hfsplus/xattr_user.c b/fs/hfsplus/xattr_user.c
--- a/fs/hfsplus/xattr_user.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/hfsplus/xattr_user.c	2021-12-23 08:35:52.000000000 +0100
@@ -14,7 +14,8 @@
 
 static int hfsplus_user_getxattr(const struct xattr_handler *handler,
 				 struct dentry *unused, struct inode *inode,
-				 const char *name, void *buffer, size_t size)
+				 const char *name, void *buffer, size_t size,
+				 int flags)
 {
 
 	return hfsplus_getxattr(inode, name, buffer, size,
diff -ruN a/fs/jffs2/security.c b/fs/jffs2/security.c
--- a/fs/jffs2/security.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/jffs2/security.c	2021-12-23 08:35:52.000000000 +0100
@@ -50,7 +50,8 @@
 /* ---- XATTR Handler for "security.*" ----------------- */
 static int jffs2_security_getxattr(const struct xattr_handler *handler,
 				   struct dentry *unused, struct inode *inode,
-				   const char *name, void *buffer, size_t size)
+				   const char *name, void *buffer, size_t size,
+				   int flags)
 {
 	return do_jffs2_getxattr(inode, JFFS2_XPREFIX_SECURITY,
 				 name, buffer, size);
diff -ruN a/fs/jffs2/xattr_trusted.c b/fs/jffs2/xattr_trusted.c
--- a/fs/jffs2/xattr_trusted.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/jffs2/xattr_trusted.c	2021-12-23 08:35:52.000000000 +0100
@@ -18,7 +18,8 @@
 
 static int jffs2_trusted_getxattr(const struct xattr_handler *handler,
 				  struct dentry *unused, struct inode *inode,
-				  const char *name, void *buffer, size_t size)
+				  const char *name, void *buffer, size_t size,
+				  int flags)
 {
 	return do_jffs2_getxattr(inode, JFFS2_XPREFIX_TRUSTED,
 				 name, buffer, size);
diff -ruN a/fs/jffs2/xattr_user.c b/fs/jffs2/xattr_user.c
--- a/fs/jffs2/xattr_user.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/jffs2/xattr_user.c	2021-12-23 08:35:52.000000000 +0100
@@ -18,7 +18,8 @@
 
 static int jffs2_user_getxattr(const struct xattr_handler *handler,
 			       struct dentry *unused, struct inode *inode,
-			       const char *name, void *buffer, size_t size)
+			       const char *name, void *buffer, size_t size,
+			       int flags)
 {
 	return do_jffs2_getxattr(inode, JFFS2_XPREFIX_USER,
 				 name, buffer, size);
diff -ruN a/fs/jfs/xattr.c b/fs/jfs/xattr.c
--- a/fs/jfs/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/jfs/xattr.c	2021-12-23 08:35:52.000000000 +0100
@@ -925,7 +925,7 @@
 
 static int jfs_xattr_get(const struct xattr_handler *handler,
 			 struct dentry *unused, struct inode *inode,
-			 const char *name, void *value, size_t size)
+			 const char *name, void *value, size_t size, int flags)
 {
 	name = xattr_full_name(handler, name);
 	return __jfs_getxattr(inode, name, value, size);
@@ -943,7 +943,8 @@
 
 static int jfs_xattr_get_os2(const struct xattr_handler *handler,
 			     struct dentry *unused, struct inode *inode,
-			     const char *name, void *value, size_t size)
+			     const char *name, void *value, size_t size,
+			     int flags)
 {
 	if (is_known_namespace(name))
 		return -EOPNOTSUPP;
diff -ruN a/fs/Kconfig b/fs/Kconfig
--- a/fs/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/Kconfig	2021-12-23 08:35:51.000000000 +0100
@@ -290,6 +290,7 @@
 source "fs/adfs/Kconfig"
 source "fs/affs/Kconfig"
 source "fs/ecryptfs/Kconfig"
+source "fs/esdfs/Kconfig"
 source "fs/hfs/Kconfig"
 source "fs/hfsplus/Kconfig"
 source "fs/befs/Kconfig"
diff -ruN a/fs/kernfs/inode.c b/fs/kernfs/inode.c
--- a/fs/kernfs/inode.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/kernfs/inode.c	2021-12-23 08:35:52.000000000 +0100
@@ -313,7 +313,8 @@
 
 static int kernfs_vfs_xattr_get(const struct xattr_handler *handler,
 				struct dentry *unused, struct inode *inode,
-				const char *suffix, void *value, size_t size)
+				const char *suffix, void *value, size_t size,
+				int flags)
 {
 	const char *name = xattr_full_name(handler, suffix);
 	struct kernfs_node *kn = inode->i_private;
diff -ruN a/fs/Makefile b/fs/Makefile
--- a/fs/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/Makefile	2021-12-23 08:35:51.000000000 +0100
@@ -88,6 +88,7 @@
 obj-$(CONFIG_HFSPLUS_FS)	+= hfsplus/ # Before hfs to find wrapped HFS+
 obj-$(CONFIG_HFS_FS)		+= hfs/
 obj-$(CONFIG_ECRYPT_FS)		+= ecryptfs/
+obj-$(CONFIG_ESD_FS)		+= esdfs/
 obj-$(CONFIG_VXFS_FS)		+= freevxfs/
 obj-$(CONFIG_NFS_FS)		+= nfs/
 obj-$(CONFIG_EXPORTFS)		+= exportfs/
diff -ruN a/fs/namei.c b/fs/namei.c
--- a/fs/namei.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/namei.c	2021-12-23 08:35:52.000000000 +0100
@@ -1020,8 +1020,8 @@
 		path_put(&last->link);
 }
 
-int sysctl_protected_symlinks __read_mostly = 0;
-int sysctl_protected_hardlinks __read_mostly = 0;
+int sysctl_protected_symlinks __read_mostly = 1;
+int sysctl_protected_hardlinks __read_mostly = 1;
 int sysctl_protected_fifos __read_mostly;
 int sysctl_protected_regular __read_mostly;
 
@@ -2484,6 +2484,9 @@
 	if (likely(!retval))
 		audit_inode(name, path->dentry,
 			    flags & LOOKUP_MOUNTPOINT ? AUDIT_INODE_NOEVAL : 0);
+#ifdef CONFIG_SECURITY_CHROMIUMOS_NO_SYMLINK_MOUNT
+	path->link_count = nd.total_link_count | PATH_LINK_COUNT_VALID;
+#endif /* CONFIG_SECURITY_CHROMIUMOS_NO_SYMLINK_MOUNT */
 	restore_nameidata();
 	return retval;
 }
@@ -2526,6 +2529,9 @@
 		*type = nd.last_type;
 		audit_inode(name, parent->dentry, AUDIT_INODE_PARENT);
 	}
+#ifdef CONFIG_SECURITY_CHROMIUMOS_NO_SYMLINK_MOUNT
+	parent->link_count = nd.total_link_count | PATH_LINK_COUNT_VALID;
+#endif /* CONFIG_SECURITY_CHROMIUMOS_NO_SYMLINK_MOUNT */
 	restore_nameidata();
 	return retval;
 }
diff -ruN a/fs/namespace.c b/fs/namespace.c
--- a/fs/namespace.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/namespace.c	2021-12-23 08:35:52.000000000 +0100
@@ -746,8 +746,14 @@
 			goto done;
 	}
 
-	if (!new)
-		new = kmalloc(sizeof(struct mountpoint), GFP_KERNEL);
+	if (!new) {
+		/*
+		 * We are allocating as GFP_NOFS to appease lockdep:
+		 * since we are holding i_mutex we should not try to
+		 * recurse into filesystem code.
+		 */
+		new = kmalloc(sizeof(struct mountpoint), GFP_NOFS);
+	}
 	if (!new)
 		return ERR_PTR(-ENOMEM);
 
@@ -3269,6 +3275,24 @@
 	if (!(flags & MS_NOATIME))
 		mnt_flags |= MNT_RELATIME;
 
+	/*
+	 * The nosymfollow option used to be extracted from data_page by an LSM.
+	 * It is now passed in as MS_NOSYMFOLLOW.  We need to also check in
+	 * the old place until all callers have been updated to use the flag.
+	 * Some callers will pass both for cross-kernel compatibility, so
+	 * only check if the new flag isn't already present.
+	 * TODO(b/152074038): Remove this check when all devices are on a kernel
+	 * that supports MS_NOSYMFOLLOW.
+	 */
+	if (data_page && !(flags & MS_NOSYMFOLLOW)) {
+		if (!strncmp((char *)data_page, "nosymfollow", 11) ||
+		    strstr((char *)data_page, ",nosymfollow")) {
+			WARN(1,
+			     "nosymfollow passed in mount data should be changed to the MS_NOSYMFOLLOW flag.");
+			flags |= MS_NOSYMFOLLOW;
+		}
+	}
+
 	/* Separate the per-mountpoint flags */
 	if (flags & MS_NOSUID)
 		mnt_flags |= MNT_NOSUID;
diff -ruN a/fs/nfs/nfs4proc.c b/fs/nfs/nfs4proc.c
--- a/fs/nfs/nfs4proc.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/nfs/nfs4proc.c	2021-12-23 08:35:52.000000000 +0100
@@ -7608,7 +7608,8 @@
 
 static int nfs4_xattr_get_nfs4_acl(const struct xattr_handler *handler,
 				   struct dentry *unused, struct inode *inode,
-				   const char *key, void *buf, size_t buflen)
+				   const char *key, void *buf, size_t buflen,
+				   int flags)
 {
 	return nfs4_proc_get_acl(inode, buf, buflen);
 }
@@ -7634,7 +7635,8 @@
 
 static int nfs4_xattr_get_nfs4_label(const struct xattr_handler *handler,
 				     struct dentry *unused, struct inode *inode,
-				     const char *key, void *buf, size_t buflen)
+				     const char *key, void *buf, size_t buflen,
+				     int flags)
 {
 	if (security_ismaclabel(key))
 		return nfs4_get_security_label(inode, buf, buflen);
@@ -7712,7 +7714,8 @@
 
 static int nfs4_xattr_get_nfs4_user(const struct xattr_handler *handler,
 				    struct dentry *unused, struct inode *inode,
-				    const char *key, void *buf, size_t buflen)
+				    const char *key, void *buf, size_t buflen,
+				    int flags)
 {
 	struct nfs_access_entry cache;
 	ssize_t ret;
diff -ruN a/fs/nsfs.c b/fs/nsfs.c
--- a/fs/nsfs.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/nsfs.c	2021-12-23 08:35:52.000000000 +0100
@@ -251,6 +251,7 @@
 	fput(file);
 	return ERR_PTR(-EINVAL);
 }
+EXPORT_SYMBOL(proc_ns_fget);
 
 /**
  * ns_match() - Returns true if current namespace matches dev/ino provided.
diff -ruN a/fs/ntfs3/xattr.c b/fs/ntfs3/xattr.c
--- a/fs/ntfs3/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ntfs3/xattr.c	2021-12-23 08:35:53.000000000 +0100
@@ -710,7 +710,7 @@
 
 static int ntfs_getxattr(const struct xattr_handler *handler, struct dentry *de,
 			 struct inode *inode, const char *name, void *buffer,
-			 size_t size)
+			 size_t size, int flags)
 {
 	int err;
 	struct ntfs_inode *ni = ntfs_i(inode);
diff -ruN a/fs/ocfs2/xattr.c b/fs/ocfs2/xattr.c
--- a/fs/ocfs2/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ocfs2/xattr.c	2021-12-23 08:35:53.000000000 +0100
@@ -7240,7 +7240,8 @@
  */
 static int ocfs2_xattr_security_get(const struct xattr_handler *handler,
 				    struct dentry *unused, struct inode *inode,
-				    const char *name, void *buffer, size_t size)
+				    const char *name, void *buffer, size_t size,
+				    int flags)
 {
 	return ocfs2_xattr_get(inode, OCFS2_XATTR_INDEX_SECURITY,
 			       name, buffer, size);
@@ -7313,7 +7314,8 @@
  */
 static int ocfs2_xattr_trusted_get(const struct xattr_handler *handler,
 				   struct dentry *unused, struct inode *inode,
-				   const char *name, void *buffer, size_t size)
+				   const char *name, void *buffer, size_t size,
+				   int flags)
 {
 	return ocfs2_xattr_get(inode, OCFS2_XATTR_INDEX_TRUSTED,
 			       name, buffer, size);
@@ -7340,7 +7342,8 @@
  */
 static int ocfs2_xattr_user_get(const struct xattr_handler *handler,
 				struct dentry *unused, struct inode *inode,
-				const char *name, void *buffer, size_t size)
+				const char *name, void *buffer, size_t size,
+				int flags)
 {
 	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
 
diff -ruN a/fs/open.c b/fs/open.c
--- a/fs/open.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/open.c	2021-12-23 08:35:53.000000000 +0100
@@ -35,6 +35,9 @@
 
 #include "internal.h"
 
+#define CREATE_TRACE_POINTS
+#include <trace/events/fs.h>
+
 int do_truncate(struct user_namespace *mnt_userns, struct dentry *dentry,
 		loff_t length, unsigned int time_attrs, struct file *filp)
 {
@@ -1216,6 +1219,7 @@
 		} else {
 			fsnotify_open(f);
 			fd_install(fd, f);
+			trace_do_sys_open(tmp->name, how->flags, how->mode);
 		}
 	}
 	putname(tmp);
diff -ruN a/fs/orangefs/xattr.c b/fs/orangefs/xattr.c
--- a/fs/orangefs/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/orangefs/xattr.c	2021-12-23 08:35:53.000000000 +0100
@@ -542,7 +542,8 @@
 				      struct inode *inode,
 				      const char *name,
 				      void *buffer,
-				      size_t size)
+				      size_t size,
+				      int flags)
 {
 	return orangefs_inode_getxattr(inode, name, buffer, size);
 
diff -ruN a/fs/overlayfs/copy_up.c b/fs/overlayfs/copy_up.c
--- a/fs/overlayfs/copy_up.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/overlayfs/copy_up.c	2021-12-23 08:35:53.000000000 +0100
@@ -1032,7 +1032,7 @@
 		dput(parent);
 		dput(next);
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 	return err;
 }
diff -ruN a/fs/overlayfs/dir.c b/fs/overlayfs/dir.c
--- a/fs/overlayfs/dir.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/overlayfs/dir.c	2021-12-23 08:35:53.000000000 +0100
@@ -570,7 +570,7 @@
 			      struct ovl_cattr *attr, bool origin)
 {
 	int err;
-	const struct cred *old_cred;
+	const struct cred *old_cred, *hold_cred = NULL;
 	struct cred *override_cred;
 	struct dentry *parent = dentry->d_parent;
 
@@ -597,13 +597,14 @@
 		override_cred->fsuid = inode->i_uid;
 		override_cred->fsgid = inode->i_gid;
 		err = security_dentry_create_files_as(dentry,
-				attr->mode, &dentry->d_name, old_cred,
+				attr->mode, &dentry->d_name,
+				old_cred ? old_cred : current_cred(),
 				override_cred);
 		if (err) {
 			put_cred(override_cred);
 			goto out_revert_creds;
 		}
-		put_cred(override_creds(override_cred));
+		hold_cred = override_creds(override_cred);
 		put_cred(override_cred);
 	}
 
@@ -613,7 +614,9 @@
 		err = ovl_create_over_whiteout(dentry, inode, attr);
 
 out_revert_creds:
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred ?: hold_cred);
+	if (old_cred && hold_cred)
+		put_cred(hold_cred);
 	return err;
 }
 
@@ -690,7 +693,7 @@
 
 	old_cred = ovl_override_creds(dentry->d_sb);
 	err = ovl_set_redirect(dentry, false);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 	return err;
 }
@@ -909,7 +912,7 @@
 		err = ovl_remove_upper(dentry, is_dir, &list);
 	else
 		err = ovl_remove_and_whiteout(dentry, &list);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	if (!err) {
 		if (is_dir)
 			clear_nlink(dentry->d_inode);
@@ -1284,7 +1287,7 @@
 out_unlock:
 	unlock_rename(new_upperdir, old_upperdir);
 out_revert_creds:
-	revert_creds(old_cred);
+	ovl_revert_creds(old->d_sb, old_cred);
 	if (update_nlink)
 		ovl_nlink_end(new);
 out_drop_write:
diff -ruN a/fs/overlayfs/file.c b/fs/overlayfs/file.c
--- a/fs/overlayfs/file.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/overlayfs/file.c	2021-12-23 08:35:53.000000000 +0100
@@ -15,6 +15,8 @@
 #include <linux/fs.h>
 #include "overlayfs.h"
 
+#define OVL_IOCB_MASK (IOCB_DSYNC | IOCB_HIPRI | IOCB_NOWAIT | IOCB_SYNC)
+
 struct ovl_aio_req {
 	struct kiocb iocb;
 	refcount_t ref;
@@ -61,7 +63,7 @@
 		realfile = open_with_fake_path(&file->f_path, flags, realinode,
 					       current_cred());
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 
 	pr_debug("open(%p[%pD2/%c], 0%o) -> (%p, 0%o)\n",
 		 file, file, ovl_whatisit(inode, realinode), file->f_flags,
@@ -205,7 +207,7 @@
 
 	old_cred = ovl_override_creds(inode->i_sb);
 	ret = vfs_llseek(real.file, offset, whence);
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 
 	file->f_pos = real.file->f_pos;
 	ovl_inode_unlock(inode);
@@ -237,22 +239,6 @@
 	touch_atime(&file->f_path);
 }
 
-static rwf_t ovl_iocb_to_rwf(int ifl)
-{
-	rwf_t flags = 0;
-
-	if (ifl & IOCB_NOWAIT)
-		flags |= RWF_NOWAIT;
-	if (ifl & IOCB_HIPRI)
-		flags |= RWF_HIPRI;
-	if (ifl & IOCB_DSYNC)
-		flags |= RWF_DSYNC;
-	if (ifl & IOCB_SYNC)
-		flags |= RWF_SYNC;
-
-	return flags;
-}
-
 static inline void ovl_aio_put(struct ovl_aio_req *aio_req)
 {
 	if (refcount_dec_and_test(&aio_req->ref)) {
@@ -313,7 +299,8 @@
 	old_cred = ovl_override_creds(file_inode(file)->i_sb);
 	if (is_sync_kiocb(iocb)) {
 		ret = vfs_iter_read(real.file, iter, &iocb->ki_pos,
-				    ovl_iocb_to_rwf(iocb->ki_flags));
+				    iocb_to_rw_flags(iocb->ki_flags,
+						     OVL_IOCB_MASK));
 	} else {
 		struct ovl_aio_req *aio_req;
 
@@ -334,7 +321,8 @@
 			ovl_aio_cleanup_handler(aio_req);
 	}
 out:
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
+
 	ovl_file_accessed(file);
 out_fdput:
 	fdput(real);
@@ -378,7 +366,7 @@
 	if (is_sync_kiocb(iocb)) {
 		file_start_write(real.file);
 		ret = vfs_iter_write(real.file, iter, &iocb->ki_pos,
-				     ovl_iocb_to_rwf(ifl));
+				     iocb_to_rw_flags(ifl, OVL_IOCB_MASK));
 		file_end_write(real.file);
 		/* Update size */
 		ovl_copyattr(ovl_inode_real(inode), inode);
@@ -407,7 +395,7 @@
 			ovl_aio_cleanup_handler(aio_req);
 	}
 out:
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 out_fdput:
 	fdput(real);
 
@@ -453,7 +441,7 @@
 	file_end_write(real.file);
 	/* Update size */
 	ovl_copyattr(inode);
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 	fdput(real);
 
 out_unlock:
@@ -480,7 +468,7 @@
 	if (file_inode(real.file) == ovl_inode_upper(file_inode(file))) {
 		old_cred = ovl_override_creds(file_inode(file)->i_sb);
 		ret = vfs_fsync_range(real.file, start, end, datasync);
-		revert_creds(old_cred);
+		ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 	}
 
 	fdput(real);
@@ -504,7 +492,7 @@
 
 	old_cred = ovl_override_creds(file_inode(file)->i_sb);
 	ret = call_mmap(vma->vm_file, vma);
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 	ovl_file_accessed(file);
 
 	return ret;
@@ -523,7 +511,7 @@
 
 	old_cred = ovl_override_creds(file_inode(file)->i_sb);
 	ret = vfs_fallocate(real.file, mode, offset, len);
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 
 	/* Update size */
 	ovl_copyattr(ovl_inode_real(inode), inode);
@@ -545,7 +533,7 @@
 
 	old_cred = ovl_override_creds(file_inode(file)->i_sb);
 	ret = vfs_fadvise(real.file, offset, len, advice);
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file)->i_sb, old_cred);
 
 	fdput(real);
 
@@ -595,7 +583,7 @@
 						flags);
 		break;
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(file_inode(file_out)->i_sb, old_cred);
 
 	/* Update size */
 	ovl_copyattr(ovl_inode_real(inode_out), inode_out);
diff -ruN a/fs/overlayfs/inode.c b/fs/overlayfs/inode.c
--- a/fs/overlayfs/inode.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/overlayfs/inode.c	2021-12-23 08:35:53.000000000 +0100
@@ -78,7 +78,7 @@
 		inode_lock(upperdentry->d_inode);
 		old_cred = ovl_override_creds(dentry->d_sb);
 		err = notify_change(&init_user_ns, upperdentry, attr, NULL);
-		revert_creds(old_cred);
+		ovl_revert_creds(dentry->d_sb, old_cred);
 		if (!err)
 			ovl_copyattr(upperdentry->d_inode, dentry->d_inode);
 		inode_unlock(upperdentry->d_inode);
@@ -270,7 +270,7 @@
 		stat->nlink = dentry->d_inode->i_nlink;
 
 out:
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 	return err;
 }
@@ -305,7 +305,7 @@
 		mask |= MAY_READ;
 	}
 	err = inode_permission(&init_user_ns, realinode, mask);
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 
 	return err;
 }
@@ -322,7 +322,7 @@
 
 	old_cred = ovl_override_creds(dentry->d_sb);
 	p = vfs_get_link(ovl_dentry_real(dentry), done);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	return p;
 }
 
@@ -353,7 +353,7 @@
 	if (!value && !upperdentry) {
 		old_cred = ovl_override_creds(dentry->d_sb);
 		err = vfs_getxattr(&init_user_ns, realdentry, name, NULL, 0);
-		revert_creds(old_cred);
+		ovl_revert_creds(dentry->d_sb, old_cred);
 		if (err < 0)
 			goto out_drop_write;
 	}
@@ -374,7 +374,7 @@
 		WARN_ON(flags != XATTR_REPLACE);
 		err = vfs_removexattr(&init_user_ns, realdentry, name);
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 	/* copy c/mtime */
 	ovl_copyattr(d_inode(realdentry), inode);
@@ -386,7 +386,7 @@
 }
 
 int ovl_xattr_get(struct dentry *dentry, struct inode *inode, const char *name,
-		  void *value, size_t size)
+		  void *value, size_t size, int flags)
 {
 	ssize_t res;
 	const struct cred *old_cred;
@@ -394,8 +394,9 @@
 		ovl_i_dentry_upper(inode) ?: ovl_dentry_lower(dentry);
 
 	old_cred = ovl_override_creds(dentry->d_sb);
-	res = vfs_getxattr(&init_user_ns, realdentry, name, value, size);
-	revert_creds(old_cred);
+	res = __vfs_getxattr(&init_user_ns, realdentry, d_inode(realdentry), name,
+			     value, size, flags);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	return res;
 }
 
@@ -423,7 +424,7 @@
 
 	old_cred = ovl_override_creds(dentry->d_sb);
 	res = vfs_listxattr(realdentry, list, size);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	if (res <= 0 || size == 0)
 		return res;
 
@@ -461,7 +462,7 @@
 
 	old_cred = ovl_override_creds(inode->i_sb);
 	acl = get_acl(realinode, type);
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 
 	return acl;
 }
@@ -495,7 +496,7 @@
 
 	old_cred = ovl_override_creds(inode->i_sb);
 	err = realinode->i_op->fiemap(realinode, fieinfo, start, len);
-	revert_creds(old_cred);
+	ovl_revert_creds(inode->i_sb, old_cred);
 
 	return err;
 }
diff -ruN a/fs/overlayfs/namei.c b/fs/overlayfs/namei.c
--- a/fs/overlayfs/namei.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/overlayfs/namei.c	2021-12-23 08:35:53.000000000 +0100
@@ -108,7 +108,8 @@
 static struct ovl_fh *ovl_get_fh(struct ovl_fs *ofs, struct dentry *dentry,
 				 enum ovl_xattr ox)
 {
-	int res, err;
+	ssize_t res;
+	int err;
 	struct ovl_fh *fh = NULL;
 
 	res = ovl_do_getxattr(ofs, dentry, ox, NULL, 0);
@@ -143,10 +144,10 @@
 	return NULL;
 
 fail:
-	pr_warn_ratelimited("failed to get origin (%i)\n", res);
+	pr_warn_ratelimited("failed to get origin (%zi)\n", res);
 	goto out;
 invalid:
-	pr_warn_ratelimited("invalid origin (%*phN)\n", res, fh);
+	pr_warn_ratelimited("invalid origin (%*phN)\n", (int)res, fh);
 	goto out;
 }
 
@@ -1106,7 +1107,7 @@
 	ovl_dentry_update_reval(dentry, upperdentry,
 			DCACHE_OP_REVALIDATE | DCACHE_OP_WEAK_REVALIDATE);
 
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	if (origin_path) {
 		dput(origin_path->dentry);
 		kfree(origin_path);
@@ -1133,7 +1134,7 @@
 	kfree(upperredirect);
 out:
 	kfree(d.redirect);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	return ERR_PTR(err);
 }
 
@@ -1185,7 +1186,7 @@
 			dput(this);
 		}
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 	return positive;
 }
diff -ruN a/fs/overlayfs/overlayfs.h b/fs/overlayfs/overlayfs.h
--- a/fs/overlayfs/overlayfs.h	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/overlayfs/overlayfs.h	2021-12-23 08:35:53.000000000 +0100
@@ -187,7 +187,8 @@
 				      size_t size)
 {
 	const char *name = ovl_xattr(ofs, ox);
-	int err = vfs_getxattr(&init_user_ns, dentry, name, value, size);
+	struct inode *ip = d_inode(dentry);
+	int err = __vfs_getxattr(&init_user_ns, dentry, ip, name, value, size, XATTR_NOSECURITY);
 	int len = (value && err > 0) ? err : 0;
 
 	pr_debug("getxattr(%pd2, \"%s\", \"%*pE\", %zu, 0) = %i\n",
@@ -280,6 +281,7 @@
 void ovl_drop_write(struct dentry *dentry);
 struct dentry *ovl_workdir(struct dentry *dentry);
 const struct cred *ovl_override_creds(struct super_block *sb);
+void ovl_revert_creds(struct super_block *sb, const struct cred *oldcred);
 int ovl_can_decode_fh(struct super_block *sb);
 struct dentry *ovl_indexdir(struct super_block *sb);
 bool ovl_index_all(struct super_block *sb);
@@ -496,7 +498,7 @@
 int ovl_xattr_set(struct dentry *dentry, struct inode *inode, const char *name,
 		  const void *value, size_t size, int flags);
 int ovl_xattr_get(struct dentry *dentry, struct inode *inode, const char *name,
-		  void *value, size_t size);
+		  void *value, size_t size, int flags);
 ssize_t ovl_listxattr(struct dentry *dentry, char *list, size_t size);
 struct posix_acl *ovl_get_acl(struct inode *inode, int type, bool rcu);
 int ovl_update_time(struct inode *inode, struct timespec64 *ts, int flags);
diff -ruN a/fs/overlayfs/ovl_entry.h b/fs/overlayfs/ovl_entry.h
--- a/fs/overlayfs/ovl_entry.h	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/overlayfs/ovl_entry.h	2021-12-23 08:35:53.000000000 +0100
@@ -20,6 +20,7 @@
 	bool metacopy;
 	bool userxattr;
 	bool ovl_volatile;
+	bool override_creds;
 };
 
 struct ovl_sb {
diff -ruN a/fs/overlayfs/readdir.c b/fs/overlayfs/readdir.c
--- a/fs/overlayfs/readdir.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/overlayfs/readdir.c	2021-12-23 08:35:53.000000000 +0100
@@ -286,7 +286,7 @@
 		}
 		inode_unlock(dir->d_inode);
 	}
-	revert_creds(old_cred);
+	ovl_revert_creds(rdd->dentry->d_sb, old_cred);
 
 	return err;
 }
@@ -967,7 +967,7 @@
 
 	old_cred = ovl_override_creds(dentry->d_sb);
 	err = ovl_dir_read_merged(dentry, list, &root);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 	if (err)
 		return err;
 
diff -ruN a/fs/overlayfs/super.c b/fs/overlayfs/super.c
--- a/fs/overlayfs/super.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/overlayfs/super.c	2021-12-23 08:35:53.000000000 +0100
@@ -53,6 +53,11 @@
 MODULE_PARM_DESC(xino_auto,
 		 "Auto enable xino feature");
 
+static bool __read_mostly ovl_override_creds_def = true;
+module_param_named(override_creds, ovl_override_creds_def, bool, 0644);
+MODULE_PARM_DESC(ovl_override_creds_def,
+		 "Use mounter's credentials for accesses");
+
 static void ovl_entry_stack_free(struct ovl_entry *oe)
 {
 	unsigned int i;
@@ -382,6 +387,9 @@
 		seq_puts(m, ",volatile");
 	if (ofs->config.userxattr)
 		seq_puts(m, ",userxattr");
+	if (ofs->config.override_creds != ovl_override_creds_def)
+		seq_show_option(m, "override_creds",
+				ofs->config.override_creds ? "on" : "off");
 	return 0;
 }
 
@@ -437,6 +445,8 @@
 	OPT_METACOPY_ON,
 	OPT_METACOPY_OFF,
 	OPT_VOLATILE,
+	OPT_OVERRIDE_CREDS_ON,
+	OPT_OVERRIDE_CREDS_OFF,
 	OPT_ERR,
 };
 
@@ -459,6 +469,8 @@
 	{OPT_METACOPY_ON,		"metacopy=on"},
 	{OPT_METACOPY_OFF,		"metacopy=off"},
 	{OPT_VOLATILE,			"volatile"},
+	{OPT_OVERRIDE_CREDS_ON,		"override_creds=on"},
+	{OPT_OVERRIDE_CREDS_OFF,	"override_creds=off"},
 	{OPT_ERR,			NULL}
 };
 
@@ -518,6 +530,7 @@
 	config->redirect_mode = kstrdup(ovl_redirect_mode_def(), GFP_KERNEL);
 	if (!config->redirect_mode)
 		return -ENOMEM;
+	config->override_creds = ovl_override_creds_def;
 
 	while ((p = ovl_next_opt(&opt)) != NULL) {
 		int token;
@@ -619,6 +632,14 @@
 			config->userxattr = true;
 			break;
 
+		case OPT_OVERRIDE_CREDS_ON:
+			config->override_creds = true;
+			break;
+
+		case OPT_OVERRIDE_CREDS_OFF:
+			config->override_creds = false;
+			break;
+
 		default:
 			pr_err("unrecognized mount option \"%s\" or missing value\n",
 					p);
@@ -1000,9 +1021,9 @@
 static int __maybe_unused
 ovl_posix_acl_xattr_get(const struct xattr_handler *handler,
 			struct dentry *dentry, struct inode *inode,
-			const char *name, void *buffer, size_t size)
+			const char *name, void *buffer, size_t size, int flags)
 {
-	return ovl_xattr_get(dentry, inode, handler->name, buffer, size);
+	return ovl_xattr_get(dentry, inode, handler->name, buffer, size, flags);
 }
 
 static int __maybe_unused
@@ -1063,7 +1084,8 @@
 
 static int ovl_own_xattr_get(const struct xattr_handler *handler,
 			     struct dentry *dentry, struct inode *inode,
-			     const char *name, void *buffer, size_t size)
+			     const char *name, void *buffer, size_t size,
+			     int flags)
 {
 	return -EOPNOTSUPP;
 }
@@ -1079,9 +1101,10 @@
 
 static int ovl_other_xattr_get(const struct xattr_handler *handler,
 			       struct dentry *dentry, struct inode *inode,
-			       const char *name, void *buffer, size_t size)
+			       const char *name, void *buffer, size_t size,
+			       int flags)
 {
-	return ovl_xattr_get(dentry, inode, name, buffer, size);
+	return ovl_xattr_get(dentry, inode, name, buffer, size, flags);
 }
 
 static int ovl_other_xattr_set(const struct xattr_handler *handler,
@@ -2138,7 +2161,6 @@
 	kfree(splitlower);
 
 	sb->s_root = root_dentry;
-
 	return 0;
 
 out_free_oe:
diff -ruN a/fs/overlayfs/util.c b/fs/overlayfs/util.c
--- a/fs/overlayfs/util.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/overlayfs/util.c	2021-12-23 08:35:53.000000000 +0100
@@ -38,9 +38,17 @@
 {
 	struct ovl_fs *ofs = sb->s_fs_info;
 
+	if (!ofs->config.override_creds)
+		return NULL;
 	return override_creds(ofs->creator_cred);
 }
 
+void ovl_revert_creds(struct super_block *sb, const struct cred *old_cred)
+{
+	if (old_cred)
+		revert_creds(old_cred);
+}
+
 /*
  * Check if underlying fs supports file handles and try to determine encoding
  * type, in order to deduce maximum inode number used by fs.
@@ -552,7 +560,7 @@
 
 bool ovl_check_origin_xattr(struct ovl_fs *ofs, struct dentry *dentry)
 {
-	int res;
+	ssize_t res;
 
 	res = ovl_do_getxattr(ofs, dentry, OVL_XATTR_ORIGIN, NULL, 0);
 
@@ -566,7 +574,7 @@
 bool ovl_check_dir_xattr(struct super_block *sb, struct dentry *dentry,
 			 enum ovl_xattr ox)
 {
-	int res;
+	ssize_t res;
 	char val;
 
 	if (!d_is_dir(dentry))
@@ -899,7 +907,7 @@
 	 * value relative to the upper inode nlink in an upper inode xattr.
 	 */
 	err = ovl_set_nlink_upper(dentry);
-	revert_creds(old_cred);
+	ovl_revert_creds(dentry->d_sb, old_cred);
 
 out:
 	if (err)
@@ -917,7 +925,7 @@
 
 		old_cred = ovl_override_creds(dentry->d_sb);
 		ovl_cleanup_index(dentry);
-		revert_creds(old_cred);
+		ovl_revert_creds(dentry->d_sb, old_cred);
 	}
 
 	ovl_inode_unlock(inode);
@@ -945,7 +953,7 @@
 /* err < 0, 0 if no metacopy xattr, 1 if metacopy xattr found */
 int ovl_check_metacopy_xattr(struct ovl_fs *ofs, struct dentry *dentry)
 {
-	int res;
+	ssize_t res;
 
 	/* Only regular files can have metacopy xattr */
 	if (!S_ISREG(d_inode(dentry)->i_mode))
@@ -967,7 +975,7 @@
 
 	return 1;
 out:
-	pr_warn_ratelimited("failed to get metacopy (%i)\n", res);
+	pr_warn_ratelimited("failed to get metacopy (%zi)\n", res);
 	return res;
 }
 
diff -ruN a/fs/posix_acl.c b/fs/posix_acl.c
--- a/fs/posix_acl.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/posix_acl.c	2021-12-23 08:35:53.000000000 +0100
@@ -888,7 +888,7 @@
 static int
 posix_acl_xattr_get(const struct xattr_handler *handler,
 		    struct dentry *unused, struct inode *inode,
-		    const char *name, void *value, size_t size)
+		    const char *name, void *value, size_t size, int flags)
 {
 	struct posix_acl *acl;
 	int error;
diff -ruN a/fs/proc/base.c b/fs/proc/base.c
--- a/fs/proc/base.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/proc/base.c	2021-12-23 08:35:53.000000000 +0100
@@ -97,6 +97,7 @@
 #include <linux/time_namespace.h>
 #include <linux/resctrl.h>
 #include <linux/cn_proc.h>
+#include <linux/cpufreq_times.h>
 #include <trace/events/oom.h>
 #include "internal.h"
 #include "fd.h"
@@ -151,6 +152,12 @@
 		NULL, &proc_pid_attr_operations,	\
 		{ .lsm = LSM })
 
+#ifdef CONFIG_SECURITY_CHROMIUMOS_READONLY_PROC_SELF_MEM
+# define PROC_PID_MEM_MODE S_IRUSR
+#else
+# define PROC_PID_MEM_MODE S_IRUSR|S_IWUSR
+#endif
+
 /*
  * Count the number of hardlinks for the pid_entry table, excluding the .
  * and .. links.
@@ -899,7 +906,11 @@
 static ssize_t mem_write(struct file *file, const char __user *buf,
 			 size_t count, loff_t *ppos)
 {
+#ifdef CONFIG_SECURITY_CHROMIUMOS_READONLY_PROC_SELF_MEM
+	return -EACCES;
+#else
 	return mem_rw(file, (char __user*)buf, count, ppos, 1);
+#endif
 }
 
 loff_t mem_lseek(struct file *file, loff_t offset, int orig)
@@ -3208,18 +3219,22 @@
 #ifdef CONFIG_NUMA
 	REG("numa_maps",  S_IRUGO, proc_pid_numa_maps_operations),
 #endif
-	REG("mem",        S_IRUSR|S_IWUSR, proc_mem_operations),
+	REG("mem",        PROC_PID_MEM_MODE, proc_mem_operations),
 	LNK("cwd",        proc_cwd_link),
 	LNK("root",       proc_root_link),
 	LNK("exe",        proc_exe_link),
 	REG("mounts",     S_IRUGO, proc_mounts_operations),
 	REG("mountinfo",  S_IRUGO, proc_mountinfo_operations),
 	REG("mountstats", S_IRUSR, proc_mountstats_operations),
+#ifdef CONFIG_PROCESS_RECLAIM
+	REG("reclaim",    S_IWUGO, proc_reclaim_operations),
+#endif
 #ifdef CONFIG_PROC_PAGE_MONITOR
 	REG("clear_refs", S_IWUSR, proc_clear_refs_operations),
 	REG("smaps",      S_IRUGO, proc_pid_smaps_operations),
 	REG("smaps_rollup", S_IRUGO, proc_pid_smaps_rollup_operations),
 	REG("pagemap",    S_IRUSR, proc_pagemap_operations),
+	REG("totmaps",    S_IRUGO, proc_totmaps_operations),
 #endif
 #ifdef CONFIG_SECURITY
 	DIR("attr",       S_IRUGO|S_IXUGO, proc_attr_dir_inode_operations, proc_attr_dir_operations),
@@ -3275,6 +3290,9 @@
 #ifdef CONFIG_LIVEPATCH
 	ONE("patch_state",  S_IRUSR, proc_pid_patch_state),
 #endif
+#ifdef CONFIG_CPU_FREQ_TIMES
+	ONE("time_in_state", 0444, proc_time_in_state_show),
+#endif
 #ifdef CONFIG_STACKLEAK_METRICS
 	ONE("stack_depth", S_IRUGO, proc_stack_depth),
 #endif
@@ -3552,7 +3570,7 @@
 #ifdef CONFIG_NUMA
 	REG("numa_maps", S_IRUGO, proc_pid_numa_maps_operations),
 #endif
-	REG("mem",       S_IRUSR|S_IWUSR, proc_mem_operations),
+	REG("mem",       PROC_PID_MEM_MODE, proc_mem_operations),
 	LNK("cwd",       proc_cwd_link),
 	LNK("root",      proc_root_link),
 	LNK("exe",       proc_exe_link),
@@ -3617,6 +3635,9 @@
 #ifdef CONFIG_SECCOMP_CACHE_DEBUG
 	ONE("seccomp_cache", S_IRUSR, proc_pid_seccomp_cache),
 #endif
+#ifdef CONFIG_CPU_FREQ_TIMES
+	ONE("time_in_state", 0444, proc_time_in_state_show),
+#endif
 };
 
 static int proc_tid_base_readdir(struct file *file, struct dir_context *ctx)
diff -ruN a/fs/proc/internal.h b/fs/proc/internal.h
--- a/fs/proc/internal.h	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/proc/internal.h	2021-12-23 08:35:53.000000000 +0100
@@ -90,6 +90,9 @@
 	const char *lsm;
 };
 
+
+extern const struct file_operations proc_totmaps_operations;
+
 struct proc_inode {
 	struct pid *pid;
 	unsigned int fd;
@@ -216,6 +219,7 @@
 extern const struct inode_operations proc_link_inode_operations;
 extern const struct inode_operations proc_pid_link_inode_operations;
 extern const struct super_operations proc_sops;
+extern const struct file_operations proc_reclaim_operations;
 
 void proc_init_kmemcache(void);
 void proc_invalidate_siblings_dcache(struct hlist_head *inodes, spinlock_t *lock);
@@ -291,6 +295,7 @@
 	struct mm_struct *mm;
 #ifdef CONFIG_MMU
 	struct vm_area_struct *tail_vma;
+	struct mem_size_stats *mss;
 #endif
 #ifdef CONFIG_NUMA
 	struct mempolicy *task_mempolicy;
diff -ruN a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c
--- a/fs/proc/task_mmu.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/proc/task_mmu.c	2021-12-23 08:35:53.000000000 +0100
@@ -19,6 +19,7 @@
 #include <linux/shmem_fs.h>
 #include <linux/uaccess.h>
 #include <linux/pkeys.h>
+#include <linux/mm_inline.h>
 
 #include <asm/elf.h>
 #include <asm/tlb.h>
@@ -123,6 +124,56 @@
 }
 #endif
 
+static void seq_print_vma_name(struct seq_file *m, struct vm_area_struct *vma)
+{
+	const char __user *name = vma_get_anon_name(vma);
+	struct mm_struct *mm = vma->vm_mm;
+
+	unsigned long page_start_vaddr;
+	unsigned long page_offset;
+	unsigned long num_pages;
+	unsigned long max_len = NAME_MAX;
+	int i;
+
+	page_start_vaddr = (unsigned long)name & PAGE_MASK;
+	page_offset = (unsigned long)name - page_start_vaddr;
+	num_pages = DIV_ROUND_UP(page_offset + max_len, PAGE_SIZE);
+
+	seq_puts(m, "[anon:");
+
+	for (i = 0; i < num_pages; i++) {
+		int len;
+		int write_len;
+		const char *kaddr;
+		long pages_pinned;
+		struct page *page;
+
+		pages_pinned = get_user_pages_remote(mm, page_start_vaddr, 1, 0,
+						     &page, NULL, NULL);
+		if (pages_pinned < 1) {
+			seq_puts(m, "<fault>]");
+			return;
+		}
+
+		kaddr = (const char *)kmap(page);
+		len = min(max_len, PAGE_SIZE - page_offset);
+		write_len = strnlen(kaddr + page_offset, len);
+		seq_write(m, kaddr + page_offset, write_len);
+		kunmap(page);
+		put_page(page);
+
+		/* if strnlen hit a null terminator then we're done */
+		if (write_len != len)
+			break;
+
+		max_len -= len;
+		page_offset = 0;
+		page_start_vaddr += PAGE_SIZE;
+	}
+
+	seq_putc(m, ']');
+}
+
 static void *m_start(struct seq_file *m, loff_t *ppos)
 {
 	struct proc_maps_private *priv = m->private;
@@ -319,8 +370,15 @@
 			goto done;
 		}
 
-		if (is_stack(vma))
+		if (is_stack(vma)) {
 			name = "[stack]";
+			goto done;
+		}
+
+		if (vma_get_anon_name(vma)) {
+			seq_pad(m, ' ');
+			seq_print_vma_name(m, vma);
+		}
 	}
 
 done:
@@ -818,6 +876,11 @@
 	smap_gather_stats(vma, &mss, 0);
 
 	show_map_vma(m, vma);
+	if (vma_get_anon_name(vma)) {
+		seq_puts(m, "Name:           ");
+		seq_print_vma_name(m, vma);
+		seq_putc(m, '\n');
+	}
 
 	SEQ_PUT_DEC("Size:           ", vma->vm_end - vma->vm_start);
 	SEQ_PUT_DEC(" kB\nKernelPageSize: ", vma_kernel_pagesize(vma));
@@ -836,6 +899,85 @@
 	return 0;
 }
 
+static void add_smaps_sum(struct mem_size_stats *mss,
+		struct mem_size_stats *mss_sum)
+{
+	mss_sum->resident += mss->resident;
+	mss_sum->pss += mss->pss;
+	mss_sum->pss_anon += mss->pss_anon;
+	mss_sum->pss_file += mss->pss_file;
+	mss_sum->pss_shmem += mss->pss_shmem;
+	mss_sum->shared_clean += mss->shared_clean;
+	mss_sum->shared_dirty += mss->shared_dirty;
+	mss_sum->private_clean += mss->private_clean;
+	mss_sum->private_dirty += mss->private_dirty;
+	mss_sum->referenced += mss->referenced;
+	mss_sum->anonymous += mss->anonymous;
+	mss_sum->anonymous_thp += mss->anonymous_thp;
+	mss_sum->swap += mss->swap;
+}
+
+static int totmaps_proc_show(struct seq_file *m, void *data)
+{
+	struct proc_maps_private *priv = m->private;
+	struct mm_struct *mm;
+	struct vm_area_struct *vma;
+	struct mem_size_stats *mss_sum = priv->mss;
+
+	/* reference to priv->task already taken */
+	/* but need to get the mm here because */
+	/* task could be in the process of exiting */
+	mm = get_task_mm(priv->task);
+	if (!mm || IS_ERR(mm))
+		return -EINVAL;
+
+	mmap_read_lock(mm);
+	hold_task_mempolicy(priv);
+
+	for (vma = mm->mmap; vma != priv->tail_vma; vma = vma->vm_next) {
+		struct mem_size_stats mss;
+
+		if (vma->vm_mm && !is_vm_hugetlb_page(vma)) {
+			memset(&mss, 0, sizeof(mss));
+			walk_page_vma(vma, &smaps_walk_ops, &mss);
+			add_smaps_sum(&mss, mss_sum);
+		}
+	}
+	seq_printf(m,
+		   "Rss:            %8lu kB\n"
+		   "Pss:            %8lu kB\n"
+		   "Pss_Anon:       %8lu kB\n"
+		   "Pss_File:       %8lu kB\n"
+		   "Pss_Shmem:      %8lu kB\n"
+		   "Shared_Clean:   %8lu kB\n"
+		   "Shared_Dirty:   %8lu kB\n"
+		   "Private_Clean:  %8lu kB\n"
+		   "Private_Dirty:  %8lu kB\n"
+		   "Referenced:     %8lu kB\n"
+		   "Anonymous:      %8lu kB\n"
+		   "AnonHugePages:  %8lu kB\n"
+		   "Swap:           %8lu kB\n",
+		   mss_sum->resident >> 10,
+		   (unsigned long)(mss_sum->pss >> (10 + PSS_SHIFT)),
+		   (unsigned long)(mss_sum->pss_anon >> (10 + PSS_SHIFT)),
+		   (unsigned long)(mss_sum->pss_file >> (10 + PSS_SHIFT)),
+		   (unsigned long)(mss_sum->pss_shmem >> (10 + PSS_SHIFT)),
+		   mss_sum->shared_clean  >> 10,
+		   mss_sum->shared_dirty  >> 10,
+		   mss_sum->private_clean >> 10,
+		   mss_sum->private_dirty >> 10,
+		   mss_sum->referenced >> 10,
+		   mss_sum->anonymous >> 10,
+		   mss_sum->anonymous_thp >> 10,
+		   mss_sum->swap >> 10);
+
+	release_task_mempolicy(priv);
+	mmap_read_unlock(mm);
+	mmput(mm);
+
+	return 0;
+}
+
 static int show_smaps_rollup(struct seq_file *m, void *v)
 {
 	struct proc_maps_private *priv = m->private;
@@ -1005,6 +1147,50 @@
 	return single_release(inode, file);
 }
 
+static int totmaps_open(struct inode *inode, struct file *file)
+{
+	struct proc_maps_private *priv;
+	int ret = -ENOMEM;
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (priv) {
+		priv->mss = kzalloc(sizeof(*priv->mss), GFP_KERNEL);
+		if (!priv->mss)
+			return -ENOMEM;
+
+		/* we need to grab references to the task_struct */
+		/* at open time, because there's a potential information */
+		/* leak where the totmaps file is opened and held open */
+		/* while the underlying pid to task mapping changes */
+		/* underneath it */
+		priv->task = get_pid_task(proc_pid(inode), PIDTYPE_PID);
+		if (!priv->task) {
+			kfree(priv->mss);
+			kfree(priv);
+			return -ESRCH;
+		}
+
+		ret = single_open(file, totmaps_proc_show, priv);
+		if (ret) {
+			put_task_struct(priv->task);
+			kfree(priv->mss);
+			kfree(priv);
+		}
+	}
+	return ret;
+}
+
+static int totmaps_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *m = file->private_data;
+	struct proc_maps_private *priv = m->private;
+
+	put_task_struct(priv->task);
+	kfree(priv->mss);
+	kfree(priv);
+	m->private = NULL;
+	return single_release(inode, file);
+}
+
 const struct file_operations proc_pid_smaps_operations = {
 	.open		= pid_smaps_open,
 	.read		= seq_read,
@@ -1019,6 +1205,13 @@
 	.release	= smaps_rollup_release,
 };
 
+const struct file_operations proc_totmaps_operations = {
+	.open		= totmaps_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= totmaps_release,
+};
+
 enum clear_refs_types {
 	CLEAR_REFS_ALL = 1,
 	CLEAR_REFS_ANON,
@@ -1692,6 +1885,342 @@
 };
 #endif /* CONFIG_PROC_PAGE_MONITOR */
 
+#ifdef CONFIG_PROCESS_RECLAIM
+enum reclaim_type {
+	RECLAIM_FILE = 1,
+	RECLAIM_ANON,
+	RECLAIM_ALL,
+	/*
+	 * For safety and backwards compatability, shmem reclaim mode
+	 * is only possible by directly using 'shmem', 'all' does not
+	 * inlcude shmem.
+	 */
+	RECLAIM_SHMEM,
+};
+
+struct walk_data {
+	enum reclaim_type type;
+};
+
+static int deactivate_pte_range(pmd_t *pmd, unsigned long addr,
+				unsigned long end, struct mm_walk *walk)
+{
+	pte_t *orig_pte, *pte, ptent;
+	spinlock_t *ptl;
+	struct page *page;
+	struct vm_area_struct *vma = walk->vma;
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long next = pmd_addr_end(addr, end);
+
+	ptl = pmd_trans_huge_lock(pmd, vma);
+	if (ptl) {
+		if (!pmd_present(*pmd))
+			goto huge_unlock;
+
+		if (is_huge_zero_pmd(*pmd))
+			goto huge_unlock;
+
+		page = pmd_page(*pmd);
+		if (page_mapcount(page) > 1)
+			goto huge_unlock;
+
+		if (next - addr != HPAGE_PMD_SIZE) {
+			int err;
+
+			get_page(page);
+			spin_unlock(ptl);
+			lock_page(page);
+			err = split_huge_page(page);
+			unlock_page(page);
+			put_page(page);
+			if (!err)
+				goto regular_page;
+			return 0;
+		}
+
+		pmdp_test_and_clear_young(vma, addr, pmd);
+		deactivate_page(page);
+huge_unlock:
+		spin_unlock(ptl);
+		return 0;
+	}
+
+	if (pmd_trans_unstable(pmd))
+		return 0;
+
+regular_page:
+	orig_pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
+	for (pte = orig_pte; addr < end; pte++, addr += PAGE_SIZE) {
+		ptent = *pte;
+
+		if (!pte_present(ptent))
+			continue;
+
+		page = vm_normal_page(vma, addr, ptent);
+		if (!page)
+			continue;
+
+		if (PageTransCompound(page))  {
+			if (page_mapcount(page) != 1)
+				break;
+			get_page(page);
+			if (!trylock_page(page)) {
+				put_page(page);
+				break;
+			}
+			pte_unmap_unlock(orig_pte, ptl);
+			if (split_huge_page(page)) {
+				unlock_page(page);
+				put_page(page);
+				pte_offset_map_lock(mm, pmd, addr, &ptl);
+				break;
+			}
+			unlock_page(page);
+			put_page(page);
+			pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
+			pte--;
+			addr -= PAGE_SIZE;
+			continue;
+		}
+
+		VM_BUG_ON_PAGE(PageTransCompound(page), page);
+
+		if (page_mapcount(page) > 1)
+			continue;
+
+		ptep_test_and_clear_young(vma, addr, pte);
+		deactivate_page(page);
+	}
+	pte_unmap_unlock(orig_pte, ptl);
+	cond_resched();
+	return 0;
+}
+
+
+static int reclaim_pte_range(pmd_t *pmd, unsigned long addr,
+				unsigned long end, struct mm_walk *walk)
+{
+	pte_t *orig_pte, *pte, ptent;
+	spinlock_t *ptl;
+	LIST_HEAD(page_list);
+	struct page *page;
+	int isolated = 0;
+	struct vm_area_struct *vma = walk->vma;
+	struct walk_data *data = (struct walk_data*)walk->private;
+	enum reclaim_type type = 0;
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long next = pmd_addr_end(addr, end);
+
+	if (data)
+		type = data->type;
+
+	ptl = pmd_trans_huge_lock(pmd, vma);
+	if (ptl) {
+		if (!pmd_present(*pmd))
+			goto huge_unlock;
+
+		if (is_huge_zero_pmd(*pmd))
+			goto huge_unlock;
+
+		page = pmd_page(*pmd);
+		if (type != RECLAIM_SHMEM && page_mapcount(page) > 1)
+			goto huge_unlock;
+
+		if (next - addr != HPAGE_PMD_SIZE) {
+			int err;
+
+			get_page(page);
+			spin_unlock(ptl);
+			lock_page(page);
+			err = split_huge_page(page);
+			unlock_page(page);
+			put_page(page);
+			if (!err)
+				goto regular_page;
+			return 0;
+		}
+
+		if (isolate_lru_page(page))
+			goto huge_unlock;
+
+		/* Clear all the references to make sure it gets reclaimed */
+		pmdp_test_and_clear_young(vma, addr, pmd);
+		ClearPageReferenced(page);
+		test_and_clear_page_young(page);
+		list_add(&page->lru, &page_list);
+huge_unlock:
+		spin_unlock(ptl);
+		reclaim_pages(&page_list);
+		return 0;
+	}
+
+	if (pmd_trans_unstable(pmd))
+		return 0;
+
+regular_page:
+	orig_pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
+	for (pte = orig_pte; addr < end; pte++, addr += PAGE_SIZE) {
+		ptent = *pte;
+		if (!pte_present(ptent))
+			continue;
+
+		page = vm_normal_page(vma, addr, ptent);
+		if (!page)
+			continue;
+
+		if (PageTransCompound(page)) {
+			if (type != RECLAIM_SHMEM && page_mapcount(page) != 1)
+				break;
+			get_page(page);
+			if (!trylock_page(page)) {
+				put_page(page);
+				break;
+			}
+			pte_unmap_unlock(orig_pte, ptl);
+
+			if (split_huge_page(page)) {
+				unlock_page(page);
+				put_page(page);
+				pte_offset_map_lock(mm, pmd, addr, &ptl);
+				break;
+			}
+			unlock_page(page);
+			put_page(page);
+			pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
+			pte--;
+			addr -= PAGE_SIZE;
+			continue;
+		}
+
+		VM_BUG_ON_PAGE(PageTransCompound(page), page);
+
+		if (!PageLRU(page))
+			continue;
+
+		if (type != RECLAIM_SHMEM && page_mapcount(page) > 1)
+			continue;
+
+		if (isolate_lru_page(page))
+			continue;
+
+		isolated++;
+		list_add(&page->lru, &page_list);
+		/* Clear all the references to make sure it gets reclaimed */
+		ptep_test_and_clear_young(vma, addr, pte);
+		ClearPageReferenced(page);
+		test_and_clear_page_young(page);
+		if (isolated >= SWAP_CLUSTER_MAX) {
+			pte_unmap_unlock(orig_pte, ptl);
+			reclaim_pages(&page_list);
+			isolated = 0;
+			pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
+			orig_pte = pte;
+		}
+	}
+
+	pte_unmap_unlock(orig_pte, ptl);
+	reclaim_pages(&page_list);
+
+	cond_resched();
+	return 0;
+}
+
+static ssize_t reclaim_write(struct file *file, const char __user *buf,
+				size_t count, loff_t *ppos)
+{
+	struct task_struct *task;
+	char buffer[PROC_NUMBUF];
+	struct mm_struct *mm;
+	struct vm_area_struct *vma;
+	enum reclaim_type type;
+	char *type_buf;
+
+	memset(buffer, 0, sizeof(buffer));
+	if (count > sizeof(buffer) - 1)
+		count = sizeof(buffer) - 1;
+
+	if (copy_from_user(buffer, buf, count))
+		return -EFAULT;
+
+	type_buf = strstrip(buffer);
+	if (!strcmp(type_buf, "file"))
+		type = RECLAIM_FILE;
+	else if (!strcmp(type_buf, "anon"))
+		type = RECLAIM_ANON;
+#ifdef CONFIG_SHMEM
+	else if (!strcmp(type_buf, "shmem"))
+		type = RECLAIM_SHMEM;
+#endif
+	else if (!strcmp(type_buf, "all"))
+		type = RECLAIM_ALL;
+	else
+		return -EINVAL;
+
+	task = get_proc_task(file->f_path.dentry->d_inode);
+	if (!task)
+		return -ESRCH;
+
+	mm = get_task_mm(task);
+	if (mm) {
+		struct mm_walk_ops reclaim_walk = {
+			.pmd_entry = reclaim_pte_range,
+		};
+
+		struct walk_data reclaim_data = {
+			.type = type,
+		};
+
+		mmap_read_lock(mm);
+		for (vma = mm->mmap; vma; vma = vma->vm_next) {
+			if (is_vm_hugetlb_page(vma))
+				continue;
+
+			if (vma->vm_flags & VM_LOCKED)
+				continue;
+
+			if (type == RECLAIM_ANON && !vma_is_anonymous(vma))
+				continue;
+			if ((type == RECLAIM_FILE || type == RECLAIM_SHMEM)
+					&& vma_is_anonymous(vma)) {
+				continue;
+			}
+
+			if (vma_is_anonymous(vma) || shmem_file(vma->vm_file)) {
+				if (get_nr_swap_pages() <= 0 ||
+					get_mm_counter(mm, MM_ANONPAGES) == 0) {
+					if (type == RECLAIM_ALL)
+						continue;
+					else
+						break;
+				}
+
+				if (shmem_file(vma->vm_file) && type != RECLAIM_SHMEM) {
+					continue;
+				}
+
+				reclaim_walk.pmd_entry = reclaim_pte_range;
+			} else {
+				reclaim_walk.pmd_entry = deactivate_pte_range;
+			}
+
+			walk_page_range(mm, vma->vm_start, vma->vm_end,
+					&reclaim_walk, (void*)&reclaim_data);
+		}
+		flush_tlb_mm(mm);
+		mmap_read_unlock(mm);
+		mmput(mm);
+	}
+	put_task_struct(task);
+
+	return count;
+}
+
+const struct file_operations proc_reclaim_operations = {
+	.write		= reclaim_write,
+	.llseek		= noop_llseek,
+};
+#endif
+
 #ifdef CONFIG_NUMA
 
 struct numa_maps {
diff -ruN a/fs/read_write.c b/fs/read_write.c
--- a/fs/read_write.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/read_write.c	2021-12-23 08:35:53.000000000 +0100
@@ -493,6 +493,8 @@
 	return ret;
 }
 
+EXPORT_SYMBOL(vfs_read);
+
 static ssize_t new_sync_write(struct file *filp, const char __user *buf, size_t len, loff_t *ppos)
 {
 	struct iovec iov = { .iov_base = (void __user *)buf, .iov_len = len };
@@ -602,6 +604,7 @@
 	file_end_write(file);
 	return ret;
 }
+EXPORT_SYMBOL(vfs_write);
 
 /* file_ppos returns &file->f_pos or NULL if file is stream */
 static inline loff_t *file_ppos(struct file *file)
diff -ruN a/fs/reiserfs/xattr_security.c b/fs/reiserfs/xattr_security.c
--- a/fs/reiserfs/xattr_security.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/reiserfs/xattr_security.c	2021-12-23 08:35:53.000000000 +0100
@@ -11,7 +11,8 @@
 
 static int
 security_get(const struct xattr_handler *handler, struct dentry *unused,
-	     struct inode *inode, const char *name, void *buffer, size_t size)
+	     struct inode *inode, const char *name, void *buffer, size_t size,
+	     int flags)
 {
 	if (IS_PRIVATE(inode))
 		return -EPERM;
diff -ruN a/fs/reiserfs/xattr_trusted.c b/fs/reiserfs/xattr_trusted.c
--- a/fs/reiserfs/xattr_trusted.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/reiserfs/xattr_trusted.c	2021-12-23 08:35:53.000000000 +0100
@@ -10,7 +10,8 @@
 
 static int
 trusted_get(const struct xattr_handler *handler, struct dentry *unused,
-	    struct inode *inode, const char *name, void *buffer, size_t size)
+	    struct inode *inode, const char *name, void *buffer, size_t size,
+	    int flags)
 {
 	if (!capable(CAP_SYS_ADMIN) || IS_PRIVATE(inode))
 		return -EPERM;
diff -ruN a/fs/reiserfs/xattr_user.c b/fs/reiserfs/xattr_user.c
--- a/fs/reiserfs/xattr_user.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/reiserfs/xattr_user.c	2021-12-23 08:35:53.000000000 +0100
@@ -9,7 +9,8 @@
 
 static int
 user_get(const struct xattr_handler *handler, struct dentry *unused,
-	 struct inode *inode, const char *name, void *buffer, size_t size)
+	 struct inode *inode, const char *name, void *buffer, size_t size,
+	 int flags)
 {
 	if (!reiserfs_xattrs_user(inode->i_sb))
 		return -EOPNOTSUPP;
diff -ruN a/fs/squashfs/xattr.c b/fs/squashfs/xattr.c
--- a/fs/squashfs/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/squashfs/xattr.c	2021-12-23 08:35:53.000000000 +0100
@@ -204,7 +204,7 @@
 				      struct dentry *unused,
 				      struct inode *inode,
 				      const char *name,
-				      void *buffer, size_t size)
+				      void *buffer, size_t size, int flags)
 {
 	return squashfs_xattr_get(inode, handler->flags, name,
 		buffer, size);
diff -ruN a/fs/sync.c b/fs/sync.c
--- a/fs/sync.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/sync.c	2021-12-23 08:35:53.000000000 +0100
@@ -9,7 +9,7 @@
 #include <linux/slab.h>
 #include <linux/export.h>
 #include <linux/namei.h>
-#include <linux/sched.h>
+#include <linux/sched/xacct.h>
 #include <linux/writeback.h>
 #include <linux/syscalls.h>
 #include <linux/linkage.h>
@@ -223,6 +223,7 @@
 	if (f.file) {
 		ret = vfs_fsync(f.file, datasync);
 		fdput(f);
+		inc_syscfs(current);
 	}
 	return ret;
 }
diff -ruN a/fs/ubifs/xattr.c b/fs/ubifs/xattr.c
--- a/fs/ubifs/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/ubifs/xattr.c	2021-12-23 08:35:53.000000000 +0100
@@ -689,7 +689,8 @@
 
 static int xattr_get(const struct xattr_handler *handler,
 			   struct dentry *dentry, struct inode *inode,
-			   const char *name, void *buffer, size_t size)
+			   const char *name, void *buffer, size_t size,
+			   int flags)
 {
 	dbg_gen("xattr '%s', ino %lu ('%pd'), buf size %zd", name,
 		inode->i_ino, dentry, size);
diff -ruN a/fs/userfaultfd.c b/fs/userfaultfd.c
--- a/fs/userfaultfd.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/userfaultfd.c	2021-12-23 08:35:53.000000000 +0100
@@ -877,7 +877,8 @@
 				 new_flags, vma->anon_vma,
 				 vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
-				 NULL_VM_UFFD_CTX);
+				 NULL_VM_UFFD_CTX,
+				 vma_get_anon_name(vma));
 		if (prev)
 			vma = prev;
 		else
@@ -1436,7 +1437,8 @@
 		prev = vma_merge(mm, prev, start, vma_end, new_flags,
 				 vma->anon_vma, vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
-				 ((struct vm_userfaultfd_ctx){ ctx }));
+				 ((struct vm_userfaultfd_ctx){ ctx }),
+				 vma_get_anon_name(vma));
 		if (prev) {
 			vma = prev;
 			goto next;
@@ -1613,7 +1615,8 @@
 		prev = vma_merge(mm, prev, start, vma_end, new_flags,
 				 vma->anon_vma, vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
-				 NULL_VM_UFFD_CTX);
+				 NULL_VM_UFFD_CTX,
+				 vma_get_anon_name(vma));
 		if (prev) {
 			vma = prev;
 			goto next;
diff -ruN a/fs/verity/signature.c b/fs/verity/signature.c
--- a/fs/verity/signature.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/verity/signature.c	2021-12-23 08:35:53.000000000 +0100
@@ -40,11 +40,38 @@
 int fsverity_verify_signature(const struct fsverity_info *vi,
 			      const u8 *signature, size_t sig_size)
 {
-	const struct inode *inode = vi->inode;
-	const struct fsverity_hash_alg *hash_alg = vi->tree_params.hash_alg;
+	unsigned int digest_algorithm =
+		vi->tree_params.hash_alg - fsverity_hash_algs;
+
+	return __fsverity_verify_signature(vi->inode, signature, sig_size,
+					   vi->file_digest, digest_algorithm);
+}
+
+/**
+ * __fsverity_verify_signature() - check a verity file's signature
+ * @inode: the file's inode
+ * @signature: the file's signature
+ * @sig_size: size of @signature. Can be 0 if there is no signature
+ * @file_digest: the file's digest
+ * @digest_algorithm: the digest algorithm used
+ *
+ * Takes the file's digest and optional signature and verifies the signature
+ * against the digest and the fs-verity keyring if appropriate
+ *
+ * Return: 0 on success (signature valid or not required); -errno on failure
+ */
+int __fsverity_verify_signature(const struct inode *inode, const u8 *signature,
+				size_t sig_size, const u8 *file_digest,
+				unsigned int digest_algorithm)
+{
 	struct fsverity_formatted_digest *d;
+	struct fsverity_hash_alg *hash_alg = fsverity_get_hash_alg(inode,
+							digest_algorithm);
 	int err;
 
+	if (IS_ERR(hash_alg))
+		return PTR_ERR(hash_alg);
+
 	if (sig_size == 0) {
 		if (fsverity_require_signatures) {
 			fsverity_err(inode,
@@ -60,7 +87,7 @@
 	memcpy(d->magic, "FSVerity", 8);
 	d->digest_algorithm = cpu_to_le16(hash_alg - fsverity_hash_algs);
 	d->digest_size = cpu_to_le16(hash_alg->digest_size);
-	memcpy(d->digest, vi->file_digest, hash_alg->digest_size);
+	memcpy(d->digest, file_digest, hash_alg->digest_size);
 
 	err = verify_pkcs7_signature(d, sizeof(*d) + hash_alg->digest_size,
 				     signature, sig_size, fsverity_keyring,
@@ -83,9 +110,10 @@
 	}
 
 	pr_debug("Valid signature for file digest %s:%*phN\n",
-		 hash_alg->name, hash_alg->digest_size, vi->file_digest);
+		 hash_alg->name, hash_alg->digest_size, file_digest);
 	return 0;
 }
+EXPORT_SYMBOL_GPL(__fsverity_verify_signature);
 
 #ifdef CONFIG_SYSCTL
 static struct ctl_table_header *fsverity_sysctl_header;
diff -ruN a/fs/xattr.c b/fs/xattr.c
--- a/fs/xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/xattr.c	2021-12-23 08:35:53.000000000 +0100
@@ -369,7 +369,7 @@
 		return PTR_ERR(handler);
 	if (!handler->get)
 		return -EOPNOTSUPP;
-	error = handler->get(handler, dentry, inode, name, NULL, 0);
+	error = handler->get(handler, dentry, inode, name, NULL, 0, 0);
 	if (error < 0)
 		return error;
 
@@ -380,33 +380,21 @@
 		memset(value, 0, error + 1);
 	}
 
-	error = handler->get(handler, dentry, inode, name, value, error);
+	error = handler->get(handler, dentry, inode, name, value, error, 0);
 	*xattr_value = value;
 	return error;
 }
 
 ssize_t
-__vfs_getxattr(struct dentry *dentry, struct inode *inode, const char *name,
-	       void *value, size_t size)
+__vfs_getxattr(struct user_namespace *mnt_userns, struct dentry *dentry,
+	       struct inode *inode, const char *name, void *value, size_t size,
+	       int flags)
 {
 	const struct xattr_handler *handler;
-
-	handler = xattr_resolve_name(inode, &name);
-	if (IS_ERR(handler))
-		return PTR_ERR(handler);
-	if (!handler->get)
-		return -EOPNOTSUPP;
-	return handler->get(handler, dentry, inode, name, value, size);
-}
-EXPORT_SYMBOL(__vfs_getxattr);
-
-ssize_t
-vfs_getxattr(struct user_namespace *mnt_userns, struct dentry *dentry,
-	     const char *name, void *value, size_t size)
-{
-	struct inode *inode = dentry->d_inode;
 	int error;
 
+	if (flags & XATTR_NOSECURITY)
+		goto nolsm;
 	error = xattr_permission(mnt_userns, inode, name, MAY_READ);
 	if (error)
 		return error;
@@ -429,7 +417,20 @@
 		return ret;
 	}
 nolsm:
-	return __vfs_getxattr(dentry, inode, name, value, size);
+	handler = xattr_resolve_name(inode, &name);
+	if (IS_ERR(handler))
+		return PTR_ERR(handler);
+	if (!handler->get)
+		return -EOPNOTSUPP;
+	return handler->get(handler, dentry, inode, name, value, size, flags);
+}
+EXPORT_SYMBOL(__vfs_getxattr);
+
+ssize_t
+vfs_getxattr(struct user_namespace *mnt_userns, struct dentry *dentry,
+	     const char *name, void *value, size_t size)
+{
+	return __vfs_getxattr(mnt_userns, dentry, dentry->d_inode, name, value, size, 0);
 }
 EXPORT_SYMBOL_GPL(vfs_getxattr);
 
diff -ruN a/fs/xfs/xfs_xattr.c b/fs/xfs/xfs_xattr.c
--- a/fs/xfs/xfs_xattr.c	2021-12-08 09:04:57.000000000 +0100
+++ b/fs/xfs/xfs_xattr.c	2021-12-23 08:35:53.000000000 +0100
@@ -21,7 +21,8 @@
 
 static int
 xfs_xattr_get(const struct xattr_handler *handler, struct dentry *unused,
-		struct inode *inode, const char *name, void *value, size_t size)
+		struct inode *inode, const char *name, void *value, size_t size,
+		int flags)
 {
 	struct xfs_da_args	args = {
 		.dp		= XFS_I(inode),
diff -ruN a/.gitignore b/.gitignore
--- a/.gitignore	2021-12-08 09:04:57.000000000 +0100
+++ b/.gitignore	2021-12-23 08:35:06.000000000 +0100
@@ -105,6 +105,9 @@
 /include/ksym/
 /arch/*/include/generated/
 
+# kernelconfig build directory
+/build/
+
 # stgit generated dirs
 patches-*
 
diff -ruN a/include/drm/drm_drv.h b/include/drm/drm_drv.h
--- a/include/drm/drm_drv.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/drm/drm_drv.h	2021-12-23 08:35:54.000000000 +0100
@@ -479,6 +479,8 @@
 			   const struct drm_driver *driver,
 			   size_t size, size_t offset);
 
+extern bool drm_master_relax;
+
 /**
  * devm_drm_dev_alloc - Resource managed allocation of a &drm_device instance
  * @parent: Parent device object
diff -ruN a/include/drm/drm_mode_object.h b/include/drm/drm_mode_object.h
--- a/include/drm/drm_mode_object.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/drm/drm_mode_object.h	2021-12-23 08:35:54.000000000 +0100
@@ -60,7 +60,7 @@
 	void (*free_cb)(struct kref *kref);
 };
 
-#define DRM_OBJECT_MAX_PROPERTY 24
+#define DRM_OBJECT_MAX_PROPERTY 64
 /**
  * struct drm_object_properties - property tracking for &drm_mode_object
  */
diff -ruN a/include/linux/alt-syscall.h b/include/linux/alt-syscall.h
--- a/include/linux/alt-syscall.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/alt-syscall.h	2021-12-23 08:35:54.000000000 +0100
@@ -0,0 +1,60 @@
+#ifndef _ALT_SYSCALL_H
+#define _ALT_SYSCALL_H
+
+#include <linux/errno.h>
+
+#ifdef CONFIG_ALT_SYSCALL
+
+#include <linux/list.h>
+#include <asm/syscall.h>
+
+#define ALT_SYS_CALL_NAME_MAX	32
+
+struct alt_sys_call_table {
+	char name[ALT_SYS_CALL_NAME_MAX + 1];
+	sys_call_ptr_t *table;
+	int size;
+#if defined(CONFIG_IA32_EMULATION) || \
+    (defined(CONFIG_ARM64) && defined(CONFIG_COMPAT))
+	sys_call_ptr_t *compat_table;
+	int compat_size;
+#endif
+	struct list_head node;
+};
+
+/*
+ * arch_dup_sys_call_table should return the default syscall table, not
+ * the current syscall table, since we want to explicitly not allow
+ * syscall table composition. A selected syscall table should be treated
+ * as a single execution personality.
+ */
+
+int arch_dup_sys_call_table(struct alt_sys_call_table *table);
+int arch_set_sys_call_table(struct alt_sys_call_table *table);
+
+int register_alt_sys_call_table(struct alt_sys_call_table *table);
+int set_alt_sys_call_table(char __user *name);
+
+#else
+
+struct alt_sys_call_table;
+
+static inline int arch_dup_sys_call_table(struct alt_sys_call_table *table)
+{
+	return -ENOSYS;
+}
+static inline int arch_set_sys_call_table(struct alt_sys_call_table *table)
+{
+	return -ENOSYS;
+}
+static inline int register_alt_sys_call_table(struct alt_sys_call_table *table)
+{
+	return -ENOSYS;
+}
+static inline int set_alt_sys_call_table(char __user *name)
+{
+	return -ENOSYS;
+}
+#endif
+
+#endif /* _ALT_SYSCALL_H */
diff -ruN a/include/linux/android_aid.h b/include/linux/android_aid.h
--- a/include/linux/android_aid.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/android_aid.h	2021-12-23 08:35:54.000000000 +0100
@@ -0,0 +1,28 @@
+/* include/linux/android_aid.h
+ *
+ * Copyright (C) 2008 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef _LINUX_ANDROID_AID_H
+#define _LINUX_ANDROID_AID_H
+
+/* AIDs that the kernel treats differently */
+#define AID_OBSOLETE_000 (3001)  /* was NET_BT_ADMIN */
+#define AID_OBSOLETE_001 (3002)  /* was NET_BT */
+#define AID_INET         (3003)
+#define AID_NET_RAW      (3004)
+#define AID_NET_ADMIN    (3005)
+#define AID_NET_BW_STATS (3006)  /* read bandwidth statistics */
+#define AID_NET_BW_ACCT  (3007)  /* change bandwidth statistics accounting */
+
+#endif
diff -ruN a/include/linux/chromeos_platform.h b/include/linux/chromeos_platform.h
--- a/include/linux/chromeos_platform.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/chromeos_platform.h	2021-12-23 08:35:54.000000000 +0100
@@ -0,0 +1,27 @@
+#ifndef _LINUX_CHROMEOS_PLATFORM_H
+#define _LINUX_CHROMEOS_PLATFORM_H
+
+#include <linux/errno.h>
+#include <linux/types.h>
+
+#ifdef CONFIG_CHROMEOS
+/*
+ * ChromeOS platform support code. Glue layer between higher level functions
+ * and per-platform firmware interfaces.
+ */
+
+/*
+ * Set the taint bit telling firmware that the currently running side needs
+ * recovery (or reinstall).
+ */
+extern int chromeos_set_need_recovery(void);
+
+#else
+
+static inline int chromeos_set_need_recovery(void)
+{
+	return -ENODEV;
+}
+#endif /* CONFIG_CHROMEOS */
+
+#endif /* _LINUX_CHROMEOS_PLATFORM_H */
diff -ruN a/include/linux/cpufreq.h b/include/linux/cpufreq.h
--- a/include/linux/cpufreq.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/cpufreq.h	2021-12-23 08:35:54.000000000 +0100
@@ -1078,14 +1078,6 @@
 }
 #endif
 
-#if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
-void sched_cpufreq_governor_change(struct cpufreq_policy *policy,
-			struct cpufreq_governor *old_gov);
-#else
-static inline void sched_cpufreq_governor_change(struct cpufreq_policy *policy,
-			struct cpufreq_governor *old_gov) { }
-#endif
-
 extern void arch_freq_prepare_all(void);
 extern unsigned int arch_freq_get_on_cpu(int cpu);
 
diff -ruN a/include/linux/cpufreq_times.h b/include/linux/cpufreq_times.h
--- a/include/linux/cpufreq_times.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/cpufreq_times.h	2021-12-23 08:35:54.000000000 +0100
@@ -0,0 +1,42 @@
+/* drivers/cpufreq/cpufreq_times.c
+ *
+ * Copyright (C) 2018 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef _LINUX_CPUFREQ_TIMES_H
+#define _LINUX_CPUFREQ_TIMES_H
+
+#include <linux/cpufreq.h>
+#include <linux/pid.h>
+
+#ifdef CONFIG_CPU_FREQ_TIMES
+void cpufreq_task_times_init(struct task_struct *p);
+void cpufreq_task_times_alloc(struct task_struct *p);
+void cpufreq_task_times_exit(struct task_struct *p);
+int proc_time_in_state_show(struct seq_file *m, struct pid_namespace *ns,
+			    struct pid *pid, struct task_struct *p);
+void cpufreq_acct_update_power(struct task_struct *p, u64 cputime);
+void cpufreq_times_create_policy(struct cpufreq_policy *policy);
+void cpufreq_times_record_transition(struct cpufreq_policy *policy,
+                                     unsigned int new_freq);
+#else
+static inline void cpufreq_task_times_init(struct task_struct *p) {}
+static inline void cpufreq_task_times_alloc(struct task_struct *p) {}
+static inline void cpufreq_task_times_exit(struct task_struct *p) {}
+static inline void cpufreq_acct_update_power(struct task_struct *p,
+					     u64 cputime) {}
+static inline void cpufreq_times_create_policy(struct cpufreq_policy *policy) {}
+static inline void cpufreq_times_record_transition(
+	struct cpufreq_policy *policy, unsigned int new_freq) {}
+#endif /* CONFIG_CPU_FREQ_TIMES */
+#endif /* _LINUX_CPUFREQ_TIMES_H */
diff -ruN a/include/linux/cpu.h b/include/linux/cpu.h
--- a/include/linux/cpu.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/cpu.h	2021-12-23 08:35:54.000000000 +0100
@@ -227,4 +227,5 @@
 extern bool cpu_mitigations_off(void);
 extern bool cpu_mitigations_auto_nosmt(void);
 
+extern bool coresched_cmd_secure(void);
 #endif /* _LINUX_CPU_H_ */
diff -ruN a/include/linux/devfreq.h b/include/linux/devfreq.h
--- a/include/linux/devfreq.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/devfreq.h	2021-12-23 08:35:54.000000000 +0100
@@ -38,6 +38,7 @@
 
 struct devfreq;
 struct devfreq_governor;
+struct devfreq_cpu_data;
 struct thermal_cooling_device;
 
 /**
@@ -288,6 +289,11 @@
 #endif
 
 #if IS_ENABLED(CONFIG_DEVFREQ_GOV_PASSIVE)
+enum devfreq_parent_dev_type {
+	DEVFREQ_PARENT_DEV,
+	CPUFREQ_PARENT_DEV,
+};
+
 /**
  * struct devfreq_passive_data - ``void *data`` fed to struct devfreq
  *	and devfreq_add_device
@@ -299,8 +305,10 @@
  *			using governors except for passive governor.
  *			If the devfreq device has the specific method to decide
  *			the next frequency, should use this callback.
- * @this:	the devfreq instance of own device.
- * @nb:		the notifier block for DEVFREQ_TRANSITION_NOTIFIER list
+ + * @parent_type	parent type of the device
+ + * @this:		the devfreq instance of own device.
+ + * @nb:		the notifier block for DEVFREQ_TRANSITION_NOTIFIER list
+ + * @cpudata:		the state min/max/current frequency of all online cpu's
  *
  * The devfreq_passive_data have to set the devfreq instance of parent
  * device with governors except for the passive governor. But, don't need to
@@ -314,9 +322,13 @@
 	/* Optional callback to decide the next frequency of passvice device */
 	int (*get_target_freq)(struct devfreq *this, unsigned long *freq);
 
+	/* Should set the type of parent device */
+	enum devfreq_parent_dev_type parent_type;
+
 	/* For passive governor's internal use. Don't need to set them */
 	struct devfreq *this;
 	struct notifier_block nb;
+	struct devfreq_cpu_data *cpudata[NR_CPUS];
 };
 #endif
 
diff -ruN a/include/linux/dma-fence-array.h b/include/linux/dma-fence-array.h
--- a/include/linux/dma-fence-array.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/dma-fence-array.h	2021-12-23 08:35:54.000000000 +0100
@@ -74,6 +74,19 @@
 	return container_of(fence, struct dma_fence_array, base);
 }
 
+/**
+ * dma_fence_array_for_each - iterate over all fences in array
+ * @fence: current fence
+ * @index: index into the array
+ * @head: potential dma_fence_array object
+ *
+ * Test if @array is a dma_fence_array object and if yes iterate over all fences
+ * in the array. If not just iterate over the fence in @array itself.
+ */
+#define dma_fence_array_for_each(fence, index, head)			\
+	for (index = 0, fence = dma_fence_array_first(head); fence;	\
+	     ++(index), fence = dma_fence_array_next(head, index))
+
 struct dma_fence_array *dma_fence_array_create(int num_fences,
 					       struct dma_fence **fences,
 					       u64 context, unsigned seqno,
@@ -81,4 +94,8 @@
 
 bool dma_fence_match_context(struct dma_fence *fence, u64 context);
 
+struct dma_fence *dma_fence_array_first(struct dma_fence *head);
+struct dma_fence *dma_fence_array_next(struct dma_fence *head,
+				       unsigned int index);
+
 #endif /* __LINUX_DMA_FENCE_ARRAY_H */
diff -ruN a/include/linux/dma-resv.h b/include/linux/dma-resv.h
--- a/include/linux/dma-resv.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/dma-resv.h	2021-12-23 08:35:54.000000000 +0100
@@ -275,6 +275,7 @@
 int dma_resv_get_fences(struct dma_resv *obj, struct dma_fence **pfence_excl,
 			unsigned *pshared_count, struct dma_fence ***pshared);
 int dma_resv_copy_fences(struct dma_resv *dst, struct dma_resv *src);
+struct dma_fence *dma_resv_get_singleton(struct dma_resv *obj);
 long dma_resv_wait_timeout(struct dma_resv *obj, bool wait_all, bool intr,
 			   unsigned long timeout);
 bool dma_resv_test_signaled(struct dma_resv *obj, bool test_all);
diff -ruN a/include/linux/eventfd.h b/include/linux/eventfd.h
--- a/include/linux/eventfd.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/eventfd.h	2021-12-23 08:35:54.000000000 +0100
@@ -27,8 +27,14 @@
 #define EFD_CLOEXEC O_CLOEXEC
 #define EFD_NONBLOCK O_NONBLOCK
 
+/*
+ * We intentionally use the value of O_NOFOLLOW for EFD_ZERO_ON_WAKE
+ * because O_NOFOLLOW would have no meaning with an eventfd.
+ */
+#define EFD_ZERO_ON_WAKE O_NOFOLLOW
+
 #define EFD_SHARED_FCNTL_FLAGS (O_CLOEXEC | O_NONBLOCK)
-#define EFD_FLAGS_SET (EFD_SHARED_FCNTL_FLAGS | EFD_SEMAPHORE)
+#define EFD_FLAGS_SET (EFD_SHARED_FCNTL_FLAGS | EFD_SEMAPHORE | EFD_ZERO_ON_WAKE)
 
 struct eventfd_ctx;
 struct file;
diff -ruN a/include/linux/fs.h b/include/linux/fs.h
--- a/include/linux/fs.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/fs.h	2021-12-23 08:35:54.000000000 +0100
@@ -3508,6 +3508,11 @@
 	return 0;
 }
 
+static inline rwf_t iocb_to_rw_flags(int ifl, int iocb_mask)
+{
+	return ifl & iocb_mask;
+}
+
 static inline ino_t parent_ino(struct dentry *dentry)
 {
 	ino_t res;
diff -ruN a/include/linux/fsverity.h b/include/linux/fsverity.h
--- a/include/linux/fsverity.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/fsverity.h	2021-12-23 08:35:54.000000000 +0100
@@ -233,4 +233,18 @@
 	return fsverity_get_info(inode) != NULL;
 }
 
+#ifdef CONFIG_FS_VERITY_BUILTIN_SIGNATURES
+int __fsverity_verify_signature(const struct inode *inode, const u8 *signature,
+				size_t sig_size, const u8 *file_digest,
+				unsigned int digest_algorithm);
+#else /* !CONFIG_FS_VERITY_BUILTIN_SIGNATURES */
+static inline int __fsverity_verify_signature(const struct inode *inode,
+				const u8 *signature, size_t sig_size,
+				const u8 *file_digest,
+				unsigned int digest_algorithm)
+{
+	return 0;
+}
+#endif /* !CONFIG_FS_VERITY_BUILTIN_SIGNATURES */
+
 #endif	/* _LINUX_FSVERITY_H */
diff -ruN a/include/linux/hid.h b/include/linux/hid.h
--- a/include/linux/hid.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/hid.h	2021-12-23 08:35:54.000000000 +0100
@@ -343,6 +343,7 @@
 #define HID_QUIRK_INPUT_PER_APP			BIT(11)
 #define HID_QUIRK_X_INVERT			BIT(12)
 #define HID_QUIRK_Y_INVERT			BIT(13)
+#define HID_QUIRK_DEVICE_IS_DIGITIZER		BIT(14)
 #define HID_QUIRK_SKIP_OUTPUT_REPORTS		BIT(16)
 #define HID_QUIRK_SKIP_OUTPUT_REPORT_ID		BIT(17)
 #define HID_QUIRK_NO_OUTPUT_REPORTS_ON_INTR_EP	BIT(18)
@@ -394,6 +395,7 @@
 
 struct hid_global {
 	unsigned usage_page;
+	/* HID Global fields are constrained by spec to 32-bits */
 	__s32    logical_minimum;
 	__s32    logical_maximum;
 	__s32    physical_minimum;
@@ -460,7 +462,7 @@
 	unsigned  maxusage;		/* maximum usage index */
 	unsigned  flags;		/* main-item flags (i.e. volatile,array,constant) */
 	unsigned  report_offset;	/* bit offset in the report */
-	unsigned  report_size;		/* size of this field in the report */
+	unsigned  report_size;		/* size of this field in the report, in bits */
 	unsigned  report_count;		/* number of this field in the report */
 	unsigned  report_type;		/* (input,output,feature) */
 	__s32    *value;		/* last known value(s) */
@@ -589,8 +591,14 @@
 	__s32 battery_max;
 	__s32 battery_report_type;
 	__s32 battery_report_id;
+	__u64 battery_serial_number;
+	__u64 battery_new_serial_number;				/* gather entire updated 64-bit SN here for end of report */
+	char battery_serial_number_str[17];				/* Space for max 16 hex digits */
 	enum hid_battery_status battery_status;
 	bool battery_avoid_query;
+	bool battery_state_changed;					/* a battery field has been changed within the current report */
+	bool battery_reported;
+	bool battery_sn_64bit;						/* whether battery S/N is 32 or 64 bits long */
 	ktime_t battery_ratelimit_time;
 #endif
 
diff -ruN a/include/linux/ieee80211.h b/include/linux/ieee80211.h
--- a/include/linux/ieee80211.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/ieee80211.h	2021-12-23 08:35:54.000000000 +0100
@@ -2084,6 +2084,7 @@
 
 #define IEEE80211_HE_VHT_MAX_AMPDU_FACTOR	20
 #define IEEE80211_HE_HT_MAX_AMPDU_FACTOR	16
+#define IEEE80211_HE_6GHZ_MAX_AMPDU_FACTOR	13
 
 /* 802.11ax HE PHY capabilities */
 #define IEEE80211_HE_PHY_CAP0_CHANNEL_WIDTH_SET_40MHZ_IN_2G		0x02
diff -ruN a/include/linux/iio/common/cros_ec_sensors_core.h b/include/linux/iio/common/cros_ec_sensors_core.h
--- a/include/linux/iio/common/cros_ec_sensors_core.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/iio/common/cros_ec_sensors_core.h	2021-12-23 08:35:54.000000000 +0100
@@ -26,7 +26,6 @@
 
 /*
  * 4 16 bit channels are allowed.
- * Good enough for current sensors, they use up to 3 16 bit vectors.
  */
 #define CROS_EC_SAMPLE_SIZE  (sizeof(s64) * 2)
 
@@ -125,5 +124,6 @@
 
 /* List of extended channel specification for all sensors. */
 extern const struct iio_chan_spec_ext_info cros_ec_sensors_ext_info[];
+extern const struct iio_chan_spec_ext_info cros_ec_sensors_limited_info[];
 
 #endif  /* __CROS_EC_SENSORS_CORE_H */
diff -ruN a/include/linux/io-pgtable.h b/include/linux/io-pgtable.h
--- a/include/linux/io-pgtable.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/io-pgtable.h	2021-12-23 08:35:54.000000000 +0100
@@ -148,6 +148,13 @@
  * @unmap:        Unmap a physically contiguous memory region.
  * @unmap_pages:  Unmap a range of virtually contiguous pages of the same size.
  * @iova_to_phys: Translate iova to physical address.
+ * @pgtable_walk: Return details of a page table walk for a given iova.
+ *                This returns the array of PTEs in a format that is
+ *                specific to the page table format.  The number of
+ *                PTEs can be format specific.  The num_ptes parameter
+ *                on input specifies the size of the ptes array, and
+ *                on output the number of PTEs filled in (which depends
+ *                on the number of PTEs walked to resolve the iova)
  *
  * These functions map directly onto the iommu_ops member functions with
  * the same names.
@@ -165,6 +172,8 @@
 			      struct iommu_iotlb_gather *gather);
 	phys_addr_t (*iova_to_phys)(struct io_pgtable_ops *ops,
 				    unsigned long iova);
+	int (*pgtable_walk)(struct io_pgtable_ops *ops, unsigned long iova,
+			    void *ptes, int *num_ptes);
 };
 
 /**
diff -ruN a/include/linux/ipv6.h b/include/linux/ipv6.h
--- a/include/linux/ipv6.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/ipv6.h	2021-12-23 08:35:54.000000000 +0100
@@ -43,6 +43,7 @@
 	__s32		accept_ra_rt_info_max_plen;
 #endif
 #endif
+	__s32		accept_ra_rt_table;
 	__s32		proxy_ndp;
 	__s32		accept_source_route;
 	__s32		accept_ra_from_local;
diff -ruN a/include/linux/kvm_host.h b/include/linux/kvm_host.h
--- a/include/linux/kvm_host.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/kvm_host.h	2021-12-23 08:35:54.000000000 +0100
@@ -862,6 +862,19 @@
 			       bool atomic, bool *async, bool write_fault,
 			       bool *writable, hva_t *hva);
 
+kvm_pfn_t gfn_to_pfn_page(struct kvm *kvm, gfn_t gfn, struct page **page);
+kvm_pfn_t gfn_to_pfn_page_prot(struct kvm *kvm, gfn_t gfn,
+			       bool write_fault, bool *writable,
+			       struct page **page);
+kvm_pfn_t gfn_to_pfn_page_memslot(struct kvm_memory_slot *slot,
+				  gfn_t gfn, struct page **page);
+kvm_pfn_t gfn_to_pfn_page_memslot_atomic(struct kvm_memory_slot *slot,
+					 gfn_t gfn, struct page **page);
+kvm_pfn_t __gfn_to_pfn_page_memslot(struct kvm_memory_slot *slot,
+				    gfn_t gfn, bool atomic, bool *async,
+				    bool write_fault, bool *writable,
+				    hva_t *hva, struct page **page);
+
 void kvm_release_pfn_clean(kvm_pfn_t pfn);
 void kvm_release_pfn_dirty(kvm_pfn_t pfn);
 void kvm_set_pfn_dirty(kvm_pfn_t pfn);
@@ -942,6 +955,10 @@
 struct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);
 kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);
 kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);
+kvm_pfn_t kvm_vcpu_gfn_to_pfn_page_atomic(struct kvm_vcpu *vcpu, gfn_t gfn,
+					  struct page **page);
+kvm_pfn_t kvm_vcpu_gfn_to_pfn_page(struct kvm_vcpu *vcpu, gfn_t gfn,
+				   struct page **page);
 int kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa, struct kvm_host_map *map);
 int kvm_map_gfn(struct kvm_vcpu *vcpu, gfn_t gfn, struct kvm_host_map *map,
 		struct gfn_to_pfn_cache *cache, bool atomic);
diff -ruN a/include/linux/low-mem-notify.h b/include/linux/low-mem-notify.h
--- a/include/linux/low-mem-notify.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/low-mem-notify.h	2021-12-23 08:35:54.000000000 +0100
@@ -0,0 +1,22 @@
+#ifndef _LINUX_LOW_MEM_NOTIFY_H
+#define _LINUX_LOW_MEM_NOTIFY_H
+
+#include <linux/types.h>
+
+#ifdef CONFIG_LOW_MEM_NOTIFY
+extern const struct file_operations low_mem_notify_fops;
+
+void low_mem_notify(void);
+bool low_mem_check(void);
+#else
+static inline void low_mem_notify(void)
+{
+}
+
+static inline bool low_mem_check(void)
+{
+	return false;
+}
+#endif
+
+#endif
diff -ruN a/include/linux/mmc/sdio_ids.h b/include/linux/mmc/sdio_ids.h
--- a/include/linux/mmc/sdio_ids.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/mmc/sdio_ids.h	2021-12-23 08:35:54.000000000 +0100
@@ -105,6 +105,7 @@
 #define SDIO_VENDOR_ID_MEDIATEK			0x037a
 #define SDIO_DEVICE_ID_MEDIATEK_MT7663		0x7663
 #define SDIO_DEVICE_ID_MEDIATEK_MT7668		0x7668
+#define SDIO_DEVICE_ID_MEDIATEK_MT7961		0x7961
 
 #define SDIO_VENDOR_ID_MICROCHIP_WILC		0x0296
 #define SDIO_DEVICE_ID_MICROCHIP_WILC1000	0x5347
diff -ruN a/include/linux/mm.h b/include/linux/mm.h
--- a/include/linux/mm.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/mm.h	2021-12-23 08:35:54.000000000 +0100
@@ -202,6 +202,7 @@
 #define DEFAULT_MAX_MAP_COUNT	(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)
 
 extern int sysctl_max_map_count;
+extern int sysctl_mmap_noexec_taint;
 
 extern unsigned long sysctl_user_reserve_kbytes;
 extern unsigned long sysctl_admin_reserve_kbytes;
@@ -210,6 +211,8 @@
 extern int sysctl_overcommit_ratio;
 extern unsigned long sysctl_overcommit_kbytes;
 
+extern int sysctl_disk_based_swap;
+
 int overcommit_ratio_handler(struct ctl_table *, int, void *, size_t *,
 		loff_t *);
 int overcommit_kbytes_handler(struct ctl_table *, int, void *, size_t *,
@@ -2548,7 +2551,7 @@
 extern struct vm_area_struct *vma_merge(struct mm_struct *,
 	struct vm_area_struct *prev, unsigned long addr, unsigned long end,
 	unsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,
-	struct mempolicy *, struct vm_userfaultfd_ctx);
+	struct mempolicy *, struct vm_userfaultfd_ctx, const char __user *);
 extern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);
 extern int __split_vma(struct mm_struct *, struct vm_area_struct *,
 	unsigned long addr, int new_below);
@@ -3245,6 +3248,7 @@
 #endif
 
 extern int sysctl_nr_trim_pages;
+extern int min_filelist_kbytes;
 
 #ifdef CONFIG_PRINTK
 void mem_dump_obj(void *object);
diff -ruN a/include/linux/mm_types.h b/include/linux/mm_types.h
--- a/include/linux/mm_types.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/mm_types.h	2021-12-23 08:35:54.000000000 +0100
@@ -350,11 +350,18 @@
 	/*
 	 * For areas with an address space and backing store,
 	 * linkage into the address_space->i_mmap interval tree.
+	 *
+	 * For private anonymous mappings, a pointer to a null terminated string
+	 * in the user process containing the name given to the vma, or NULL
+	 * if unnamed.
 	 */
-	struct {
-		struct rb_node rb;
-		unsigned long rb_subtree_last;
-	} shared;
+	union {
+		struct {
+			struct rb_node rb;
+			unsigned long rb_subtree_last;
+		} shared;
+		const char __user *anon_name;
+	};
 
 	/*
 	 * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma
@@ -809,4 +816,13 @@
 	unsigned long val;
 } swp_entry_t;
 
+/* Return the name for an anonymous mapping or NULL for a file-backed mapping */
+static inline const char __user *vma_get_anon_name(struct vm_area_struct *vma)
+{
+	if (vma->vm_file)
+		return NULL;
+
+	return vma->anon_name;
+}
+
 #endif /* _LINUX_MM_TYPES_H */
diff -ruN a/include/linux/netfilter/xt_quota2.h b/include/linux/netfilter/xt_quota2.h
--- a/include/linux/netfilter/xt_quota2.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/netfilter/xt_quota2.h	2021-12-23 08:35:54.000000000 +0100
@@ -0,0 +1,26 @@
+#ifndef _XT_QUOTA_H
+#define _XT_QUOTA_H
+#include <linux/types.h>
+
+enum xt_quota_flags {
+	XT_QUOTA_INVERT    = 1 << 0,
+	XT_QUOTA_GROW      = 1 << 1,
+	XT_QUOTA_PACKET    = 1 << 2,
+	XT_QUOTA_NO_CHANGE = 1 << 3,
+	XT_QUOTA_MASK      = 0x0F,
+};
+
+struct xt_quota_counter;
+
+struct xt_quota_mtinfo2 {
+	char name[15];
+	u_int8_t flags;
+
+	/* Comparison-invariant */
+	aligned_u64 quota;
+
+	/* Used internally by the kernel */
+	struct xt_quota_counter *master __attribute__((aligned(8)));
+};
+
+#endif /* _XT_QUOTA_H */
diff -ruN a/include/linux/nmi.h b/include/linux/nmi.h
--- a/include/linux/nmi.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/nmi.h	2021-12-23 08:35:54.000000000 +0100
@@ -45,6 +45,8 @@
 extern void touch_softlockup_watchdog_sync(void);
 extern void touch_all_softlockup_watchdogs(void);
 extern unsigned int  softlockup_panic;
+DECLARE_PER_CPU(unsigned long, hrtimer_interrupts);
+DECLARE_PER_CPU(unsigned long, hrtimer_interrupts_saved);
 
 extern int lockup_detector_online_cpu(unsigned int cpu);
 extern int lockup_detector_offline_cpu(unsigned int cpu);
@@ -81,14 +83,14 @@
 #define NMI_WATCHDOG_ENABLED      (1 << NMI_WATCHDOG_ENABLED_BIT)
 #define SOFT_WATCHDOG_ENABLED     (1 << SOFT_WATCHDOG_ENABLED_BIT)
 
-#if defined(CONFIG_HARDLOCKUP_DETECTOR)
+#if defined(CONFIG_HARDLOCKUP_DETECTOR_CORE)
 extern void hardlockup_detector_disable(void);
 extern unsigned int hardlockup_panic;
 #else
 static inline void hardlockup_detector_disable(void) {}
 #endif
 
-#if defined(CONFIG_HAVE_NMI_WATCHDOG) || defined(CONFIG_HARDLOCKUP_DETECTOR)
+#if defined(CONFIG_HAVE_NMI_WATCHDOG) || defined(CONFIG_HARDLOCKUP_DETECTOR_CORE)
 # define NMI_WATCHDOG_SYSCTL_PERM	0644
 #else
 # define NMI_WATCHDOG_SYSCTL_PERM	0444
@@ -122,6 +124,14 @@
 int watchdog_nmi_enable(unsigned int cpu);
 void watchdog_nmi_disable(unsigned int cpu);
 
+#ifdef CONFIG_HARDLOCKUP_DETECTOR_BUDDY_CPU
+extern void buddy_cpu_touch_watchdog(void);
+void watchdog_check_hardlockup(void);
+#else
+static inline void buddy_cpu_touch_watchdog(void) {}
+static inline void watchdog_check_hardlockup(void) {}
+#endif
+
 /**
  * touch_nmi_watchdog - restart NMI watchdog timeout.
  *
@@ -132,6 +142,7 @@
 static inline void touch_nmi_watchdog(void)
 {
 	arch_touch_nmi_watchdog();
+	buddy_cpu_touch_watchdog();
 	touch_softlockup_watchdog();
 }
 
@@ -195,7 +206,7 @@
 #endif
 
 #if defined(CONFIG_HARDLOCKUP_CHECK_TIMESTAMP) && \
-    defined(CONFIG_HARDLOCKUP_DETECTOR)
+    defined(CONFIG_HARDLOCKUP_DETECTOR_CORE)
 void watchdog_update_hrtimer_threshold(u64 period);
 #else
 static inline void watchdog_update_hrtimer_threshold(u64 period) { }
diff -ruN a/include/linux/path.h b/include/linux/path.h
--- a/include/linux/path.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/path.h	2021-12-23 08:35:54.000000000 +0100
@@ -5,9 +5,14 @@
 struct dentry;
 struct vfsmount;
 
+#define PATH_LINK_COUNT_VALID 0x80000000
+
 struct path {
 	struct vfsmount *mnt;
 	struct dentry *dentry;
+#ifdef CONFIG_SECURITY_CHROMIUMOS_NO_SYMLINK_MOUNT
+	int link_count;
+#endif /* CONFIG_SECURITY_CHROMIUMOS_NO_SYMLINK_MOUNT */
 } __randomize_layout;
 
 extern void path_get(const struct path *);
diff -ruN a/include/linux/pkglist.h b/include/linux/pkglist.h
--- a/include/linux/pkglist.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/pkglist.h	2021-12-23 08:35:55.000000000 +0100
@@ -0,0 +1,38 @@
+#ifndef _PKGLIST_H_
+#define _PKGLIST_H_
+
+#include <linux/dcache.h>
+#include <linux/uidgid.h>
+
+#define QSTR_LITERAL(string) QSTR_INIT(string, sizeof(string)-1)
+
+static inline bool str_case_eq(const char *s1, const char *s2)
+{
+	return !strcasecmp(s1, s2);
+}
+
+static inline bool str_n_case_eq(const char *s1, const char *s2, size_t len)
+{
+	return !strncasecmp(s1, s2, len);
+}
+
+static inline bool qstr_case_eq(const struct qstr *q1, const struct qstr *q2)
+{
+	return q1->len == q2->len && str_case_eq(q1->name, q2->name);
+}
+
+#define BY_NAME		BIT(0)
+#define BY_USERID	BIT(1)
+
+struct pkg_list {
+	struct list_head list;
+	void (*update)(int flags, const struct qstr *name, uint32_t userid);
+};
+
+kuid_t pkglist_get_appid(const char *key);
+kgid_t pkglist_get_ext_gid(const char *key);
+bool pkglist_user_is_excluded(const char *key, uint32_t user);
+kuid_t pkglist_get_allowed_appid(const char *key, uint32_t user);
+void pkglist_register_update_listener(struct pkg_list *pkg);
+void pkglist_unregister_update_listener(struct pkg_list *pkg);
+#endif
diff -ruN a/include/linux/platform_data/cros_ec_commands.h b/include/linux/platform_data/cros_ec_commands.h
--- a/include/linux/platform_data/cros_ec_commands.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/platform_data/cros_ec_commands.h	2021-12-23 08:35:55.000000000 +0100
@@ -2340,6 +2340,12 @@
 	 */
 	MOTIONSENSE_CMD_SENSOR_SCALE = 18,
 
+	/*
+	 * Activity management
+	 * Retrieve current status of given activity.
+	 */
+	MOTIONSENSE_CMD_GET_ACTIVITY = 20,
+
 	/* Number of motionsense sub-commands. */
 	MOTIONSENSE_NUM_CMDS
 };
@@ -2354,6 +2360,7 @@
 	MOTIONSENSE_TYPE_ACTIVITY = 5,
 	MOTIONSENSE_TYPE_BARO = 6,
 	MOTIONSENSE_TYPE_SYNC = 7,
+	MOTIONSENSE_TYPE_LIGHT_RGB = 8,
 	MOTIONSENSE_TYPE_MAX,
 };
 
@@ -2387,6 +2394,7 @@
 	MOTIONSENSE_CHIP_LSM6DS3 = 17,
 	MOTIONSENSE_CHIP_LSM6DSO = 18,
 	MOTIONSENSE_CHIP_LNG2DM = 19,
+	MOTIONSENSE_CHIP_TCS3400 = 20,
 	MOTIONSENSE_CHIP_MAX,
 };
 
@@ -2399,6 +2407,11 @@
 	MOTIONSENSE_ORIENTATION_UNKNOWN = 4,
 };
 
+struct ec_response_activity_data {
+	uint8_t activity; /* motionsensor_activity */
+	uint8_t state;
+} __ec_todo_packed;
+
 struct ec_response_motion_sensor_data {
 	/* Flags for each sensor. */
 	uint8_t flags;
@@ -2406,15 +2419,14 @@
 	uint8_t sensor_num;
 	/* Each sensor is up to 3-axis. */
 	union {
-		int16_t             data[3];
+		int16_t                                  data[3];
 		struct __ec_todo_packed {
-			uint16_t    reserved;
-			uint32_t    timestamp;
+			uint16_t                         reserved;
+			uint32_t                         timestamp;
 		};
 		struct __ec_todo_unpacked {
-			uint8_t     activity; /* motionsensor_activity */
-			uint8_t     state;
-			int16_t     add_info[2];
+			struct ec_response_activity_data activity_data;
+			int16_t                          add_info[2];
 		};
 	};
 } __ec_todo_packed;
@@ -2446,6 +2458,7 @@
 	MOTIONSENSE_ACTIVITY_SIG_MOTION = 1,
 	MOTIONSENSE_ACTIVITY_DOUBLE_TAP = 2,
 	MOTIONSENSE_ACTIVITY_ORIENTATION = 3,
+	MOTIONSENSE_ACTIVITY_BODY_DETECTION = 4,
 };
 
 struct ec_motion_sense_activity {
@@ -2529,14 +2542,20 @@
 
 		/*
 		 * Used for MOTIONSENSE_CMD_INFO, MOTIONSENSE_CMD_DATA
-		 * and MOTIONSENSE_CMD_PERFORM_CALIB.
 		 */
 		struct __ec_todo_unpacked {
 			uint8_t sensor_num;
-		} info, info_3, data, fifo_flush, perform_calib,
-				list_activities;
+		} info, info_3, data, fifo_flush, list_activities;
 
 		/*
+		 * Used for MOTIONSENSE_CMD_PERFORM_CALIB:
+		 * Allow entering/exiting the calibration mode.
+		 */
+		struct __ec_todo_unpacked {
+			uint8_t sensor_num;
+			uint8_t enable;
+		} perform_calib;
+		/*
 		 * Used for MOTIONSENSE_CMD_EC_RATE, MOTIONSENSE_CMD_SENSOR_ODR
 		 * and MOTIONSENSE_CMD_SENSOR_RANGE.
 		 */
@@ -2623,6 +2642,7 @@
 			uint32_t max_data_vector;
 		} fifo_read;
 
+		/* Used for MOTIONSENSE_CMD_SET_ACTIVITY */
 		struct ec_motion_sense_activity set_activity;
 
 		/* Used for MOTIONSENSE_CMD_LID_ANGLE */
@@ -2668,6 +2688,14 @@
 			 */
 			int16_t hys_degree;
 		} tablet_mode_threshold;
+
+		/*
+		 * Used for MOTIONSENSE_CMD_GET_ACTIVITY.
+		 */
+		struct __ec_todo_unpacked {
+			uint8_t sensor_num;
+			uint8_t activity;  /* enum motionsensor_activity */
+		} get_activity;
 	};
 } __ec_todo_packed;
 
@@ -2785,6 +2813,10 @@
 			uint16_t hys_degree;
 		} tablet_mode_threshold;
 
+		/* USED for MOTIONSENSE_CMD_GET_ACTIVITY. */
+		struct __ec_todo_unpacked {
+			uint8_t     state;
+		} get_activity;
 	};
 } __ec_todo_packed;
 
@@ -5528,6 +5560,53 @@
 	}
 
 /*****************************************************************************/
+/* Locate peripheral chips
+ *
+ * Return values:
+ * EC_RES_UNAVAILABLE: The chip type is supported but not found on system.
+ * EC_RES_INVALID_PARAM: The chip type was unrecognized.
+ * EC_RES_OVERFLOW: The index number exceeded the number of chip instances.
+ */
+#define EC_CMD_LOCATE_CHIP 0x0126
+
+enum ec_chip_type {
+	EC_CHIP_TYPE_CBI_EEPROM = 0,
+	EC_CHIP_TYPE_TCPC = 1,
+	EC_CHIP_TYPE_COUNT,
+	EC_CHIP_TYPE_MAX = 0xFF,
+};
+
+enum ec_bus_type {
+	EC_BUS_TYPE_I2C = 0,
+	EC_BUS_TYPE_EMBEDDED = 1,
+	EC_BUS_TYPE_COUNT,
+	EC_BUS_TYPE_MAX = 0xFF,
+};
+
+struct ec_i2c_info {
+	uint16_t port;		/* Physical port for device */
+	uint16_t addr_flags;	/* 7-bit (or 10-bit) address */
+};
+
+struct ec_params_locate_chip {
+	uint8_t type;		/* enum ec_chip_type */
+	uint8_t index;		/* Specifies one instance of chip type */
+	/* Used for type specific parameters in future */
+	union {
+		uint16_t reserved;
+	};
+} __ec_align2;
+
+
+struct ec_response_locate_chip {
+	uint8_t bus_type;	/* enum ec_bus_type */
+	uint8_t reserved;	/* Aligning the following union to 2 bytes */
+	union {
+		struct ec_i2c_info i2c_info;
+	};
+} __ec_align2;
+
+/*****************************************************************************/
 /* Voltage regulator controls */
 
 /*
diff -ruN a/include/linux/platform_data/cros_ec_pd_update.h b/include/linux/platform_data/cros_ec_pd_update.h
--- a/include/linux/platform_data/cros_ec_pd_update.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/platform_data/cros_ec_pd_update.h	2021-12-23 08:35:55.000000000 +0100
@@ -0,0 +1,121 @@
+/*
+ * cros_ec_pd - Chrome OS EC Power Delivery Device Driver
+ *
+ * Copyright (C) 2014 Google, Inc
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef __CROS_EC_PD_UPDATE_H
+#define __CROS_EC_PD_UPDATE_H
+
+#include <linux/types.h>
+#include <linux/platform_data/cros_ec_commands.h>
+#include <linux/platform_data/cros_ec_proto.h>
+
+enum cros_ec_pd_device_type {
+	PD_DEVICE_TYPE_NONE = 0,
+	PD_DEVICE_TYPE_ZINGER = 1,
+	PD_DEVICE_TYPE_DINGDONG = 3,
+	PD_DEVICE_TYPE_HOHO = 4,
+	PD_DEVICE_TYPE_COUNT,
+};
+
+#define USB_VID_GOOGLE 0x18d1
+
+#define USB_PID_DINGDONG 0x5011
+#define USB_PID_HOHO     0x5010
+#define USB_PID_ZINGER   0x5012
+
+struct cros_ec_pd_firmware_image {
+	unsigned int id_major;
+	unsigned int id_minor;
+	uint16_t usb_vid;
+	uint16_t usb_pid;
+	char *filename;
+	ssize_t rw_image_size;
+	uint8_t hash[PD_RW_HASH_SIZE];
+	uint8_t (*update_hashes)[][PD_RW_HASH_SIZE];
+	int update_hash_count;
+};
+
+struct cros_ec_pd_update_data {
+	struct device *dev;
+
+	struct delayed_work work;
+	struct workqueue_struct *workqueue;
+	struct notifier_block notifier;
+
+	int num_ports;
+	int force_update;
+	int is_suspending;
+
+	u32 pd_status;
+	struct mutex lock;
+};
+
+#define PD_ID_MAJOR_SHIFT 0
+#define PD_ID_MAJOR_MASK  0x03ff
+#define PD_ID_MINOR_SHIFT 10
+#define PD_ID_MINOR_MASK  0xfc00
+
+#define MAJOR_MINOR_TO_DEV_ID(major, minor) \
+	((((major) << PD_ID_MAJOR_SHIFT) & PD_ID_MAJOR_MASK) | \
+	(((minor) << PD_ID_MINOR_SHIFT) & PD_ID_MINOR_MASK))
+
+enum cros_ec_pd_find_update_firmware_result {
+	PD_DO_UPDATE,
+	PD_ALREADY_HAVE_LATEST,
+	PD_UNKNOWN_DEVICE,
+	PD_UNKNOWN_RW,
+};
+
+/* Send 96 bytes per write command when flashing PD device */
+#define PD_FLASH_WRITE_STEP 96
+
+/*
+ * Wait 2s to start an update check after scheduling. This helps to remove
+ * needless extra update checks (ex. if a PD device is reset several times
+ * immediately after insertion) and fixes load issues on resume.
+ */
+#define PD_UPDATE_CHECK_DELAY msecs_to_jiffies(2000)
+
+/**
+ * cros_ec_pd_get_status - Get info about a possible PD device attached to a
+ * given port. Returns 0 on success, <0 on failure.
+ *
+ * @dev: PD device
+ * @pd_dev: EC PD device
+ * @port: Port # on device
+ * @hash_entry: Stores received PD device RW FW info, on success
+ * @discovery_entry: Stores received PD device USB info, if device present
+ */
+int cros_ec_pd_get_status(
+		struct device *dev,
+		struct cros_ec_dev *pd_dev,
+		int port,
+		struct ec_params_usb_pd_rw_hash_entry *hash_entry,
+		struct ec_params_usb_pd_discovery_entry *discovery_entry);
+
+/* Store our PD device pointer so we can send update-related commands. */
+extern struct cros_ec_dev *cros_ec_pd_ec;
+
+/*
+ * firmware_images - Keep this updated with the latest RW FW + hash for each
+ * PD device. Entries should be primary sorted by id_major and secondary
+ * sorted by id_minor.
+ * The array is terminated with an empty image to  save passing size.
+ */
+extern const struct cros_ec_pd_firmware_image cros_ec_pd_firmware_images[];
+
+
+
+
+#endif  /* __CROS_EC_PD_UPDATE_H */
diff -ruN a/include/linux/platform_data/cros_ec_proto.h b/include/linux/platform_data/cros_ec_proto.h
--- a/include/linux/platform_data/cros_ec_proto.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/platform_data/cros_ec_proto.h	2021-12-23 08:35:55.000000000 +0100
@@ -11,6 +11,7 @@
 #include <linux/device.h>
 #include <linux/mutex.h>
 #include <linux/notifier.h>
+#include <linux/power_supply.h>
 
 #include <linux/platform_data/cros_ec_commands.h>
 
@@ -115,6 +116,7 @@
  *            code.
  * @pkt_xfer: Send packet to EC and get response.
  * @lock: One transaction at a time.
+ * @charger: Charger connected to the EC, if any.
  * @mkbp_event_supported: 0 if MKBP not supported. Otherwise its value is
  *                        the maximum supported version of the MKBP host event
  *                        command + 1.
@@ -159,6 +161,7 @@
 			struct cros_ec_command *msg);
 	int (*pkt_xfer)(struct cros_ec_device *ec,
 			struct cros_ec_command *msg);
+	struct power_supply *charger;
 	struct mutex lock;
 	u8 mkbp_event_supported;
 	bool host_sleep_v1;
@@ -231,6 +234,9 @@
 
 int cros_ec_get_sensor_count(struct cros_ec_dev *ec);
 
+int cros_ec_command(struct cros_ec_device *ec_dev, unsigned int version, int command, void *outdata,
+		    int outsize, void *indata, int insize);
+
 /**
  * cros_ec_get_time_ns() - Return time in ns.
  *
diff -ruN a/include/linux/platform_data/wilco-ec.h b/include/linux/platform_data/wilco-ec.h
--- a/include/linux/platform_data/wilco-ec.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/platform_data/wilco-ec.h	2021-12-23 08:35:55.000000000 +0100
@@ -34,6 +34,7 @@
  * @debugfs_pdev: The child platform_device used by the debugfs sub-driver.
  * @rtc_pdev: The child platform_device used by the RTC sub-driver.
  * @charger_pdev: Child platform_device used by the charger config sub-driver.
+ * @charge_schedule_pdev: Child pdev used by the charge schedule sub-driver.
  * @telem_pdev: The child platform_device used by the telemetry sub-driver.
  */
 struct wilco_ec_device {
@@ -47,6 +48,7 @@
 	struct platform_device *debugfs_pdev;
 	struct platform_device *rtc_pdev;
 	struct platform_device *charger_pdev;
+	struct platform_device *charge_schedule_pdev;
 	struct platform_device *telem_pdev;
 };
 
diff -ruN a/include/linux/platform_data/x86/soc.h b/include/linux/platform_data/x86/soc.h
--- a/include/linux/platform_data/x86/soc.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/platform_data/x86/soc.h	2021-12-23 08:35:55.000000000 +0100
@@ -0,0 +1,65 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Helpers for Intel SoC model detection
+ *
+ * Copyright (c) 2019, Intel Corporation.
+ */
+
+#ifndef __PLATFORM_DATA_X86_SOC_H
+#define __PLATFORM_DATA_X86_SOC_H
+
+#if IS_ENABLED(CONFIG_X86)
+
+#include <asm/cpu_device_id.h>
+#include <asm/intel-family.h>
+
+#define SOC_INTEL_IS_CPU(soc, type)				\
+static inline bool soc_intel_is_##soc(void)			\
+{								\
+	static const struct x86_cpu_id soc##_cpu_ids[] = {	\
+		X86_MATCH_INTEL_FAM6_MODEL(type, NULL),		\
+		{}						\
+	};							\
+	const struct x86_cpu_id *id;				\
+								\
+	id = x86_match_cpu(soc##_cpu_ids);			\
+	if (id)							\
+		return true;					\
+	return false;						\
+}
+
+SOC_INTEL_IS_CPU(byt, ATOM_SILVERMONT);
+SOC_INTEL_IS_CPU(cht, ATOM_AIRMONT);
+SOC_INTEL_IS_CPU(apl, ATOM_GOLDMONT);
+SOC_INTEL_IS_CPU(glk, ATOM_GOLDMONT_PLUS);
+SOC_INTEL_IS_CPU(cml, KABYLAKE_L);
+
+#else /* IS_ENABLED(CONFIG_X86) */
+
+static inline bool soc_intel_is_byt(void)
+{
+	return false;
+}
+
+static inline bool soc_intel_is_cht(void)
+{
+	return false;
+}
+
+static inline bool soc_intel_is_apl(void)
+{
+	return false;
+}
+
+static inline bool soc_intel_is_glk(void)
+{
+	return false;
+}
+
+static inline bool soc_intel_is_cml(void)
+{
+	return false;
+}
+#endif /* IS_ENABLED(CONFIG_X86) */
+
+#endif /* __PLATFORM_DATA_X86_SOC_H */
diff -ruN a/include/linux/pm_opp.h b/include/linux/pm_opp.h
--- a/include/linux/pm_opp.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/pm_opp.h	2021-12-23 08:35:55.000000000 +0100
@@ -94,6 +94,9 @@
 
 unsigned long dev_pm_opp_get_voltage(struct dev_pm_opp *opp);
 
+unsigned long dev_pm_opp_get_voltage_supply(struct dev_pm_opp *opp,
+					    unsigned int index);
+
 unsigned long dev_pm_opp_get_freq(struct dev_pm_opp *opp);
 
 unsigned int dev_pm_opp_get_level(struct dev_pm_opp *opp);
diff -ruN a/include/linux/pwm.h b/include/linux/pwm.h
--- a/include/linux/pwm.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/pwm.h	2021-12-23 08:35:55.000000000 +0100
@@ -48,6 +48,17 @@
 	PWMF_EXPORTED = 1 << 1,
 };
 
+/**
+ * enum pwm_output_type - output type of the PWM signal
+ * @PWM_OUTPUT_FIXED: PWM output is fixed until a change request
+ * @PWM_OUTPUT_MODULATED: PWM output is modulated in hardware
+ * autonomously with a predefined pattern
+ */
+enum pwm_output_type {
+	PWM_OUTPUT_FIXED = 1 << 0,
+	PWM_OUTPUT_MODULATED = 1 << 1,
+};
+
 /*
  * struct pwm_state - state of a PWM channel
  * @period: PWM period (in nanoseconds)
@@ -63,6 +74,7 @@
 	u64 period;
 	u64 duty_cycle;
 	enum pwm_polarity polarity;
+	enum pwm_output_type output_type;
 	bool enabled;
 	bool usage_power;
 };
@@ -156,6 +168,16 @@
 	return state.polarity;
 }
 
+static inline enum pwm_output_type pwm_get_output_type(
+		const struct pwm_device *pwm)
+{
+	struct pwm_state state;
+
+	pwm_get_state(pwm, &state);
+
+	return state.output_type;
+}
+
 static inline void pwm_get_args(const struct pwm_device *pwm,
 				struct pwm_args *args)
 {
@@ -260,6 +282,7 @@
  * @get_state: get the current PWM state. This function is only
  *	       called once per PWM device when the PWM chip is
  *	       registered.
+ * @get_output_type_supported: get the supported output type of this PWM
  * @owner: helps prevent removal of modules exporting active PWMs
  * @config: configure duty cycles and period length for this PWM
  * @set_polarity: configure the polarity of this PWM
@@ -275,6 +298,8 @@
 		     const struct pwm_state *state);
 	void (*get_state)(struct pwm_chip *chip, struct pwm_device *pwm,
 			  struct pwm_state *state);
+	int (*get_output_type_supported)(struct pwm_chip *chip,
+			struct pwm_device *pwm);
 	struct module *owner;
 
 	/* Only used by legacy drivers */
@@ -330,6 +355,24 @@
 int pwm_adjust_config(struct pwm_device *pwm);
 
 /**
+ * pwm_get_output_type_supported() - obtain output type of a PWM device.
+ * @pwm: PWM device
+ *
+ * Returns:  output type supported by the PWM device
+ */
+static inline int pwm_get_output_type_supported(struct pwm_device *pwm)
+{
+	if (!pwm)
+		return -EINVAL;
+
+	if (pwm->chip->ops->get_output_type_supported)
+		return pwm->chip->ops->get_output_type_supported(pwm->chip,
+				pwm);
+
+	return PWM_OUTPUT_FIXED;
+}
+
+/**
  * pwm_config() - change a PWM device configuration
  * @pwm: PWM device
  * @duty_ns: "on" time (in nanoseconds)
@@ -447,6 +490,11 @@
 	return -ENOTSUPP;
 }
 
+static inline int pwm_get_output_type_supported(struct pwm_device *pwm)
+{
+	return -EINVAL;
+}
+
 static inline int pwm_config(struct pwm_device *pwm, int duty_ns,
 			     int period_ns)
 {
diff -ruN a/include/linux/rmap.h b/include/linux/rmap.h
--- a/include/linux/rmap.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/rmap.h	2021-12-23 08:35:55.000000000 +0100
@@ -12,6 +12,10 @@
 #include <linux/memcontrol.h>
 #include <linux/highmem.h>
 
+extern int isolate_lru_page(struct page *page);
+extern void putback_lru_page(struct page *page);
+extern unsigned long reclaim_pages(struct list_head *page_list);
+
 /*
  * The anon_vma heads a list of private "related" vmas, to scan if
  * an anonymous page pointing to this anon_vma needs to be unmapped:
diff -ruN a/include/linux/sched/smt.h b/include/linux/sched/smt.h
--- a/include/linux/sched/smt.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/sched/smt.h	2021-12-23 08:35:55.000000000 +0100
@@ -17,4 +17,8 @@
 
 void arch_smt_update(void);
 
+#ifdef CONFIG_SCHED_CORE
+extern struct static_key_true sched_coresched_supported;
+#endif
+
 #endif
diff -ruN a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
--- a/include/linux/sched/sysctl.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/sched/sysctl.h	2021-12-23 08:35:55.000000000 +0100
@@ -35,6 +35,9 @@
 	SCHED_TUNABLESCALING_END,
 };
 
+extern unsigned int sysctl_iowait_reset_ticks;
+extern unsigned int sysctl_iowait_apply_ticks;
+
 /*
  *  control realtime throttling:
  *
diff -ruN a/include/linux/sched/xacct.h b/include/linux/sched/xacct.h
--- a/include/linux/sched/xacct.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/sched/xacct.h	2021-12-23 08:35:55.000000000 +0100
@@ -28,6 +28,11 @@
 {
 	tsk->ioac.syscw++;
 }
+
+static inline void inc_syscfs(struct task_struct *tsk)
+{
+	tsk->ioac.syscfs++;
+}
 #else
 static inline void add_rchar(struct task_struct *tsk, ssize_t amt)
 {
@@ -44,6 +49,10 @@
 static inline void inc_syscw(struct task_struct *tsk)
 {
 }
+
+static inline void inc_syscfs(struct task_struct *tsk)
+{
+}
 #endif
 
 #endif /* _LINUX_SCHED_XACCT_H */
diff -ruN a/include/linux/sched.h b/include/linux/sched.h
--- a/include/linux/sched.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/sched.h	2021-12-23 08:35:55.000000000 +0100
@@ -999,6 +999,10 @@
 	u64				stimescaled;
 #endif
 	u64				gtime;
+#ifdef CONFIG_CPU_FREQ_TIMES
+	u64				*time_in_state;
+	unsigned int			max_state;
+#endif
 	struct prev_cputime		prev_cputime;
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 	struct vtime			vtime;
@@ -1915,6 +1919,7 @@
 
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
+extern int wake_up_process_prefer_current_cpu(struct task_struct *tsk);
 extern void wake_up_new_task(struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
diff -ruN a/include/linux/suspend.h b/include/linux/suspend.h
--- a/include/linux/suspend.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/suspend.h	2021-12-23 08:35:55.000000000 +0100
@@ -517,6 +517,7 @@
 extern bool pm_save_wakeup_count(unsigned int count);
 extern void pm_wakep_autosleep_enabled(bool set);
 extern void pm_print_active_wakeup_sources(void);
+extern void pm_get_active_wakeup_sources(char *pending_sources, size_t max);
 
 extern void lock_system_sleep(void);
 extern void unlock_system_sleep(void);
diff -ruN a/include/linux/syscalls.h b/include/linux/syscalls.h
--- a/include/linux/syscalls.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/syscalls.h	2021-12-23 08:35:55.000000000 +0100
@@ -1381,4 +1381,25 @@
 		int __user *optlen);
 int __sys_setsockopt(int fd, int level, int optname, char __user *optval,
 		int optlen);
+
+#ifdef CONFIG_ALT_SYSCALL
+
+/* Only used with ALT_SYSCALL enabled */
+
+int ksys_prctl(int option, unsigned long arg2, unsigned long arg3,
+	       unsigned long arg4, unsigned long arg5);
+int ksys_setpriority(int which, int who, int niceval);
+int ksys_getpriority(int which, int who);
+int ksys_perf_event_open(
+		struct perf_event_attr __user *attr_uptr,
+		pid_t pid, int cpu, int group_fd, unsigned long flags);
+int ksys_clock_adjtime(const clockid_t which_clock, struct __kernel_timex __user * utx);
+int ksys_adjtimex(struct __kernel_timex __user *txc_p);
+int ksys_getcpu(unsigned __user *cpu, unsigned __user *node,
+		struct getcpu_cache __user *cache);
+int ksys_clock_adjtime32(clockid_t which_clock,
+			 struct old_timex32 __user *utp);
+int ksys_adjtimex_time32(struct old_timex32 __user *utp);
+#endif /* CONFIG_ALT_SYSCALL */
+
 #endif
diff -ruN a/include/linux/sysrq.h b/include/linux/sysrq.h
--- a/include/linux/sysrq.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/sysrq.h	2021-12-23 08:35:55.000000000 +0100
@@ -28,6 +28,7 @@
 #define SYSRQ_ENABLE_SIGNAL	0x0040
 #define SYSRQ_ENABLE_BOOT	0x0080
 #define SYSRQ_ENABLE_RTNICE	0x0100
+#define SYSRQ_ENABLE_CROS_XKEY	0x1000
 
 struct sysrq_key_op {
 	void (* const handler)(int);
diff -ruN a/include/linux/task_io_accounting.h b/include/linux/task_io_accounting.h
--- a/include/linux/task_io_accounting.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/task_io_accounting.h	2021-12-23 08:35:55.000000000 +0100
@@ -19,6 +19,8 @@
 	u64 syscr;
 	/* # of write syscalls */
 	u64 syscw;
+	/* # of fsync syscalls */
+	u64 syscfs;
 #endif /* CONFIG_TASK_XACCT */
 
 #ifdef CONFIG_TASK_IO_ACCOUNTING
diff -ruN a/include/linux/task_io_accounting_ops.h b/include/linux/task_io_accounting_ops.h
--- a/include/linux/task_io_accounting_ops.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/task_io_accounting_ops.h	2021-12-23 08:35:55.000000000 +0100
@@ -97,6 +97,7 @@
 	dst->wchar += src->wchar;
 	dst->syscr += src->syscr;
 	dst->syscw += src->syscw;
+	dst->syscfs += src->syscfs;
 }
 #else
 static inline void task_chr_io_accounting_add(struct task_io_accounting *dst,
diff -ruN a/include/linux/thermal.h b/include/linux/thermal.h
--- a/include/linux/thermal.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/thermal.h	2021-12-23 08:35:55.000000000 +0100
@@ -13,6 +13,7 @@
 #include <linux/of.h>
 #include <linux/idr.h>
 #include <linux/device.h>
+#include <linux/notifier.h>
 #include <linux/sysfs.h>
 #include <linux/workqueue.h>
 #include <uapi/linux/thermal.h>
@@ -443,4 +444,7 @@
 { return -ENODEV; }
 #endif /* CONFIG_THERMAL */
 
+extern int register_thermal_notifier(struct notifier_block *);
+extern int unregister_thermal_notifier(struct notifier_block *);
+
 #endif /* __THERMAL_H__ */
diff -ruN a/include/linux/tpm.h b/include/linux/tpm.h
--- a/include/linux/tpm.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/tpm.h	2021-12-23 08:35:55.000000000 +0100
@@ -138,6 +138,8 @@
 	int dev_num;		/* /dev/tpm# */
 	unsigned long is_open;	/* only one allowed */
 
+	bool is_suspended;
+
 	char hwrng_name[64];
 	struct hwrng hwrng;
 
@@ -153,7 +155,7 @@
 
 	struct dentry *bios_dir[TPM_NUM_EVENT_LOG_FILES];
 
-	const struct attribute_group *groups[3 + TPM_MAX_HASHES];
+	const struct attribute_group *groups[4 + TPM_MAX_HASHES];
 	unsigned int groups_cnt;
 
 	u32 nr_allocated_banks;
@@ -253,7 +255,34 @@
 };
 
 enum tpm2_properties {
-	TPM_PT_TOTAL_COMMANDS	= 0x0129,
+	TPM_PT_TOTAL_COMMANDS		= 0x0129,
+	TPM2_PT_NONE			= 0,
+	TPM2_PT_GROUP			= 0x100,
+	TPM2_PT_FIXED			= TPM2_PT_GROUP,
+	TPM2_PT_VAR			= TPM2_PT_GROUP * 2,
+	TPM2_PT_PERMANENT		= TPM2_PT_VAR + 0,
+	TPM2_PT_STARTUP_CLEAR		= TPM2_PT_VAR + 1,
+	TPM2_PT_LOCKOUT_COUNTER		= TPM2_PT_VAR + 14,
+	TPM2_PT_MAX_AUTH_FAIL		= TPM2_PT_VAR + 15,
+	TPM2_PT_LOCKOUT_INTERVAL	= TPM2_PT_VAR + 16,
+	TPM2_PT_LOCKOUT_RECOVERY	= TPM2_PT_VAR + 17,
+};
+
+enum tpm2_attr_permanent {
+	TPM2_ATTR_OWNER_AUTH_SET	= BIT(0),
+	TPM2_ATTR_ENDORSEMENT_AUTH_SET	= BIT(1),
+	TPM2_ATTR_LOCKOUT_AUTH_SET	= BIT(2),
+	TPM2_ATTR_DISABLE_CLEAR		= BIT(8),
+	TPM2_ATTR_IN_LOCKOUT		= BIT(9),
+	TPM2_ATTR_TPM_GENERATED_EPS	= BIT(10),
+};
+
+enum tpm2_attr_startup_clear {
+	TPM2_ATTR_PH_ENABLE		= BIT(0),
+	TPM2_ATTR_SH_ENABLE		= BIT(1),
+	TPM2_ATTR_EH_ENABLE		= BIT(2),
+	TPM2_ATTR_PH_ENABLE_NV		= BIT(3),
+	TPM2_ATTR_ORDERLY		= BIT(31),
 };
 
 enum tpm2_startup_types {
diff -ruN a/include/linux/usb/composite.h b/include/linux/usb/composite.h
--- a/include/linux/usb/composite.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/usb/composite.h	2021-12-23 08:35:55.000000000 +0100
@@ -592,6 +592,7 @@
 	struct config_group group;
 	struct list_head cfs_list;
 	struct usb_function_driver *fd;
+	struct usb_function *f;
 	int (*set_inst_name)(struct usb_function_instance *inst,
 			      const char *name);
 	void (*free_func_inst)(struct usb_function_instance *inst);
diff -ruN a/include/linux/usb/f_accessory.h b/include/linux/usb/f_accessory.h
--- a/include/linux/usb/f_accessory.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/usb/f_accessory.h	2021-12-23 08:35:55.000000000 +0100
@@ -0,0 +1,23 @@
+/*
+ * Gadget Function Driver for Android USB accessories
+ *
+ * Copyright (C) 2011 Google, Inc.
+ * Author: Mike Lockwood <lockwood@android.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef __LINUX_USB_F_ACCESSORY_H
+#define __LINUX_USB_F_ACCESSORY_H
+
+#include <uapi/linux/usb/f_accessory.h>
+
+#endif /* __LINUX_USB_F_ACCESSORY_H */
diff -ruN a/include/linux/wakeup_reason.h b/include/linux/wakeup_reason.h
--- a/include/linux/wakeup_reason.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/wakeup_reason.h	2021-12-23 08:35:55.000000000 +0100
@@ -0,0 +1,37 @@
+/*
+ * include/linux/wakeup_reason.h
+ *
+ * Logs the reason which caused the kernel to resume
+ * from the suspend mode.
+ *
+ * Copyright (C) 2014 Google, Inc.
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _LINUX_WAKEUP_REASON_H
+#define _LINUX_WAKEUP_REASON_H
+
+#define MAX_SUSPEND_ABORT_LEN 256
+
+#ifdef CONFIG_SUSPEND
+void log_irq_wakeup_reason(int irq);
+void log_threaded_irq_wakeup_reason(int irq, int parent_irq);
+void log_suspend_abort_reason(const char *fmt, ...);
+void log_abnormal_wakeup_reason(const char *fmt, ...);
+void clear_wakeup_reasons(void);
+#else
+static inline void log_irq_wakeup_reason(int irq) { }
+static inline void log_threaded_irq_wakeup_reason(int irq, int parent_irq) { }
+static inline void log_suspend_abort_reason(const char *fmt, ...) { }
+static inline void log_abnormal_wakeup_reason(const char *fmt, ...) { }
+static inline void clear_wakeup_reasons(void) { }
+#endif
+
+#endif /* _LINUX_WAKEUP_REASON_H */
diff -ruN a/include/linux/wwan.h b/include/linux/wwan.h
--- a/include/linux/wwan.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/wwan.h	2021-12-23 08:35:55.000000000 +0100
@@ -171,4 +171,9 @@
 
 void wwan_unregister_ops(struct device *parent);
 
+/*
+ * Default WWAN interface MTU value
+ */
+#define WWAN_DEFAULT_MTU       1500
+
 #endif /* __WWAN_H */
diff -ruN a/include/linux/xattr.h b/include/linux/xattr.h
--- a/include/linux/xattr.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/linux/xattr.h	2021-12-23 08:35:55.000000000 +0100
@@ -32,10 +32,10 @@
 	const char *prefix;
 	int flags;      /* fs private flags */
 	bool (*list)(struct dentry *dentry);
-	int (*get)(const struct xattr_handler *, struct dentry *dentry,
+	int (*get)(const struct xattr_handler *handler, struct dentry *dentry,
 		   struct inode *inode, const char *name, void *buffer,
-		   size_t size);
-	int (*set)(const struct xattr_handler *,
+		   size_t size, int flags);
+	int (*set)(const struct xattr_handler *handler,
 		   struct user_namespace *mnt_userns, struct dentry *dentry,
 		   struct inode *inode, const char *name, const void *buffer,
 		   size_t size, int flags);
@@ -49,7 +49,9 @@
 	size_t value_len;
 };
 
-ssize_t __vfs_getxattr(struct dentry *, struct inode *, const char *, void *, size_t);
+ssize_t __vfs_getxattr(struct user_namespace *mnt_userns,
+		       struct dentry *dentry, struct inode *inode,
+		       const char *name, void *buffer, size_t size, int flags);
 ssize_t vfs_getxattr(struct user_namespace *, struct dentry *, const char *,
 		     void *, size_t);
 ssize_t vfs_listxattr(struct dentry *d, char *list, size_t size);
diff -ruN a/include/media/ipu-isys.h b/include/media/ipu-isys.h
--- a/include/media/ipu-isys.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/media/ipu-isys.h	2021-12-23 08:35:55.000000000 +0100
@@ -0,0 +1,44 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (C) 2014 - 2020 Intel Corporation */
+
+#ifndef MEDIA_IPU_H
+#define MEDIA_IPU_H
+
+#include <linux/i2c.h>
+#include <linux/clkdev.h>
+#include <media/v4l2-async.h>
+
+#define IPU_ISYS_MAX_CSI2_LANES		4
+
+struct ipu_isys_csi2_config {
+	unsigned int nlanes;
+	unsigned int port;
+};
+
+struct ipu_isys_subdev_i2c_info {
+	struct i2c_board_info board_info;
+	int i2c_adapter_id;
+	char i2c_adapter_bdf[32];
+};
+
+struct ipu_isys_subdev_info {
+	struct ipu_isys_csi2_config *csi2;
+	struct ipu_isys_subdev_i2c_info i2c;
+};
+
+struct ipu_isys_clk_mapping {
+	struct clk_lookup clkdev_data;
+	char *platform_clock_name;
+};
+
+struct ipu_isys_subdev_pdata {
+	struct ipu_isys_subdev_info **subdevs;
+	struct ipu_isys_clk_mapping *clk_map;
+};
+
+struct sensor_async_subdev {
+	struct v4l2_async_subdev asd;
+	struct ipu_isys_csi2_config csi2;
+};
+
+#endif /* MEDIA_IPU_H */
diff -ruN a/include/media/v4l2-ctrls.h b/include/media/v4l2-ctrls.h
--- a/include/media/v4l2-ctrls.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/media/v4l2-ctrls.h	2021-12-23 08:35:55.000000000 +0100
@@ -18,6 +18,7 @@
  * This will move to the public headers once this API is fully stable.
  */
 #include <media/hevc-ctrls.h>
+#include <media/vp9-ctrls.h>
 
 /* forward references */
 struct file;
diff -ruN a/include/media/videobuf2-core.h b/include/media/videobuf2-core.h
--- a/include/media/videobuf2-core.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/media/videobuf2-core.h	2021-12-23 08:35:55.000000000 +0100
@@ -20,7 +20,7 @@
 #include <media/media-request.h>
 #include <media/frame_vector.h>
 
-#define VB2_MAX_FRAME	(32)
+#define VB2_MAX_FRAME	(64)
 #define VB2_MAX_PLANES	(8)
 
 /**
@@ -267,10 +267,10 @@
 	 *			after the 'buf_finish' op is called.
 	 * copied_timestamp:	the timestamp of this capture buffer was copied
 	 *			from an output buffer.
-	 * need_cache_sync_on_prepare: when set buffer's ->prepare() function
-	 *			performs cache sync/invalidation.
-	 * need_cache_sync_on_finish: when set buffer's ->finish() function
-	 *			performs cache sync/invalidation.
+	 * skip_cache_sync_on_prepare: when set buffer's ->prepare() function
+	 *			skips cache sync/invalidation.
+	 * skip_cache_sync_on_finish: when set buffer's ->finish() function
+	 *			skips cache sync/invalidation.
 	 * queued_entry:	entry on the queued buffers list, which holds
 	 *			all buffers queued from userspace
 	 * done_entry:		entry on the list that stores all buffers ready
@@ -281,8 +281,8 @@
 	unsigned int		synced:1;
 	unsigned int		prepared:1;
 	unsigned int		copied_timestamp:1;
-	unsigned int		need_cache_sync_on_prepare:1;
-	unsigned int		need_cache_sync_on_finish:1;
+	unsigned int		skip_cache_sync_on_prepare:1;
+	unsigned int		skip_cache_sync_on_finish:1;
 
 	struct vb2_plane	planes[VB2_MAX_PLANES];
 	struct list_head	queued_entry;
@@ -504,6 +504,8 @@
  * @allow_cache_hints: when set user-space can pass cache management hints in
  *		order to skip cache flush/invalidation on ->prepare() or/and
  *		->finish().
+ * @non_coherent_mem: when set queue will attempt to allocate buffers using
+ *		non-coherent memory.
  * @lock:	pointer to a mutex that protects the &struct vb2_queue. The
  *		driver can set this to a mutex to let the v4l2 core serialize
  *		the queuing ioctls. If the driver wants to handle locking
@@ -583,6 +585,7 @@
 	unsigned int			uses_qbuf:1;
 	unsigned int			uses_requests:1;
 	unsigned int			allow_cache_hints:1;
+	unsigned int			non_coherent_mem:1;
 
 	struct mutex			*lock;
 	void				*owner;
@@ -748,6 +751,8 @@
  * vb2_core_reqbufs() - Initiate streaming.
  * @q:		pointer to &struct vb2_queue with videobuf2 queue.
  * @memory:	memory type, as defined by &enum vb2_memory.
+ * @flags:	auxiliary queue/buffer management flags. Currently, the only
+ *		used flag is %V4L2_MEMORY_FLAG_NON_COHERENT.
  * @count:	requested buffer count.
  *
  * Videobuf2 core helper to implement VIDIOC_REQBUF() operation. It is called
@@ -772,12 +777,13 @@
  * Return: returns zero on success; an error code otherwise.
  */
 int vb2_core_reqbufs(struct vb2_queue *q, enum vb2_memory memory,
-		    unsigned int *count);
+		     unsigned int flags, unsigned int *count);
 
 /**
  * vb2_core_create_bufs() - Allocate buffers and any required auxiliary structs
  * @q: pointer to &struct vb2_queue with videobuf2 queue.
  * @memory: memory type, as defined by &enum vb2_memory.
+ * @flags: auxiliary queue/buffer management flags.
  * @count: requested buffer count.
  * @requested_planes: number of planes requested.
  * @requested_sizes: array with the size of the planes.
@@ -795,7 +801,7 @@
  * Return: returns zero on success; an error code otherwise.
  */
 int vb2_core_create_bufs(struct vb2_queue *q, enum vb2_memory memory,
-			 unsigned int *count,
+			 unsigned int flags, unsigned int *count,
 			 unsigned int requested_planes,
 			 const unsigned int requested_sizes[]);
 
diff -ruN a/include/media/vp9-ctrls.h b/include/media/vp9-ctrls.h
--- a/include/media/vp9-ctrls.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/media/vp9-ctrls.h	2021-12-23 08:35:55.000000000 +0100
@@ -0,0 +1,492 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * These are the VP9 state controls for use with stateless VP9
+ * codec drivers.
+ *
+ * It turns out that these structs are not stable yet and will undergo
+ * more changes. So keep them private until they are stable and ready to
+ * become part of the official public API.
+ */
+
+#ifndef _VP9_CTRLS_H_
+#define _VP9_CTRLS_H_
+
+#include <linux/types.h>
+
+#define V4L2_PIX_FMT_VP9_FRAME v4l2_fourcc('V', 'P', '9', 'F')
+
+#define V4L2_CID_MPEG_VIDEO_VP9_FRAME_CONTEXT(i)	(V4L2_CID_CODEC_BASE + 4000 + (i))
+#define V4L2_CID_MPEG_VIDEO_VP9_FRAME_DECODE_PARAMS	(V4L2_CID_CODEC_BASE + 4004)
+#define V4L2_CTRL_TYPE_VP9_FRAME_CONTEXT		0x400
+#define V4L2_CTRL_TYPE_VP9_FRAME_DECODE_PARAMS		0x404
+
+/**
+ * enum v4l2_vp9_loop_filter_flags - VP9 loop filter flags
+ *
+ * @V4L2_VP9_LOOP_FILTER_FLAG_DELTA_ENABLED: the filter level depends on
+ *					     the mode and reference frame used
+ *					     to predict a block
+ * @V4L2_VP9_LOOP_FILTER_FLAG_DELTA_UPDATE: the bitstream contains additional
+ *					    syntax elements that specify which
+ *					    mode and reference frame deltas
+ *					    are to be updated
+ *
+ * Those are the flags you should pass to &v4l2_vp9_loop_filter.flags. See
+ * section '7.2.8 Loop filter semantics' of the VP9 specification for more
+ * details.
+ */
+enum v4l2_vp9_loop_filter_flags {
+	V4L2_VP9_LOOP_FILTER_FLAG_DELTA_ENABLED = 1 << 0,
+	V4L2_VP9_LOOP_FILTER_FLAG_DELTA_UPDATE = 1 << 1,
+};
+
+/**
+ * struct v4l2_vp9_loop_filter - VP9 loop filter parameters
+ *
+ * @flags: combination of V4L2_VP9_LOOP_FILTER_FLAG_* flags
+ * @level: indicates the loop filter strength
+ * @sharpness: indicates the sharpness level
+ * @ref_deltas: contains the adjustment needed for the filter level based on
+ *		the chosen reference frame
+ * @mode_deltas: contains the adjustment needed for the filter level based on
+ *		 the chosen mode
+ * @level_lookup: level lookup table
+ *
+ * This structure contains all loop filter related parameters. See sections
+ * '7.2.8 Loop filter semantics' and '8.8.1 Loop filter frame init process'
+ * of the VP9 specification for more details.
+ */
+struct v4l2_vp9_loop_filter {
+	__u8 flags;
+	__u8 level;
+	__u8 sharpness;
+	__s8 ref_deltas[4];
+	__s8 mode_deltas[2];
+	__u8 level_lookup[8][4][2];
+};
+
+/**
+ * struct v4l2_vp9_quantization - VP9 quantization parameters
+ *
+ * @base_q_idx: indicates the base frame qindex
+ * @delta_q_y_dc: indicates the Y DC quantizer relative to base_q_idx
+ * @delta_q_uv_dc: indicates the UV DC quantizer relative to base_q_idx
+ * @delta_q_uv_ac indicates the UV AC quantizer relative to base_q_idx
+ * @padding: padding bytes to align things on 64 bits. Must be set to 0
+ *
+ * Encodes the quantization parameters. See section '7.2.9 Quantization params
+ * syntax' of the VP9 specification for more details.
+ */
+struct v4l2_vp9_quantization {
+	/* TODO LOSSLESS flag missing? */
+	__u8 base_q_idx;
+	__s8 delta_q_y_dc;
+	__s8 delta_q_uv_dc;
+	__s8 delta_q_uv_ac;
+	__u8 padding[4];
+};
+
+/**
+ * enum v4l2_vp9_segmentation_flags - VP9 segmentation flags
+ *
+ * @V4L2_VP9_SEGMENTATION_FLAG_ENABLED: indicates that this frame makes use of
+ *					the segmentation tool
+ * @V4L2_VP9_SEGMENTATION_FLAG_UPDATE_MAP: indicates that the segmentation map
+ *					   should be updated during the
+ *					   decoding of this frame
+ * @V4L2_VP9_SEGMENTATION_FLAG_TEMPORAL_UPDATE: indicates that the updates to
+ *						the segmentation map are coded
+ *						relative to the existing
+ *						segmentation map
+ * @V4L2_VP9_SEGMENTATION_FLAG_UPDATE_DATA: indicates that new parameters are
+ *					    about to be specified for each
+ *					    segment
+ * @V4L2_VP9_SEGMENTATION_FLAG_ABS_OR_DELTA_UPDATE: indicates that the
+ *						    segmentation parameters
+ *						    represent the actual values
+ *						    to be used
+ *
+ * Those are the flags you should pass to &v4l2_vp9_segmentation.flags. See
+ * section '7.2.10 Segmentation params syntax' of the VP9 specification for
+ * more details.
+ */
+enum v4l2_vp9_segmentation_flags {
+	V4L2_VP9_SEGMENTATION_FLAG_ENABLED = 1 << 0,
+	V4L2_VP9_SEGMENTATION_FLAG_UPDATE_MAP = 1 << 1,
+	V4L2_VP9_SEGMENTATION_FLAG_TEMPORAL_UPDATE = 1 << 2,
+	V4L2_VP9_SEGMENTATION_FLAG_UPDATE_DATA = 1 << 3,
+	V4L2_VP9_SEGMENTATION_FLAG_ABS_OR_DELTA_UPDATE = 1 << 4,
+};
+
+#define V4L2_VP9_SEGMENT_FEATURE_ENABLED(id)	(1 << (id))
+#define V4L2_VP9_SEGMENT_FEATURE_ENABLED_MASK	0xf
+
+/**
+ * enum v4l2_vp9_segment_feature - VP9 segment feature IDs
+ *
+ * @V4L2_VP9_SEGMENT_FEATURE_QP_DELTA: QP delta segment feature
+ * @V4L2_VP9_SEGMENT_FEATURE_LF: loop filter segment feature
+ * @V4L2_VP9_SEGMENT_FEATURE_REF_FRAME: reference frame segment feature
+ * @V4L2_VP9_SEGMENT_FEATURE_SKIP: skip segment feature
+ * @V4L2_VP9_SEGMENT_FEATURE_CNT: number of segment features
+ *
+ * Segment feature IDs. See section '7.2.10 Segmentation params syntax' of the
+ * VP9 specification for more details.
+ */
+enum v4l2_vp9_segment_feature {
+	V4L2_VP9_SEGMENT_FEATURE_QP_DELTA,
+	V4L2_VP9_SEGMENT_FEATURE_LF,
+	V4L2_VP9_SEGMENT_FEATURE_REF_FRAME,
+	V4L2_VP9_SEGMENT_FEATURE_SKIP,
+	V4L2_VP9_SEGMENT_FEATURE_CNT,
+};
+
+/**
+ * struct v4l2_vp9_segmentation - VP9 segmentation parameters
+ *
+ * @flags: combination of V4L2_VP9_SEGMENTATION_FLAG_* flags
+ * @tree_probs: specifies the probability values to be used when
+ *              decoding a Segment-ID. See '5.15. Segmentation map'
+ *              section of the VP9 specification for more details.
+ * @pred_prob: specifies the probability values to be used when decoding a
+ *	       Predicted-Segment-ID. See '6.4.14. Get segment id syntax'
+ *	       section of :ref:`vp9` for more details..
+ * @padding: padding used to make things aligned on 64 bits. Shall be zero
+ *	     filled
+ * @feature_enabled: bitmask defining which features are enabled in each
+ *		     segment
+ * @feature_data: data attached to each feature. Data entry is only valid if
+ *		  the feature is enabled
+ *
+ * Encodes the quantization parameters. See section '7.2.10 Segmentation
+ * params syntax' of the VP9 specification for more details.
+ */
+struct v4l2_vp9_segmentation {
+	__u8 flags;
+	__u8 tree_probs[7];
+	__u8 pred_probs[3];
+	__u8 padding[5];
+	__u8 feature_enabled[8];
+	__s16 feature_data[8][4];
+};
+
+/**
+ * enum v4l2_vp9_intra_prediction_mode - VP9 Intra prediction modes
+ *
+ * @V4L2_VP9_INTRA_PRED_DC: DC intra prediction
+ * @V4L2_VP9_INTRA_PRED_MODE_V: vertical intra prediction
+ * @V4L2_VP9_INTRA_PRED_MODE_H: horizontal intra prediction
+ * @V4L2_VP9_INTRA_PRED_MODE_D45: D45 intra prediction
+ * @V4L2_VP9_INTRA_PRED_MODE_D135: D135 intra prediction
+ * @V4L2_VP9_INTRA_PRED_MODE_D117: D117 intra prediction
+ * @V4L2_VP9_INTRA_PRED_MODE_D153: D153 intra prediction
+ * @V4L2_VP9_INTRA_PRED_MODE_D207: D207 intra prediction
+ * @V4L2_VP9_INTRA_PRED_MODE_D63: D63 intra prediction
+ * @V4L2_VP9_INTRA_PRED_MODE_TM: True Motion intra prediction
+ *
+ * See section '7.4.5 Intra frame mode info semantics' for more details.
+ */
+// TODO where is this used??
+enum v4l2_vp9_intra_prediction_mode {
+	V4L2_VP9_INTRA_PRED_MODE_DC,
+	V4L2_VP9_INTRA_PRED_MODE_V,
+	V4L2_VP9_INTRA_PRED_MODE_H,
+	V4L2_VP9_INTRA_PRED_MODE_D45,
+	V4L2_VP9_INTRA_PRED_MODE_D135,
+	V4L2_VP9_INTRA_PRED_MODE_D117,
+	V4L2_VP9_INTRA_PRED_MODE_D153,
+	V4L2_VP9_INTRA_PRED_MODE_D207,
+	V4L2_VP9_INTRA_PRED_MODE_D63,
+	V4L2_VP9_INTRA_PRED_MODE_TM,
+};
+
+/**
+ * struct v4l2_vp9_mv_probabilities - VP9 Motion vector probabilities
+ * @joint: motion vector joint probabilities
+ * @sign: motion vector sign probabilities
+ * @class_: motion vector class probabilities
+ * @class0_bit: motion vector class0 bit probabilities
+ * @bits: motion vector bits probabilities
+ * @class0_fr: motion vector class0 fractional bit probabilities
+ * @fr: motion vector fractional bit probabilities
+ * @class0_hp: motion vector class0 high precision fractional bit probabilities
+ * @hp: motion vector high precision fractional bit probabilities
+ */
+struct v4l2_vp9_mv_probabilities {
+	__u8 joint[3];
+	__u8 sign[2];
+	__u8 class_[2][10];
+	__u8 class0_bit[2];
+	__u8 bits[2][10];
+	__u8 class0_fr[2][2][3];
+	__u8 fr[2][3];
+	__u8 class0_hp[2];
+	__u8 hp[2];
+};
+
+/**
+ * struct v4l2_vp9_probabilities - VP9 Probabilities
+ *
+ * @tx8: TX 8x8 probabilities
+ * @tx16: TX 16x16 probabilities
+ * @tx32: TX 32x32 probabilities
+ * @coef: coefficient probabilities
+ * @skip: skip probabilities
+ * @inter_mode: inter mode probabilities
+ * @interp_filter: interpolation filter probabilities
+ * @is_inter: is inter-block probabilities
+ * @comp_mode: compound prediction mode probabilities
+ * @single_ref: single ref probabilities
+ * @comp_ref: compound ref probabilities
+ * @y_mode: Y prediction mode probabilities
+ * @uv_mode: UV prediction mode probabilities
+ * @partition: partition probabilities
+ * @mv: motion vector probabilities
+ *
+ * Structure containing most VP9 probabilities. See the VP9 specification
+ * for more details.
+ */
+struct v4l2_vp9_probabilities {
+	__u8 tx8[2][1];
+	__u8 tx16[2][2];
+	__u8 tx32[2][3];
+	__u8 coef[4][2][2][6][6][3];
+	__u8 skip[3];
+	__u8 inter_mode[7][3];
+	__u8 interp_filter[4][2];
+	__u8 is_inter[4];
+	__u8 comp_mode[5];
+	__u8 single_ref[5][2];
+	__u8 comp_ref[5];
+	__u8 y_mode[4][9];
+	__u8 uv_mode[10][9];
+	__u8 partition[16][3];
+
+	struct v4l2_vp9_mv_probabilities mv;
+};
+
+/**
+ * enum v4l2_vp9_reset_frame_context - Valid values for
+ *			&v4l2_ctrl_vp9_frame_decode_params->reset_frame_context
+ *
+ * @V4L2_VP9_RESET_FRAME_CTX_NONE: don't reset any frame context
+ * @V4L2_VP9_RESET_FRAME_CTX_SPEC: reset the frame context pointed by
+ *			&v4l2_ctrl_vp9_frame_decode_params.frame_context_idx
+ * @V4L2_VP9_RESET_FRAME_CTX_ALL: reset all frame contexts
+ *
+ * See section '7.2 Uncompressed header semantics' of the VP9 specification
+ * for more details.
+ */
+enum v4l2_vp9_reset_frame_context {
+	V4L2_VP9_RESET_FRAME_CTX_NONE,
+	V4L2_VP9_RESET_FRAME_CTX_SPEC,
+	V4L2_VP9_RESET_FRAME_CTX_ALL,
+};
+
+/**
+ * enum v4l2_vp9_interpolation_filter - VP9 interpolation filter types
+ *
+ * @V4L2_VP9_INTERP_FILTER_8TAP: height tap filter
+ * @V4L2_VP9_INTERP_FILTER_8TAP_SMOOTH: height tap smooth filter
+ * @V4L2_VP9_INTERP_FILTER_8TAP_SHARP: height tap sharp filter
+ * @V4L2_VP9_INTERP_FILTER_BILINEAR: bilinear filter
+ * @V4L2_VP9_INTERP_FILTER_SWITCHABLE: filter selection is signaled at the
+ *				       block level
+ *
+ * See section '7.2.7 Interpolation filter semantics' of the VP9 specification
+ * for more details.
+ */
+enum v4l2_vp9_interpolation_filter {
+	V4L2_VP9_INTERP_FILTER_8TAP,
+	V4L2_VP9_INTERP_FILTER_8TAP_SMOOTH,
+	V4L2_VP9_INTERP_FILTER_8TAP_SHARP,
+	V4L2_VP9_INTERP_FILTER_BILINEAR,
+	V4L2_VP9_INTERP_FILTER_SWITCHABLE,
+};
+
+/**
+ * enum v4l2_vp9_reference_mode - VP9 reference modes
+ *
+ * @V4L2_VP9_REF_MODE_SINGLE: indicates that all the inter blocks use only a
+ *			      single reference frame to generate motion
+ *			      compensated prediction
+ * @V4L2_VP9_REF_MODE_COMPOUND: requires all the inter blocks to use compound
+ *				mode. Single reference frame prediction is not
+ *				allowed
+ * @V4L2_VP9_REF_MODE_SELECT: allows each individual inter block to select
+ *			      between single and compound prediction modes
+ *
+ * See section '7.3.6 Frame reference mode semantics' of the VP9 specification
+ * for more details.
+ */
+enum v4l2_vp9_reference_mode {
+	V4L2_VP9_REF_MODE_SINGLE,
+	V4L2_VP9_REF_MODE_COMPOUND,
+	V4L2_VP9_REF_MODE_SELECT,
+};
+
+/**
+ * enum v4l2_vp9_tx_mode - VP9 TX modes
+ *
+ * @V4L2_VP9_TX_MODE_ONLY_4X4: transform size is 4x4
+ * @V4L2_VP9_TX_MODE_ALLOW_8X8: transform size can be up to 8x8
+ * @V4L2_VP9_TX_MODE_ALLOW_16X16: transform size can be up to 16x16
+ * @V4L2_VP9_TX_MODE_ALLOW_32X32: transform size can be up to 32x32
+ * @V4L2_VP9_TX_MODE_SELECT: bitstream contains transform size for each block
+ *
+ * See section '7.3.1 Tx mode semantics' of the VP9 specification for more
+ * details.
+ */
+enum v4l2_vp9_tx_mode {
+	V4L2_VP9_TX_MODE_ONLY_4X4,
+	V4L2_VP9_TX_MODE_ALLOW_8X8,
+	V4L2_VP9_TX_MODE_ALLOW_16X16,
+	V4L2_VP9_TX_MODE_ALLOW_32X32,
+	V4L2_VP9_TX_MODE_SELECT,
+};
+
+/**
+ * enum v4l2_vp9_ref_id - VP9 Reference frame IDs
+ *
+ * @V4L2_REF_ID_LAST: last reference frame
+ * @V4L2_REF_ID_GOLDEN: golden reference frame
+ * @V4L2_REF_ID_ALTREF: alternative reference frame
+ * @V4L2_REF_ID_CNT: number of reference frames
+ *
+ * See section '7.4.12 Ref frames semantics' of the VP9 specification for more
+ * details.
+ */
+enum v4l2_vp9_ref_id {
+	V4L2_REF_ID_LAST,
+	V4L2_REF_ID_GOLDEN,
+	V4L2_REF_ID_ALTREF,
+	V4L2_REF_ID_CNT,
+};
+
+/**
+ * enum v4l2_vp9_frame_flags - VP9 frame flags
+ * @V4L2_VP9_FRAME_FLAG_KEY_FRAME: the frame is a key frame
+ * @V4L2_VP9_FRAME_FLAG_SHOW_FRAME: the frame should be displayed
+ * @V4L2_VP9_FRAME_FLAG_ERROR_RESILIENT: the decoding should be error resilient
+ * @V4L2_VP9_FRAME_FLAG_INTRA_ONLY: the frame does not reference other frames
+ * @V4L2_VP9_FRAME_FLAG_ALLOW_HIGH_PREC_MV: the frame might can high precision
+ *					    motion vectors
+ * @V4L2_VP9_FRAME_FLAG_REFRESH_FRAME_CTX: frame context should be updated
+ *					   after decoding
+ * @V4L2_VP9_FRAME_FLAG_PARALLEL_DEC_MODE: parallel decoding is used
+ * @V4L2_VP9_FRAME_FLAG_X_SUBSAMPLING: vertical subsampling is enabled
+ * @V4L2_VP9_FRAME_FLAG_Y_SUBSAMPLING: horizontal subsampling is enabled
+ * @V4L2_VP9_FRAME_FLAG_COLOR_RANGE_FULL_SWING: full UV range is used
+ *
+ * Check the VP9 specification for more details.
+ */
+enum v4l2_vp9_frame_flags {
+	V4L2_VP9_FRAME_FLAG_KEY_FRAME = 1 << 0,
+	V4L2_VP9_FRAME_FLAG_SHOW_FRAME = 1 << 1,
+	V4L2_VP9_FRAME_FLAG_ERROR_RESILIENT = 1 << 2,
+	V4L2_VP9_FRAME_FLAG_INTRA_ONLY = 1 << 3,
+	V4L2_VP9_FRAME_FLAG_ALLOW_HIGH_PREC_MV = 1 << 4,
+	V4L2_VP9_FRAME_FLAG_REFRESH_FRAME_CTX = 1 << 5,
+	V4L2_VP9_FRAME_FLAG_PARALLEL_DEC_MODE = 1 << 6,
+	V4L2_VP9_FRAME_FLAG_X_SUBSAMPLING = 1 << 7,
+	V4L2_VP9_FRAME_FLAG_Y_SUBSAMPLING = 1 << 8,
+	V4L2_VP9_FRAME_FLAG_COLOR_RANGE_FULL_SWING = 1 << 9,
+};
+
+#define V4L2_VP9_PROFILE_MAX		3
+
+/**
+ * struct v4l2_ctrl_vp9_frame_decode_params - VP9 frame decoding control
+ *
+ * @flags: combination of V4L2_VP9_FRAME_FLAG_* flags
+ * @compressed_header_size: compressed header size in bytes
+ * @uncompressed_header_size: uncompressed header size in bytes
+ * @profile: VP9 profile. Can be 0, 1, 2 or 3
+ * @reset_frame_context: specifies whether the frame context should be reset
+ *			 to default values. See &v4l2_vp9_reset_frame_context
+ *			 for more details
+ * @frame_context_idx: frame context that should be used/updated
+ * @bit_depth: bits per components. Can be 8, 10 or 12. Note that not all
+ *	       profiles support 10 and/or 12 bits depths
+ * @interpolation_filter: specifies the filter selection used for performing
+ *			  inter prediction. See &v4l2_vp9_interpolation_filter
+ *			  for more details
+ * @tile_cols_log2: specifies the base 2 logarithm of the width of each tile
+ *		    (where the width is measured in units of 8x8 blocks).
+ *		    Shall be less than or equal to 6
+ * @tile_rows_log2: specifies the base 2 logarithm of the height of each tile
+ *		    (where the height is measured in units of 8x8 blocks)
+ * @tx_mode: specifies the TX mode. See &v4l2_vp9_tx_mode for more details
+ * @reference_mode: specifies the type of inter prediction to be used. See
+ *		    &v4l2_vp9_reference_mode for more details
+ * @ref_frame_sign_biases: intended direction in time of the motion vector for
+ *                         each reference frame (0: backward, 1: forward).
+ *                         Only the first V4L2_REF_ID_CNT are used.
+ * @padding: needed to make this struct 64 bit aligned. Shall be filled with
+ *	     zeros
+ * @frame_width_minus_1: add 1 to it and you'll get the frame width expressed
+ *			 in pixels
+ * @frame_height_minus_1: add 1 to it and you'll get the frame height expressed
+ *			  in pixels
+ * @frame_width_minus_1: add 1 to it and you'll get the expected render width
+ *			 expressed in pixels. This is not used during the
+ *			 decoding process but might be used by HW scalers to
+ *			 prepare a frame that's ready for scanout
+ * @frame_height_minus_1: add 1 to it and you'll get the expected render height
+ *			 expressed in pixels. This is not used during the
+ *			 decoding process but might be used by HW scalers to
+ *			 prepare a frame that's ready for scanout
+ * @refs: array of reference frames, identified by timestamp. See
+          &v4l2_vp9_ref_id for more details
+ * @lf: loop filter parameters. See &v4l2_vp9_loop_filter for more details
+ * @quant: quantization parameters. See &v4l2_vp9_quantization for more details
+ * @seg: segmentation parameters. See &v4l2_vp9_segmentation for more details
+ * @probs: probabilities. See &v4l2_vp9_probabilities for more details
+ */
+struct v4l2_ctrl_vp9_frame_decode_params {
+	__u32 flags;
+	__u16 compressed_header_size;
+	__u16 uncompressed_header_size;
+	__u8 profile;
+	__u8 reset_frame_context;
+	__u8 frame_context_idx;
+	__u8 bit_depth;
+	__u8 interpolation_filter;
+	__u8 tile_cols_log2;
+	__u8 tile_rows_log2;
+	__u8 tx_mode;
+	__u8 reference_mode;
+	__u8 ref_frame_sign_biases;
+	__u8 padding[5];
+	__u16 frame_width_minus_1;
+	__u16 frame_height_minus_1;
+	__u16 render_width_minus_1;
+	__u16 render_height_minus_1;
+	__u64 refs[V4L2_REF_ID_CNT];
+	struct v4l2_vp9_loop_filter lf;
+	struct v4l2_vp9_quantization quant;
+	struct v4l2_vp9_segmentation seg;
+	struct v4l2_vp9_probabilities probs;
+};
+
+#define V4L2_VP9_NUM_FRAME_CTX	4
+
+/**
+ * struct v4l2_ctrl_vp9_frame_ctx - VP9 frame context control
+ *
+ * @probs: VP9 probabilities
+ *
+ * This control is accessed in both direction. The user should initialize the
+ * 4 contexts with default values just after starting the stream. Then before
+ * decoding a frame it should query the current frame context (the one passed
+ * through &v4l2_ctrl_vp9_frame_decode_params.frame_context_idx) to initialize
+ * &v4l2_ctrl_vp9_frame_decode_params.probs. The probs are then adjusted based
+ * on the bitstream info and passed to the kernel. The codec should update
+ * the frame context after the frame has been decoded, so that next time
+ * userspace query this context it contains the updated probabilities.
+ */
+struct v4l2_ctrl_vp9_frame_ctx {
+	struct v4l2_vp9_probabilities probs;
+};
+
+#endif /* _VP9_CTRLS_H_ */
diff -ruN a/include/net/addrconf.h b/include/net/addrconf.h
--- a/include/net/addrconf.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/net/addrconf.h	2021-12-23 08:35:55.000000000 +0100
@@ -269,6 +269,18 @@
 void addrconf_prefix_rcv(struct net_device *dev,
 			 u8 *opt, int len, bool sllao);
 
+/* Determines into what table to put autoconf PIO/RIO/default routes
+ * learned on this device.
+ *
+ * - If 0, use the same table for every device. This puts routes into
+ *   one of RT_TABLE_{PREFIX,INFO,DFLT} depending on the type of route
+ *   (but note that these three are currently all equal to
+ *   RT6_TABLE_MAIN).
+ * - If > 0, use the specified table.
+ * - If < 0, put routes into table dev->ifindex + (-rt_table).
+ */
+u32 addrconf_rt_table(const struct net_device *dev, u32 default_table);
+
 /*
  *	anycast prototypes (anycast.c)
  */
diff -ruN a/include/net/netns/core.h b/include/net/netns/core.h
--- a/include/net/netns/core.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/net/netns/core.h	2021-12-23 08:35:55.000000000 +0100
@@ -10,6 +10,7 @@
 	struct ctl_table_header	*sysctl_hdr;
 
 	int	sysctl_somaxconn;
+	int	sysctl_android_paranoid;
 
 #ifdef CONFIG_PROC_FS
 	int __percpu *sock_inuse;
diff -ruN a/include/net/netns/ipv4.h b/include/net/netns/ipv4.h
--- a/include/net/netns/ipv4.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/net/netns/ipv4.h	2021-12-23 08:35:55.000000000 +0100
@@ -112,6 +112,7 @@
 #ifdef CONFIG_NET_L3_MASTER_DEV
 	u8 sysctl_tcp_l3mdev_accept;
 #endif
+	int sysctl_tcp_default_init_rwnd;
 	u8 sysctl_tcp_mtu_probing;
 	int sysctl_tcp_mtu_probe_floor;
 	int sysctl_tcp_base_mss;
diff -ruN a/include/net/sock.h b/include/net/sock.h
--- a/include/net/sock.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/net/sock.h	2021-12-23 08:35:55.000000000 +0100
@@ -2731,6 +2731,8 @@
 		   struct user_namespace *user_ns, int cap);
 bool sk_capable(const struct sock *sk, int cap);
 bool sk_net_capable(const struct sock *sk, int cap);
+bool inet_sk_allowed(struct net *net, gid_t gid);
+bool android_ns_capable(struct net *net, int cap);
 
 void sk_get_meminfo(const struct sock *sk, u32 *meminfo);
 
diff -ruN a/include/sound/da7219.h b/include/sound/da7219.h
--- a/include/sound/da7219.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/sound/da7219.h	2021-12-23 08:35:56.000000000 +0100
@@ -40,6 +40,8 @@
 
 	const char *dai_clk_names[DA7219_DAI_NUM_CLKS];
 
+	const char *mclk_name;
+
 	/* Mic */
 	enum da7219_micbias_voltage micbias_lvl;
 	enum da7219_mic_amp_in_sel mic_amp_in_sel;
diff -ruN a/include/sound/hdmi-codec.h b/include/sound/hdmi-codec.h
--- a/include/sound/hdmi-codec.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/sound/hdmi-codec.h	2021-12-23 08:35:56.000000000 +0100
@@ -55,6 +55,13 @@
 typedef void (*hdmi_codec_plugged_cb)(struct device *dev,
 				      bool plugged);
 
+enum {
+	HDMI_CODEC_TRIGGER_EVENT_STOP,
+	HDMI_CODEC_TRIGGER_EVENT_START,
+	HDMI_CODEC_TRIGGER_EVENT_SUSPEND,
+	HDMI_CODEC_TRIGGER_EVENT_RESUME,
+};
+
 struct hdmi_codec_pdata;
 struct hdmi_codec_ops {
 	/*
@@ -82,6 +89,12 @@
 		       struct hdmi_codec_params *hparms);
 
 	/*
+	 * PCM trigger callback.
+	 * Optional
+	 */
+	int (*trigger)(struct device *dev, int event);
+
+	/*
 	 * Shuts down the audio stream.
 	 * Mandatory
 	 */
diff -ruN a/include/sound/rt5682.h b/include/sound/rt5682.h
--- a/include/sound/rt5682.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/sound/rt5682.h	2021-12-23 08:35:56.000000000 +0100
@@ -43,6 +43,7 @@
 	bool dmic_clk_driving_high;
 
 	const char *dai_clk_names[RT5682_DAI_NUM_CLKS];
+	const char *mclk_name;
 };
 
 #endif
diff -ruN a/include/sound/rt5682s.h b/include/sound/rt5682s.h
--- a/include/sound/rt5682s.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/sound/rt5682s.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,48 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * linux/sound/rt5682s.h -- Platform data for RT5682I-VS
+ *
+ * Copyright 2021 Realtek Microelectronics
+ */
+
+#ifndef __LINUX_SND_RT5682S_H
+#define __LINUX_SND_RT5682S_H
+
+enum rt5682s_dmic1_data_pin {
+	RT5682S_DMIC1_DATA_NULL,
+	RT5682S_DMIC1_DATA_GPIO2,
+	RT5682S_DMIC1_DATA_GPIO5,
+};
+
+enum rt5682s_dmic1_clk_pin {
+	RT5682S_DMIC1_CLK_NULL,
+	RT5682S_DMIC1_CLK_GPIO1,
+	RT5682S_DMIC1_CLK_GPIO3,
+};
+
+enum rt5682s_jd_src {
+	RT5682S_JD_NULL,
+	RT5682S_JD1,
+};
+
+enum rt5682s_dai_clks {
+	RT5682S_DAI_WCLK_IDX,
+	RT5682S_DAI_BCLK_IDX,
+	RT5682S_DAI_NUM_CLKS,
+};
+
+struct rt5682s_platform_data {
+
+	int ldo1_en; /* GPIO for LDO1_EN */
+
+	enum rt5682s_dmic1_data_pin dmic1_data_pin;
+	enum rt5682s_dmic1_clk_pin dmic1_clk_pin;
+	enum rt5682s_jd_src jd_src;
+	unsigned int dmic_clk_rate;
+	unsigned int dmic_delay;
+	bool dmic_clk_driving_high;
+
+	const char *dai_clk_names[RT5682S_DAI_NUM_CLKS];
+};
+
+#endif
diff -ruN a/include/sound/soc-acpi.h b/include/sound/soc-acpi.h
--- a/include/sound/soc-acpi.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/sound/soc-acpi.h	2021-12-23 08:35:56.000000000 +0100
@@ -129,6 +129,8 @@
  * all firmware/topology related fields.
  *
  * @id: ACPI ID (usually the codec's) used to find a matching machine driver.
+ * @comp_ids: list of compatible audio codecs using the same machine driver,
+ * firmware and topology
  * @link_mask: describes required board layout, e.g. for SoundWire.
  * @links: array of link _ADR descriptors, null terminated.
  * @drv_name: machine driver name
@@ -146,6 +148,7 @@
 /* Descriptor for SST ASoC machine driver */
 struct snd_soc_acpi_mach {
 	const u8 id[ACPI_ID_LEN];
+	const struct snd_soc_acpi_codecs *comp_ids;
 	const u32 link_mask;
 	const struct snd_soc_acpi_link_adr *links;
 	const char *drv_name;
diff -ruN a/include/sound/soc-dpcm.h b/include/sound/soc-dpcm.h
--- a/include/sound/soc-dpcm.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/sound/soc-dpcm.h	2021-12-23 08:35:56.000000000 +0100
@@ -159,6 +159,7 @@
 int dpcm_be_dai_prepare(struct snd_soc_pcm_runtime *fe, int stream);
 int dpcm_dapm_stream_event(struct snd_soc_pcm_runtime *fe, int dir,
 	int event);
+bool dpcm_end_walk_at_be(struct snd_soc_dapm_widget *widget, enum snd_soc_dapm_direction dir);
 
 #define dpcm_be_dai_startup_rollback(fe, stream, last)	\
 						dpcm_be_dai_stop(fe, stream, 0, last)
diff -ruN a/include/sound/soc-topology.h b/include/sound/soc-topology.h
--- a/include/sound/soc-topology.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/sound/soc-topology.h	2021-12-23 08:35:56.000000000 +0100
@@ -151,7 +151,7 @@
 		struct snd_soc_tplg_hdr *);
 
 	/* completion - called at completion of firmware loading */
-	void (*complete)(struct snd_soc_component *);
+	int (*complete)(struct snd_soc_component *comp);
 
 	/* manifest - optional to inform component of manifest */
 	int (*manifest)(struct snd_soc_component *, int index,
diff -ruN a/include/sound/sof/dai.h b/include/sound/sof/dai.h
--- a/include/sound/sof/dai.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/sound/sof/dai.h	2021-12-23 08:35:56.000000000 +0100
@@ -50,6 +50,13 @@
 #define SOF_DAI_FMT_INV_MASK		0x0f00
 #define SOF_DAI_FMT_CLOCK_PROVIDER_MASK	0xf000
 
+/* DAI_CONFIG flags */
+#define SOF_DAI_CONFIG_FLAGS_MASK	0x3
+#define SOF_DAI_CONFIG_FLAGS_NONE	(0 << 0) /**< DAI_CONFIG sent without stage information */
+#define SOF_DAI_CONFIG_FLAGS_HW_PARAMS	(1 << 0) /**< DAI_CONFIG sent during hw_params stage */
+#define SOF_DAI_CONFIG_FLAGS_HW_FREE	(2 << 0) /**< DAI_CONFIG sent during hw_free stage */
+#define SOF_DAI_CONFIG_FLAGS_RFU	(3 << 0) /**< not used, reserved for future use */
+
 /** \brief Types of DAI */
 enum sof_ipc_dai_type {
 	SOF_DAI_INTEL_NONE = 0,		/**< None */
@@ -69,7 +76,8 @@
 
 	/* physical protocol and clocking */
 	uint16_t format;	/**< SOF_DAI_FMT_ */
-	uint16_t reserved16;	/**< alignment */
+	uint8_t group_id;	/**< group ID, 0 means no group (ABI 3.17) */
+	uint8_t flags;		/**< SOF_DAI_CONFIG_FLAGS_ (ABI 3.19) */
 
 	/* reserved for future use */
 	uint32_t reserved[8];
diff -ruN a/include/sound/sof/dai-intel.h b/include/sound/sof/dai-intel.h
--- a/include/sound/sof/dai-intel.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/sound/sof/dai-intel.h	2021-12-23 08:35:56.000000000 +0100
@@ -48,6 +48,10 @@
 #define SOF_DAI_INTEL_SSP_CLKCTRL_FS_KA			BIT(4)
 /* bclk idle */
 #define SOF_DAI_INTEL_SSP_CLKCTRL_BCLK_IDLE_HIGH	BIT(5)
+/* mclk early start */
+#define SOF_DAI_INTEL_SSP_CLKCTRL_MCLK_ES               BIT(6)
+/* bclk early start */
+#define SOF_DAI_INTEL_SSP_CLKCTRL_BCLK_ES               BIT(7)
 
 /* DMIC max. four controllers for eight microphone channels */
 #define SOF_DAI_INTEL_DMIC_NUM_CTRL			4
diff -ruN a/include/trace/events/fs.h b/include/trace/events/fs.h
--- a/include/trace/events/fs.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/trace/events/fs.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,53 @@
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM fs
+
+#if !defined(_TRACE_FS_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_FS_H
+
+#include <linux/fs.h>
+#include <linux/tracepoint.h>
+
+TRACE_EVENT(do_sys_open,
+
+	TP_PROTO(const char *filename, int flags, int mode),
+
+	TP_ARGS(filename, flags, mode),
+
+	TP_STRUCT__entry(
+		__string(	filename, filename		)
+		__field(	int, flags			)
+		__field(	int, mode			)
+	),
+
+	TP_fast_assign(
+		__assign_str(filename, filename);
+		__entry->flags = flags;
+		__entry->mode = mode;
+	),
+
+	TP_printk("\"%s\" %x %o",
+		  __get_str(filename), __entry->flags, __entry->mode)
+);
+
+TRACE_EVENT(open_exec,
+
+	TP_PROTO(const char *filename),
+
+	TP_ARGS(filename),
+
+	TP_STRUCT__entry(
+		__string(	filename, filename		)
+	),
+
+	TP_fast_assign(
+		__assign_str(filename, filename);
+	),
+
+	TP_printk("\"%s\"",
+		  __get_str(filename))
+);
+
+#endif /* _TRACE_FS_H */
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff -ruN a/include/trace/events/sched.h b/include/trace/events/sched.h
--- a/include/trace/events/sched.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/trace/events/sched.h	2021-12-23 08:35:56.000000000 +0100
@@ -485,6 +485,30 @@
 	     TP_ARGS(tsk, delay));
 
 /*
+ * Tracepoint for recording the cause of uninterruptible sleep.
+ */
+TRACE_EVENT(sched_blocked_reason,
+
+	TP_PROTO(struct task_struct *tsk),
+
+	TP_ARGS(tsk),
+
+	TP_STRUCT__entry(
+		__field( pid_t,	pid	)
+		__field( void*, caller	)
+		__field( bool, io_wait	)
+	),
+
+	TP_fast_assign(
+		__entry->pid	= tsk->pid;
+		__entry->caller = (void *)get_wchan(tsk);
+		__entry->io_wait = tsk->in_iowait;
+	),
+
+	TP_printk("pid=%d iowait=%d caller=%pS", __entry->pid, __entry->io_wait, __entry->caller)
+);
+
+/*
  * Tracepoint for accounting runtime (time the task is executing
  * on a CPU).
  */
diff -ruN a/include/uapi/drm/evdi_drm.h b/include/uapi/drm/evdi_drm.h
--- a/include/uapi/drm/evdi_drm.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/uapi/drm/evdi_drm.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,132 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note 
+ *
+ * Copyright (c) 2015 - 2020 DisplayLink (UK) Ltd.
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License v2. See the file COPYING in the main directory of this archive for
+ * more details.
+ */
+
+#ifndef __UAPI_EVDI_DRM_H__
+#define __UAPI_EVDI_DRM_H__
+
+#ifdef __KERNEL__
+#include <linux/types.h>
+#else
+#include <stdint.h>
+#endif
+
+#include "drm.h"
+
+/* Output events sent from driver to evdi lib */
+#define DRM_EVDI_EVENT_UPDATE_READY  0x80000000
+#define DRM_EVDI_EVENT_DPMS          0x80000001
+#define DRM_EVDI_EVENT_MODE_CHANGED  0x80000002
+#define DRM_EVDI_EVENT_CRTC_STATE    0x80000003
+#define DRM_EVDI_EVENT_CURSOR_SET    0x80000004
+#define DRM_EVDI_EVENT_CURSOR_MOVE   0x80000005
+#define DRM_EVDI_EVENT_DDCCI_DATA    0x80000006
+
+struct drm_evdi_event_update_ready {
+	struct drm_event base;
+};
+
+struct drm_evdi_event_dpms {
+	struct drm_event base;
+	int32_t mode;
+};
+
+struct drm_evdi_event_mode_changed {
+	struct drm_event base;
+	int32_t hdisplay;
+	int32_t vdisplay;
+	int32_t vrefresh;
+	int32_t bits_per_pixel;
+	uint32_t pixel_format;
+};
+
+struct drm_evdi_event_crtc_state {
+	struct drm_event base;
+	int32_t state;
+};
+
+struct drm_evdi_connect {
+	int32_t connected;
+	int32_t dev_index;
+	const unsigned char * __user edid;
+	uint32_t edid_length;
+	uint32_t sku_area_limit;
+};
+
+struct drm_evdi_request_update {
+	int32_t reserved;
+};
+
+enum drm_evdi_grabpix_mode {
+	EVDI_GRABPIX_MODE_RECTS = 0,
+	EVDI_GRABPIX_MODE_DIRTY = 1,
+};
+
+struct drm_evdi_grabpix {
+	enum drm_evdi_grabpix_mode mode;
+	int32_t buf_width;
+	int32_t buf_height;
+	int32_t buf_byte_stride;
+	unsigned char __user *buffer;
+	int32_t num_rects;
+	struct drm_clip_rect __user *rects;
+};
+
+struct drm_evdi_event_cursor_set {
+	struct drm_event base;
+	int32_t hot_x;
+	int32_t hot_y;
+	uint32_t width;
+	uint32_t height;
+	uint8_t enabled;
+	uint32_t buffer_handle;
+	uint32_t buffer_length;
+	uint32_t pixel_format;
+	uint32_t stride;
+};
+
+struct drm_evdi_event_cursor_move {
+	struct drm_event base;
+	int32_t x;
+	int32_t y;
+};
+
+struct drm_evdi_ddcci_response {
+	const unsigned char * __user buffer;
+	uint32_t buffer_length;
+	uint8_t result;
+};
+
+#define DDCCI_BUFFER_SIZE 64
+
+struct drm_evdi_event_ddcci_data {
+	struct drm_event base;
+	unsigned char buffer[DDCCI_BUFFER_SIZE];
+	uint32_t buffer_length;
+	uint16_t flags;
+	uint16_t address;
+};
+
+/* Input ioctls from evdi lib to driver */
+#define DRM_EVDI_CONNECT          0x00
+#define DRM_EVDI_REQUEST_UPDATE   0x01
+#define DRM_EVDI_GRABPIX          0x02
+#define DRM_EVDI_DDCCI_RESPONSE   0x03
+/* LAST_IOCTL 0x5F -- 96 driver specific ioctls to use */
+
+#define DRM_IOCTL_EVDI_CONNECT DRM_IOWR(DRM_COMMAND_BASE +  \
+	DRM_EVDI_CONNECT, struct drm_evdi_connect)
+#define DRM_IOCTL_EVDI_REQUEST_UPDATE DRM_IOWR(DRM_COMMAND_BASE +  \
+	DRM_EVDI_REQUEST_UPDATE, struct drm_evdi_request_update)
+#define DRM_IOCTL_EVDI_GRABPIX DRM_IOWR(DRM_COMMAND_BASE +  \
+	DRM_EVDI_GRABPIX, struct drm_evdi_grabpix)
+#define DRM_IOCTL_EVDI_DDCCI_RESPONSE DRM_IOWR(DRM_COMMAND_BASE +  \
+	DRM_EVDI_DDCCI_RESPONSE, struct drm_evdi_ddcci_response)
+
+#endif /* __EVDI_UAPI_DRM_H__ */
+
diff -ruN a/include/uapi/drm/mediatek_drm.h b/include/uapi/drm/mediatek_drm.h
--- a/include/uapi/drm/mediatek_drm.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/uapi/drm/mediatek_drm.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,65 @@
+/*
+ * Copyright (c) 2015 MediaTek Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef _UAPI_MEDIATEK_DRM_H
+#define _UAPI_MEDIATEK_DRM_H
+
+#include <drm/drm.h>
+
+#ifdef __KERNEL__
+#include <linux/types.h>
+#else
+#include <stdint.h>
+#endif
+
+/**
+ * User-desired buffer creation information structure.
+ *
+ * @size: user-desired memory allocation size.
+ *	- this size value would be page-aligned internally.
+ * @flags: user request for setting memory type or cache attributes.
+ * @handle: returned a handle to created gem object.
+ *	- this handle will be set by gem module of kernel side.
+ */
+struct drm_mtk_gem_create {
+	uint64_t size;
+	uint32_t flags;
+	uint32_t handle;
+};
+
+/**
+ * A structure for getting buffer offset.
+ *
+ * @handle: a pointer to gem object created.
+ * @pad: just padding to be 64-bit aligned.
+ * @offset: relatived offset value of the memory region allocated.
+ *     - this value should be set by user.
+ */
+struct drm_mtk_gem_map_off {
+	uint32_t handle;
+	uint32_t pad;
+	uint64_t offset;
+};
+
+#define DRM_MTK_GEM_CREATE		0x00
+#define DRM_MTK_GEM_MAP_OFFSET		0x01
+
+#define DRM_IOCTL_MTK_GEM_CREATE	DRM_IOWR(DRM_COMMAND_BASE + \
+		DRM_MTK_GEM_CREATE, struct drm_mtk_gem_create)
+
+#define DRM_IOCTL_MTK_GEM_MAP_OFFSET	DRM_IOWR(DRM_COMMAND_BASE + \
+		DRM_MTK_GEM_MAP_OFFSET, struct drm_mtk_gem_map_off)
+
+
+#endif /* _UAPI_MEDIATEK_DRM_H */
diff -ruN a/include/uapi/drm/rockchip_drm.h b/include/uapi/drm/rockchip_drm.h
--- a/include/uapi/drm/rockchip_drm.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/uapi/drm/rockchip_drm.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,58 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+
+/*
+ * Copyright (c) Fuzhou Rockchip Electronics Co.Ltd
+ * Authors:
+ *       Mark Yao <yzq@rock-chips.com>
+ *
+ * base on exynos_drm.h
+ */
+
+#ifndef _UAPI_ROCKCHIP_DRM_H
+#define _UAPI_ROCKCHIP_DRM_H
+
+#ifndef __KERNEL__
+#include <stdint.h>
+#endif
+
+#include <drm/drm.h>
+#include <linux/types.h>
+
+/**
+ * User-desired buffer creation information structure.
+ *
+ * @size: user-desired memory allocation size.
+ * @flags: user request for setting memory type or cache attributes.
+ * @handle: returned a handle to created gem object.
+ *     - this handle will be set by gem module of kernel side.
+ */
+struct drm_rockchip_gem_create {
+	uint64_t size;
+	uint32_t flags;
+	uint32_t handle;
+};
+
+/**
+ * A structure for getting buffer offset.
+ *
+ * @handle: a pointer to gem object created.
+ * @pad: just padding to be 64-bit aligned.
+ * @offset: relatived offset value of the memory region allocated.
+ *     - this value should be set by user.
+ */
+struct drm_rockchip_gem_map_off {
+	uint32_t handle;
+	uint32_t pad;
+	uint64_t offset;
+};
+
+#define DRM_ROCKCHIP_GEM_CREATE		0x00
+#define DRM_ROCKCHIP_GEM_MAP_OFFSET	0x01
+
+#define DRM_IOCTL_ROCKCHIP_GEM_CREATE	DRM_IOWR(DRM_COMMAND_BASE + \
+		DRM_ROCKCHIP_GEM_CREATE, struct drm_rockchip_gem_create)
+
+#define DRM_IOCTL_ROCKCHIP_GEM_MAP_OFFSET	DRM_IOWR(DRM_COMMAND_BASE + \
+		DRM_ROCKCHIP_GEM_MAP_OFFSET, struct drm_rockchip_gem_map_off)
+
+#endif /* _UAPI_ROCKCHIP_DRM_H */
diff -ruN a/include/uapi/drm/virtgpu_drm.h b/include/uapi/drm/virtgpu_drm.h
--- a/include/uapi/drm/virtgpu_drm.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/drm/virtgpu_drm.h	2021-12-23 08:35:56.000000000 +0100
@@ -47,12 +47,15 @@
 #define DRM_VIRTGPU_WAIT     0x08
 #define DRM_VIRTGPU_GET_CAPS  0x09
 #define DRM_VIRTGPU_RESOURCE_CREATE_BLOB 0x0a
+#define DRM_VIRTGPU_CONTEXT_INIT 0x0b
 
 #define VIRTGPU_EXECBUF_FENCE_FD_IN	0x01
 #define VIRTGPU_EXECBUF_FENCE_FD_OUT	0x02
+#define VIRTGPU_EXECBUF_RING_IDX	0x04
 #define VIRTGPU_EXECBUF_FLAGS  (\
 		VIRTGPU_EXECBUF_FENCE_FD_IN |\
 		VIRTGPU_EXECBUF_FENCE_FD_OUT |\
+		VIRTGPU_EXECBUF_RING_IDX |\
 		0)
 
 struct drm_virtgpu_map {
@@ -68,6 +71,8 @@
 	__u64 bo_handles;
 	__u32 num_bo_handles;
 	__s32 fence_fd; /* in/out fence fd (see VIRTGPU_EXECBUF_FENCE_FD_IN/OUT) */
+	__u32 ring_idx; /* command ring index (see VIRTGPU_EXECBUF_RING_IDX) */
+	__u32 pad;
 };
 
 #define VIRTGPU_PARAM_3D_FEATURES 1 /* do we have 3D features in the hw */
@@ -75,6 +80,8 @@
 #define VIRTGPU_PARAM_RESOURCE_BLOB 3 /* DRM_VIRTGPU_RESOURCE_CREATE_BLOB */
 #define VIRTGPU_PARAM_HOST_VISIBLE 4 /* Host blob resources are mappable */
 #define VIRTGPU_PARAM_CROSS_DEVICE 5 /* Cross virtio-device resource sharing  */
+#define VIRTGPU_PARAM_CONTEXT_INIT 6 /* DRM_VIRTGPU_CONTEXT_INIT */
+#define VIRTGPU_PARAM_SUPPORTED_CAPSET_IDs 7 /* Bitmask of supported capability set ids */
 
 struct drm_virtgpu_getparam {
 	__u64 param;
@@ -107,6 +114,34 @@
 	__u32 blob_mem;
 };
 
+/* CHROMIUM */
+struct drm_virtgpu_resource_info_cros {
+	__u32 bo_handle;
+	__u32 res_handle;
+	__u32 size;
+
+/* Always returns res_handle, size, and blob_mem.
+ * !! RETURN SEMANTICS ARE CHANGED BY THIS COMMIT.
+ * !! User space changes are likely required for anything relying on
+ * !! getting extended info from VIRTGPU_RESOURCE_INFO_TYPE_DEFAULT.
+ */
+#define VIRTGPU_RESOURCE_INFO_TYPE_DEFAULT 0
+/* Always returns res_handle, size, and "extended info".
+ * !! Produces an error (EINVAL) for blob resources created with blob_mem ==
+ * !! VIRTGPU_BLOB_MEM_GUEST, which doesn't use host storage.
+ */
+#define VIRTGPU_RESOURCE_INFO_TYPE_EXTENDED 1
+	union {
+		__u32 type; /* in, VIRTGPU_RESOURCE_INFO_TYPE_* */
+		__u32 blob_mem;
+		__u32 stride;
+		__u32 strides[4]; /* strides[0] is accessible with stride. */
+	};
+	__u32 num_planes;
+	__u32 offsets[4];
+	__u64 format_modifier;
+};
+
 struct drm_virtgpu_3d_box {
 	__u32 x;
 	__u32 y;
@@ -173,6 +208,22 @@
 	__u64 blob_id;
 };
 
+#define VIRTGPU_CONTEXT_PARAM_CAPSET_ID       0x0001
+#define VIRTGPU_CONTEXT_PARAM_NUM_RINGS       0x0002
+#define VIRTGPU_CONTEXT_PARAM_POLL_RINGS_MASK 0x0003
+struct drm_virtgpu_context_set_param {
+	__u64 param;
+	__u64 value;
+};
+
+struct drm_virtgpu_context_init {
+	__u32 num_params;
+	__u32 pad;
+
+	/* pointer to drm_virtgpu_context_set_param array */
+	__u64 ctx_set_params;
+};
+
 #define DRM_IOCTL_VIRTGPU_MAP \
 	DRM_IOWR(DRM_COMMAND_BASE + DRM_VIRTGPU_MAP, struct drm_virtgpu_map)
 
@@ -192,6 +243,11 @@
 	DRM_IOWR(DRM_COMMAND_BASE + DRM_VIRTGPU_RESOURCE_INFO, \
 		 struct drm_virtgpu_resource_info)
 
+/* same ioctl number as DRM_IOCTL_VIRTGPU_RESOURCE_INFO */
+#define DRM_IOCTL_VIRTGPU_RESOURCE_INFO_CROS \
+	DRM_IOWR(DRM_COMMAND_BASE + DRM_VIRTGPU_RESOURCE_INFO, \
+		 struct drm_virtgpu_resource_info_cros)
+
 #define DRM_IOCTL_VIRTGPU_TRANSFER_FROM_HOST \
 	DRM_IOWR(DRM_COMMAND_BASE + DRM_VIRTGPU_TRANSFER_FROM_HOST,	\
 		struct drm_virtgpu_3d_transfer_from_host)
@@ -212,6 +268,10 @@
 	DRM_IOWR(DRM_COMMAND_BASE + DRM_VIRTGPU_RESOURCE_CREATE_BLOB,	\
 		struct drm_virtgpu_resource_create_blob)
 
+#define DRM_IOCTL_VIRTGPU_CONTEXT_INIT					\
+	DRM_IOWR(DRM_COMMAND_BASE + DRM_VIRTGPU_CONTEXT_INIT,		\
+		struct drm_virtgpu_context_init)
+
 #if defined(__cplusplus)
 }
 #endif
diff -ruN a/include/uapi/linux/dma-buf.h b/include/uapi/linux/dma-buf.h
--- a/include/uapi/linux/dma-buf.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/dma-buf.h	2021-12-23 08:35:56.000000000 +0100
@@ -85,6 +85,40 @@
 
 #define DMA_BUF_NAME_LEN	32
 
+/**
+ * struct dma_buf_export_sync_file - Get a sync_file from a dma-buf
+ *
+ * Userspace can perform a DMA_BUF_IOCTL_EXPORT_SYNC_FILE to retrieve the
+ * current set of fences on a dma-buf file descriptor as a sync_file.  CPU
+ * waits via poll() or other driver-specific mechanisms typically wait on
+ * whatever fences are on the dma-buf at the time the wait begins.  This
+ * is similar except that it takes a snapshot of the current fences on the
+ * dma-buf for waiting later instead of waiting immediately.  This is
+ * useful for modern graphics APIs such as Vulkan which assume an explicit
+ * synchronization model but still need to inter-operate with dma-buf.
+ */
+struct dma_buf_export_sync_file {
+	/**
+	 * @flags: Read/write flags
+	 *
+	 * Must be DMA_BUF_SYNC_READ, DMA_BUF_SYNC_WRITE, or both.
+	 *
+	 * If DMA_BUF_SYNC_READ is set and DMA_BUF_SYNC_WRITE is not set,
+	 * the returned sync file waits on any writers of the dma-buf to
+	 * complete.  Waiting on the returned sync file is equivalent to
+	 * poll() with POLLIN.
+	 *
+	 * If DMA_BUF_SYNC_WRITE is set, the returned sync file waits on
+	 * any users of the dma-buf (read or write) to complete.  Waiting
+	 * on the returned sync file is equivalent to poll() with POLLOUT.
+	 * If both DMA_BUF_SYNC_WRITE and DMA_BUF_SYNC_READ are set, this
+	 * is equivalent to just DMA_BUF_SYNC_WRITE.
+	 */
+	__u32 flags;
+	/** @fd: Returned sync file descriptor */
+	__s32 fd;
+};
+
 #define DMA_BUF_BASE		'b'
 #define DMA_BUF_IOCTL_SYNC	_IOW(DMA_BUF_BASE, 0, struct dma_buf_sync)
 
@@ -94,5 +128,6 @@
 #define DMA_BUF_SET_NAME	_IOW(DMA_BUF_BASE, 1, const char *)
 #define DMA_BUF_SET_NAME_A	_IOW(DMA_BUF_BASE, 1, __u32)
 #define DMA_BUF_SET_NAME_B	_IOW(DMA_BUF_BASE, 1, __u64)
+#define DMA_BUF_IOCTL_EXPORT_SYNC_FILE	_IOWR(DMA_BUF_BASE, 2, struct dma_buf_export_sync_file)
 
 #endif
diff -ruN a/include/uapi/linux/fuse.h b/include/uapi/linux/fuse.h
--- a/include/uapi/linux/fuse.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/fuse.h	2021-12-23 08:35:56.000000000 +0100
@@ -367,6 +367,7 @@
 #define FUSE_SUBMOUNTS		(1 << 27)
 #define FUSE_HANDLE_KILLPRIV_V2	(1 << 28)
 #define FUSE_SETXATTR_EXT	(1 << 29)
+#define FUSE_PASSTHROUGH	(1 << 31)
 
 /**
  * CUSE INIT request/reply flags
@@ -520,6 +521,9 @@
 	/* Reserved opcodes: helpful to detect structure endian-ness */
 	CUSE_INIT_BSWAP_RESERVED	= 1048576,	/* CUSE_INIT << 8 */
 	FUSE_INIT_BSWAP_RESERVED	= 436207616,	/* FUSE_INIT << 24 */
+
+	/* Chrome OS extensions */
+	FUSE_CHROMEOS_TMPFILE	= 0xffffffff,	/* u32::MAX */
 };
 
 enum fuse_notify_code {
@@ -636,10 +640,15 @@
 	uint32_t	open_flags;	/* FUSE_OPEN_... */
 };
 
+struct fuse_chromeos_tmpfile_in {
+	uint32_t mode;
+	uint32_t umask;
+};
+
 struct fuse_open_out {
 	uint64_t	fh;
 	uint32_t	open_flags;
-	uint32_t	padding;
+	uint32_t	passthrough_fh;
 };
 
 struct fuse_release_in {
@@ -846,6 +855,14 @@
 	uint32_t	padding;
 };
 
+/* fuse_passthrough_out for passthrough V1 */
+struct fuse_passthrough_out {
+	uint32_t	fd;
+	/* For future implementation */
+	uint32_t	len;
+	void		*vec;
+};
+
 struct fuse_out_header {
 	uint32_t	len;
 	int32_t		error;
@@ -923,6 +940,8 @@
 /* Device ioctls: */
 #define FUSE_DEV_IOC_MAGIC		229
 #define FUSE_DEV_IOC_CLONE		_IOR(FUSE_DEV_IOC_MAGIC, 0, uint32_t)
+/* 127 is reserved for the V1 interface implementation in Android */
+#define FUSE_DEV_IOC_PASSTHROUGH_OPEN	_IOW(FUSE_DEV_IOC_MAGIC, 127, struct fuse_passthrough_out)
 
 struct fuse_lseek_in {
 	uint64_t	fh;
diff -ruN a/include/uapi/linux/futex.h b/include/uapi/linux/futex.h
--- a/include/uapi/linux/futex.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/futex.h	2021-12-23 08:35:56.000000000 +0100
@@ -22,6 +22,7 @@
 #define FUTEX_WAIT_REQUEUE_PI	11
 #define FUTEX_CMP_REQUEUE_PI	12
 #define FUTEX_LOCK_PI2		13
+#define FUTEX_SWAP		14
 
 #define FUTEX_PRIVATE_FLAG	128
 #define FUTEX_CLOCK_REALTIME	256
@@ -42,6 +43,7 @@
 					 FUTEX_PRIVATE_FLAG)
 #define FUTEX_CMP_REQUEUE_PI_PRIVATE	(FUTEX_CMP_REQUEUE_PI | \
 					 FUTEX_PRIVATE_FLAG)
+#define FUTEX_SWAP_PRIVATE		(FUTEX_SWAP | FUTEX_PRIVATE_FLAG)
 
 /*
  * Support for robust futexes: the kernel cleans up held futexes at
diff -ruN a/include/uapi/linux/iio/types.h b/include/uapi/linux/iio/types.h
--- a/include/uapi/linux/iio/types.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/iio/types.h	2021-12-23 08:35:56.000000000 +0100
@@ -95,6 +95,7 @@
 	IIO_MOD_ETHANOL,
 	IIO_MOD_H2,
 	IIO_MOD_O2,
+	IIO_MOD_DOUBLE_TAP,
 };
 
 enum iio_event_type {
diff -ruN a/include/uapi/linux/ipu-isys.h b/include/uapi/linux/ipu-isys.h
--- a/include/uapi/linux/ipu-isys.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/uapi/linux/ipu-isys.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/* Copyright (C) 2016 - 2020 Intel Corporation */
+
+#ifndef UAPI_LINUX_IPU_ISYS_H
+#define UAPI_LINUX_IPU_ISYS_H
+
+#define V4L2_CID_IPU_BASE	(V4L2_CID_USER_BASE + 0x1080)
+
+#define V4L2_CID_IPU_STORE_CSI2_HEADER	(V4L2_CID_IPU_BASE + 2)
+#define V4L2_CID_IPU_ISYS_COMPRESSION	(V4L2_CID_IPU_BASE + 3)
+
+#define VIDIOC_IPU_GET_DRIVER_VERSION \
+	_IOWR('v', BASE_VIDIOC_PRIVATE + 3, uint32_t)
+
+#endif /* UAPI_LINUX_IPU_ISYS_H */
diff -ruN a/include/uapi/linux/ipu-psys.h b/include/uapi/linux/ipu-psys.h
--- a/include/uapi/linux/ipu-psys.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/uapi/linux/ipu-psys.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,121 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/* Copyright (C) 2013 - 2020 Intel Corporation */
+
+#ifndef _UAPI_IPU_PSYS_H
+#define _UAPI_IPU_PSYS_H
+
+#ifdef __KERNEL__
+#include <linux/types.h>
+#else
+#include <stdint.h>
+#endif
+
+struct ipu_psys_capability {
+	uint32_t version;
+	uint8_t driver[20];
+	uint32_t pg_count;
+	uint8_t dev_model[32];
+	uint32_t reserved[17];
+} __attribute__ ((packed));
+
+struct ipu_psys_event {
+	uint32_t type;		/* IPU_PSYS_EVENT_TYPE_ */
+	uint64_t user_token;
+	uint64_t issue_id;
+	uint32_t buffer_idx;
+	uint32_t error;
+	int32_t reserved[2];
+} __attribute__ ((packed));
+
+#define IPU_PSYS_EVENT_TYPE_CMD_COMPLETE	1
+#define IPU_PSYS_EVENT_TYPE_BUFFER_COMPLETE	2
+
+/**
+ * struct ipu_psys_buffer - for input/output terminals
+ * @len:	total allocated size @ base address
+ * @userptr:	user pointer
+ * @fd:		DMA-BUF handle
+ * @data_offset:offset to valid data
+ * @bytes_used:	amount of valid data including offset
+ * @flags:	flags
+ */
+struct ipu_psys_buffer {
+	uint64_t len;
+	union {
+		int fd;
+		void __user *userptr;
+		uint64_t reserved;
+	} base;
+	uint32_t data_offset;
+	uint32_t bytes_used;
+	uint32_t flags;
+	uint32_t reserved[2];
+} __attribute__ ((packed));
+
+#define IPU_BUFFER_FLAG_INPUT	(1 << 0)
+#define IPU_BUFFER_FLAG_OUTPUT	(1 << 1)
+#define IPU_BUFFER_FLAG_MAPPED	(1 << 2)
+#define IPU_BUFFER_FLAG_NO_FLUSH	(1 << 3)
+#define IPU_BUFFER_FLAG_DMA_HANDLE	(1 << 4)
+#define IPU_BUFFER_FLAG_USERPTR	(1 << 5)
+
+#define	IPU_PSYS_CMD_PRIORITY_HIGH	0
+#define	IPU_PSYS_CMD_PRIORITY_MED	1
+#define	IPU_PSYS_CMD_PRIORITY_LOW	2
+#define	IPU_PSYS_CMD_PRIORITY_NUM	3
+
+/**
+ * struct ipu_psys_command - processing command
+ * @issue_id:		unique id for the command set by user
+ * @user_token:		token of the command
+ * @priority:		priority of the command
+ * @pg_manifest:	userspace pointer to program group manifest
+ * @buffers:		userspace pointers to array of psys dma buf structs
+ * @pg:			process group DMA-BUF handle
+ * @pg_manifest_size:	size of program group manifest
+ * @bufcount:		number of buffers in buffers array
+ * @min_psys_freq:	minimum psys frequency in MHz used for this cmd
+ * @frame_counter:      counter of current frame synced between isys and psys
+ * @kernel_enable_bitmap:       enable bits for each individual kernel
+ * @terminal_enable_bitmap:     enable bits for each individual terminals
+ * @routing_enable_bitmap:      enable bits for each individual routing
+ * @rbm:                        enable bits for routing
+ *
+ * Specifies a processing command with input and output buffers.
+ */
+struct ipu_psys_command {
+	uint64_t issue_id;
+	uint64_t user_token;
+	uint32_t priority;
+	void __user *pg_manifest;
+	struct ipu_psys_buffer __user *buffers;
+	int pg;
+	uint32_t pg_manifest_size;
+	uint32_t bufcount;
+	uint32_t min_psys_freq;
+	uint32_t frame_counter;
+	uint32_t kernel_enable_bitmap[4];
+	uint32_t terminal_enable_bitmap[4];
+	uint32_t routing_enable_bitmap[4];
+	uint32_t rbm[5];
+	uint32_t reserved[2];
+} __attribute__ ((packed));
+
+struct ipu_psys_manifest {
+	uint32_t index;
+	uint32_t size;
+	void __user *manifest;
+	uint32_t reserved[5];
+} __attribute__ ((packed));
+
+#define IPU_IOC_QUERYCAP _IOR('A', 1, struct ipu_psys_capability)
+#define IPU_IOC_MAPBUF _IOWR('A', 2, int)
+#define IPU_IOC_UNMAPBUF _IOWR('A', 3, int)
+#define IPU_IOC_GETBUF _IOWR('A', 4, struct ipu_psys_buffer)
+#define IPU_IOC_PUTBUF _IOWR('A', 5, struct ipu_psys_buffer)
+#define IPU_IOC_QCMD _IOWR('A', 6, struct ipu_psys_command)
+#define IPU_IOC_DQEVENT _IOWR('A', 7, struct ipu_psys_event)
+#define IPU_IOC_CMD_CANCEL _IOWR('A', 8, struct ipu_psys_command)
+#define IPU_IOC_GET_MANIFEST _IOWR('A', 9, struct ipu_psys_manifest)
+
+#endif /* _UAPI_IPU_PSYS_H */
diff -ruN a/include/uapi/linux/magic.h b/include/uapi/linux/magic.h
--- a/include/uapi/linux/magic.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/magic.h	2021-12-23 08:35:56.000000000 +0100
@@ -58,6 +58,8 @@
 #define REISER2FS_SUPER_MAGIC_STRING	"ReIsEr2Fs"
 #define REISER2FS_JR_SUPER_MAGIC_STRING	"ReIsEr3Fs"
 
+#define ESDFS_SUPER_MAGIC	0x00035df5
+
 #define SMB_SUPER_MAGIC		0x517B
 #define CGROUP_SUPER_MAGIC	0x27e0eb
 #define CGROUP2_SUPER_MAGIC	0x63677270
diff -ruN a/include/uapi/linux/netfilter/xt_IDLETIMER.h b/include/uapi/linux/netfilter/xt_IDLETIMER.h
--- a/include/uapi/linux/netfilter/xt_IDLETIMER.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/netfilter/xt_IDLETIMER.h	2021-12-23 08:35:56.000000000 +0100
@@ -48,7 +48,7 @@
 
 	char label[MAX_IDLETIMER_LABEL_SIZE];
 
-	__u8 send_nl_msg;   /* unused: for compatibility with Android */
+	__u8 send_nl_msg;
 	__u8 timer_type;
 
 	/* for kernel module internal use only */
diff -ruN a/include/uapi/linux/prctl.h b/include/uapi/linux/prctl.h
--- a/include/uapi/linux/prctl.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/prctl.h	2021-12-23 08:35:56.000000000 +0100
@@ -155,6 +155,9 @@
 #define PR_SET_PTRACER 0x59616d61
 # define PR_SET_PTRACER_ANY ((unsigned long)-1)
 
+#define PR_ALT_SYSCALL 0x43724f53
+# define PR_ALT_SYSCALL_SET_SYSCALL_TABLE 1
+
 #define PR_SET_CHILD_SUBREAPER	36
 #define PR_GET_CHILD_SUBREAPER	37
 
@@ -269,4 +272,7 @@
 # define PR_SCHED_CORE_SHARE_FROM	3 /* pull core_sched cookie to pid */
 # define PR_SCHED_CORE_MAX		4
 
+#define PR_SET_VMA		0x53564d41
+# define PR_SET_VMA_ANON_NAME		0
+
 #endif /* _LINUX_PRCTL_H */
diff -ruN a/include/uapi/linux/usb/f_accessory.h b/include/uapi/linux/usb/f_accessory.h
--- a/include/uapi/linux/usb/f_accessory.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/uapi/linux/usb/f_accessory.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,146 @@
+/*
+ * Gadget Function Driver for Android USB accessories
+ *
+ * Copyright (C) 2011 Google, Inc.
+ * Author: Mike Lockwood <lockwood@android.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ */
+
+#ifndef _UAPI_LINUX_USB_F_ACCESSORY_H
+#define _UAPI_LINUX_USB_F_ACCESSORY_H
+
+/* Use Google Vendor ID when in accessory mode */
+#define USB_ACCESSORY_VENDOR_ID 0x18D1
+
+
+/* Product ID to use when in accessory mode */
+#define USB_ACCESSORY_PRODUCT_ID 0x2D00
+
+/* Product ID to use when in accessory mode and adb is enabled */
+#define USB_ACCESSORY_ADB_PRODUCT_ID 0x2D01
+
+/* Indexes for strings sent by the host via ACCESSORY_SEND_STRING */
+#define ACCESSORY_STRING_MANUFACTURER   0
+#define ACCESSORY_STRING_MODEL          1
+#define ACCESSORY_STRING_DESCRIPTION    2
+#define ACCESSORY_STRING_VERSION        3
+#define ACCESSORY_STRING_URI            4
+#define ACCESSORY_STRING_SERIAL         5
+
+/* Control request for retrieving device's protocol version
+ *
+ *	requestType:    USB_DIR_IN | USB_TYPE_VENDOR
+ *	request:        ACCESSORY_GET_PROTOCOL
+ *	value:          0
+ *	index:          0
+ *	data            version number (16 bits little endian)
+ *                     1 for original accessory support
+ *                     2 adds HID and device to host audio support
+ */
+#define ACCESSORY_GET_PROTOCOL  51
+
+/* Control request for host to send a string to the device
+ *
+ *	requestType:    USB_DIR_OUT | USB_TYPE_VENDOR
+ *	request:        ACCESSORY_SEND_STRING
+ *	value:          0
+ *	index:          string ID
+ *	data            zero terminated UTF8 string
+ *
+ *  The device can later retrieve these strings via the
+ *  ACCESSORY_GET_STRING_* ioctls
+ */
+#define ACCESSORY_SEND_STRING   52
+
+/* Control request for starting device in accessory mode.
+ * The host sends this after setting all its strings to the device.
+ *
+ *	requestType:    USB_DIR_OUT | USB_TYPE_VENDOR
+ *	request:        ACCESSORY_START
+ *	value:          0
+ *	index:          0
+ *	data            none
+ */
+#define ACCESSORY_START         53
+
+/* Control request for registering a HID device.
+ * Upon registering, a unique ID is sent by the accessory in the
+ * value parameter. This ID will be used for future commands for
+ * the device
+ *
+ *	requestType:    USB_DIR_OUT | USB_TYPE_VENDOR
+ *	request:        ACCESSORY_REGISTER_HID_DEVICE
+ *	value:          Accessory assigned ID for the HID device
+ *	index:          total length of the HID report descriptor
+ *	data            none
+ */
+#define ACCESSORY_REGISTER_HID         54
+
+/* Control request for unregistering a HID device.
+ *
+ *	requestType:    USB_DIR_OUT | USB_TYPE_VENDOR
+ *	request:        ACCESSORY_REGISTER_HID
+ *	value:          Accessory assigned ID for the HID device
+ *	index:          0
+ *	data            none
+ */
+#define ACCESSORY_UNREGISTER_HID         55
+
+/* Control request for sending the HID report descriptor.
+ * If the HID descriptor is longer than the endpoint zero max packet size,
+ * the descriptor will be sent in multiple ACCESSORY_SET_HID_REPORT_DESC
+ * commands. The data for the descriptor must be sent sequentially
+ * if multiple packets are needed.
+ *
+ *	requestType:    USB_DIR_OUT | USB_TYPE_VENDOR
+ *	request:        ACCESSORY_SET_HID_REPORT_DESC
+ *	value:          Accessory assigned ID for the HID device
+ *	index:          offset of data in descriptor
+ *                      (needed when HID descriptor is too big for one packet)
+ *	data            the HID report descriptor
+ */
+#define ACCESSORY_SET_HID_REPORT_DESC         56
+
+/* Control request for sending HID events.
+ *
+ *	requestType:    USB_DIR_OUT | USB_TYPE_VENDOR
+ *	request:        ACCESSORY_SEND_HID_EVENT
+ *	value:          Accessory assigned ID for the HID device
+ *	index:          0
+ *	data            the HID report for the event
+ */
+#define ACCESSORY_SEND_HID_EVENT         57
+
+/* Control request for setting the audio mode.
+ *
+ *	requestType:	USB_DIR_OUT | USB_TYPE_VENDOR
+ *	request:        ACCESSORY_SET_AUDIO_MODE
+ *	value:          0 - no audio
+ *                     1 - device to host, 44100 16-bit stereo PCM
+ *	index:          0
+ *	data            none
+ */
+#define ACCESSORY_SET_AUDIO_MODE         58
+
+/* ioctls for retrieving strings set by the host */
+#define ACCESSORY_GET_STRING_MANUFACTURER   _IOW('M', 1, char[256])
+#define ACCESSORY_GET_STRING_MODEL          _IOW('M', 2, char[256])
+#define ACCESSORY_GET_STRING_DESCRIPTION    _IOW('M', 3, char[256])
+#define ACCESSORY_GET_STRING_VERSION        _IOW('M', 4, char[256])
+#define ACCESSORY_GET_STRING_URI            _IOW('M', 5, char[256])
+#define ACCESSORY_GET_STRING_SERIAL         _IOW('M', 6, char[256])
+/* returns 1 if there is a start request pending */
+#define ACCESSORY_IS_START_REQUESTED        _IO('M', 7)
+/* returns audio mode (set via the ACCESSORY_SET_AUDIO_MODE control request) */
+#define ACCESSORY_GET_AUDIO_MODE            _IO('M', 8)
+
+#endif /* _UAPI_LINUX_USB_F_ACCESSORY_H */
diff -ruN a/include/uapi/linux/usb/video.h b/include/uapi/linux/usb/video.h
--- a/include/uapi/linux/usb/video.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/usb/video.h	2021-12-23 08:35:56.000000000 +0100
@@ -104,6 +104,7 @@
 #define UVC_CT_ROLL_ABSOLUTE_CONTROL			0x0f
 #define UVC_CT_ROLL_RELATIVE_CONTROL			0x10
 #define UVC_CT_PRIVACY_CONTROL				0x11
+#define UVC_CT_REGION_OF_INTEREST_CONTROL		0x14
 
 /* A.9.5. Processing Unit Control Selectors */
 #define UVC_PU_CONTROL_UNDEFINED			0x00
diff -ruN a/include/uapi/linux/v4l2-common.h b/include/uapi/linux/v4l2-common.h
--- a/include/uapi/linux/v4l2-common.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/v4l2-common.h	2021-12-23 08:35:56.000000000 +0100
@@ -78,6 +78,14 @@
 #define V4L2_SEL_TGT_COMPOSE_BOUNDS	0x0102
 /* Current composing area plus all padding pixels */
 #define V4L2_SEL_TGT_COMPOSE_PADDED	0x0103
+/* Current Region of Interest area */
+#define V4L2_SEL_TGT_ROI		0x0200
+/* Default Region of Interest area */
+#define V4L2_SEL_TGT_ROI_DEFAULT	0x0201
+/* Region of Interest minimum values */
+#define V4L2_SEL_TGT_ROI_BOUNDS_MIN	0x0202
+/* Region of Interest maximum values */
+#define V4L2_SEL_TGT_ROI_BOUNDS_MAX	0x0203
 
 /* Selection flags */
 #define V4L2_SEL_FLAG_GE		(1 << 0)
diff -ruN a/include/uapi/linux/v4l2-controls.h b/include/uapi/linux/v4l2-controls.h
--- a/include/uapi/linux/v4l2-controls.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/v4l2-controls.h	2021-12-23 08:35:56.000000000 +0100
@@ -987,7 +987,6 @@
 
 #define V4L2_CID_PAN_SPEED			(V4L2_CID_CAMERA_CLASS_BASE+32)
 #define V4L2_CID_TILT_SPEED			(V4L2_CID_CAMERA_CLASS_BASE+33)
-
 #define V4L2_CID_CAMERA_ORIENTATION		(V4L2_CID_CAMERA_CLASS_BASE+34)
 #define V4L2_CAMERA_ORIENTATION_FRONT		0
 #define V4L2_CAMERA_ORIENTATION_BACK		1
@@ -995,6 +994,25 @@
 
 #define V4L2_CID_CAMERA_SENSOR_ROTATION		(V4L2_CID_CAMERA_CLASS_BASE+35)
 
+/*
+ * senozhatsky@ b:191930245
+ *
+ * These are FROMLIST defines. Use very high value to avoid collisions
+ * with upstream patches. Controls classes are USHRT_MAX apart from each
+ * other, but the lower 0x900 are not being used. This leaves us with the
+ * USHRT_MAX - 0x900 values. Use SHRT_MAX.
+ */
+#define V4L2_CID_REGION_OF_INTEREST_AUTO	\
+	(V4L2_CID_CAMERA_CLASS_BASE + SHRT_MAX)
+#define V4L2_CID_REGION_OF_INTEREST_AUTO_EXPOSURE		(1 << 0)
+#define V4L2_CID_REGION_OF_INTEREST_AUTO_IRIS			(1 << 1)
+#define V4L2_CID_REGION_OF_INTEREST_AUTO_WHITE_BALANCE		(1 << 2)
+#define V4L2_CID_REGION_OF_INTEREST_AUTO_FOCUS			(1 << 3)
+#define V4L2_CID_REGION_OF_INTEREST_AUTO_FACE_DETECT		(1 << 4)
+#define V4L2_CID_REGION_OF_INTEREST_AUTO_DETECT_AND_TRACK	(1 << 5)
+#define V4L2_CID_REGION_OF_INTEREST_AUTO_IMAGE_STABILIZATION	(1 << 6)
+#define V4L2_CID_REGION_OF_INTEREST_AUTO_HIGHER_QUALITY	(1 << 7)
+
 /* FM Modulator class control IDs */
 
 #define V4L2_CID_FM_TX_CLASS_BASE		(V4L2_CTRL_CLASS_FM_TX | 0x900)
diff -ruN a/include/uapi/linux/videodev2.h b/include/uapi/linux/videodev2.h
--- a/include/uapi/linux/videodev2.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/videodev2.h	2021-12-23 08:35:56.000000000 +0100
@@ -70,7 +70,7 @@
  * Common stuff for both V4L1 and V4L2
  * Moved from videodev.h
  */
-#define VIDEO_MAX_FRAME               32
+#define VIDEO_MAX_FRAME               64
 #define VIDEO_MAX_PLANES               8
 
 /*
@@ -603,6 +603,9 @@
 #define V4L2_PIX_FMT_NV42    v4l2_fourcc('N', 'V', '4', '2') /* 24  Y/CrCb 4:4:4  */
 #define V4L2_PIX_FMT_HM12    v4l2_fourcc('H', 'M', '1', '2') /*  8  YUV 4:2:0 16x16 macroblocks */
 
+/* UBWC 8-bit Y/CbCr 4:2:0  */
+#define V4L2_PIX_FMT_NV12_UBWC        v4l2_fourcc('Q', '1', '2', '8')
+
 /* two non contiguous planes - one Y, one Cr + Cb interleaved  */
 #define V4L2_PIX_FMT_NV12M   v4l2_fourcc('N', 'M', '1', '2') /* 12  Y/CbCr 4:2:0  */
 #define V4L2_PIX_FMT_NV21M   v4l2_fourcc('N', 'M', '2', '1') /* 21  Y/CrCb 4:2:0  */
@@ -733,6 +736,7 @@
 #define V4L2_PIX_FMT_Y12I     v4l2_fourcc('Y', '1', '2', 'I') /* Greyscale 12-bit L/R interleaved */
 #define V4L2_PIX_FMT_Z16      v4l2_fourcc('Z', '1', '6', ' ') /* Depth data 16-bit */
 #define V4L2_PIX_FMT_MT21C    v4l2_fourcc('M', 'T', '2', '1') /* Mediatek compressed block mode  */
+#define V4L2_PIX_FMT_MM21     v4l2_fourcc('M', 'M', '2', '1') /* Mediatek 8-bit block mode, two non-contiguous planes */
 #define V4L2_PIX_FMT_INZI     v4l2_fourcc('I', 'N', 'Z', 'I') /* Intel Planar Greyscale 10-bit and Depth 16-bit */
 #define V4L2_PIX_FMT_SUNXI_TILED_NV12 v4l2_fourcc('S', 'T', '1', '2') /* Sunxi Tiled NV12 Format */
 #define V4L2_PIX_FMT_CNF4     v4l2_fourcc('C', 'N', 'F', '4') /* Intel 4-bit packed depth confidence information */
@@ -953,9 +957,12 @@
 	__u32			type;		/* enum v4l2_buf_type */
 	__u32			memory;		/* enum v4l2_memory */
 	__u32			capabilities;
-	__u32			reserved[1];
+	__u8			flags;
+	__u8			reserved[3];
 };
 
+#define V4L2_MEMORY_FLAG_NON_COHERENT			(1 << 0)
+
 /* capabilities for struct v4l2_requestbuffers and v4l2_create_buffers */
 #define V4L2_BUF_CAP_SUPPORTS_MMAP			(1 << 0)
 #define V4L2_BUF_CAP_SUPPORTS_USERPTR			(1 << 1)
@@ -2499,6 +2506,9 @@
  * @memory:	enum v4l2_memory; buffer memory type
  * @format:	frame format, for which buffers are requested
  * @capabilities: capabilities of this buffer type.
+ * @flags:	additional buffer management attributes (ignored unless the
+ *		queue has V4L2_BUF_CAP_SUPPORTS_MMAP_CACHE_HINTS capability
+ *		and configured for MMAP streaming I/O).
  * @reserved:	future extensions
  */
 struct v4l2_create_buffers {
@@ -2507,7 +2517,8 @@
 	__u32			memory;
 	struct v4l2_format	format;
 	__u32			capabilities;
-	__u32			reserved[7];
+	__u32			flags;
+	__u32			reserved[6];
 };
 
 /*
diff -ruN a/include/uapi/linux/virtio_gpu.h b/include/uapi/linux/virtio_gpu.h
--- a/include/uapi/linux/virtio_gpu.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/virtio_gpu.h	2021-12-23 08:35:56.000000000 +0100
@@ -59,6 +59,11 @@
  * VIRTIO_GPU_CMD_RESOURCE_CREATE_BLOB
  */
 #define VIRTIO_GPU_F_RESOURCE_BLOB       3
+/*
+ * VIRTIO_GPU_CMD_CREATE_CONTEXT with
+ * context_init
+ */
+#define VIRTIO_GPU_F_CONTEXT_INIT        4
 
 enum virtio_gpu_ctrl_type {
 	VIRTIO_GPU_UNDEFINED = 0,
@@ -104,6 +109,11 @@
 	VIRTIO_GPU_RESP_OK_RESOURCE_UUID,
 	VIRTIO_GPU_RESP_OK_MAP_INFO,
 
+	/* CHROMIUM: legacy responses */
+	VIRTIO_GPU_RESP_OK_RESOURCE_PLANE_INFO_LEGACY = 0x1104,
+	/* CHROMIUM: success responses */
+	VIRTIO_GPU_RESP_OK_RESOURCE_PLANE_INFO = 0x11FF,
+
 	/* error responses */
 	VIRTIO_GPU_RESP_ERR_UNSPEC = 0x1200,
 	VIRTIO_GPU_RESP_ERR_OUT_OF_MEMORY,
@@ -269,10 +279,11 @@
 };
 
 /* VIRTIO_GPU_CMD_CTX_CREATE */
+#define VIRTIO_GPU_CONTEXT_INIT_CAPSET_ID_MASK 0x00ff
 struct virtio_gpu_ctx_create {
 	struct virtio_gpu_ctrl_hdr hdr;
 	__le32 nlen;
-	__le32 padding;
+	__le32 context_init;
 	char debug_name[64];
 };
 
@@ -342,6 +353,15 @@
 	__u8 edid[1024];
 };
 
+/* VIRTIO_GPU_RESP_OK_RESOURCE_PLANE_INFO */
+struct virtio_gpu_resp_resource_plane_info {
+	struct virtio_gpu_ctrl_hdr hdr;
+	__le32 num_planes;
+	__le64 format_modifier;
+	__le32 strides[4];
+	__le32 offsets[4];
+};
+
 #define VIRTIO_GPU_EVENT_DISPLAY (1 << 0)
 
 struct virtio_gpu_config {
diff -ruN a/include/uapi/linux/virtio_ids.h b/include/uapi/linux/virtio_ids.h
--- a/include/uapi/linux/virtio_ids.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/virtio_ids.h	2021-12-23 08:35:56.000000000 +0100
@@ -81,4 +81,8 @@
 #define VIRTIO_TRANS_ID_RNG		1005 /* transitional virtio rng */
 #define VIRTIO_TRANS_ID_9P		1009 /* transitional virtio 9p console */
 
+/* Chrome OS-specific devices */
+#define VIRTIO_ID_WL           63 /* virtio wayland */
+#define VIRTIO_ID_TPM          62 /* virtio tpm */
+
 #endif /* _LINUX_VIRTIO_IDS_H */
diff -ruN a/include/uapi/linux/virtio_video.h b/include/uapi/linux/virtio_video.h
--- a/include/uapi/linux/virtio_video.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/uapi/linux/virtio_video.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,482 @@
+/* SPDX-License-Identifier: BSD-3-Clause */
+/*
+ * Virtio Video Device
+ *
+ * This header is BSD licensed so anyone can use the definitions
+ * to implement compatible drivers/servers:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the name of IBM nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
+ * FOR A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL IBM OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
+ * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
+ * OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * Copyright (C) 2019 OpenSynergy GmbH.
+ */
+
+#ifndef _UAPI_LINUX_VIRTIO_VIDEO_H
+#define _UAPI_LINUX_VIRTIO_VIDEO_H
+
+#include <linux/types.h>
+#include <linux/virtio_config.h>
+
+/*
+ * Feature bits
+ */
+
+/* Guest pages can be used for video buffers. */
+#define VIRTIO_VIDEO_F_RESOURCE_GUEST_PAGES 0
+/*
+ * The host can process buffers even if they are non-contiguous memory such as
+ * scatter-gather lists.
+ */
+#define VIRTIO_VIDEO_F_RESOURCE_NON_CONTIG 1
+/* Objects exported by another virtio device can be used for video buffers */
+#define VIRTIO_VIDEO_F_RESOURCE_VIRTIO_OBJECT 2
+
+/*
+ * Image formats
+ */
+
+enum virtio_video_format {
+	/* Raw formats */
+	VIRTIO_VIDEO_FORMAT_RAW_MIN = 1,
+	VIRTIO_VIDEO_FORMAT_ARGB8888 = VIRTIO_VIDEO_FORMAT_RAW_MIN,
+	VIRTIO_VIDEO_FORMAT_BGRA8888,
+	VIRTIO_VIDEO_FORMAT_NV12, /* 12  Y/CbCr 4:2:0  */
+	VIRTIO_VIDEO_FORMAT_YUV420, /* 12  YUV 4:2:0     */
+	VIRTIO_VIDEO_FORMAT_YVU420, /* 12  YVU 4:2:0     */
+	VIRTIO_VIDEO_FORMAT_RAW_MAX = VIRTIO_VIDEO_FORMAT_YVU420,
+
+	/* Coded formats */
+	VIRTIO_VIDEO_FORMAT_CODED_MIN = 0x1000,
+	VIRTIO_VIDEO_FORMAT_MPEG2 =
+		VIRTIO_VIDEO_FORMAT_CODED_MIN, /* MPEG-2 Part 2 */
+	VIRTIO_VIDEO_FORMAT_MPEG4, /* MPEG-4 Part 2 */
+	VIRTIO_VIDEO_FORMAT_H264, /* H.264 */
+	VIRTIO_VIDEO_FORMAT_HEVC, /* HEVC aka H.265*/
+	VIRTIO_VIDEO_FORMAT_VP8, /* VP8 */
+	VIRTIO_VIDEO_FORMAT_VP9, /* VP9 */
+	VIRTIO_VIDEO_FORMAT_CODED_MAX = VIRTIO_VIDEO_FORMAT_VP9,
+};
+
+enum virtio_video_profile {
+	/* H.264 */
+	VIRTIO_VIDEO_PROFILE_H264_MIN = 0x100,
+	VIRTIO_VIDEO_PROFILE_H264_BASELINE = VIRTIO_VIDEO_PROFILE_H264_MIN,
+	VIRTIO_VIDEO_PROFILE_H264_MAIN,
+	VIRTIO_VIDEO_PROFILE_H264_EXTENDED,
+	VIRTIO_VIDEO_PROFILE_H264_HIGH,
+	VIRTIO_VIDEO_PROFILE_H264_HIGH10PROFILE,
+	VIRTIO_VIDEO_PROFILE_H264_HIGH422PROFILE,
+	VIRTIO_VIDEO_PROFILE_H264_HIGH444PREDICTIVEPROFILE,
+	VIRTIO_VIDEO_PROFILE_H264_SCALABLEBASELINE,
+	VIRTIO_VIDEO_PROFILE_H264_SCALABLEHIGH,
+	VIRTIO_VIDEO_PROFILE_H264_STEREOHIGH,
+	VIRTIO_VIDEO_PROFILE_H264_MULTIVIEWHIGH,
+	VIRTIO_VIDEO_PROFILE_H264_MAX = VIRTIO_VIDEO_PROFILE_H264_MULTIVIEWHIGH,
+
+	/* HEVC */
+	VIRTIO_VIDEO_PROFILE_HEVC_MIN = 0x200,
+	VIRTIO_VIDEO_PROFILE_HEVC_MAIN = VIRTIO_VIDEO_PROFILE_HEVC_MIN,
+	VIRTIO_VIDEO_PROFILE_HEVC_MAIN10,
+	VIRTIO_VIDEO_PROFILE_HEVC_MAIN_STILL_PICTURE,
+	VIRTIO_VIDEO_PROFILE_HEVC_MAX =
+		VIRTIO_VIDEO_PROFILE_HEVC_MAIN_STILL_PICTURE,
+
+	/* VP8 */
+	VIRTIO_VIDEO_PROFILE_VP8_MIN = 0x300,
+	VIRTIO_VIDEO_PROFILE_VP8_PROFILE0 = VIRTIO_VIDEO_PROFILE_VP8_MIN,
+	VIRTIO_VIDEO_PROFILE_VP8_PROFILE1,
+	VIRTIO_VIDEO_PROFILE_VP8_PROFILE2,
+	VIRTIO_VIDEO_PROFILE_VP8_PROFILE3,
+	VIRTIO_VIDEO_PROFILE_VP8_MAX = VIRTIO_VIDEO_PROFILE_VP8_PROFILE3,
+
+	/* VP9 */
+	VIRTIO_VIDEO_PROFILE_VP9_MIN = 0x400,
+	VIRTIO_VIDEO_PROFILE_VP9_PROFILE0 = VIRTIO_VIDEO_PROFILE_VP9_MIN,
+	VIRTIO_VIDEO_PROFILE_VP9_PROFILE1,
+	VIRTIO_VIDEO_PROFILE_VP9_PROFILE2,
+	VIRTIO_VIDEO_PROFILE_VP9_PROFILE3,
+	VIRTIO_VIDEO_PROFILE_VP9_MAX = VIRTIO_VIDEO_PROFILE_VP9_PROFILE3,
+};
+
+enum virtio_video_level {
+	/* H.264 */
+	VIRTIO_VIDEO_LEVEL_H264_MIN = 0x100,
+	VIRTIO_VIDEO_LEVEL_H264_1_0 = VIRTIO_VIDEO_LEVEL_H264_MIN,
+	VIRTIO_VIDEO_LEVEL_H264_1_1,
+	VIRTIO_VIDEO_LEVEL_H264_1_2,
+	VIRTIO_VIDEO_LEVEL_H264_1_3,
+	VIRTIO_VIDEO_LEVEL_H264_2_0,
+	VIRTIO_VIDEO_LEVEL_H264_2_1,
+	VIRTIO_VIDEO_LEVEL_H264_2_2,
+	VIRTIO_VIDEO_LEVEL_H264_3_0,
+	VIRTIO_VIDEO_LEVEL_H264_3_1,
+	VIRTIO_VIDEO_LEVEL_H264_3_2,
+	VIRTIO_VIDEO_LEVEL_H264_4_0,
+	VIRTIO_VIDEO_LEVEL_H264_4_1,
+	VIRTIO_VIDEO_LEVEL_H264_4_2,
+	VIRTIO_VIDEO_LEVEL_H264_5_0,
+	VIRTIO_VIDEO_LEVEL_H264_5_1,
+	VIRTIO_VIDEO_LEVEL_H264_MAX = VIRTIO_VIDEO_LEVEL_H264_5_1,
+};
+
+/*
+ * Config
+ */
+
+struct virtio_video_config {
+	__le32 version;
+	__le32 max_caps_length;
+	__le32 max_resp_length;
+};
+
+/*
+ * Commands
+ */
+
+enum virtio_video_cmd_type {
+	/* Command */
+	VIRTIO_VIDEO_CMD_QUERY_CAPABILITY = 0x0100,
+	VIRTIO_VIDEO_CMD_STREAM_CREATE,
+	VIRTIO_VIDEO_CMD_STREAM_DESTROY,
+	VIRTIO_VIDEO_CMD_STREAM_DRAIN,
+	VIRTIO_VIDEO_CMD_RESOURCE_CREATE,
+	VIRTIO_VIDEO_CMD_RESOURCE_QUEUE,
+	VIRTIO_VIDEO_CMD_RESOURCE_DESTROY_ALL,
+	VIRTIO_VIDEO_CMD_QUEUE_CLEAR,
+	VIRTIO_VIDEO_CMD_GET_PARAMS,
+	VIRTIO_VIDEO_CMD_SET_PARAMS,
+	VIRTIO_VIDEO_CMD_QUERY_CONTROL,
+	VIRTIO_VIDEO_CMD_GET_CONTROL,
+	VIRTIO_VIDEO_CMD_SET_CONTROL,
+
+	/* Response */
+	VIRTIO_VIDEO_RESP_OK_NODATA = 0x0200,
+	VIRTIO_VIDEO_RESP_OK_QUERY_CAPABILITY,
+	VIRTIO_VIDEO_RESP_OK_RESOURCE_QUEUE,
+	VIRTIO_VIDEO_RESP_OK_GET_PARAMS,
+	VIRTIO_VIDEO_RESP_OK_QUERY_CONTROL,
+	VIRTIO_VIDEO_RESP_OK_GET_CONTROL,
+
+	VIRTIO_VIDEO_RESP_ERR_INVALID_OPERATION = 0x0300,
+	VIRTIO_VIDEO_RESP_ERR_OUT_OF_MEMORY,
+	VIRTIO_VIDEO_RESP_ERR_INVALID_STREAM_ID,
+	VIRTIO_VIDEO_RESP_ERR_INVALID_RESOURCE_ID,
+	VIRTIO_VIDEO_RESP_ERR_INVALID_PARAMETER,
+	VIRTIO_VIDEO_RESP_ERR_UNSUPPORTED_CONTROL,
+};
+
+struct virtio_video_cmd_hdr {
+	__le32 type; /* One of enum virtio_video_cmd_type */
+	__le32 stream_id;
+};
+
+/* VIRTIO_VIDEO_CMD_QUERY_CAPABILITY */
+enum virtio_video_queue_type {
+	VIRTIO_VIDEO_QUEUE_TYPE_INPUT = 0x100,
+	VIRTIO_VIDEO_QUEUE_TYPE_OUTPUT,
+};
+
+struct virtio_video_query_capability {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 queue_type; /* One of VIRTIO_VIDEO_QUEUE_TYPE_* types */
+	__u8 padding[4];
+};
+
+enum virtio_video_planes_layout_flag {
+	VIRTIO_VIDEO_PLANES_LAYOUT_SINGLE_BUFFER = 1 << 0,
+	VIRTIO_VIDEO_PLANES_LAYOUT_PER_PLANE = 1 << 1,
+};
+
+struct virtio_video_format_range {
+	__le32 min;
+	__le32 max;
+	__le32 step;
+	__u8 padding[4];
+};
+
+struct virtio_video_format_frame {
+	struct virtio_video_format_range width;
+	struct virtio_video_format_range height;
+	__le32 num_rates;
+	__u8 padding[4];
+	/* Followed by struct virtio_video_format_range frame_rates[] */
+};
+
+struct virtio_video_format_desc {
+	__le64 mask;
+	__le32 format; /* One of VIRTIO_VIDEO_FORMAT_* types */
+	__le32 planes_layout; /* Bitmask with VIRTIO_VIDEO_PLANES_LAYOUT_* */
+	__le32 plane_align;
+	__le32 num_frames;
+	/* Followed by struct virtio_video_format_frame frames[] */
+};
+
+struct virtio_video_query_capability_resp {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 num_descs;
+	__u8 padding[4];
+	/* Followed by struct virtio_video_format_desc descs[] */
+};
+
+/* VIRTIO_VIDEO_CMD_STREAM_CREATE */
+enum virtio_video_mem_type {
+	VIRTIO_VIDEO_MEM_TYPE_GUEST_PAGES,
+	VIRTIO_VIDEO_MEM_TYPE_VIRTIO_OBJECT,
+};
+
+struct virtio_video_stream_create {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 in_mem_type; /* One of VIRTIO_VIDEO_MEM_TYPE_* types */
+	__le32 out_mem_type; /* One of VIRTIO_VIDEO_MEM_TYPE_* types */
+	__le32 coded_format; /* One of VIRTIO_VIDEO_FORMAT_* types */
+	__u8 padding[4];
+	__u8 tag[64];
+};
+
+/* VIRTIO_VIDEO_CMD_STREAM_DESTROY */
+struct virtio_video_stream_destroy {
+	struct virtio_video_cmd_hdr hdr;
+};
+
+/* VIRTIO_VIDEO_CMD_STREAM_DRAIN */
+struct virtio_video_stream_drain {
+	struct virtio_video_cmd_hdr hdr;
+};
+
+/* VIRTIO_VIDEO_CMD_RESOURCE_CREATE */
+struct virtio_video_mem_entry {
+	__le64 addr;
+	__le32 length;
+	__u8 padding[4];
+};
+
+struct virtio_video_object_entry {
+	__u8 uuid[16];
+};
+
+#define VIRTIO_VIDEO_MAX_PLANES 8
+
+struct virtio_video_resource_create {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 queue_type; /* One of VIRTIO_VIDEO_QUEUE_TYPE_* types */
+	__le32 resource_id;
+	__le32 planes_layout;
+	__le32 num_planes;
+	__le32 plane_offsets[VIRTIO_VIDEO_MAX_PLANES];
+	__le32 num_entries[VIRTIO_VIDEO_MAX_PLANES];
+	/**
+	 * Followed by either
+	 * - struct virtio_video_mem_entry entries[]
+	 *   for VIRTIO_VIDEO_MEM_TYPE_GUEST_PAGES
+	 * - struct virtio_video_object_entry entries[]
+	 *   for VIRTIO_VIDEO_MEM_TYPE_VIRTIO_OBJECT
+	 */
+};
+
+/* VIRTIO_VIDEO_CMD_RESOURCE_QUEUE */
+struct virtio_video_resource_queue {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 queue_type; /* One of VIRTIO_VIDEO_QUEUE_TYPE_* types */
+	__le32 resource_id;
+	__le64 timestamp;
+	__le32 num_data_sizes;
+	__le32 data_sizes[VIRTIO_VIDEO_MAX_PLANES];
+	__u8 padding[4];
+};
+
+enum virtio_video_buffer_flag {
+	VIRTIO_VIDEO_BUFFER_FLAG_ERR = 0x0001,
+	VIRTIO_VIDEO_BUFFER_FLAG_EOS = 0x0002,
+
+	/* Encoder only */
+	VIRTIO_VIDEO_BUFFER_FLAG_IFRAME = 0x0004,
+	VIRTIO_VIDEO_BUFFER_FLAG_PFRAME = 0x0008,
+	VIRTIO_VIDEO_BUFFER_FLAG_BFRAME = 0x0010,
+};
+
+struct virtio_video_resource_queue_resp {
+	struct virtio_video_cmd_hdr hdr;
+	__le64 timestamp;
+	__le32 flags; /* One of VIRTIO_VIDEO_BUFFER_FLAG_* flags */
+	__le32 size; /* Encoded size */
+};
+
+/* VIRTIO_VIDEO_CMD_RESOURCE_DESTROY_ALL */
+struct virtio_video_resource_destroy_all {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 queue_type; /* One of VIRTIO_VIDEO_QUEUE_TYPE_* types */
+	__u8 padding[4];
+};
+
+/* VIRTIO_VIDEO_CMD_QUEUE_CLEAR */
+struct virtio_video_queue_clear {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 queue_type; /* One of VIRTIO_VIDEO_QUEUE_TYPE_* types */
+	__u8 padding[4];
+};
+
+/* VIRTIO_VIDEO_CMD_GET_PARAMS */
+struct virtio_video_plane_format {
+	__le32 plane_size;
+	__le32 stride;
+};
+
+struct virtio_video_crop {
+	__le32 left;
+	__le32 top;
+	__le32 width;
+	__le32 height;
+};
+
+struct virtio_video_params {
+	__le32 queue_type; /* One of VIRTIO_VIDEO_QUEUE_TYPE_* types */
+	__le32 format; /* One of VIRTIO_VIDEO_FORMAT_* types */
+	__le32 frame_width;
+	__le32 frame_height;
+	__le32 min_buffers;
+	__le32 max_buffers;
+	struct virtio_video_crop crop;
+	__le32 frame_rate;
+	__le32 num_planes;
+	struct virtio_video_plane_format plane_formats[VIRTIO_VIDEO_MAX_PLANES];
+};
+
+struct virtio_video_get_params {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 queue_type; /* One of VIRTIO_VIDEO_QUEUE_TYPE_* types */
+	__u8 padding[4];
+};
+
+struct virtio_video_get_params_resp {
+	struct virtio_video_cmd_hdr hdr;
+	struct virtio_video_params params;
+};
+
+/* VIRTIO_VIDEO_CMD_SET_PARAMS */
+struct virtio_video_set_params {
+	struct virtio_video_cmd_hdr hdr;
+	struct virtio_video_params params;
+};
+
+/* VIRTIO_VIDEO_CMD_QUERY_CONTROL */
+enum virtio_video_control_type {
+	VIRTIO_VIDEO_CONTROL_BITRATE = 1,
+	VIRTIO_VIDEO_CONTROL_PROFILE,
+	VIRTIO_VIDEO_CONTROL_LEVEL,
+};
+
+struct virtio_video_query_control_profile {
+	__le32 format; /* One of VIRTIO_VIDEO_FORMAT_* */
+	__u8 padding[4];
+};
+
+struct virtio_video_query_control_level {
+	__le32 format; /* One of VIRTIO_VIDEO_FORMAT_* */
+	__u8 padding[4];
+};
+
+struct virtio_video_query_control {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 control; /* One of VIRTIO_VIDEO_CONTROL_* types */
+	__u8 padding[4];
+	/*
+	 * Followed by a value of struct virtio_video_query_control_*
+	 * in accordance with the value of control.
+	 */
+};
+
+struct virtio_video_query_control_resp_profile {
+	__le32 num;
+	__u8 padding[4];
+	/* Followed by an array le32 profiles[] */
+};
+
+struct virtio_video_query_control_resp_level {
+	__le32 num;
+	__u8 padding[4];
+	/* Followed by an array le32 level[] */
+};
+
+struct virtio_video_query_control_resp {
+	struct virtio_video_cmd_hdr hdr;
+	/* Followed by one of struct virtio_video_query_control_resp_* */
+};
+
+/* VIRTIO_VIDEO_CMD_GET_CONTROL */
+struct virtio_video_get_control {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 control; /* One of VIRTIO_VIDEO_CONTROL_* types */
+	__u8 padding[4];
+};
+
+struct virtio_video_control_val_bitrate {
+	__le32 bitrate;
+	__u8 padding[4];
+};
+
+struct virtio_video_control_val_profile {
+	__le32 profile;
+	__u8 padding[4];
+};
+
+struct virtio_video_control_val_level {
+	__le32 level;
+	__u8 padding[4];
+};
+
+struct virtio_video_get_control_resp {
+	struct virtio_video_cmd_hdr hdr;
+	/* Followed by one of struct virtio_video_control_val_* */
+};
+
+/* VIRTIO_VIDEO_CMD_SET_CONTROL */
+struct virtio_video_set_control {
+	struct virtio_video_cmd_hdr hdr;
+	__le32 control; /* One of VIRTIO_VIDEO_CONTROL_* types */
+	__u8 padding[4];
+	/* Followed by one of struct virtio_video_control_val_* */
+};
+
+struct virtio_video_set_control_resp {
+	struct virtio_video_cmd_hdr hdr;
+};
+
+/*
+ * Events
+ */
+
+enum virtio_video_event_type {
+	/* For all devices */
+	VIRTIO_VIDEO_EVENT_ERROR = 0x0100,
+
+	/* For decoder only */
+	VIRTIO_VIDEO_EVENT_DECODER_RESOLUTION_CHANGED = 0x0200,
+};
+
+struct virtio_video_event {
+	__le32 event_type; /* One of VIRTIO_VIDEO_EVENT_* types */
+	__le32 stream_id;
+};
+
+#endif /* _UAPI_LINUX_VIRTIO_VIDEO_H */
diff -ruN a/include/uapi/linux/virtio_wl.h b/include/uapi/linux/virtio_wl.h
--- a/include/uapi/linux/virtio_wl.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/uapi/linux/virtio_wl.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,154 @@
+#ifndef _LINUX_VIRTIO_WL_H
+#define _LINUX_VIRTIO_WL_H
+/*
+ * This header is BSD licensed so anyone can use the definitions to implement
+ * compatible drivers/servers.
+ */
+#include <linux/virtio_ids.h>
+#include <linux/virtio_config.h>
+#include <linux/virtwl.h>
+
+#define VIRTWL_IN_BUFFER_SIZE 4096
+#define VIRTWL_OUT_BUFFER_SIZE 4096
+#define VIRTWL_VQ_IN 0
+#define VIRTWL_VQ_OUT 1
+#define VIRTWL_QUEUE_COUNT 2
+#define VIRTWL_MAX_ALLOC 0x800
+#define VIRTWL_PFN_SHIFT 12
+
+/* Enables the transition to new flag semantics */
+#define VIRTIO_WL_F_TRANS_FLAGS 1
+/* Enables send fence support with virtio_wl_ctrl_vfd_send_vfd_v2 */
+#define VIRTIO_WL_F_SEND_FENCES 2
+
+struct virtio_wl_config {
+};
+
+/*
+ * The structure of each of these is virtio_wl_ctrl_hdr or one of its subclasses
+ * where noted.
+ */
+enum virtio_wl_ctrl_type {
+	VIRTIO_WL_CMD_VFD_NEW = 0x100, /* virtio_wl_ctrl_vfd_new */
+	VIRTIO_WL_CMD_VFD_CLOSE, /* virtio_wl_ctrl_vfd */
+	VIRTIO_WL_CMD_VFD_SEND, /* virtio_wl_ctrl_vfd_send + data */
+	VIRTIO_WL_CMD_VFD_RECV, /* virtio_wl_ctrl_vfd_recv + data */
+	VIRTIO_WL_CMD_VFD_NEW_CTX, /* virtio_wl_ctrl_vfd_new */
+	VIRTIO_WL_CMD_VFD_NEW_PIPE, /* virtio_wl_ctrl_vfd_new */
+	VIRTIO_WL_CMD_VFD_HUP, /* virtio_wl_ctrl_vfd */
+	VIRTIO_WL_CMD_VFD_NEW_DMABUF, /* virtio_wl_ctrl_vfd_new */
+	VIRTIO_WL_CMD_VFD_DMABUF_SYNC, /* virtio_wl_ctrl_vfd_dmabuf_sync */
+	VIRTIO_WL_CMD_VFD_SEND_FOREIGN_ID, /* virtio_wl_ctrl_vfd_send + data */
+	VIRTIO_WL_CMD_VFD_NEW_CTX_NAMED, /* virtio_wl_ctrl_vfd_new */
+
+	VIRTIO_WL_RESP_OK = 0x1000,
+	VIRTIO_WL_RESP_VFD_NEW = 0x1001, /* virtio_wl_ctrl_vfd_new */
+	VIRTIO_WL_RESP_VFD_NEW_DMABUF = 0x1002, /* virtio_wl_ctrl_vfd_new */
+
+	VIRTIO_WL_RESP_ERR = 0x1100,
+	VIRTIO_WL_RESP_OUT_OF_MEMORY,
+	VIRTIO_WL_RESP_INVALID_ID,
+	VIRTIO_WL_RESP_INVALID_TYPE,
+	VIRTIO_WL_RESP_INVALID_FLAGS,
+	VIRTIO_WL_RESP_INVALID_CMD,
+};
+
+struct virtio_wl_ctrl_hdr {
+	__le32 type; /* one of virtio_wl_ctrl_type */
+	__le32 flags; /* always 0 */
+};
+
+enum virtio_wl_vfd_flags {
+	VIRTIO_WL_VFD_WRITE = 0x1, /* intended to be written by guest */
+	VIRTIO_WL_VFD_READ = 0x2, /* intended to be read by guest */
+};
+
+struct virtio_wl_ctrl_vfd {
+	struct virtio_wl_ctrl_hdr hdr;
+	__le32 vfd_id;
+};
+
+/*
+ * If this command is sent to the guest, it indicates that the VFD has been
+ * created and the fields indicate the properties of the VFD being offered.
+ *
+ * If this command is sent to the host, it represents a request to create a VFD
+ * of the given properties. The pfn field is ignored by the host.
+ */
+struct virtio_wl_ctrl_vfd_new {
+	struct virtio_wl_ctrl_hdr hdr;
+	__le32 vfd_id; /* MSB indicates device allocated vfd */
+	__le32 flags; /* virtio_wl_vfd_flags */
+	__le64 pfn; /* first guest physical page frame number if VFD_MAP */
+	__le32 size; /* size in bytes if VIRTIO_WL_CMD_VFD_NEW* */
+	union {
+		/* buffer description if VIRTIO_WL_CMD_VFD_NEW_DMABUF */
+		struct {
+			__le32 width; /* width in pixels */
+			__le32 height; /* height in pixels */
+			__le32 format; /* fourcc format */
+			__le32 stride0; /* return stride0 */
+			__le32 stride1; /* return stride1 */
+			__le32 stride2; /* return stride2 */
+			__le32 offset0; /* return offset0 */
+			__le32 offset1; /* return offset1 */
+			__le32 offset2; /* return offset2 */
+		} dmabuf;
+		/* name of socket if VIRTIO_WL_CMD_VFD_NEW_CTX_NAMED */
+		char name[32];
+	};
+};
+
+
+enum virtio_wl_ctrl_vfd_send_kind {
+	/* The id after this one indicates an ordinary vfd_id. */
+	VIRTIO_WL_CTRL_VFD_SEND_KIND_LOCAL,
+	/* The id after this one is a virtio-gpu resource id. */
+	VIRTIO_WL_CTRL_VFD_SEND_KIND_VIRTGPU,
+	VIRTIO_WL_CTRL_VFD_SEND_KIND_VIRTGPU_FENCE,
+	VIRTIO_WL_CTRL_VFD_SEND_KIND_VIRTGPU_SIGNALED_FENCE,
+};
+
+struct virtio_wl_ctrl_vfd_send_vfd {
+	__le32 kind; /* virtio_wl_ctrl_vfd_send_kind */
+	__le32 id;
+};
+
+struct virtio_wl_ctrl_vfd_send_vfd_v2 {
+	__le32 kind; /* virtio_wl_ctrl_vfd_send_kind */
+	union {
+		/* For KIND_LOCAL and KIND_VIRTGPU */
+		__le32 id;
+		/* For KIND_VIRTGPU_FENCE */
+		__le64 seqno;
+	};
+};
+
+struct virtio_wl_ctrl_vfd_send {
+	struct virtio_wl_ctrl_hdr hdr;
+	__le32 vfd_id;
+	__le32 vfd_count; /* struct is followed by this many IDs */
+
+	/*
+	 * If hdr.type == VIRTIO_WL_CMD_VFD_SEND_FOREIGN_ID, there is a
+	 * vfd_count array of virtio_wl_ctrl_vfd_send_vfd. Otherwise, there is a
+	 * vfd_count array of vfd_ids.
+	 */
+
+	/* the remainder is raw data */
+};
+
+struct virtio_wl_ctrl_vfd_recv {
+	struct virtio_wl_ctrl_hdr hdr;
+	__le32 vfd_id;
+	__le32 vfd_count; /* struct is followed by this many IDs */
+	/* the remainder is raw data */
+};
+
+struct virtio_wl_ctrl_vfd_dmabuf_sync {
+	struct virtio_wl_ctrl_hdr hdr;
+	__le32 vfd_id;
+	__le32 flags;
+};
+
+#endif /* _LINUX_VIRTIO_WL_H */
diff -ruN a/include/uapi/linux/virtwl.h b/include/uapi/linux/virtwl.h
--- a/include/uapi/linux/virtwl.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/uapi/linux/virtwl.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,67 @@
+#ifndef _LINUX_VIRTWL_H
+#define _LINUX_VIRTWL_H
+
+#include <asm/ioctl.h>
+#include <linux/types.h>
+
+#define VIRTWL_SEND_MAX_ALLOCS 28
+
+#define VIRTWL_IOCTL_BASE 'w'
+#define VIRTWL_IO(nr)		_IO(VIRTWL_IOCTL_BASE, nr)
+#define VIRTWL_IOR(nr, type)	_IOR(VIRTWL_IOCTL_BASE, nr, type)
+#define VIRTWL_IOW(nr, type)	_IOW(VIRTWL_IOCTL_BASE, nr, type)
+#define VIRTWL_IOWR(nr, type)	_IOWR(VIRTWL_IOCTL_BASE, nr, type)
+
+enum virtwl_ioctl_new_type {
+	VIRTWL_IOCTL_NEW_CTX, /* open a new wayland connection context */
+	VIRTWL_IOCTL_NEW_ALLOC, /* create a new virtwl shm allocation */
+	/* create a new virtwl pipe that is readable via the returned fd */
+	VIRTWL_IOCTL_NEW_PIPE_READ,
+	/* create a new virtwl pipe that is writable via the returned fd */
+	VIRTWL_IOCTL_NEW_PIPE_WRITE,
+	/* create a new virtwl dmabuf that is writable via the returned fd */
+	VIRTWL_IOCTL_NEW_DMABUF,
+	VIRTWL_IOCTL_NEW_CTX_NAMED, /* open a new named connection context */
+};
+
+struct virtwl_ioctl_new {
+	__u32 type; /* VIRTWL_IOCTL_NEW_* */
+	int fd; /* return fd */
+	__u32 flags; /* currently always 0 */
+	union {
+		/* size of allocation if type == VIRTWL_IOCTL_NEW_ALLOC */
+		__u32 size;
+		/* buffer description if type == VIRTWL_IOCTL_NEW_DMABUF */
+		struct {
+			__u32 width; /* width in pixels */
+			__u32 height; /* height in pixels */
+			__u32 format; /* fourcc format */
+			__u32 stride0; /* return stride0 */
+			__u32 stride1; /* return stride1 */
+			__u32 stride2; /* return stride2 */
+			__u32 offset0; /* return offset0 */
+			__u32 offset1; /* return offset1 */
+			__u32 offset2; /* return offset2 */
+		} dmabuf;
+		/* name of socket if type == VIRTIO_WL_CMD_VFD_NEW_CTX_NAMED */
+		char name[32];
+	};
+};
+
+struct virtwl_ioctl_txn {
+	int fds[VIRTWL_SEND_MAX_ALLOCS];
+	__u32 len;
+	__u8 data[0];
+};
+
+struct virtwl_ioctl_dmabuf_sync {
+	__u32 flags; /* synchronization flags (see dma-buf.h) */
+};
+
+#define VIRTWL_IOCTL_NEW VIRTWL_IOWR(0x00, struct virtwl_ioctl_new)
+#define VIRTWL_IOCTL_SEND VIRTWL_IOR(0x01, struct virtwl_ioctl_txn)
+#define VIRTWL_IOCTL_RECV VIRTWL_IOW(0x02, struct virtwl_ioctl_txn)
+#define VIRTWL_IOCTL_DMABUF_SYNC VIRTWL_IOR(0x03, \
+					    struct virtwl_ioctl_dmabuf_sync)
+
+#endif /* _LINUX_VIRTWL_H */
diff -ruN a/include/uapi/linux/xattr.h b/include/uapi/linux/xattr.h
--- a/include/uapi/linux/xattr.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/linux/xattr.h	2021-12-23 08:35:56.000000000 +0100
@@ -18,8 +18,11 @@
 #if __UAPI_DEF_XATTR
 #define __USE_KERNEL_XATTR_DEFS
 
-#define XATTR_CREATE	0x1	/* set value, fail if attr already exists */
-#define XATTR_REPLACE	0x2	/* set value, fail if attr does not exist */
+#define XATTR_CREATE	 0x1	/* set value, fail if attr already exists */
+#define XATTR_REPLACE	 0x2	/* set value, fail if attr does not exist */
+#ifdef __KERNEL__ /* following is kernel internal, colocated for maintenance */
+#define XATTR_NOSECURITY 0x4	/* get value, do not involve security check */
+#endif
 #endif
 
 /* Namespaces */
diff -ruN a/include/uapi/nl80211-vnd-realtek.h b/include/uapi/nl80211-vnd-realtek.h
--- a/include/uapi/nl80211-vnd-realtek.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/uapi/nl80211-vnd-realtek.h	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,72 @@
+/* SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause */
+/* Copyright(c) 2018-2019  Realtek Corporation
+ */
+#ifndef _UAPI_NL80211_VND_REALTEK_H
+#define _UAPI_NL80211_VND_REALTEK_H
+
+/**
+ * This vendor ID is the value of atrribute %NL80211_ATTR_VENDOR_ID used by
+ * %NL80211_CMD_VENDOR to send vendor command.
+ */
+#define REALTEK_NL80211_VENDOR_ID	0x00E04C
+
+/**
+ * enum realtek_nl80211_vndcmd - supported vendor subcmds
+ *
+ * @REALTEK_NL80211_VNDCMD_SET_SAR: set SAR power limit
+ *	%realtek_vndcmd_sar_band within attribute %REALTEK_VNDCMD_ATTR_SAR_BAND
+ *	and corresponding power limit attribute %REALTEK_VNDCMD_ATTR_SAR_POWER.
+ *	The two attributes are in nested attribute %REALTEK_VNDCMD_ATTR_SAR_RULES.
+ */
+enum realtek_nl80211_vndcmd {
+	REALTEK_NL80211_VNDCMD_SET_SAR = 0x88,
+};
+
+/**
+ * enum realtek_vndcmd_sar_band - bands of SAR power limit
+ *
+ * @REALTEK_VNDCMD_SAR_BAND_2G: all channels of 2G band
+ * @REALTEK_VNDCMD_SAR_BAND_5G_BAND1: channels of 5G band1 (5.15~5.35G)
+ * @REALTEK_VNDCMD_SAR_BAND_5G_BAND2: channels of 5G band2 (5.35~5.47G)
+ *	5G band2 isn't used by rtw88 by now, so don't need to set SAR power
+ *	limit for this band. But we still enumerate this band as a placeholder
+ *	for the furture.
+ * @REALTEK_VNDCMD_SAR_BAND_5G_BAND3: channels of 5G band3 (5.47~5.725G)
+ * @REALTEK_VNDCMD_SAR_BAND_5G_BAND4: channels of 5G band4 (5.725~5.95G)
+ */
+enum realtek_vndcmd_sar_band {
+	REALTEK_VNDCMD_SAR_BAND_2G,
+	REALTEK_VNDCMD_SAR_BAND_5G_BAND1,
+	REALTEK_VNDCMD_SAR_BAND_5G_BAND2,
+	REALTEK_VNDCMD_SAR_BAND_5G_BAND3,
+	REALTEK_VNDCMD_SAR_BAND_5G_BAND4,
+
+	REALTEK_VNDCMD_SAR_BAND_NR,
+};
+
+/**
+ * enum realtek_vndcmd_sar_rule_attr - attributes of vendor command
+ *	%REALTEK_NL80211_VNDCMD_SET_SAR
+ *
+ * @REALTEK_VNDCMD_ATTR_SAR_RULES: nested attribute to hold SAR rules containing
+ *	band and corresponding power limit.
+ *
+ * @REALTEK_VNDCMD_ATTR_SAR_BAND: an attribute within %REALTEK_VNDCMD_ATTR_SAR_RULES,
+ *	and its value is %realtek_vndcmd_sar_band (u32 data type).
+ * @REALTEK_VNDCMD_ATTR_SAR_POWER: an attribute within %REALTEK_VNDCMD_ATTR_SAR_RULES.
+ *	SAR power limit is 'u8' type and in unit of 0.125 dBm, so its range is
+ *	0 to 31.875 dBm.
+ */
+enum realtek_vndcmd_sar_rule_attr {
+	__REALTEK_VNDCMD_SAR_RULE_ATTR_INVALID,
+
+	REALTEK_VNDCMD_ATTR_SAR_RULES,
+	REALTEK_VNDCMD_ATTR_SAR_BAND,
+	REALTEK_VNDCMD_ATTR_SAR_POWER,
+
+	/* keep last */
+	__REALTEK_VNDCMD_SAR_RULE_ATTR_AFTER_LAST,
+	REALTEK_VNDCMD_SAR_RULE_ATTR_MAX = __REALTEK_VNDCMD_SAR_RULE_ATTR_AFTER_LAST - 1,
+};
+
+#endif /* _UAPI_NL80211_VND_REALTEK_H */
diff -ruN a/include/uapi/sound/sof/tokens.h b/include/uapi/sound/sof/tokens.h
--- a/include/uapi/sound/sof/tokens.h	2021-12-08 09:04:57.000000000 +0100
+++ b/include/uapi/sound/sof/tokens.h	2021-12-23 08:35:56.000000000 +0100
@@ -51,6 +51,7 @@
 #define SOF_TKN_SCHED_CORE			203
 #define SOF_TKN_SCHED_FRAMES			204
 #define SOF_TKN_SCHED_TIME_DOMAIN		205
+#define SOF_TKN_SCHED_DYNAMIC_PIPELINE		206
 
 /* volume */
 #define SOF_TKN_VOLUME_RAMP_STEP_TYPE		250
diff -ruN a/init/do_mounts.c b/init/do_mounts.c
--- a/init/do_mounts.c	2021-12-08 09:04:57.000000000 +0100
+++ b/init/do_mounts.c	2021-12-23 08:35:56.000000000 +0100
@@ -606,6 +606,7 @@
 		ssleep(root_delay);
 	}
 
+#if 0
 	/*
 	 * wait for the known devices to complete their probing
 	 *
@@ -614,6 +615,8 @@
 	 * for the touchpad of a laptop to initialize.
 	 */
 	wait_for_device_probe();
+#endif
+	async_synchronize_full();
 
 	md_run_setup();
 
diff -ruN a/init/init_task.c b/init/init_task.c
--- a/init/init_task.c	2021-12-08 09:04:57.000000000 +0100
+++ b/init/init_task.c	2021-12-23 08:35:56.000000000 +0100
@@ -13,6 +13,9 @@
 #include <linux/numa.h>
 #include <linux/scs.h>
 
+#include <linux/alt-syscall.h>
+
+#include <asm/pgtable.h>
 #include <linux/uaccess.h>
 
 static struct signal_struct init_signals = {
diff -ruN a/init/Kconfig b/init/Kconfig
--- a/init/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/init/Kconfig	2021-12-23 08:35:56.000000000 +0100
@@ -139,6 +139,7 @@
 
 config WERROR
 	bool "Compile the kernel with warnings as errors"
+	depends on !COMPILE_TEST || ARM64 || X86
 	default COMPILE_TEST
 	help
 	  A kernel build should not cause any compiler warnings, and this
@@ -365,6 +366,15 @@
 	  used to provide more virtual memory than the actual RAM present
 	  in your computer.  If unsure say Y.
 
+config DISK_BASED_SWAP
+	bool "Allow disk-based swap files in Chromium OS kernels"
+	depends on SWAP
+	default n
+	help
+	  By default, the Chromium OS kernel allows swapping only to
+	  zram devices. This option allows you to use disk-based files
+	  as swap devices too.  If unsure say N.
+
 config SYSVIPC
 	bool "System V IPC"
 	help
diff -ruN a/kernel/alt-syscall.c b/kernel/alt-syscall.c
--- a/kernel/alt-syscall.c	1970-01-01 01:00:00.000000000 +0100
+++ b/kernel/alt-syscall.c	2021-12-23 08:35:56.000000000 +0100
@@ -0,0 +1,66 @@
+/*
+ * Alternate Syscall Table Infrastructure
+ *
+ * Copyright 2014 Google Inc. All Rights Reserved
+ *
+ * Authors:
+ *      Kees Cook   <keescook@chromium.org>
+ *      Will Drewry <wad@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/alt-syscall.h>
+
+static LIST_HEAD(alt_sys_call_tables);
+static DEFINE_SPINLOCK(alt_sys_call_tables_lock);
+
+/* XXX: there is no "unregister" yet. */
+int register_alt_sys_call_table(struct alt_sys_call_table *entry)
+{
+	if (!entry)
+		return -EINVAL;
+
+	spin_lock(&alt_sys_call_tables_lock);
+	list_add(&entry->node, &alt_sys_call_tables);
+	spin_unlock(&alt_sys_call_tables_lock);
+
+	pr_info("table '%s' available.\n", entry->name);
+
+	return 0;
+}
+
+int set_alt_sys_call_table(char * __user uname)
+{
+	char name[ALT_SYS_CALL_NAME_MAX + 1] = { };
+	struct alt_sys_call_table *entry;
+
+	if (copy_from_user(name, uname, ALT_SYS_CALL_NAME_MAX))
+		return -EFAULT;
+
+	spin_lock(&alt_sys_call_tables_lock);
+	list_for_each_entry(entry, &alt_sys_call_tables, node) {
+		if (!strcmp(entry->name, name)) {
+			if (arch_set_sys_call_table(entry))
+				continue;
+			spin_unlock(&alt_sys_call_tables_lock);
+			return 0;
+		}
+	}
+	spin_unlock(&alt_sys_call_tables_lock);
+
+	return -ENOENT;
+}
diff -ruN a/kernel/audit.c b/kernel/audit.c
--- a/kernel/audit.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/audit.c	2021-12-23 08:35:56.000000000 +0100
@@ -2093,6 +2093,12 @@
 	if (prefix)
 		audit_log_format(ab, "%s", prefix);
 
+	/* The process may be exiting. */
+	if (!current->fs) {
+		audit_log_format(ab, "<unknown>");
+		return;
+	}
+
 	/* We will allow 11 spaces for ' (deleted)' to be appended */
 	pathname = kmalloc(PATH_MAX+11, ab->gfp_mask);
 	if (!pathname) {
diff -ruN a/kernel/cgroup/cgroup-v1.c b/kernel/cgroup/cgroup-v1.c
--- a/kernel/cgroup/cgroup-v1.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/cgroup/cgroup-v1.c	2021-12-23 08:35:56.000000000 +0100
@@ -514,7 +514,8 @@
 	tcred = get_task_cred(task);
 	if (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&
 	    !uid_eq(cred->euid, tcred->uid) &&
-	    !uid_eq(cred->euid, tcred->suid))
+	    !uid_eq(cred->euid, tcred->suid) &&
+	    !ns_capable(tcred->user_ns, CAP_SYS_NICE))
 		ret = -EACCES;
 	put_cred(tcred);
 	if (ret)
diff -ruN a/kernel/cpu.c b/kernel/cpu.c
--- a/kernel/cpu.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/cpu.c	2021-12-23 08:35:57.000000000 +0100
@@ -34,6 +34,7 @@
 #include <linux/percpu-rwsem.h>
 #include <linux/cpuset.h>
 #include <linux/random.h>
+#include <uapi/linux/sched/types.h>
 
 #include <trace/events/power.h>
 #define CREATE_TRACE_POINTS
@@ -1319,6 +1320,25 @@
 	complete_ap_thread(st, true);
 }
 
+static int switch_to_rt_policy(void)
+{
+	struct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 };
+	unsigned int policy = current->policy;
+
+	if (policy == SCHED_NORMAL)
+		/* Switch to SCHED_FIFO from SCHED_NORMAL. */
+		return sched_setscheduler_nocheck(current, SCHED_FIFO, &param);
+	else
+		return 1;
+}
+
+static int switch_to_fair_policy(void)
+{
+	struct sched_param param = { .sched_priority = 0 };
+
+	return sched_setscheduler_nocheck(current, SCHED_NORMAL, &param);
+}
+
 /* Requires cpu_add_remove_lock to be held */
 static int _cpu_up(unsigned int cpu, int tasks_frozen, enum cpuhp_state target)
 {
@@ -1383,6 +1403,7 @@
 static int cpu_up(unsigned int cpu, enum cpuhp_state target)
 {
 	int err = 0;
+	int switch_err;
 
 	if (!cpu_possible(cpu)) {
 		pr_err("can't online cpu %d because it is not configured as may-hotadd at boot time\n",
@@ -1393,9 +1414,21 @@
 		return -EINVAL;
 	}
 
+	/*
+	 * CPU hotplug operations consists of many steps and each step
+	 * calls a callback of core kernel subsystem. CPU hotplug-in
+	 * operation may get preempted by other CFS tasks and whole
+	 * operation of cpu hotplug in CPU gets delayed. Switch the
+	 * current task to SCHED_FIFO from SCHED_NORMAL, so that
+	 * hotplug in operation may complete quickly in heavy loaded
+	 * conditions and new CPU will start handle the workload.
+	 */
+
+	switch_err = switch_to_rt_policy();
+
 	err = try_online_node(cpu_to_node(cpu));
 	if (err)
-		return err;
+		goto switch_out;
 
 	cpu_maps_update_begin();
 
@@ -1411,6 +1444,14 @@
 	err = _cpu_up(cpu, 0, target);
 out:
 	cpu_maps_update_done();
+switch_out:
+	if (!switch_err) {
+		switch_err = switch_to_fair_policy();
+		if (switch_err)
+			pr_err("Hotplug policy switch err=%d Task %s pid=%d\n",
+				switch_err, current->comm, current->pid);
+	}
+
 	return err;
 }
 
@@ -1550,6 +1591,7 @@
 void thaw_secondary_cpus(void)
 {
 	int cpu, error;
+	struct device *cpu_device;
 
 	/* Allow everyone to use the CPU hotplug again */
 	cpu_maps_update_begin();
@@ -1567,6 +1609,12 @@
 		trace_suspend_resume(TPS("CPU_ON"), cpu, false);
 		if (!error) {
 			pr_info("CPU%d is up\n", cpu);
+			cpu_device = get_cpu_device(cpu);
+			if (!cpu_device)
+				pr_err("%s: failed to get cpu%d device\n",
+				       __func__, cpu);
+			else
+				kobject_uevent(&cpu_device->kobj, KOBJ_ONLINE);
 			continue;
 		}
 		pr_warn("Error taking CPU%d up: %d\n", cpu, error);
@@ -2710,3 +2758,48 @@
 	return cpu_mitigations == CPU_MITIGATIONS_AUTO_NOSMT;
 }
 EXPORT_SYMBOL_GPL(cpu_mitigations_auto_nosmt);
+
+#ifdef CONFIG_SCHED_CORE
+/*
+ * These are used for a global "coresched=" cmdline option for controlling
+ * core scheduling. Note that core sched may be needed for usecases other
+ * than security as well.
+ */
+enum coresched_cmds {
+	CORE_SCHED_OFF,
+	CORE_SCHED_SECURE,
+	CORE_SCHED_ON,
+};
+
+static enum coresched_cmds coresched_cmd __ro_after_init = CORE_SCHED_SECURE;
+
+static int __init coresched_parse_cmdline(char *arg)
+{
+	if (!strcmp(arg, "off"))
+		coresched_cmd = CORE_SCHED_OFF;
+	else if (!strcmp(arg, "on"))
+		coresched_cmd = CORE_SCHED_ON;
+	else if (!strcmp(arg, "secure"))
+		/*
+		 * On x86, coresched=secure means coresched is enabled only if
+		 * system has MDS/L1TF vulnerability (see x86/bugs.c).
+		 */
+		coresched_cmd = CORE_SCHED_SECURE;
+	else
+		pr_crit("Unsupported coresched=%s, defaulting to secure.\n",
+			arg);
+
+	if (coresched_cmd == CORE_SCHED_OFF)
+		static_branch_disable(&sched_coresched_supported);
+
+	return 0;
+}
+early_param("coresched", coresched_parse_cmdline);
+
+/* coresched=secure */
+bool coresched_cmd_secure(void)
+{
+	return coresched_cmd == CORE_SCHED_SECURE;
+}
+EXPORT_SYMBOL_GPL(coresched_cmd_secure);
+#endif
diff -ruN a/kernel/events/core.c b/kernel/events/core.c
--- a/kernel/events/core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/events/core.c	2021-12-23 08:35:57.000000000 +0100
@@ -11987,9 +11987,8 @@
  * @group_fd:		group leader event fd
  * @flags:		perf event open flags
  */
-SYSCALL_DEFINE5(perf_event_open,
-		struct perf_event_attr __user *, attr_uptr,
-		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
+int ksys_perf_event_open(struct perf_event_attr __user * attr_uptr, pid_t pid,
+			 int cpu, int group_fd, unsigned long flags)
 {
 	struct perf_event *group_leader = NULL, *output_event = NULL;
 	struct perf_event *event, *sibling;
@@ -12434,6 +12433,13 @@
 	return err;
 }
 
+SYSCALL_DEFINE5(perf_event_open,
+		struct perf_event_attr __user *, attr_uptr,
+		pid_t, pid, int, cpu, int, group_fd, unsigned long, flags)
+{
+	return ksys_perf_event_open(attr_uptr, pid, cpu, group_fd, flags);
+}
+
 /**
  * perf_event_create_kernel_counter
  *
diff -ruN a/kernel/fork.c b/kernel/fork.c
--- a/kernel/fork.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/fork.c	2021-12-23 08:35:57.000000000 +0100
@@ -97,6 +97,7 @@
 #include <linux/scs.h>
 #include <linux/io_uring.h>
 #include <linux/bpf.h>
+#include <linux/cpufreq_times.h>
 
 #include <asm/pgalloc.h>
 #include <linux/uaccess.h>
@@ -446,6 +447,7 @@
 #ifdef CONFIG_SECCOMP
 	WARN_ON_ONCE(tsk->seccomp.filter);
 #endif
+	cpufreq_task_times_exit(tsk);
 	release_user_cpus_ptr(tsk);
 	scs_release(tsk);
 
@@ -2035,6 +2037,8 @@
 		siginitsetinv(&p->blocked, sigmask(SIGKILL)|sigmask(SIGSTOP));
 	}
 
+	cpufreq_task_times_init(p);
+
 	/*
 	 * This _must_ happen before we call free_task(), i.e. before we jump
 	 * to any of the bad_fork_* labels. This is to avoid freeing
@@ -2588,6 +2592,8 @@
 	if (IS_ERR(p))
 		return PTR_ERR(p);
 
+	cpufreq_task_times_alloc(p);
+
 	/*
 	 * Do this prior waking up the new thread - the thread pointer
 	 * might get invalid after that point, if the thread exits quickly.
diff -ruN a/kernel/futex/core.c b/kernel/futex/core.c
--- a/kernel/futex/core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/futex/core.c	2021-12-23 08:35:57.000000000 +0100
@@ -1238,7 +1238,7 @@
 	 *  tsk->futex_state =               } else {
 	 *	FUTEX_STATE_DEAD;              if (tsk->futex_state !=
 	 *					  FUTEX_STATE_DEAD)
-	 *				         return -EAGAIN;
+	 *					 return -EAGAIN;
 	 *				       return -ESRCH; <--- FAIL
 	 *				     }
 	 *
@@ -1645,16 +1645,16 @@
 }
 
 /*
- * Wake up waiters matching bitset queued on this futex (uaddr).
+ * Prepare wake queue matching bitset queued on this futex (uaddr).
  */
 static int
-futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)
+prepare_wake_q(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset,
+	       struct wake_q_head *wake_q)
 {
 	struct futex_hash_bucket *hb;
 	struct futex_q *this, *next;
 	union futex_key key = FUTEX_KEY_INIT;
 	int ret;
-	DEFINE_WAKE_Q(wake_q);
 
 	if (!bitset)
 		return -EINVAL;
@@ -1682,14 +1682,28 @@
 			if (!(this->bitset & bitset))
 				continue;
 
-			mark_wake_futex(&wake_q, this);
+			mark_wake_futex(wake_q, this);
 			if (++ret >= nr_wake)
 				break;
 		}
 	}
 
 	spin_unlock(&hb->lock);
+	return ret;
+}
+
+/*
+ * Wake up waiters matching bitset queued on this futex (uaddr).
+ */
+static int
+futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)
+{
+	int ret;
+	DEFINE_WAKE_Q(wake_q);
+
+	ret = prepare_wake_q(uaddr, flags, nr_wake, bitset, &wake_q);
 	wake_up_q(&wake_q);
+
 	return ret;
 }
 
@@ -2822,9 +2836,12 @@
  * @hb:		the futex hash bucket, must be locked by the caller
  * @q:		the futex_q to queue up on
  * @timeout:	the prepared hrtimer_sleeper, or null for no timeout
+ * @next:	if present, wake next and hint to the scheduler that we'd
+ *		prefer to execute it locally.
  */
 static void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,
-				struct hrtimer_sleeper *timeout)
+				struct hrtimer_sleeper *timeout,
+				struct task_struct *next)
 {
 	/*
 	 * The task state is guaranteed to be set before another task can
@@ -2849,10 +2866,25 @@
 		 * flagged for rescheduling. Only call schedule if there
 		 * is no timeout, or if it has yet to expire.
 		 */
-		if (!timeout || timeout->task)
+		if (!timeout || timeout->task) {
+			if (next) {
+#ifdef CONFIG_SMP
+				wake_up_process_prefer_current_cpu(next);
+#else
+				wake_up_process(next);
+#endif
+				put_task_struct(next);
+				next = NULL;
+			}
 			freezable_schedule();
+		}
 	}
 	__set_current_state(TASK_RUNNING);
+
+	if (next) {
+		wake_up_process(next);
+		put_task_struct(next);
+	}
 }
 
 /**
@@ -2927,7 +2959,7 @@
 }
 
 static int futex_wait(u32 __user *uaddr, unsigned int flags, u32 val,
-		      ktime_t *abs_time, u32 bitset)
+		      ktime_t *abs_time, u32 bitset, struct task_struct *next)
 {
 	struct hrtimer_sleeper timeout, *to;
 	struct restart_block *restart;
@@ -2951,7 +2983,8 @@
 		goto out;
 
 	/* queue_me and wait for wakeup, timeout, or a signal. */
-	futex_wait_queue_me(hb, &q, to);
+	futex_wait_queue_me(hb, &q, to, next);
+	next = NULL;
 
 	/* If we were woken (and unqueued), we succeeded, whatever. */
 	ret = 0;
@@ -2982,6 +3015,10 @@
 	ret = set_restart_fn(restart, futex_wait_restart);
 
 out:
+	if (next) {
+		wake_up_process(next);
+		put_task_struct(next);
+	}
 	if (to) {
 		hrtimer_cancel(&to->timer);
 		destroy_hrtimer_on_stack(&to->timer);
@@ -2989,7 +3026,6 @@
 	return ret;
 }
 
-
 static long futex_wait_restart(struct restart_block *restart)
 {
 	u32 __user *uaddr = restart->futex.uaddr;
@@ -3001,10 +3037,29 @@
 	}
 	restart->fn = do_no_restart_syscall;
 
-	return (long)futex_wait(uaddr, restart->futex.flags,
-				restart->futex.val, tp, restart->futex.bitset);
+	return (long)futex_wait(uaddr, restart->futex.flags, restart->futex.val,
+				tp, restart->futex.bitset, NULL);
 }
 
+static int futex_swap(u32 __user *uaddr, unsigned int flags, u32 val,
+		      ktime_t *abs_time, u32 __user *uaddr2)
+{
+	u32 bitset = FUTEX_BITSET_MATCH_ANY;
+	struct task_struct *next = NULL;
+	DEFINE_WAKE_Q(wake_q);
+	int ret;
+
+	ret = prepare_wake_q(uaddr2, flags, 1, bitset, &wake_q);
+	if (ret < 0)
+		return ret;
+	if (!wake_q_empty(&wake_q)) {
+		/* At most one wakee can be present. Pull it out. */
+		next = container_of(wake_q.first, struct task_struct, wake_q);
+		next->wake_q.next = NULL;
+	}
+
+	return futex_wait(uaddr, flags, val, abs_time, bitset, next);
+}
 
 /*
  * Userspace tried a 0 -> TID atomic transition of the futex value
@@ -3460,7 +3515,7 @@
 	}
 
 	/* Queue the futex_q, drop the hb lock, wait for wakeup. */
-	futex_wait_queue_me(hb, &q, to);
+	futex_wait_queue_me(hb, &q, to, NULL);
 
 	switch (futex_requeue_pi_wakeup_sync(&q)) {
 	case Q_REQUEUE_PI_IGNORE:
@@ -3961,7 +4016,7 @@
 		val3 = FUTEX_BITSET_MATCH_ANY;
 		fallthrough;
 	case FUTEX_WAIT_BITSET:
-		return futex_wait(uaddr, flags, val, timeout, val3);
+		return futex_wait(uaddr, flags, val, timeout, val3, NULL);
 	case FUTEX_WAKE:
 		val3 = FUTEX_BITSET_MATCH_ANY;
 		fallthrough;
@@ -3988,6 +4043,8 @@
 					     uaddr2);
 	case FUTEX_CMP_REQUEUE_PI:
 		return futex_requeue(uaddr, flags, uaddr2, val, val2, &val3, 1);
+	case FUTEX_SWAP:
+		return futex_swap(uaddr, flags, val, timeout, uaddr2);
 	}
 	return -ENOSYS;
 }
@@ -4000,6 +4057,7 @@
 	case FUTEX_LOCK_PI2:
 	case FUTEX_WAIT_BITSET:
 	case FUTEX_WAIT_REQUEUE_PI:
+	case FUTEX_SWAP:
 		return true;
 	}
 	return false;
@@ -4012,7 +4070,7 @@
 		return -EINVAL;
 
 	*t = timespec64_to_ktime(*ts);
-	if (cmd == FUTEX_WAIT)
+	if (cmd == FUTEX_WAIT || cmd == FUTEX_SWAP)
 		*t = ktime_add_safe(ktime_get(), *t);
 	else if (cmd != FUTEX_LOCK_PI && !(op & FUTEX_CLOCK_REALTIME))
 		*t = timens_ktime_to_host(CLOCK_MONOTONIC, *t);
diff -ruN a/kernel/hung_task.c b/kernel/hung_task.c
--- a/kernel/hung_task.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/hung_task.c	2021-12-23 08:35:57.000000000 +0100
@@ -210,8 +210,10 @@
 		trigger_all_cpu_backtrace();
 	}
 
-	if (hung_task_call_panic)
+	if (hung_task_call_panic) {
+		show_state_filter(TASK_UNINTERRUPTIBLE);
 		panic("hung_task: blocked tasks");
+	}
 }
 
 static long hung_timeout_jiffies(unsigned long last_checked,
diff -ruN a/kernel/irq/chip.c b/kernel/irq/chip.c
--- a/kernel/irq/chip.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/irq/chip.c	2021-12-23 08:35:57.000000000 +0100
@@ -14,6 +14,7 @@
 #include <linux/interrupt.h>
 #include <linux/kernel_stat.h>
 #include <linux/irqdomain.h>
+#include <linux/wakeup_reason.h>
 
 #include <trace/events/irq.h>
 
@@ -510,8 +511,22 @@
 	 * If the interrupt is not in progress and is not an armed
 	 * wakeup interrupt, proceed.
 	 */
-	if (!irqd_has_set(&desc->irq_data, mask))
+	if (!irqd_has_set(&desc->irq_data, mask)) {
+#ifdef CONFIG_PM_SLEEP
+		if (unlikely(desc->no_suspend_depth &&
+			     irqd_is_wakeup_set(&desc->irq_data))) {
+			unsigned int irq = irq_desc_get_irq(desc);
+			const char *name = "(unnamed)";
+
+			if (desc->action && desc->action->name)
+				name = desc->action->name;
+
+			log_abnormal_wakeup_reason("misconfigured IRQ %u %s",
+						   irq, name);
+		}
+#endif
 		return true;
+	}
 
 	/*
 	 * If the interrupt is an armed wakeup source, mark it pending
diff -ruN a/kernel/Makefile b/kernel/Makefile
--- a/kernel/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/Makefile	2021-12-23 08:35:56.000000000 +0100
@@ -54,6 +54,8 @@
 obj-y += dma/
 obj-y += entry/
 
+obj-$(CONFIG_ALT_SYSCALL) += alt-syscall.o
+
 obj-$(CONFIG_KCMP) += kcmp.o
 obj-$(CONFIG_FREEZER) += freezer.o
 obj-$(CONFIG_PROFILING) += profile.o
@@ -95,6 +97,7 @@
 obj-$(CONFIG_KGDB) += debug/
 obj-$(CONFIG_DETECT_HUNG_TASK) += hung_task.o
 obj-$(CONFIG_LOCKUP_DETECTOR) += watchdog.o
+obj-$(CONFIG_HARDLOCKUP_DETECTOR_BUDDY_CPU) += watchdog_buddy_cpu.o
 obj-$(CONFIG_HARDLOCKUP_DETECTOR_PERF) += watchdog_hld.o
 obj-$(CONFIG_SECCOMP) += seccomp.o
 obj-$(CONFIG_RELAY) += relay.o
diff -ruN a/kernel/power/Makefile b/kernel/power/Makefile
--- a/kernel/power/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/power/Makefile	2021-12-23 08:35:57.000000000 +0100
@@ -17,4 +17,5 @@
 
 obj-$(CONFIG_MAGIC_SYSRQ)	+= poweroff.o
 
+obj-$(CONFIG_SUSPEND)		+= wakeup_reason.o
 obj-$(CONFIG_ENERGY_MODEL)	+= energy_model.o
diff -ruN a/kernel/power/process.c b/kernel/power/process.c
--- a/kernel/power/process.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/power/process.c	2021-12-23 08:35:57.000000000 +0100
@@ -85,18 +85,21 @@
 	elapsed = ktime_sub(end, start);
 	elapsed_msecs = ktime_to_ms(elapsed);
 
-	if (todo) {
+	if (wakeup) {
 		pr_cont("\n");
-		pr_err("Freezing of tasks %s after %d.%03d seconds "
-		       "(%d tasks refusing to freeze, wq_busy=%d):\n",
-		       wakeup ? "aborted" : "failed",
+		pr_err("Freezing of tasks aborted after %d.%03d seconds",
+		       elapsed_msecs / 1000, elapsed_msecs % 1000);
+	} else if (todo) {
+		pr_cont("\n");
+		pr_err("Freezing of tasks failed after %d.%03d seconds"
+		       " (%d tasks refusing to freeze, wq_busy=%d):\n",
 		       elapsed_msecs / 1000, elapsed_msecs % 1000,
 		       todo - wq_busy, wq_busy);
 
 		if (wq_busy)
 			show_all_workqueues();
 
-		if (!wakeup || pm_debug_messages_on) {
+		if (pm_debug_messages_on) {
 			read_lock(&tasklist_lock);
 			for_each_process_thread(g, p) {
 				if (p != current && !freezer_should_skip(p)
diff -ruN a/kernel/power/suspend.c b/kernel/power/suspend.c
--- a/kernel/power/suspend.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/power/suspend.c	2021-12-23 08:35:57.000000000 +0100
@@ -30,6 +30,7 @@
 #include <trace/events/power.h>
 #include <linux/compiler.h>
 #include <linux/moduleparam.h>
+#include <linux/wakeup_reason.h>
 
 #include "power.h"
 
@@ -139,6 +140,8 @@
 			break;
 		}
 
+		clear_wakeup_reasons();
+
 		s2idle_enter();
 	}
 
@@ -361,6 +363,7 @@
 	if (!error)
 		return 0;
 
+	log_suspend_abort_reason("One or more tasks refusing to freeze");
 	suspend_stats.failed_freeze++;
 	dpm_save_failed_step(SUSPEND_FREEZE);
 	pm_notifier_call_chain(PM_POST_SUSPEND);
@@ -390,7 +393,7 @@
  */
 static int suspend_enter(suspend_state_t state, bool *wakeup)
 {
-	int error;
+	int error, last_dev;
 
 	error = platform_suspend_prepare(state);
 	if (error)
@@ -398,7 +401,11 @@
 
 	error = dpm_suspend_late(PMSG_SUSPEND);
 	if (error) {
+		last_dev = suspend_stats.last_failed_dev + REC_FAILED_NUM - 1;
+		last_dev %= REC_FAILED_NUM;
 		pr_err("late suspend of devices failed\n");
+		log_suspend_abort_reason("late suspend of %s device failed",
+					 suspend_stats.failed_devs[last_dev]);
 		goto Platform_finish;
 	}
 	error = platform_suspend_prepare_late(state);
@@ -407,7 +414,11 @@
 
 	error = dpm_suspend_noirq(PMSG_SUSPEND);
 	if (error) {
+		last_dev = suspend_stats.last_failed_dev + REC_FAILED_NUM - 1;
+		last_dev %= REC_FAILED_NUM;
 		pr_err("noirq suspend of devices failed\n");
+		log_suspend_abort_reason("noirq suspend of %s device failed",
+					 suspend_stats.failed_devs[last_dev]);
 		goto Platform_early_resume;
 	}
 	error = platform_suspend_prepare_noirq(state);
@@ -423,8 +434,10 @@
 	}
 
 	error = suspend_disable_secondary_cpus();
-	if (error || suspend_test(TEST_CPUS))
+	if (error || suspend_test(TEST_CPUS)) {
+		log_suspend_abort_reason("Disabling non-boot cpus failed");
 		goto Enable_cpus;
+	}
 
 	arch_suspend_disable_irqs();
 	BUG_ON(!irqs_disabled());
@@ -495,6 +508,8 @@
 	error = dpm_suspend_start(PMSG_SUSPEND);
 	if (error) {
 		pr_err("Some devices failed to suspend, or early wake event detected\n");
+		log_suspend_abort_reason(
+				"Some devices failed to suspend, or early wake event detected");
 		goto Recover_platform;
 	}
 	suspend_test_finish("suspend devices");
diff -ruN a/kernel/power/wakeup_reason.c b/kernel/power/wakeup_reason.c
--- a/kernel/power/wakeup_reason.c	1970-01-01 01:00:00.000000000 +0100
+++ b/kernel/power/wakeup_reason.c	2021-12-23 08:35:57.000000000 +0100
@@ -0,0 +1,438 @@
+/*
+ * kernel/power/wakeup_reason.c
+ *
+ * Logs the reasons which caused the kernel to resume from
+ * the suspend mode.
+ *
+ * Copyright (C) 2020 Google, Inc.
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/wakeup_reason.h>
+#include <linux/kernel.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/kobject.h>
+#include <linux/sysfs.h>
+#include <linux/init.h>
+#include <linux/spinlock.h>
+#include <linux/notifier.h>
+#include <linux/suspend.h>
+#include <linux/slab.h>
+
+/*
+ * struct wakeup_irq_node - stores data and relationships for IRQs logged as
+ * either base or nested wakeup reasons during suspend/resume flow.
+ * @siblings - for membership on leaf or parent IRQ lists
+ * @irq      - the IRQ number
+ * @irq_name - the name associated with the IRQ, or a default if none
+ */
+struct wakeup_irq_node {
+	struct list_head siblings;
+	int irq;
+	const char *irq_name;
+};
+
+enum wakeup_reason_flag {
+	RESUME_NONE = 0,
+	RESUME_IRQ,
+	RESUME_ABORT,
+	RESUME_ABNORMAL,
+};
+
+static DEFINE_SPINLOCK(wakeup_reason_lock);
+
+static LIST_HEAD(leaf_irqs);   /* kept in ascending IRQ sorted order */
+static LIST_HEAD(parent_irqs); /* unordered */
+
+static struct kmem_cache *wakeup_irq_nodes_cache;
+
+static const char *default_irq_name = "(unnamed)";
+
+static struct kobject *kobj;
+
+static bool capture_reasons;
+static int wakeup_reason;
+static char non_irq_wake_reason[MAX_SUSPEND_ABORT_LEN];
+
+static ktime_t last_monotime; /* monotonic time before last suspend */
+static ktime_t curr_monotime; /* monotonic time after last suspend */
+static ktime_t last_stime; /* monotonic boottime offset before last suspend */
+static ktime_t curr_stime; /* monotonic boottime offset after last suspend */
+
+static void init_node(struct wakeup_irq_node *p, int irq)
+{
+	struct irq_desc *desc;
+
+	INIT_LIST_HEAD(&p->siblings);
+
+	p->irq = irq;
+	desc = irq_to_desc(irq);
+	if (desc && desc->action && desc->action->name)
+		p->irq_name = desc->action->name;
+	else
+		p->irq_name = default_irq_name;
+}
+
+static struct wakeup_irq_node *create_node(int irq)
+{
+	struct wakeup_irq_node *result;
+
+	result = kmem_cache_alloc(wakeup_irq_nodes_cache, GFP_ATOMIC);
+	if (unlikely(!result))
+		pr_warn("Failed to log wakeup IRQ %d\n", irq);
+	else
+		init_node(result, irq);
+
+	return result;
+}
+
+static void delete_list(struct list_head *head)
+{
+	struct wakeup_irq_node *n;
+
+	while (!list_empty(head)) {
+		n = list_first_entry(head, struct wakeup_irq_node, siblings);
+		list_del(&n->siblings);
+		kmem_cache_free(wakeup_irq_nodes_cache, n);
+	}
+}
+
+static bool add_sibling_node_sorted(struct list_head *head, int irq)
+{
+	struct wakeup_irq_node *n = NULL;
+	struct list_head *predecessor = head;
+
+	if (unlikely(WARN_ON(!head)))
+		return NULL;
+
+	if (!list_empty(head))
+		list_for_each_entry(n, head, siblings) {
+			if (n->irq < irq)
+				predecessor = &n->siblings;
+			else if (n->irq == irq)
+				return true;
+			else
+				break;
+		}
+
+	n = create_node(irq);
+	if (n) {
+		list_add(&n->siblings, predecessor);
+		return true;
+	}
+
+	return false;
+}
+
+static struct wakeup_irq_node *find_node_in_list(struct list_head *head,
+						 int irq)
+{
+	struct wakeup_irq_node *n;
+
+	if (unlikely(WARN_ON(!head)))
+		return NULL;
+
+	list_for_each_entry(n, head, siblings)
+		if (n->irq == irq)
+			return n;
+
+	return NULL;
+}
+
+void log_irq_wakeup_reason(int irq)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&wakeup_reason_lock, flags);
+	if (wakeup_reason == RESUME_ABNORMAL || wakeup_reason == RESUME_ABORT) {
+		spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+		return;
+	}
+
+	if (!capture_reasons) {
+		spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+		return;
+	}
+
+	if (find_node_in_list(&parent_irqs, irq) == NULL)
+		add_sibling_node_sorted(&leaf_irqs, irq);
+
+	wakeup_reason = RESUME_IRQ;
+	spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+}
+
+void log_threaded_irq_wakeup_reason(int irq, int parent_irq)
+{
+	struct wakeup_irq_node *parent;
+	unsigned long flags;
+
+	/*
+	 * Intentionally unsynchronized.  Calls that come in after we have
+	 * resumed should have a fast exit path since there's no work to be
+	 * done, any any coherence issue that could cause a wrong value here is
+	 * both highly improbable - given the set/clear timing - and very low
+	 * impact (parent IRQ gets logged instead of the specific child).
+	 */
+	if (!capture_reasons)
+		return;
+
+	spin_lock_irqsave(&wakeup_reason_lock, flags);
+
+	if (wakeup_reason == RESUME_ABNORMAL || wakeup_reason == RESUME_ABORT) {
+		spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+		return;
+	}
+
+	if (!capture_reasons || (find_node_in_list(&leaf_irqs, irq) != NULL)) {
+		spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+		return;
+	}
+
+	parent = find_node_in_list(&parent_irqs, parent_irq);
+	if (parent != NULL)
+		add_sibling_node_sorted(&leaf_irqs, irq);
+	else {
+		parent = find_node_in_list(&leaf_irqs, parent_irq);
+		if (parent != NULL) {
+			list_del_init(&parent->siblings);
+			list_add_tail(&parent->siblings, &parent_irqs);
+			add_sibling_node_sorted(&leaf_irqs, irq);
+		}
+	}
+
+	spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+}
+EXPORT_SYMBOL_GPL(log_threaded_irq_wakeup_reason);
+
+static void __log_abort_or_abnormal_wake(bool abort, const char *fmt,
+					 va_list args)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&wakeup_reason_lock, flags);
+
+	/* Suspend abort or abnormal wake reason has already been logged. */
+	if (wakeup_reason != RESUME_NONE) {
+		spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+		return;
+	}
+
+	if (abort)
+		wakeup_reason = RESUME_ABORT;
+	else
+		wakeup_reason = RESUME_ABNORMAL;
+
+	vsnprintf(non_irq_wake_reason, MAX_SUSPEND_ABORT_LEN, fmt, args);
+
+	spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+}
+
+void log_suspend_abort_reason(const char *fmt, ...)
+{
+	va_list args;
+
+	va_start(args, fmt);
+	__log_abort_or_abnormal_wake(true, fmt, args);
+	va_end(args);
+}
+EXPORT_SYMBOL_GPL(log_suspend_abort_reason);
+
+void log_abnormal_wakeup_reason(const char *fmt, ...)
+{
+	va_list args;
+
+	va_start(args, fmt);
+	__log_abort_or_abnormal_wake(false, fmt, args);
+	va_end(args);
+}
+EXPORT_SYMBOL_GPL(log_abnormal_wakeup_reason);
+
+void clear_wakeup_reasons(void)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&wakeup_reason_lock, flags);
+
+	delete_list(&leaf_irqs);
+	delete_list(&parent_irqs);
+	wakeup_reason = RESUME_NONE;
+	capture_reasons = true;
+
+	spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+}
+
+static void print_wakeup_sources(void)
+{
+	struct wakeup_irq_node *n;
+	unsigned long flags;
+
+	spin_lock_irqsave(&wakeup_reason_lock, flags);
+
+	capture_reasons = false;
+
+	if (wakeup_reason == RESUME_ABORT) {
+		pr_info("Abort: %s\n", non_irq_wake_reason);
+		spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+		return;
+	}
+
+	if (wakeup_reason == RESUME_IRQ && !list_empty(&leaf_irqs))
+		list_for_each_entry(n, &leaf_irqs, siblings)
+			pr_info("Resume caused by IRQ %d, %s\n", n->irq,
+				n->irq_name);
+	else if (wakeup_reason == RESUME_ABNORMAL)
+		pr_info("Resume caused by %s\n", non_irq_wake_reason);
+	else
+		pr_info("Resume cause unknown\n");
+
+	spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+}
+
+static ssize_t last_resume_reason_show(struct kobject *kobj,
+				       struct kobj_attribute *attr, char *buf)
+{
+	ssize_t buf_offset = 0;
+	struct wakeup_irq_node *n;
+	unsigned long flags;
+
+	spin_lock_irqsave(&wakeup_reason_lock, flags);
+
+	if (wakeup_reason == RESUME_ABORT) {
+		buf_offset = scnprintf(buf, PAGE_SIZE, "Abort: %s",
+				       non_irq_wake_reason);
+		spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+		return buf_offset;
+	}
+
+	if (wakeup_reason == RESUME_IRQ && !list_empty(&leaf_irqs))
+		list_for_each_entry(n, &leaf_irqs, siblings)
+			buf_offset += scnprintf(buf + buf_offset,
+						PAGE_SIZE - buf_offset,
+						"%d %s\n", n->irq, n->irq_name);
+	else if (wakeup_reason == RESUME_ABNORMAL)
+		buf_offset = scnprintf(buf, PAGE_SIZE, "-1 %s",
+				       non_irq_wake_reason);
+
+	spin_unlock_irqrestore(&wakeup_reason_lock, flags);
+
+	return buf_offset;
+}
+
+static ssize_t last_suspend_time_show(struct kobject *kobj,
+			struct kobj_attribute *attr, char *buf)
+{
+	struct timespec64 sleep_time;
+	struct timespec64 total_time;
+	struct timespec64 suspend_resume_time;
+
+	/*
+	 * total_time is calculated from monotonic bootoffsets because
+	 * unlike CLOCK_MONOTONIC it include the time spent in suspend state.
+	 */
+	total_time = ktime_to_timespec64(ktime_sub(curr_stime, last_stime));
+
+	/*
+	 * suspend_resume_time is calculated as monotonic (CLOCK_MONOTONIC)
+	 * time interval before entering suspend and post suspend.
+	 */
+	suspend_resume_time =
+		ktime_to_timespec64(ktime_sub(curr_monotime, last_monotime));
+
+	/* sleep_time = total_time - suspend_resume_time */
+	sleep_time = timespec64_sub(total_time, suspend_resume_time);
+
+	/* Export suspend_resume_time and sleep_time in pair here. */
+	return sprintf(buf, "%llu.%09lu %llu.%09lu\n",
+		       (unsigned long long)suspend_resume_time.tv_sec,
+		       suspend_resume_time.tv_nsec,
+		       (unsigned long long)sleep_time.tv_sec,
+		       sleep_time.tv_nsec);
+}
+
+static struct kobj_attribute resume_reason = __ATTR_RO(last_resume_reason);
+static struct kobj_attribute suspend_time = __ATTR_RO(last_suspend_time);
+
+static struct attribute *attrs[] = {
+	&resume_reason.attr,
+	&suspend_time.attr,
+	NULL,
+};
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+/* Detects a suspend and clears all the previous wake up reasons*/
+static int wakeup_reason_pm_event(struct notifier_block *notifier,
+		unsigned long pm_event, void *unused)
+{
+	switch (pm_event) {
+	case PM_SUSPEND_PREPARE:
+		/* monotonic time since boot */
+		last_monotime = ktime_get();
+		/* monotonic time since boot including the time spent in suspend */
+		last_stime = ktime_get_boottime();
+		clear_wakeup_reasons();
+		break;
+	case PM_POST_SUSPEND:
+		/* monotonic time since boot */
+		curr_monotime = ktime_get();
+		/* monotonic time since boot including the time spent in suspend */
+		curr_stime = ktime_get_boottime();
+		print_wakeup_sources();
+		break;
+	default:
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block wakeup_reason_pm_notifier_block = {
+	.notifier_call = wakeup_reason_pm_event,
+};
+
+static int __init wakeup_reason_init(void)
+{
+	if (register_pm_notifier(&wakeup_reason_pm_notifier_block)) {
+		pr_warn("[%s] failed to register PM notifier\n", __func__);
+		goto fail;
+	}
+
+	kobj = kobject_create_and_add("wakeup_reasons", kernel_kobj);
+	if (!kobj) {
+		pr_warn("[%s] failed to create a sysfs kobject\n", __func__);
+		goto fail_unregister_pm_notifier;
+	}
+
+	if (sysfs_create_group(kobj, &attr_group)) {
+		pr_warn("[%s] failed to create a sysfs group\n", __func__);
+		goto fail_kobject_put;
+	}
+
+	wakeup_irq_nodes_cache =
+		kmem_cache_create("wakeup_irq_node_cache",
+				  sizeof(struct wakeup_irq_node), 0, 0, NULL);
+	if (!wakeup_irq_nodes_cache)
+		goto fail_remove_group;
+
+	return 0;
+
+fail_remove_group:
+	sysfs_remove_group(kobj, &attr_group);
+fail_kobject_put:
+	kobject_put(kobj);
+fail_unregister_pm_notifier:
+	unregister_pm_notifier(&wakeup_reason_pm_notifier_block);
+fail:
+	return 1;
+}
+
+late_initcall(wakeup_reason_init);
diff -ruN a/kernel/printk/printk.c b/kernel/printk/printk.c
--- a/kernel/printk/printk.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/printk/printk.c	2021-12-23 08:35:57.000000000 +0100
@@ -2377,16 +2377,6 @@
 	char *s, *options, *brl_options = NULL;
 	int idx;
 
-	/*
-	 * console="" or console=null have been suggested as a way to
-	 * disable console output. Use ttynull that has been created
-	 * for exactly this purpose.
-	 */
-	if (str[0] == 0 || strcmp(str, "null") == 0) {
-		__add_preferred_console("ttynull", 0, NULL, NULL, true);
-		return 1;
-	}
-
 	if (_braille_console_setup(&str, &brl_options))
 		return 1;
 
diff -ruN a/kernel/sched/core.c b/kernel/sched/core.c
--- a/kernel/sched/core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/sched/core.c	2021-12-23 08:35:57.000000000 +0100
@@ -76,6 +76,9 @@
  */
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
 
+unsigned int sysctl_iowait_reset_ticks = 20;
+unsigned int sysctl_iowait_apply_ticks = 10;
+
 /*
  * period over which we measure -rt task CPU usage in us.
  * default: 1s
@@ -320,11 +323,14 @@
 	static_branch_disable(&__sched_core_enabled);
 }
 
+DEFINE_STATIC_KEY_TRUE(sched_coresched_supported);
+
 void sched_core_get(void)
 {
 	if (atomic_inc_not_zero(&sched_core_count))
 		return;
-
+	if (!static_branch_likely(&sched_coresched_supported))
+		return;
 	mutex_lock(&sched_core_mutex);
 	if (!atomic_read(&sched_core_count))
 		__sched_core_enable();
@@ -336,6 +342,8 @@
 
 static void __sched_core_put(struct work_struct *work)
 {
+	if (!static_branch_likely(&sched_coresched_supported))
+		return;
 	if (atomic_dec_and_mutex_lock(&sched_core_count, &sched_core_mutex)) {
 		__sched_core_disable();
 		mutex_unlock(&sched_core_mutex);
@@ -4014,6 +4022,9 @@
 	if (READ_ONCE(p->on_rq) && ttwu_runnable(p, wake_flags))
 		goto unlock;
 
+	if (READ_ONCE(p->__state) & TASK_UNINTERRUPTIBLE)
+		trace_sched_blocked_reason(p);
+
 #ifdef CONFIG_SMP
 	/*
 	 * Ensure we load p->on_cpu _after_ p->on_rq, otherwise it would be
@@ -8779,6 +8790,11 @@
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
+int wake_up_process_prefer_current_cpu(struct task_struct *next)
+{
+	return try_to_wake_up(next, TASK_NORMAL, WF_CURRENT_CPU);
+}
+
 #ifdef CONFIG_HOTPLUG_CPU
 /*
  * Ensure that the idle task is using init_mm right before its CPU goes
@@ -10167,6 +10183,27 @@
 	cpu_uclamp_print(sf, UCLAMP_MAX);
 	return 0;
 }
+
+static int cpu_uclamp_ls_write_u64(struct cgroup_subsys_state *css,
+				   struct cftype *cftype, u64 ls)
+{
+	struct task_group *tg;
+
+	if (ls > 1)
+		return -EINVAL;
+	tg = css_tg(css);
+	tg->latency_sensitive = (unsigned int) ls;
+
+	return 0;
+}
+
+static u64 cpu_uclamp_ls_read_u64(struct cgroup_subsys_state *css,
+				  struct cftype *cft)
+{
+	struct task_group *tg = css_tg(css);
+
+	return (u64) tg->latency_sensitive;
+}
 #endif /* CONFIG_UCLAMP_TASK_GROUP */
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -10603,6 +10640,12 @@
 		.seq_show = cpu_uclamp_max_show,
 		.write = cpu_uclamp_max_write,
 	},
+	{
+		.name = "uclamp.latency_sensitive",
+		.flags = CFTYPE_NOT_ON_ROOT,
+		.read_u64 = cpu_uclamp_ls_read_u64,
+		.write_u64 = cpu_uclamp_ls_write_u64,
+	},
 #endif
 	{ }	/* Terminate */
 };
@@ -10797,6 +10840,12 @@
 		.seq_show = cpu_uclamp_max_show,
 		.write = cpu_uclamp_max_write,
 	},
+	{
+		.name = "uclamp.latency_sensitive",
+		.flags = CFTYPE_NOT_ON_ROOT,
+		.read_u64 = cpu_uclamp_ls_read_u64,
+		.write_u64 = cpu_uclamp_ls_write_u64,
+	},
 #endif
 	{ }	/* terminate */
 };
diff -ruN a/kernel/sched/core_sched.c b/kernel/sched/core_sched.c
--- a/kernel/sched/core_sched.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/sched/core_sched.c	2021-12-23 08:35:57.000000000 +0100
@@ -138,6 +138,9 @@
 	    (cmd != PR_SCHED_CORE_GET && uaddr))
 		return -EINVAL;
 
+	if (!static_branch_likely(&sched_coresched_supported))
+		return 0;
+
 	rcu_read_lock();
 	if (pid == 0) {
 		task = current;
diff -ruN a/kernel/sched/cpufreq_schedutil.c b/kernel/sched/cpufreq_schedutil.c
--- a/kernel/sched/cpufreq_schedutil.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/sched/cpufreq_schedutil.c	2021-12-23 08:35:57.000000000 +0100
@@ -27,6 +27,7 @@
 	struct list_head	tunables_hook;
 
 	raw_spinlock_t		update_lock;
+	u64			last_update;
 	u64			last_freq_update_time;
 	s64			freq_update_delay_ns;
 	unsigned int		next_freq;
@@ -187,9 +188,13 @@
 			       bool set_iowait_boost)
 {
 	s64 delta_ns = time - sg_cpu->last_update;
+	unsigned int ticks = TICK_NSEC;
+
+	if (sysctl_iowait_reset_ticks)
+		ticks = sysctl_iowait_reset_ticks * TICK_NSEC;
 
-	/* Reset boost only if a tick has elapsed since last request */
-	if (delta_ns <= TICK_NSEC)
+	/* Reset boost only if enough ticks has elapsed since last request. */
+	if (delta_ns <= ticks)
 		return false;
 
 	sg_cpu->iowait_boost = set_iowait_boost ? IOWAIT_BOOST_MIN : 0;
@@ -261,6 +266,7 @@
  */
 static void sugov_iowait_apply(struct sugov_cpu *sg_cpu, u64 time)
 {
+	struct sugov_policy *sg_policy = sg_cpu->sg_policy;
 	unsigned long boost;
 
 	/* No boost currently required */
@@ -271,7 +277,9 @@
 	if (sugov_iowait_reset(sg_cpu, time, false))
 		return;
 
-	if (!sg_cpu->iowait_boost_pending) {
+	if (!sg_cpu->iowait_boost_pending &&
+	    (!sysctl_iowait_apply_ticks ||
+	     (time - sg_policy->last_update > (sysctl_iowait_apply_ticks * TICK_NSEC)))) {
 		/*
 		 * No boost pending; reduce the boost value.
 		 */
@@ -450,6 +458,14 @@
 		if (!sugov_update_next_freq(sg_policy, time, next_f))
 			goto unlock;
 
+		/*
+		 * Required for ensuring iowait decay does not happen too
+		 * quickly.  This can happen, for example, if a neighboring CPU
+		 * does a cpufreq update immediately after a CPU that just
+		 * completed I/O.
+		 */
+		sg_policy->last_update = time;
+
 		if (sg_policy->policy->fast_switch_enabled)
 			cpufreq_driver_fast_switch(sg_policy->policy, next_f);
 		else
@@ -835,29 +851,3 @@
 #endif
 
 cpufreq_governor_init(schedutil_gov);
-
-#ifdef CONFIG_ENERGY_MODEL
-static void rebuild_sd_workfn(struct work_struct *work)
-{
-	rebuild_sched_domains_energy();
-}
-static DECLARE_WORK(rebuild_sd_work, rebuild_sd_workfn);
-
-/*
- * EAS shouldn't be attempted without sugov, so rebuild the sched_domains
- * on governor changes to make sure the scheduler knows about it.
- */
-void sched_cpufreq_governor_change(struct cpufreq_policy *policy,
-				  struct cpufreq_governor *old_gov)
-{
-	if (old_gov == &schedutil_gov || policy->governor == &schedutil_gov) {
-		/*
-		 * When called from the cpufreq_register_driver() path, the
-		 * cpu_hotplug_lock is already held, so use a work item to
-		 * avoid nested locking in rebuild_sched_domains().
-		 */
-		schedule_work(&rebuild_sd_work);
-	}
-
-}
-#endif
diff -ruN a/kernel/sched/cputime.c b/kernel/sched/cputime.c
--- a/kernel/sched/cputime.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/sched/cputime.c	2021-12-23 08:35:57.000000000 +0100
@@ -2,6 +2,7 @@
 /*
  * Simple CPU accounting cgroup controller
  */
+#include <linux/cpufreq_times.h>
 #include "sched.h"
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
@@ -130,6 +131,9 @@
 
 	/* Account for user time used */
 	acct_account_cputime(p);
+
+	/* Account power usage for user time */
+	cpufreq_acct_update_power(p, cputime);
 }
 
 /*
@@ -174,6 +178,9 @@
 
 	/* Account for system time used */
 	acct_account_cputime(p);
+
+	/* Account power usage for system time */
+	cpufreq_acct_update_power(p, cputime);
 }
 
 /*
diff -ruN a/kernel/sched/debug.c b/kernel/sched/debug.c
--- a/kernel/sched/debug.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/sched/debug.c	2021-12-23 08:35:57.000000000 +0100
@@ -820,6 +820,10 @@
 		"sysctl_sched_tunable_scaling",
 		sysctl_sched_tunable_scaling,
 		sched_tunable_scaling_names[sysctl_sched_tunable_scaling]);
+#ifdef CONFIG_SCHED_CORE
+	SEQ_printf(m, "  .%-40s: %d\n", "core_sched_enabled",
+		   !!static_branch_likely(&__sched_core_enabled));
+#endif
 	SEQ_printf(m, "\n");
 }
 
@@ -1054,6 +1058,10 @@
 		__PS("clock-delta", t1-t0);
 	}
 
+#ifdef CONFIG_SCHED_CORE
+	__PS("core_cookie", p->core_cookie);
+#endif
+
 	sched_show_numa(p, m);
 }
 
diff -ruN a/kernel/sched/fair.c b/kernel/sched/fair.c
--- a/kernel/sched/fair.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/sched/fair.c	2021-12-23 08:35:57.000000000 +0100
@@ -6783,14 +6783,19 @@
  * other use-cases too. So, until someone finds a better way to solve this,
  * let's keep things simple by re-using the existing slow path.
  */
-static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
+static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu, int sync)
 {
 	unsigned long prev_delta = ULONG_MAX, best_delta = ULONG_MAX;
 	unsigned long p_util_min = uclamp_is_used() ? uclamp_eff_value(p, UCLAMP_MIN) : 0;
 	unsigned long p_util_max = uclamp_is_used() ? uclamp_eff_value(p, UCLAMP_MAX) : 1024;
 	struct root_domain *rd = cpu_rq(smp_processor_id())->rd;
+	int max_spare_cap_cpu_ls = prev_cpu, best_idle_cpu = -1;
 	int cpu, best_energy_cpu = prev_cpu, target = -1;
+	unsigned long max_spare_cap_ls = 0, target_cap;
 	unsigned long cpu_cap, util, base_energy = 0;
+	bool boosted, latency_sensitive = false;
+	unsigned int min_exit_lat = UINT_MAX;
+	struct cpuidle_state *idle;
 	struct sched_domain *sd;
 	struct perf_domain *pd;
 
@@ -6797,6 +6802,14 @@
 	if (!pd || READ_ONCE(rd->overutilized))
 		goto unlock;
 
+	cpu = smp_processor_id();
+	if (sync && cpu_rq(cpu)->nr_running == 1 &&
+	    cpumask_test_cpu(cpu, p->cpus_ptr) &&
+	    task_fits_cpu(p, cpu)) {
+		rcu_read_unlock();
+		return cpu;
+	}
+
 	/*
 	 * Energy-aware wake-up happens on the lowest sched_domain starting
 	 * from sd_asym_cpucapacity spanning over this_cpu and prev_cpu.
@@ -6813,6 +6826,10 @@
 	if (!task_util_est(p))
 		goto unlock;
 
+	latency_sensitive = uclamp_latency_sensitive(p);
+	boosted = uclamp_boosted(p);
+	target_cap = boosted ? 0 : ULONG_MAX;
+
 	for (; pd; pd = pd->next) {
 		unsigned long cur_delta, spare_cap, max_spare_cap = 0;
 		bool compute_prev_delta = false;
@@ -6839,7 +6856,7 @@
 			if (!fits_capacity(util, cpu_cap))
 				continue;
 
-			if (cpu == prev_cpu) {
+			if (!latency_sensitive && cpu == prev_cpu) {
 				/* Always use prev_cpu as a candidate. */
 				compute_prev_delta = true;
 			} else if (spare_cap > max_spare_cap) {
@@ -6850,9 +6867,32 @@
 				max_spare_cap = spare_cap;
 				max_spare_cap_cpu = cpu;
 			}
+
+			if (!latency_sensitive)
+				continue;
+
+			if (idle_cpu(cpu)) {
+				cpu_cap = capacity_orig_of(cpu);
+				if (boosted && cpu_cap < target_cap)
+					continue;
+				if (!boosted && cpu_cap > target_cap)
+					continue;
+				idle = idle_get_state(cpu_rq(cpu));
+				if (idle && idle->exit_latency > min_exit_lat &&
+						cpu_cap == target_cap)
+					continue;
+
+				if (idle)
+					min_exit_lat = idle->exit_latency;
+				target_cap = cpu_cap;
+				best_idle_cpu = cpu;
+			} else if (spare_cap > max_spare_cap_ls) {
+				max_spare_cap_ls = spare_cap;
+				max_spare_cap_cpu_ls = cpu;
+			}
 		}
 
-		if (max_spare_cap_cpu < 0 && !compute_prev_delta)
+		if (!latency_sensitive && max_spare_cap_cpu < 0 && !compute_prev_delta)
 			continue;
 
 		/* Compute the 'base' energy of the pd, without @p */
@@ -6882,6 +6922,9 @@
 	}
 	rcu_read_unlock();
 
+	if (latency_sensitive)
+		return best_idle_cpu >= 0 ? best_idle_cpu : max_spare_cap_cpu_ls;
+
 	/*
 	 * Pick the best CPU if prev_cpu cannot be used, or if it saves at
 	 * least 6% of the energy used by prev_cpu.
@@ -6923,11 +6966,14 @@
 	 * required for stable ->cpus_allowed
 	 */
 	lockdep_assert_held(&p->pi_lock);
+	if ((wake_flags & WF_CURRENT_CPU) && cpumask_test_cpu(cpu, p->cpus_ptr))
+		return cpu;
+
 	if (wake_flags & WF_TTWU) {
 		record_wakee(p);
 
 		if (sched_energy_enabled()) {
-			new_cpu = find_energy_efficient_cpu(p, prev_cpu);
+			new_cpu = find_energy_efficient_cpu(p, prev_cpu, sync);
 			if (new_cpu >= 0)
 				return new_cpu;
 			new_cpu = prev_cpu;
@@ -8602,6 +8648,99 @@
 }
 
 /**
+ * asym_smt_can_pull_tasks - Check whether the load balancing CPU can pull tasks
+ * @dst_cpu:	Destination CPU of the load balancing
+ * @sds:	Load-balancing data with statistics of the local group
+ * @sgs:	Load-balancing statistics of the candidate busiest group
+ * @sg:		The candidate busiest group
+ *
+ * Check the state of the SMT siblings of both @sds::local and @sg and decide
+ * if @dst_cpu can pull tasks.
+ *
+ * If @dst_cpu does not have SMT siblings, it can pull tasks if two or more of
+ * the SMT siblings of @sg are busy. If only one CPU in @sg is busy, pull tasks
+ * only if @dst_cpu has higher priority.
+ *
+ * If both @dst_cpu and @sg have SMT siblings, and @sg has exactly one more
+ * busy CPU than @sds::local, let @dst_cpu pull tasks if it has higher priority.
+ * Bigger imbalances in the number of busy CPUs will be dealt with in
+ * update_sd_pick_busiest().
+ *
+ * If @sg does not have SMT siblings, only pull tasks if all of the SMT siblings
+ * of @dst_cpu are idle and @sg has lower priority.
+ */
+static bool asym_smt_can_pull_tasks(int dst_cpu, struct sd_lb_stats *sds,
+				    struct sg_lb_stats *sgs,
+				    struct sched_group *sg)
+{
+#ifdef CONFIG_SCHED_SMT
+	bool local_is_smt, sg_is_smt;
+	int sg_busy_cpus;
+
+	local_is_smt = sds->local->flags & SD_SHARE_CPUCAPACITY;
+	sg_is_smt = sg->flags & SD_SHARE_CPUCAPACITY;
+
+	sg_busy_cpus = sgs->group_weight - sgs->idle_cpus;
+
+	if (!local_is_smt) {
+		/*
+		 * If we are here, @dst_cpu is idle and does not have SMT
+		 * siblings. Pull tasks if candidate group has two or more
+		 * busy CPUs.
+		 */
+		if (sg_busy_cpus >= 2) /* implies sg_is_smt */
+			return true;
+
+		/*
+		 * @dst_cpu does not have SMT siblings. @sg may have SMT
+		 * siblings and only one is busy. In such case, @dst_cpu
+		 * can help if it has higher priority and is idle (i.e.,
+		 * it has no running tasks).
+		 */
+		return sched_asym_prefer(dst_cpu, sg->asym_prefer_cpu);
+	}
+
+	/* @dst_cpu has SMT siblings. */
+
+	if (sg_is_smt) {
+		int local_busy_cpus = sds->local->group_weight -
+				      sds->local_stat.idle_cpus;
+		int busy_cpus_delta = sg_busy_cpus - local_busy_cpus;
+
+		if (busy_cpus_delta == 1)
+			return sched_asym_prefer(dst_cpu, sg->asym_prefer_cpu);
+
+		return false;
+	}
+
+	/*
+	 * @sg does not have SMT siblings. Ensure that @sds::local does not end
+	 * up with more than one busy SMT sibling and only pull tasks if there
+	 * are not busy CPUs (i.e., no CPU has running tasks).
+	 */
+	if (!sds->local_stat.sum_nr_running)
+		return sched_asym_prefer(dst_cpu, sg->asym_prefer_cpu);
+
+	return false;
+#else
+	/* Always return false so that callers deal with non-SMT cases. */
+	return false;
+#endif
+}
+
+static inline bool
+sched_asym(struct lb_env *env, struct sd_lb_stats *sds,  struct sg_lb_stats *sgs,
+	   struct sched_group *group)
+{
+	/* Only do SMT checks if either local or candidate have SMT siblings */
+	if ((sds->local->flags & SD_SHARE_CPUCAPACITY) ||
+	    (group->flags & SD_SHARE_CPUCAPACITY))
+		return asym_smt_can_pull_tasks(env->dst_cpu, sds, sgs, group);
+
+	return sched_asym_prefer(env->dst_cpu, group->asym_prefer_cpu);
+}
+
+/**
  * update_sg_lb_stats - Update sched_group's statistics for load balancing.
  * @env: The load balancing environment.
  * @group: sched_group whose statistics are to be updated.
@@ -8609,6 +8748,7 @@
  * @sg_status: Holds flag indicating the status of the sched_group
  */
 static inline void update_sg_lb_stats(struct lb_env *env,
+				      struct sd_lb_stats *sds,
 				      struct sched_group *group,
 				      struct sg_lb_stats *sgs,
 				      int *sg_status)
@@ -8617,7 +8757,7 @@
 
 	memset(sgs, 0, sizeof(*sgs));
 
-	local_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(group));
+	local_group = group == sds->local;
 
 	for_each_cpu_and(i, sched_group_span(group), env->cpus) {
 		struct rq *rq = cpu_rq(i);
@@ -8660,18 +8800,17 @@
 		}
 	}
 
-	/* Check if dst CPU is idle and preferred to this group */
-	if (env->sd->flags & SD_ASYM_PACKING &&
-	    env->idle != CPU_NOT_IDLE &&
-	    sgs->sum_h_nr_running &&
-	    sched_asym_prefer(env->dst_cpu, group->asym_prefer_cpu)) {
-		sgs->group_asym_packing = 1;
-	}
-
 	sgs->group_capacity = group->sgc->capacity;
 
 	sgs->group_weight = group->group_weight;
 
+	/* Check if dst CPU is idle and preferred to this group */
+	if (!local_group && env->sd->flags & SD_ASYM_PACKING &&
+	    env->idle != CPU_NOT_IDLE && sgs->sum_h_nr_running &&
+	    sched_asym(env, sds, sgs, group)) {
+		sgs->group_asym_packing = 1;
+	}
+
 	sgs->group_type = group_classify(env->sd->imbalance_pct, group, sgs);
 
 	/* Computing avg_load makes sense only when group is overloaded */
@@ -9180,7 +9319,7 @@
 				update_group_capacity(env->sd, env->dst_cpu);
 		}
 
-		update_sg_lb_stats(env, sg, sgs, &sg_status);
+		update_sg_lb_stats(env, sds, sg, sgs, &sg_status);
 
 		if (local_group)
 			goto next_group;
@@ -9603,6 +9742,12 @@
 		    nr_running == 1)
 			continue;
 
+		/* Make sure we only pull tasks from a CPU of lower priority */
+		if ((env->sd->flags & SD_ASYM_PACKING) &&
+		    sched_asym_prefer(i, env->dst_cpu) &&
+		    nr_running == 1)
+			continue;
+
 		switch (env->migration_type) {
 		case migrate_load:
 			/*
diff -ruN a/kernel/sched/sched.h b/kernel/sched/sched.h
--- a/kernel/sched/sched.h	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/sched/sched.h	2021-12-23 08:35:57.000000000 +0100
@@ -436,6 +436,8 @@
 	struct uclamp_se	uclamp_req[UCLAMP_CNT];
 	/* Effective clamp values used for a task group */
 	struct uclamp_se	uclamp[UCLAMP_CNT];
+	/* Latency-sensitive flag used for a task group */
+	unsigned int		latency_sensitive;
 #endif
 
 };
@@ -1810,6 +1812,7 @@
 	unsigned int		group_weight;
 	struct sched_group_capacity *sgc;
 	int			asym_prefer_cpu;	/* CPU of highest priority in group */
+	int			flags;
 
 	/*
 	 * The CPUs this group covers.
@@ -2049,6 +2052,7 @@
 
 #define WF_SYNC     0x10 /* Waker goes to sleep after wakeup */
 #define WF_MIGRATED 0x20 /* Internal use, task got migrated */
+#define WF_CURRENT_CPU		0x80		/* Prefer to move wakee to the current CPU */
 
 #ifdef CONFIG_SMP
 static_assert(WF_EXEC == SD_BALANCE_EXEC);
@@ -2400,6 +2404,9 @@
 extern const_debug unsigned int sysctl_sched_nr_migrate;
 extern const_debug unsigned int sysctl_sched_migration_cost;
 
+extern unsigned int sysctl_iowait_reset_ticks;
+extern unsigned int sysctl_iowait_apply_ticks;
+
 #ifdef CONFIG_SCHED_DEBUG
 extern unsigned int sysctl_sched_latency;
 extern unsigned int sysctl_sched_min_granularity;
@@ -2880,6 +2887,11 @@
 	return clamp(util, min_util, max_util);
 }
 
+static inline bool uclamp_boosted(struct task_struct *p)
+{
+	return uclamp_eff_value(p, UCLAMP_MIN) > 0;
+}
+
 /*
  * When uclamp is compiled in, the aggregation at rq level is 'turned off'
  * by default in the fast path and only gets turned on once userspace performs
@@ -2900,12 +2912,36 @@
 {
 }
 
+static inline bool uclamp_boosted(struct task_struct *p)
+{
+	return false;
+}
+
 static inline bool uclamp_rq_is_idle(struct rq *rq)
 {
 	return false;
 }
 #endif /* CONFIG_UCLAMP_TASK */
 
+#ifdef CONFIG_UCLAMP_TASK_GROUP
+static inline bool uclamp_latency_sensitive(struct task_struct *p)
+{
+	struct cgroup_subsys_state *css = task_css(p, cpu_cgrp_id);
+	struct task_group *tg;
+
+	if (!css)
+		return false;
+	tg = container_of(css, struct task_group, css);
+
+	return tg->latency_sensitive;
+}
+#else
+static inline bool uclamp_latency_sensitive(struct task_struct *p)
+{
+	return false;
+}
+#endif /* CONFIG_UCLAMP_TASK_GROUP */
+
 #ifdef arch_scale_freq_capacity
 # ifndef arch_scale_freq_invariant
 #  define arch_scale_freq_invariant()	true
diff -ruN a/kernel/sched/topology.c b/kernel/sched/topology.c
--- a/kernel/sched/topology.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/sched/topology.c	2021-12-23 08:35:57.000000000 +0100
@@ -327,8 +327,7 @@
  *    2. the SD_ASYM_CPUCAPACITY flag is set in the sched_domain hierarchy.
  *    3. no SMT is detected.
  *    4. the EM complexity is low enough to keep scheduling overheads low;
- *    5. schedutil is driving the frequency of all CPUs of the rd;
- *    6. frequency invariance support is present;
+ *    5. frequency invariance support is present;
  *
  * The complexity of the Energy Model is defined as:
  *
@@ -348,15 +347,12 @@
  */
 #define EM_MAX_COMPLEXITY 2048
 
-extern struct cpufreq_governor schedutil_gov;
 static bool build_perf_domains(const struct cpumask *cpu_map)
 {
 	int i, nr_pd = 0, nr_ps = 0, nr_cpus = cpumask_weight(cpu_map);
 	struct perf_domain *pd = NULL, *tmp;
 	int cpu = cpumask_first(cpu_map);
 	struct root_domain *rd = cpu_rq(cpu)->rd;
-	struct cpufreq_policy *policy;
-	struct cpufreq_governor *gov;
 
 	if (!sysctl_sched_energy_aware)
 		goto free;
@@ -390,19 +386,6 @@
 		if (find_pd(pd, i))
 			continue;
 
-		/* Do not attempt EAS if schedutil is not being used. */
-		policy = cpufreq_cpu_get(i);
-		if (!policy)
-			goto free;
-		gov = policy->governor;
-		cpufreq_cpu_put(policy);
-		if (gov != &schedutil_gov) {
-			if (rd->pd)
-				pr_warn("rd %*pbl: Disabling EAS, schedutil is mandatory\n",
-						cpumask_pr_args(cpu_map));
-			goto free;
-		}
-
 		/* Create the new pd and add it to the local list. */
 		tmp = pd_init(i);
 		if (!tmp)
@@ -716,8 +699,20 @@
 		tmp = sd;
 		sd = sd->parent;
 		destroy_sched_domain(tmp);
-		if (sd)
+		if (sd) {
+			struct sched_group *sg = sd->groups;
+
+			/*
+			 * sched groups hold the flags of the child sched
+			 * domain for convenience. Clear such flags since
+			 * the child is being destroyed.
+			 */
+			do {
+				sg->flags = 0;
+			} while (sg != sd->groups);
+
 			sd->child = NULL;
+		}
 	}
 
 	for (tmp = sd; tmp; tmp = tmp->parent)
@@ -916,10 +911,12 @@
 		return NULL;
 
 	sg_span = sched_group_span(sg);
-	if (sd->child)
+	if (sd->child) {
 		cpumask_copy(sg_span, sched_domain_span(sd->child));
-	else
+		sg->flags = sd->child->flags;
+	} else {
 		cpumask_copy(sg_span, sched_domain_span(sd));
+	}
 
 	atomic_inc(&sg->ref);
 	return sg;
@@ -1169,6 +1166,7 @@
 	if (child) {
 		cpumask_copy(sched_group_span(sg), sched_domain_span(child));
 		cpumask_copy(group_balance_mask(sg), sched_group_span(sg));
+		sg->flags = child->flags;
 	} else {
 		cpumask_set_cpu(cpu, sched_group_span(sg));
 		cpumask_set_cpu(cpu, group_balance_mask(sg));
diff -ruN a/kernel/sys.c b/kernel/sys.c
--- a/kernel/sys.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/sys.c	2021-12-23 08:35:57.000000000 +0100
@@ -42,9 +42,12 @@
 #include <linux/version.h>
 #include <linux/ctype.h>
 #include <linux/syscall_user_dispatch.h>
+#include <linux/mm.h>
+#include <linux/mempolicy.h>
 
 #include <linux/compat.h>
 #include <linux/syscalls.h>
+#include <linux/alt-syscall.h>
 #include <linux/kprobes.h>
 #include <linux/user_namespace.h>
 #include <linux/time_namespace.h>
@@ -200,7 +203,7 @@
 	return error;
 }
 
-SYSCALL_DEFINE3(setpriority, int, which, int, who, int, niceval)
+int ksys_setpriority(int which, int who, int niceval)
 {
 	struct task_struct *g, *p;
 	struct user_struct *user;
@@ -264,13 +267,18 @@
 	return error;
 }
 
+SYSCALL_DEFINE3(setpriority, int, which, int, who, int, niceval)
+{
+	return ksys_setpriority(which, who, niceval);
+}
+
 /*
  * Ugh. To avoid negative return values, "getpriority()" will
  * not return the normal nice-value, but a negated value that
  * has been offset by 20 (ie it returns 40..1 instead of -20..19)
  * to stay compatible.
  */
-SYSCALL_DEFINE2(getpriority, int, which, int, who)
+int ksys_getpriority(int which, int who)
 {
 	struct task_struct *g, *p;
 	struct user_struct *user;
@@ -335,6 +343,11 @@
 	return retval;
 }
 
+SYSCALL_DEFINE2(getpriority, int, which, int, who)
+{
+	return ksys_getpriority(which, who);
+}
+
 /*
  * Unprivileged users may change the real gid to the effective gid
  * or vice versa.  (BSD-style)
@@ -2259,10 +2272,157 @@
 	return -EINVAL;
 }
 
+#ifdef CONFIG_MMU
+static int prctl_update_vma_anon_name(struct vm_area_struct *vma,
+		struct vm_area_struct **prev,
+		unsigned long start, unsigned long end,
+		const char __user *name_addr)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	int error = 0;
+	pgoff_t pgoff;
+
+	if (name_addr == vma_get_anon_name(vma)) {
+		*prev = vma;
+		goto out;
+	}
+
+	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
+	*prev = vma_merge(mm, *prev, start, end, vma->vm_flags, vma->anon_vma,
+				vma->vm_file, pgoff, vma_policy(vma),
+				vma->vm_userfaultfd_ctx, name_addr);
+	if (*prev) {
+		vma = *prev;
+		goto success;
+	}
+
+	*prev = vma;
+
+	if (start != vma->vm_start) {
+		error = split_vma(mm, vma, start, 1);
+		if (error)
+			goto out;
+	}
+
+	if (end != vma->vm_end) {
+		error = split_vma(mm, vma, end, 0);
+		if (error)
+			goto out;
+	}
+
+success:
+	if (!vma->vm_file)
+		vma->anon_name = name_addr;
+
+out:
+	if (error == -ENOMEM)
+		error = -EAGAIN;
+	return error;
+}
+
+static int prctl_set_vma_anon_name(unsigned long start, unsigned long end,
+			unsigned long arg)
+{
+	unsigned long tmp;
+	struct vm_area_struct *vma, *prev;
+	int unmapped_error = 0;
+	int error = -EINVAL;
+
+	/*
+	 * If the interval [start,end) covers some unmapped address
+	 * ranges, just ignore them, but return -ENOMEM at the end.
+	 * - this matches the handling in madvise.
+	 */
+	vma = find_vma_prev(current->mm, start, &prev);
+	if (vma && start > vma->vm_start)
+		prev = vma;
+
+	for (;;) {
+		/* Still start < end. */
+		error = -ENOMEM;
+		if (!vma)
+			return error;
+
+		/* Here start < (end|vma->vm_end). */
+		if (start < vma->vm_start) {
+			unmapped_error = -ENOMEM;
+			start = vma->vm_start;
+			if (start >= end)
+				return error;
+		}
+
+		/* Here vma->vm_start <= start < (end|vma->vm_end) */
+		tmp = vma->vm_end;
+		if (end < tmp)
+			tmp = end;
+
+		/* Here vma->vm_start <= start < tmp <= (end|vma->vm_end). */
+		error = prctl_update_vma_anon_name(vma, &prev, start, tmp,
+				(const char __user *)arg);
+		if (error)
+			return error;
+		start = tmp;
+		if (prev && start < prev->vm_end)
+			start = prev->vm_end;
+		error = unmapped_error;
+		if (start >= end)
+			return error;
+		if (prev)
+			vma = prev->vm_next;
+		else	/* madvise_remove dropped mmap_lock */
+			vma = find_vma(current->mm, start);
+	}
+}
+
+static int prctl_set_vma(unsigned long opt, unsigned long start,
+		unsigned long len_in, unsigned long arg)
+{
+	struct mm_struct *mm = current->mm;
+	int error;
+	unsigned long len;
+	unsigned long end;
+
+	if (start & ~PAGE_MASK)
+		return -EINVAL;
+	len = (len_in + ~PAGE_MASK) & PAGE_MASK;
+
+	/* Check to see whether len was rounded up from small -ve to zero */
+	if (len_in && !len)
+		return -EINVAL;
+
+	end = start + len;
+	if (end < start)
+		return -EINVAL;
+
+	if (end == start)
+		return 0;
+
+	mmap_write_lock(mm);
+
+	switch (opt) {
+	case PR_SET_VMA_ANON_NAME:
+		error = prctl_set_vma_anon_name(start, end, arg);
+		break;
+	default:
+		error = -EINVAL;
+	}
+
+	mmap_write_unlock(mm);
+
+	return error;
+}
+#else /* CONFIG_MMU */
+static int prctl_set_vma(unsigned long opt, unsigned long start,
+		unsigned long len_in, unsigned long arg)
+{
+	return -EINVAL;
+}
+#endif
+
 #define PR_IO_FLUSHER (PF_MEMALLOC_NOIO | PF_LOCAL_THROTTLE)
 
-SYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,
-		unsigned long, arg4, unsigned long, arg5)
+int ksys_prctl(int option, unsigned long arg2, unsigned long arg3,
+	       unsigned long arg4, unsigned long arg5)
 {
 	struct task_struct *me = current;
 	unsigned char comm[sizeof(me->comm)];
@@ -2345,6 +2505,12 @@
 	case PR_SET_SECCOMP:
 		error = prctl_set_seccomp(arg2, (char __user *)arg3);
 		break;
+	case PR_ALT_SYSCALL:
+		if (arg2 == PR_ALT_SYSCALL_SET_SYSCALL_TABLE)
+			error = set_alt_sys_call_table((char __user *)arg3);
+		else
+			error = -EINVAL;
+		break;
 	case PR_GET_TSC:
 		error = GET_TSC_CTL(arg2);
 		break;
@@ -2473,6 +2639,9 @@
 			return -EINVAL;
 		error = arch_prctl_spec_ctrl_set(me, arg2, arg3);
 		break;
+	case PR_SET_VMA:
+		error = prctl_set_vma(arg2, arg3, arg4, arg5);
+		break;
 	case PR_PAC_RESET_KEYS:
 		if (arg3 || arg4 || arg5)
 			return -EINVAL;
@@ -2537,8 +2706,14 @@
 	return error;
 }
 
-SYSCALL_DEFINE3(getcpu, unsigned __user *, cpup, unsigned __user *, nodep,
-		struct getcpu_cache __user *, unused)
+SYSCALL_DEFINE5(prctl, int, option, unsigned long, arg2, unsigned long, arg3,
+		unsigned long, arg4, unsigned long, arg5)
+{
+	return ksys_prctl(option, arg2, arg3, arg4, arg5);
+}
+
+int ksys_getcpu(unsigned __user *cpup, unsigned __user *nodep,
+		struct getcpu_cache __user *unused)
 {
 	int err = 0;
 	int cpu = raw_smp_processor_id();
@@ -2550,6 +2725,12 @@
 	return err ? -EFAULT : 0;
 }
 
+SYSCALL_DEFINE3(getcpu, unsigned __user *, cpup, unsigned __user *, nodep,
+		struct getcpu_cache __user *, unused)
+{
+	return ksys_getcpu(cpup, nodep, unused);
+}
+
 /**
  * do_sysinfo - fill in sysinfo struct
  * @info: pointer to buffer to fill
diff -ruN a/kernel/sysctl.c b/kernel/sysctl.c
--- a/kernel/sysctl.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/sysctl.c	2021-12-23 08:35:57.000000000 +0100
@@ -1778,6 +1778,20 @@
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec,
 	},
+	{
+		.procname	= "iowait_reset_ticks",
+		.data		= &sysctl_iowait_reset_ticks,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
+	{
+		.procname	= "iowait_apply_ticks",
+		.data		= &sysctl_iowait_apply_ticks,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
 #ifdef CONFIG_SCHEDSTATS
 	{
 		.procname	= "sched_schedstats",
@@ -3074,6 +3088,15 @@
 		.mode		= 0644,
 		.proc_handler	= mmap_min_addr_handler,
 	},
+	{
+		.procname	= "mmap_noexec_taint",
+		.data		= &sysctl_mmap_noexec_taint,
+		.maxlen		= sizeof(sysctl_mmap_noexec_taint),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
 #endif
 #ifdef CONFIG_NUMA
 	{
@@ -3145,6 +3168,13 @@
 		.mode		= 0644,
 		.proc_handler	= proc_doulongvec_minmax,
 	},
+	{
+		.procname	= "min_filelist_kbytes",
+		.data		= &min_filelist_kbytes,
+		.maxlen		= sizeof(min_filelist_kbytes),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec,
+	},
 #ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS
 	{
 		.procname	= "mmap_rnd_bits",
@@ -3175,6 +3205,17 @@
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
+#endif
+#ifdef CONFIG_DISK_BASED_SWAP
+	{
+		.procname	= "disk_based_swap",
+		.data		= &sysctl_disk_based_swap,
+		.maxlen		= sizeof(sysctl_disk_based_swap),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_ONE,
 	},
 #endif
diff -ruN a/kernel/time/posix-timers.c b/kernel/time/posix-timers.c
--- a/kernel/time/posix-timers.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/time/posix-timers.c	2021-12-23 08:35:57.000000000 +0100
@@ -1109,8 +1109,8 @@
 	return kc->clock_adj(which_clock, ktx);
 }
 
-SYSCALL_DEFINE2(clock_adjtime, const clockid_t, which_clock,
-		struct __kernel_timex __user *, utx)
+int ksys_clock_adjtime(const clockid_t which_clock,
+		       struct __kernel_timex __user * utx)
 {
 	struct __kernel_timex ktx;
 	int err;
@@ -1126,6 +1126,12 @@
 	return err;
 }
 
+SYSCALL_DEFINE2(clock_adjtime, const clockid_t, which_clock,
+		struct __kernel_timex __user *, utx)
+{
+	return ksys_clock_adjtime(which_clock, utx);
+}
+
 SYSCALL_DEFINE2(clock_getres, const clockid_t, which_clock,
 		struct __kernel_timespec __user *, tp)
 {
@@ -1179,8 +1185,7 @@
 	return err;
 }
 
-SYSCALL_DEFINE2(clock_adjtime32, clockid_t, which_clock,
-		struct old_timex32 __user *, utp)
+int ksys_clock_adjtime32(clockid_t which_clock, struct old_timex32 __user * utp)
 {
 	struct __kernel_timex ktx;
 	int err;
@@ -1197,6 +1202,12 @@
 	return err;
 }
 
+SYSCALL_DEFINE2(clock_adjtime32, clockid_t, which_clock,
+		struct old_timex32 __user *, utp)
+{
+	return ksys_clock_adjtime32(which_clock, utp);
+}
+
 SYSCALL_DEFINE2(clock_getres_time32, clockid_t, which_clock,
 		struct old_timespec32 __user *, tp)
 {
diff -ruN a/kernel/time/time.c b/kernel/time/time.c
--- a/kernel/time/time.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/time/time.c	2021-12-23 08:35:57.000000000 +0100
@@ -266,7 +266,7 @@
 #endif
 
 #ifdef CONFIG_64BIT
-SYSCALL_DEFINE1(adjtimex, struct __kernel_timex __user *, txc_p)
+int ksys_adjtimex(struct __kernel_timex __user * txc_p)
 {
 	struct __kernel_timex txc;		/* Local copy of parameter */
 	int ret;
@@ -280,6 +280,11 @@
 	ret = do_adjtimex(&txc);
 	return copy_to_user(txc_p, &txc, sizeof(struct __kernel_timex)) ? -EFAULT : ret;
 }
+
+SYSCALL_DEFINE1(adjtimex, struct __kernel_timex __user *, txc_p)
+{
+	return ksys_adjtimex(txc_p);
+}
 #endif
 
 #ifdef CONFIG_COMPAT_32BIT_TIME
@@ -346,7 +351,7 @@
 	return 0;
 }
 
-SYSCALL_DEFINE1(adjtimex_time32, struct old_timex32 __user *, utp)
+int ksys_adjtimex_time32(struct old_timex32 __user * utp)
 {
 	struct __kernel_timex txc;
 	int err, ret;
@@ -363,6 +368,12 @@
 
 	return ret;
 }
+
+SYSCALL_DEFINE1(adjtimex_time32, struct old_timex32 __user *, utp)
+{
+	return ksys_adjtimex_time32(utp);
+}
+
 #endif
 
 /*
diff -ruN a/kernel/user_namespace.c b/kernel/user_namespace.c
--- a/kernel/user_namespace.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/user_namespace.c	2021-12-23 08:35:57.000000000 +0100
@@ -1382,6 +1382,7 @@
 	.owner		= userns_owner,
 	.get_parent	= ns_get_owner,
 };
+EXPORT_SYMBOL(userns_operations);
 
 static __init int user_namespaces_init(void)
 {
diff -ruN a/kernel/watchdog_buddy_cpu.c b/kernel/watchdog_buddy_cpu.c
--- a/kernel/watchdog_buddy_cpu.c	1970-01-01 01:00:00.000000000 +0100
+++ b/kernel/watchdog_buddy_cpu.c	2021-12-23 08:35:57.000000000 +0100
@@ -0,0 +1,128 @@
+// SPDX-License-Identifier: GPL-2.0
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/kernel.h>
+#include <linux/nmi.h>
+#include <linux/percpu-defs.h>
+
+static DEFINE_PER_CPU(bool, watchdog_touch);
+static DEFINE_PER_CPU(bool, hard_watchdog_warn);
+static cpumask_t __read_mostly watchdog_cpus;
+
+int __init watchdog_nmi_probe(void)
+{
+	return 0;
+}
+
+notrace void buddy_cpu_touch_watchdog(void)
+{
+	/*
+	 * Using __raw here because some code paths have
+	 * preemption enabled.  If preemption is enabled
+	 * then interrupts should be enabled too, in which
+	 * case we shouldn't have to worry about the watchdog
+	 * going off.
+	 */
+	raw_cpu_write(watchdog_touch, true);
+}
+EXPORT_SYMBOL_GPL(buddy_cpu_touch_watchdog);
+
+static unsigned int watchdog_next_cpu(unsigned int cpu)
+{
+	cpumask_t cpus = watchdog_cpus;
+	unsigned int next_cpu;
+
+	next_cpu = cpumask_next(cpu, &cpus);
+	if (next_cpu >= nr_cpu_ids)
+		next_cpu = cpumask_first(&cpus);
+
+	if (next_cpu == cpu)
+		return nr_cpu_ids;
+
+	return next_cpu;
+}
+
+int watchdog_nmi_enable(unsigned int cpu)
+{
+	/*
+	 * The new cpu will be marked online before the first hrtimer interrupt
+	 * runs on it.  If another cpu tests for a hardlockup on the new cpu
+	 * before it has run its first hrtimer, it will get a false positive.
+	 * Touch the watchdog on the new cpu to delay the first check for at
+	 * least 3 sampling periods to guarantee one hrtimer has run on the new
+	 * cpu.
+	 */
+	per_cpu(watchdog_touch, cpu) = true;
+	smp_wmb();
+	cpumask_set_cpu(cpu, &watchdog_cpus);
+	return 0;
+}
+
+void watchdog_nmi_disable(unsigned int cpu)
+{
+	unsigned int next_cpu = watchdog_next_cpu(cpu);
+
+	/*
+	 * Offlining this cpu will cause the cpu before this one to start
+	 * checking the one after this one.  If this cpu just finished checking
+	 * the next cpu and updating hrtimer_interrupts_saved, and then the
+	 * previous cpu checks it within one sample period, it will trigger a
+	 * false positive.  Touch the watchdog on the next cpu to prevent it.
+	 */
+	if (next_cpu < nr_cpu_ids)
+		per_cpu(watchdog_touch, next_cpu) = true;
+	smp_wmb();
+	cpumask_clear_cpu(cpu, &watchdog_cpus);
+}
+
+static int is_hardlockup_buddy_cpu(unsigned int cpu)
+{
+	unsigned long hrint = per_cpu(hrtimer_interrupts, cpu);
+
+	if (per_cpu(hrtimer_interrupts_saved, cpu) == hrint)
+		return 1;
+
+	per_cpu(hrtimer_interrupts_saved, cpu) = hrint;
+	return 0;
+}
+
+void watchdog_check_hardlockup(void)
+{
+	unsigned int next_cpu;
+
+	/*
+	 * Test for hardlockups every 3 samples.  The sample period is
+	 *  watchdog_thresh * 2 / 5, so 3 samples gets us back to slightly over
+	 *  watchdog_thresh (over by 20%).
+	 */
+	if (__this_cpu_read(hrtimer_interrupts) % 3 != 0)
+		return;
+
+	/* check for a hardlockup on the next cpu */
+	next_cpu = watchdog_next_cpu(smp_processor_id());
+	if (next_cpu >= nr_cpu_ids)
+		return;
+
+	smp_rmb();
+
+	if (per_cpu(watchdog_touch, next_cpu) == true) {
+		per_cpu(watchdog_touch, next_cpu) = false;
+		return;
+	}
+
+	if (is_hardlockup_buddy_cpu(next_cpu)) {
+		/* only warn once */
+		if (per_cpu(hard_watchdog_warn, next_cpu) == true)
+			return;
+
+		if (hardlockup_panic)
+			panic("Watchdog detected hard LOCKUP on cpu %u", next_cpu);
+		else
+			WARN(1, "Watchdog detected hard LOCKUP on cpu %u", next_cpu);
+
+		per_cpu(hard_watchdog_warn, next_cpu) = true;
+	} else {
+		per_cpu(hard_watchdog_warn, next_cpu) = false;
+	}
+}
diff -ruN a/kernel/watchdog.c b/kernel/watchdog.c
--- a/kernel/watchdog.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/watchdog.c	2021-12-23 08:35:57.000000000 +0100
@@ -29,7 +29,7 @@
 
 static DEFINE_MUTEX(watchdog_mutex);
 
-#if defined(CONFIG_HARDLOCKUP_DETECTOR) || defined(CONFIG_HAVE_NMI_WATCHDOG)
+#if defined(CONFIG_HARDLOCKUP_DETECTOR_CORE) || defined(CONFIG_HAVE_NMI_WATCHDOG)
 # define WATCHDOG_DEFAULT	(SOFT_WATCHDOG_ENABLED | NMI_WATCHDOG_ENABLED)
 # define NMI_WATCHDOG_DEFAULT	1
 #else
@@ -47,7 +47,7 @@
 struct cpumask watchdog_cpumask __read_mostly;
 unsigned long *watchdog_cpumask_bits = cpumask_bits(&watchdog_cpumask);
 
-#ifdef CONFIG_HARDLOCKUP_DETECTOR
+#ifdef CONFIG_HARDLOCKUP_DETECTOR_CORE
 
 # ifdef CONFIG_SMP
 int __read_mostly sysctl_hardlockup_all_cpu_backtrace;
@@ -85,7 +85,9 @@
 }
 __setup("nmi_watchdog=", hardlockup_panic_setup);
 
-#endif /* CONFIG_HARDLOCKUP_DETECTOR */
+#endif /* CONFIG_HARDLOCKUP_DETECTOR_CORE */
+
+#ifdef CONFIG_HARDLOCKUP_DETECTOR
 
 /*
  * These functions can be overridden if an architecture implements its
@@ -106,6 +108,13 @@
 	hardlockup_detector_perf_disable();
 }
 
+#else
+
+int __weak watchdog_nmi_enable(unsigned int cpu) { return 0; }
+void __weak watchdog_nmi_disable(unsigned int cpu) { return; }
+
+#endif /* CONFIG_HARDLOCKUP_DETECTOR */
+
 /* Return 0, if a NMI watchdog is available. Error code otherwise */
 int __weak __init watchdog_nmi_probe(void)
 {
@@ -179,8 +188,8 @@
 static DEFINE_PER_CPU(unsigned long, watchdog_report_ts);
 static DEFINE_PER_CPU(struct hrtimer, watchdog_hrtimer);
 static DEFINE_PER_CPU(bool, softlockup_touch_sync);
-static DEFINE_PER_CPU(unsigned long, hrtimer_interrupts);
-static DEFINE_PER_CPU(unsigned long, hrtimer_interrupts_saved);
+DEFINE_PER_CPU(unsigned long, hrtimer_interrupts);
+DEFINE_PER_CPU(unsigned long, hrtimer_interrupts_saved);
 static unsigned long soft_lockup_nmi_warn;
 
 static int __init nowatchdog_setup(char *str)
@@ -364,6 +373,9 @@
 	/* kick the hardlockup detector */
 	watchdog_interrupt_count();
 
+	/* test for hardlockups */
+	watchdog_check_hardlockup();
+
 	/* kick the softlockup detector */
 	if (completion_done(this_cpu_ptr(&softlockup_completion))) {
 		reinit_completion(this_cpu_ptr(&softlockup_completion));
diff -ruN a/kernel/watchdog_hld.c b/kernel/watchdog_hld.c
--- a/kernel/watchdog_hld.c	2021-12-08 09:04:57.000000000 +0100
+++ b/kernel/watchdog_hld.c	2021-12-23 08:35:57.000000000 +0100
@@ -170,7 +170,17 @@
 	struct perf_event *evt;
 
 	wd_attr = &wd_hw_attr;
-	wd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh);
+	/*
+	 * TODO: revert this 3x factor once the NMI timer is constant
+	 * upstream and the fix backported here, see
+	 * https://partnerissuetracker.corp.google.com/issues/35587084
+	 * On some systems the turbo frequency can go higher than 5/2
+	 * times the TSC_MHz.  This makes this timer tick too fast and
+	 * trigger spurious hard LOCKUPs. Slow it down by a factor of
+	 * 3 as a temporary workaround.
+	 * See also https://crrev.com/c/502789/
+	 */
+	wd_attr->sample_period = hw_nmi_get_sample_period(watchdog_thresh) * 3;
 
 	/* Try to register using hardware perf events */
 	evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL,
diff -ruN a/lib/Kconfig.debug b/lib/Kconfig.debug
--- a/lib/Kconfig.debug	2021-12-08 09:04:57.000000000 +0100
+++ b/lib/Kconfig.debug	2021-12-23 08:35:57.000000000 +0100
@@ -1054,6 +1054,9 @@
 config HARDLOCKUP_CHECK_TIMESTAMP
 	bool
 
+config HARDLOCKUP_DETECTOR_CORE
+	bool
+
 #
 # arch/ can define HAVE_HARDLOCKUP_DETECTOR_ARCH to provide their own hard
 # lockup detector rather than the perf based detector.
@@ -1063,6 +1066,7 @@
 	depends on DEBUG_KERNEL && !S390
 	depends on HAVE_HARDLOCKUP_DETECTOR_PERF || HAVE_HARDLOCKUP_DETECTOR_ARCH
 	select LOCKUP_DETECTOR
+	select HARDLOCKUP_DETECTOR_CORE
 	select HARDLOCKUP_DETECTOR_PERF if HAVE_HARDLOCKUP_DETECTOR_PERF
 	help
 	  Say Y here to enable the kernel to act as a watchdog to detect
@@ -1073,9 +1077,22 @@
 	  chance to run.  The current stack trace is displayed upon detection
 	  and the system will stay locked up.
 
+config HARDLOCKUP_DETECTOR_BUDDY_CPU
+	bool "Buddy CPU hardlockup detector"
+	depends on DEBUG_KERNEL && SMP
+	depends on !HARDLOCKUP_DETECTOR && !HAVE_NMI_WATCHDOG
+	depends on !S390
+	select HARDLOCKUP_DETECTOR_CORE
+	select SOFTLOCKUP_DETECTOR
+	help
+	  Say Y here to enable a hardlockup detector where CPUs check
+	  each other for lockup. Each cpu uses its softlockup hrtimer
+	  to check that the next cpu is processing hrtimer interrupts by
+	  verifying that a counter is increasing.
+
 config BOOTPARAM_HARDLOCKUP_PANIC
 	bool "Panic (Reboot) On Hard Lockups"
-	depends on HARDLOCKUP_DETECTOR
+	depends on HARDLOCKUP_DETECTOR_CORE
 	help
 	  Say Y here to enable the kernel to panic on "hard lockups",
 	  which are bugs that cause the kernel to loop in kernel
@@ -1086,7 +1103,7 @@
 
 config BOOTPARAM_HARDLOCKUP_PANIC_VALUE
 	int
-	depends on HARDLOCKUP_DETECTOR
+	depends on HARDLOCKUP_DETECTOR_CORE
 	range 0 1
 	default 0 if !BOOTPARAM_HARDLOCKUP_PANIC
 	default 1 if BOOTPARAM_HARDLOCKUP_PANIC
diff -ruN a/lib/zstd/decompress.c b/lib/zstd/decompress.c
--- a/lib/zstd/decompress.c	2021-12-08 09:04:57.000000000 +0100
+++ b/lib/zstd/decompress.c	2021-12-23 08:35:58.000000000 +0100
@@ -1192,6 +1192,8 @@
 		else {
 			if (longOffsets) {
 				int const extraBits = ofBits - MIN(ofBits, STREAM_ACCUMULATOR_MIN);
+				// Check array bounds for gcc 4.9.x.
+				BUG_ON(ofCode >= ARRAY_SIZE(OF_base));
 				offset = OF_base[ofCode] + (BIT_readBitsFast(&seqState->DStream, ofBits - extraBits) << extraBits);
 				if (ZSTD_32bits() || extraBits)
 					BIT_reloadDStream(&seqState->DStream);
diff -ruN a/MAINTAINERS b/MAINTAINERS
--- a/MAINTAINERS	2021-12-08 09:04:57.000000000 +0100
+++ b/MAINTAINERS	2021-12-23 08:35:08.000000000 +0100
@@ -4426,6 +4426,12 @@
 F:	Documentation/devicetree/bindings/sound/google,cros-ec-codec.yaml
 F:	sound/soc/codecs/cros_ec_codec.*
 
+CHROMEOS EC UART DRIVER
+M:	Bhanu Prakash Maiya <bhanumaiya@chromium.org>
+R:	Enric Balletbo i Serra <enric.balletbo@collabora.com>
+S:	Maintained
+F:	drivers/platform/chrome/cros_ec_uart.c
+
 CHROMEOS EC SUBDRIVERS
 M:	Benson Leung <bleung@chromium.org>
 M:	Enric Balletbo i Serra <enric.balletbo@collabora.com>
@@ -11891,6 +11897,17 @@
 F:	drivers/net/dsa/mt7530.*
 F:	net/dsa/tag_mtk.c
 
+MEDIATEK T7XX 5G WWAN MODEM DRIVER
+M:	Chandrashekar Devegowda <chandrashekar.devegowda@intel.com>
+M:	Intel Corporation <linuxwwan@intel.com>
+R:	Chiranjeevi Rapolu <chiranjeevi.rapolu@linux.intel.com>
+R:	Liu Haijun <haijun.liu@mediatek.com>
+R:	M Chetan Kumar <m.chetan.kumar@linux.intel.com>
+R:	Ricardo Martinez <ricardo.martinez@linux.intel.com>
+L:	netdev@vger.kernel.org
+S:	Maintained
+F:	drivers/net/wwan/t7xx/
+
 MEDIATEK USB3 DRD IP DRIVER
 M:	Chunfeng Yun <chunfeng.yun@mediatek.com>
 L:	linux-usb@vger.kernel.org
@@ -13271,6 +13288,12 @@
 F:	Documentation/scsi/NinjaSCSI.rst
 F:	drivers/scsi/nsp32*
 
+NINTENDO HID DRIVER
+M:	Daniel J. Ogorchock <djogorchock@gmail.com>
+L:	linux-input@vger.kernel.org
+S:	Maintained
+F:	drivers/hid/hid-nintendo*
+
 NIOS2 ARCHITECTURE
 M:	Dinh Nguyen <dinguyen@kernel.org>
 S:	Maintained
@@ -15895,6 +15918,12 @@
 S:	Maintained
 F:	drivers/net/wireless/realtek/rtw88/
 
+REALTEK WIRELESS DRIVER (rtw89)
+M:	Ping-Ke Shih <pkshih@realtek.com>
+L:	linux-wireless@vger.kernel.org
+S:	Maintained
+F:	drivers/net/wireless/realtek/rtw89/
+
 REDPINE WIRELESS DRIVER
 M:	Amitkumar Karwar <amitkarwar@gmail.com>
 M:	Siva Rebbagondla <siva8118@gmail.com>
diff -ruN a/mm/Kconfig b/mm/Kconfig
--- a/mm/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/Kconfig	2021-12-23 08:35:58.000000000 +0100
@@ -231,6 +231,17 @@
 	  those pages to another entity, such as a hypervisor, so that the
 	  memory can be freed within the host for other uses.
 
+config PROCESS_RECLAIM
+	bool "Enable process reclaim"
+	depends on PROC_FS && MMU
+	help
+	 It allows to reclaim pages of the process by /proc/pid/reclaim.
+
+	 (echo file > /proc/PID/reclaim) reclaims file-backed pages only.
+	 (echo anon > /proc/PID/reclaim) reclaims anonymous pages only.
+	 (echo all > /proc/PID/reclaim) reclaims all pages.
+
+	 Any other value is ignored.
 #
 # support for page migration
 #
@@ -321,6 +332,23 @@
 	  This value can be changed after boot using the
 	  /proc/sys/vm/mmap_min_addr tunable.
 
+config MMAP_NOEXEC_TAINT
+	int "Turns on tainting of mmap()d files from noexec mountpoints"
+	default 1 if MMU
+	default 0 if !MMU
+	help
+	  By default, the ability to change the protections of a virtual
+	  memory area to allow execution depend on if the vma has the
+	  VM_MAYEXEC flag.  When mapping regions from files, VM_MAYEXEC
+	  will be unset if the containing mountpoint is mounted MNT_NOEXEC.
+	  By setting the value to 0, any mmap()d region may be later
+	  mprotect()d with PROT_EXEC.
+
+	  If unsure, keep the value set to 1.
+
+	  This value can be changed after boot using the
+	  /proc/sys/vm/mmap_noexec_taint tunable.
+
 config ARCH_SUPPORTS_MEMORY_FAILURE
 	bool
 
@@ -897,6 +925,17 @@
 config SECRETMEM
 	def_bool ARCH_HAS_SET_DIRECT_MAP && !EMBEDDED
 
+config LOW_MEM_NOTIFY
+	bool "Create device that lets processes detect low-memory conditions"
+	default n
+	help
+	  A process can poll the /dev/low_mem device to be notified of
+	  low-memory conditions.  The process can then attempt to free memory
+	  before a OOM condition develops and the OOM killer takes over.  This
+	  is meant to be used in systems with no or very little swap space.  In
+	  the presence of large swap space, the system is likely to become
+	  unusable before the OOM killer is triggered.
+
 source "mm/damon/Kconfig"
 
 endmenu
diff -ruN a/mm/low-mem-notify.c b/mm/low-mem-notify.c
--- a/mm/low-mem-notify.c	1970-01-01 01:00:00.000000000 +0100
+++ b/mm/low-mem-notify.c	2021-12-23 08:35:58.000000000 +0100
@@ -0,0 +1,396 @@
+/*
+ * mm/low-mem-notify.c
+ *
+ * Sends low-memory notifications to processes via /dev/low-mem.
+ *
+ * Copyright (C) 2012 The Chromium OS Authors
+ * This program is free software, released under the GPL.
+ * Based on a proposal by Minchan Kim
+ *
+ * A process that polls /dev/low-mem is notified of a low-memory situation.
+ * The intent is to allow the process to free some memory before the OOM killer
+ * is invoked.
+ *
+ * A low-memory condition is estimated by subtracting anonymous memory
+ * (i.e. process data segments), kernel memory, and a fixed amount of
+ * file-backed memory from total memory.  This is just a heuristic, as in
+ * general we don't know how much memory can be reclaimed before we try to
+ * reclaim it, and that's too expensive or too late.
+ *
+ * This is tailored to Chromium OS, where a single program (the browser)
+ * controls most of the memory, and (currently) no swap space is used.
+ */
+
+
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/wait.h>
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/mm.h>
+#include <linux/ctype.h>
+#include <linux/ratelimit.h>
+#include <linux/stddef.h>
+#include <linux/swap.h>
+
+#define MB (1 << 20)
+
+static DECLARE_WAIT_QUEUE_HEAD(low_mem_wait);
+static atomic_t low_mem_state = ATOMIC_INIT(0);
+
+/* We support up to this many different thresholds. */
+#define LOW_MEM_THRESHOLD_MAX 5
+
+/* This is a list of thresholds in pages and should be in ascending order. */
+static unsigned long low_mem_thresholds[LOW_MEM_THRESHOLD_MAX] = {
+	50 * MB / PAGE_SIZE
+};
+static unsigned int low_mem_threshold_count = 1;
+
+static bool low_mem_margin_enabled = true;
+static unsigned int low_mem_ram_vs_swap_weight = 4;
+
+void low_mem_notify(void)
+{
+	atomic_set(&low_mem_state, true);
+	wake_up(&low_mem_wait);
+}
+
+/*
+ * Compute available memory used by files that can be reclaimed quickly.
+ */
+static unsigned long get_available_file_mem(void)
+{
+	unsigned long file_mem =
+			global_node_page_state(NR_ACTIVE_FILE) +
+			global_node_page_state(NR_INACTIVE_FILE);
+	unsigned long dirty_mem = global_node_page_state(NR_FILE_DIRTY);
+	unsigned long min_file_mem = min_filelist_kbytes >> (PAGE_SHIFT - 10);
+	unsigned long clean_file_mem = file_mem > dirty_mem ?
+			file_mem - dirty_mem : 0;
+	/* Conservatively estimate the amount of available_file_mem */
+	unsigned long available_file_mem = clean_file_mem > min_file_mem ?
+			clean_file_mem - min_file_mem : 0;
+	return available_file_mem;
+}
+
+/*
+ * Available anonymous memory.
+ */
+static unsigned long get_available_anon_mem(void)
+{
+	return global_node_page_state(NR_ACTIVE_ANON) +
+		global_node_page_state(NR_INACTIVE_ANON);
+}
+
+/*
+ * Compute "available" memory, that is either free memory or memory that can be
+ * reclaimed quickly, adjusted for the presence of swap.
+ */
+static unsigned long get_available_mem_adj(void)
+{
+	/* free_mem is completely unallocated; clean file-backed memory
+	 * (file_mem - dirty_mem) is easy to reclaim, except for the last
+	 * min_filelist_kbytes. totalreserve_pages is the reserve of pages that
+	 * are not available to user space.
+	 */
+	unsigned long raw_free_mem = global_zone_page_state(NR_FREE_PAGES);
+	unsigned long free_mem = raw_free_mem > totalreserve_pages ?
+			raw_free_mem - totalreserve_pages : 0;
+	unsigned long available_mem = free_mem + get_available_file_mem();
+	unsigned long swappable_pages = min_t(unsigned long,
+			get_nr_swap_pages(), get_available_anon_mem());
+	/*
+	 * The contribution of swap is reduced by a factor of
+	 * low_mem_ram_vs_swap_weight.
+	 */
+	return available_mem + swappable_pages / low_mem_ram_vs_swap_weight;
+}
+
+#ifdef CONFIG_SYSFS
+static void low_mem_threshold_notify(void);
+#else
+static void low_mem_threshold_notify(void)
+{
+}
+#endif
+
+/*
+ * Returns TRUE if we are in a low memory state.
+ */
+bool low_mem_check(void)
+{
+	static bool was_low_mem;	/* = false, as per style guide */
+	static atomic_t in_low_mem_check = ATOMIC_INIT(0);
+	/* last observed threshold */
+	static unsigned int low_mem_threshold_last = UINT_MAX;
+	/* Limit logging low memory to once per second. */
+	static DEFINE_RATELIMIT_STATE(low_mem_logging_ratelimit, 1 * HZ, 1);
+
+	/* We declare a low-memory condition when a combination of RAM and swap
+	 * space is low.
+	 */
+	unsigned long available_mem = get_available_mem_adj();
+	/*
+	 * For backwards compatibility with the older margin interface, we will
+	 * trigger the /dev/chromeos-low_mem device when we are below the
+	 * lowest threshold
+	 */
+	bool is_low_mem = available_mem < low_mem_thresholds[0];
+	unsigned int threshold_lowest = UINT_MAX;
+	int i;
+
+	if (!low_mem_margin_enabled)
+		return false;
+
+	if (atomic_read(&in_low_mem_check) || atomic_xchg(&in_low_mem_check, 1))
+		return was_low_mem;
+
+	if (unlikely(is_low_mem && !was_low_mem) &&
+	    __ratelimit(&low_mem_logging_ratelimit)) {
+		pr_info("entering low_mem (avail RAM = %lu kB, avail swap %lu kB, avail file %lu kB, anon mem: %lu kB)\n",
+			available_mem * PAGE_SIZE / 1024,
+			get_nr_swap_pages() * PAGE_SIZE / 1024,
+			get_available_file_mem() * PAGE_SIZE / 1024,
+			get_available_anon_mem() * PAGE_SIZE / 1024);
+	}
+	was_low_mem = is_low_mem;
+
+	if (is_low_mem)
+		low_mem_notify();
+
+	for (i = 0; i < low_mem_threshold_count; i++) {
+		if (available_mem < low_mem_thresholds[i]) {
+			threshold_lowest = i;
+			break;
+		}
+	}
+
+	/* we crossed one or more thresholds */
+	if (unlikely(threshold_lowest < low_mem_threshold_last))
+		low_mem_threshold_notify();
+
+	low_mem_threshold_last = threshold_lowest;
+
+	atomic_set(&in_low_mem_check, 0);
+
+	return is_low_mem;
+}
+
+static int low_mem_notify_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static int low_mem_notify_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static __poll_t low_mem_notify_poll(struct file *file, poll_table *wait)
+{
+	/* Update state to reflect any recent freeing. */
+	atomic_set(&low_mem_state, low_mem_check());
+
+	poll_wait(file, &low_mem_wait, wait);
+
+	if (low_mem_margin_enabled && atomic_read(&low_mem_state))
+		return POLLIN;
+
+	return 0;
+}
+
+const struct file_operations low_mem_notify_fops = {
+	.open = low_mem_notify_open,
+	.release = low_mem_notify_release,
+	.poll = low_mem_notify_poll,
+};
+EXPORT_SYMBOL(low_mem_notify_fops);
+
+#ifdef CONFIG_SYSFS
+
+#define LOW_MEM_ATTR(_name)				      \
+	static struct kobj_attribute low_mem_##_name##_attr = \
+		__ATTR(_name, 0644, low_mem_##_name##_show,   \
+		       low_mem_##_name##_store)
+
+static ssize_t low_mem_margin_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buf)
+{
+	int i;
+	ssize_t written = 0;
+
+	if (!low_mem_margin_enabled || !low_mem_threshold_count)
+		return sprintf(buf, "off\n");
+
+	for (i = 0; i < low_mem_threshold_count; i++)
+		written += sprintf(buf + written, "%lu ",
+			    low_mem_thresholds[i] * PAGE_SIZE / MB);
+	written += sprintf(buf + written, "\n");
+	return written;
+}
+
+static ssize_t low_mem_margin_store(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buf, size_t count)
+{
+	int i = 0, consumed = 0;
+	const char *start = buf;
+	char *endp;
+	unsigned long thresholds[LOW_MEM_THRESHOLD_MAX];
+
+	memset(thresholds, 0, sizeof(thresholds));
+	/*
+	 * Even though the API does not say anything about this, the string in
+	 * buf is zero-terminated (as long as count < PAGE_SIZE) because buf is
+	 * a newly allocated zero-filled page.  Most other sysfs handlers rely
+	 * on this too.
+	 */
+	if (strncmp("off", buf, 3) == 0) {
+		pr_info("low_mem: disabling notifier\n");
+		low_mem_margin_enabled = false;
+		return count;
+	}
+	if (strncmp("on", buf, 2) == 0) {
+		pr_info("low_mem: enabling notifier\n");
+		low_mem_margin_enabled = true;
+		return count;
+	}
+	/*
+	 * This takes a space separated list of thresholds in ascending order,
+	 * and a trailing newline is optional.
+	 */
+	while (consumed < count) {
+		if (i >= LOW_MEM_THRESHOLD_MAX) {
+			pr_warn("low-mem: too many thresholds");
+			return -EINVAL;
+		}
+		/* special case for trailing newline */
+		if (*start == '\n')
+			break;
+
+		thresholds[i] = simple_strtoul(start, &endp, 0);
+		if ((endp == start) && *endp != '\n')
+			return -EINVAL;
+
+		/* make sure each is larger than the last one */
+		if (i && thresholds[i] <= thresholds[i - 1]) {
+			pr_warn("low-mem: thresholds not in increasing order: %lu then %lu\n",
+				thresholds[i - 1], thresholds[i]);
+			return -EINVAL;
+		}
+
+		if (thresholds[i] * (MB / PAGE_SIZE) > totalram_pages()) {
+			pr_warn("low-mem: threshold too high\n");
+			return -EINVAL;
+		}
+
+		consumed += endp - start + 1;
+		start = endp + 1;
+		i++;
+	}
+
+	low_mem_threshold_count = i;
+	low_mem_margin_enabled = !!low_mem_threshold_count;
+
+	/* Convert to pages outside the allocator fast path. */
+	for (i = 0; i < low_mem_threshold_count; i++) {
+		low_mem_thresholds[i] = thresholds[i] * (MB / PAGE_SIZE);
+		pr_info("low_mem: threshold[%d] %lu MB\n", i,
+			low_mem_thresholds[i] * PAGE_SIZE / MB);
+	}
+
+	return count;
+}
+LOW_MEM_ATTR(margin);
+
+static ssize_t low_mem_ram_vs_swap_weight_show(struct kobject *kobj,
+					       struct kobj_attribute *attr,
+					       char *buf)
+{
+	return sprintf(buf, "%u\n", low_mem_ram_vs_swap_weight);
+}
+
+static ssize_t low_mem_ram_vs_swap_weight_store(struct kobject *kobj,
+						struct kobj_attribute *attr,
+						const char *buf, size_t count)
+{
+	int err;
+	unsigned weight;
+
+	err = kstrtouint(buf, 10, &weight);
+	if (err)
+		return -EINVAL;
+
+	/* The special value 0 represents infinity. */
+	low_mem_ram_vs_swap_weight = !weight ? -1 : weight;
+	pr_info("low_mem: setting ram weight to %u\n",
+		low_mem_ram_vs_swap_weight);
+	return count;
+}
+LOW_MEM_ATTR(ram_vs_swap_weight);
+
+static ssize_t low_mem_available_show(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      char *buf)
+{
+	unsigned long available_mem = get_available_mem_adj();
+
+	return sprintf(buf, "%lu\n",
+		       available_mem / (MB / PAGE_SIZE));
+}
+
+static ssize_t low_mem_available_store(struct kobject *kobj,
+				       struct kobj_attribute *attr,
+				       const char *buf, size_t count)
+{
+	return -EINVAL;
+}
+LOW_MEM_ATTR(available);
+
+static struct attribute *low_mem_attrs[] = {
+	&low_mem_margin_attr.attr,
+	&low_mem_ram_vs_swap_weight_attr.attr,
+	&low_mem_available_attr.attr,
+	NULL,
+};
+
+static struct attribute_group low_mem_attr_group = {
+	.attrs = low_mem_attrs,
+	.name = "chromeos-low_mem",
+};
+
+static struct kernfs_node *low_mem_available_dirent;
+
+static void low_mem_threshold_notify(void)
+{
+	if (low_mem_available_dirent)
+		sysfs_notify_dirent(low_mem_available_dirent);
+}
+
+static int __init low_mem_init(void)
+{
+	int err;
+	struct kernfs_node *low_mem_node;
+
+	err = sysfs_create_group(mm_kobj, &low_mem_attr_group);
+	if (err) {
+		pr_err("low_mem: register sysfs failed\n");
+		return err;
+	}
+
+	low_mem_node = sysfs_get_dirent(mm_kobj->sd, "chromeos-low_mem");
+	if (low_mem_node) {
+		low_mem_available_dirent =
+		    sysfs_get_dirent(low_mem_node, "available");
+		sysfs_put(low_mem_node);
+	}
+
+	if (!low_mem_available_dirent)
+		pr_warn("unable to find dirent for \"available\" attribute\n");
+
+	return 0;
+}
+module_init(low_mem_init)
+
+#endif
diff -ruN a/mm/madvise.c b/mm/madvise.c
--- a/mm/madvise.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/madvise.c	2021-12-23 08:35:58.000000000 +0100
@@ -138,7 +138,7 @@
 	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
 	*prev = vma_merge(mm, *prev, start, end, new_flags, vma->anon_vma,
 			  vma->vm_file, pgoff, vma_policy(vma),
-			  vma->vm_userfaultfd_ctx);
+			  vma->vm_userfaultfd_ctx, vma_get_anon_name(vma));
 	if (*prev) {
 		vma = *prev;
 		goto success;
diff -ruN a/mm/Makefile b/mm/Makefile
--- a/mm/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/Makefile	2021-12-23 08:35:58.000000000 +0100
@@ -117,6 +117,7 @@
 obj-$(CONFIG_CMA_SYSFS) += cma_sysfs.o
 obj-$(CONFIG_USERFAULTFD) += userfaultfd.o
 obj-$(CONFIG_IDLE_PAGE_TRACKING) += page_idle.o
+obj-$(CONFIG_LOW_MEM_NOTIFY) += low-mem-notify.o
 obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o
 obj-$(CONFIG_DAMON) += damon/
 obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
diff -ruN a/mm/mempolicy.c b/mm/mempolicy.c
--- a/mm/mempolicy.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/mempolicy.c	2021-12-23 08:35:58.000000000 +0100
@@ -810,7 +810,8 @@
 			((vmstart - vma->vm_start) >> PAGE_SHIFT);
 		prev = vma_merge(mm, prev, vmstart, vmend, vma->vm_flags,
 				 vma->anon_vma, vma->vm_file, pgoff,
-				 new_pol, vma->vm_userfaultfd_ctx);
+				 new_pol, vma->vm_userfaultfd_ctx,
+				 vma_get_anon_name(vma));
 		if (prev) {
 			vma = prev;
 			next = vma->vm_next;
diff -ruN a/mm/mlock.c b/mm/mlock.c
--- a/mm/mlock.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/mlock.c	2021-12-23 08:35:58.000000000 +0100
@@ -511,7 +511,7 @@
 	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
 	*prev = vma_merge(mm, *prev, start, end, newflags, vma->anon_vma,
 			  vma->vm_file, pgoff, vma_policy(vma),
-			  vma->vm_userfaultfd_ctx);
+			  vma->vm_userfaultfd_ctx, vma_get_anon_name(vma));
 	if (*prev) {
 		vma = *prev;
 		goto success;
diff -ruN a/mm/mmap.c b/mm/mmap.c
--- a/mm/mmap.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/mmap.c	2021-12-23 08:35:58.000000000 +0100
@@ -1029,7 +1029,8 @@
  */
 static inline int is_mergeable_vma(struct vm_area_struct *vma,
 				struct file *file, unsigned long vm_flags,
-				struct vm_userfaultfd_ctx vm_userfaultfd_ctx)
+				struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
+				const char __user *anon_name)
 {
 	/*
 	 * VM_SOFTDIRTY should not prevent from VMA merging, if we
@@ -1047,6 +1048,8 @@
 		return 0;
 	if (!is_mergeable_vm_userfaultfd_ctx(vma, vm_userfaultfd_ctx))
 		return 0;
+	if (vma_get_anon_name(vma) != anon_name)
+		return 0;
 	return 1;
 }
 
@@ -1079,9 +1082,10 @@
 can_vma_merge_before(struct vm_area_struct *vma, unsigned long vm_flags,
 		     struct anon_vma *anon_vma, struct file *file,
 		     pgoff_t vm_pgoff,
-		     struct vm_userfaultfd_ctx vm_userfaultfd_ctx)
+		     struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
+		     const char __user *anon_name)
 {
-	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx) &&
+	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx, anon_name) &&
 	    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
 		if (vma->vm_pgoff == vm_pgoff)
 			return 1;
@@ -1100,9 +1104,10 @@
 can_vma_merge_after(struct vm_area_struct *vma, unsigned long vm_flags,
 		    struct anon_vma *anon_vma, struct file *file,
 		    pgoff_t vm_pgoff,
-		    struct vm_userfaultfd_ctx vm_userfaultfd_ctx)
+		    struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
+		    const char __user *anon_name)
 {
-	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx) &&
+	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx, anon_name) &&
 	    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
 		pgoff_t vm_pglen;
 		vm_pglen = vma_pages(vma);
@@ -1113,9 +1118,9 @@
 }
 
 /*
- * Given a mapping request (addr,end,vm_flags,file,pgoff), figure out
- * whether that can be merged with its predecessor or its successor.
- * Or both (it neatly fills a hole).
+ * Given a mapping request (addr,end,vm_flags,file,pgoff,anon_name),
+ * figure out whether that can be merged with its predecessor or its
+ * successor.  Or both (it neatly fills a hole).
  *
  * In most cases - when called for mmap, brk or mremap - [addr,end) is
  * certain not to be mapped by the time vma_merge is called; but when
@@ -1160,7 +1165,8 @@
 			unsigned long end, unsigned long vm_flags,
 			struct anon_vma *anon_vma, struct file *file,
 			pgoff_t pgoff, struct mempolicy *policy,
-			struct vm_userfaultfd_ctx vm_userfaultfd_ctx)
+			struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
+			const char __user *anon_name)
 {
 	pgoff_t pglen = (end - addr) >> PAGE_SHIFT;
 	struct vm_area_struct *area, *next;
@@ -1190,7 +1196,8 @@
 			mpol_equal(vma_policy(prev), policy) &&
 			can_vma_merge_after(prev, vm_flags,
 					    anon_vma, file, pgoff,
-					    vm_userfaultfd_ctx)) {
+					    vm_userfaultfd_ctx,
+					    anon_name)) {
 		/*
 		 * OK, it can.  Can we now merge in the successor as well?
 		 */
@@ -1199,7 +1206,8 @@
 				can_vma_merge_before(next, vm_flags,
 						     anon_vma, file,
 						     pgoff+pglen,
-						     vm_userfaultfd_ctx) &&
+						     vm_userfaultfd_ctx,
+						     anon_name) &&
 				is_mergeable_anon_vma(prev->anon_vma,
 						      next->anon_vma, NULL)) {
 							/* cases 1, 6 */
@@ -1222,7 +1230,8 @@
 			mpol_equal(policy, vma_policy(next)) &&
 			can_vma_merge_before(next, vm_flags,
 					     anon_vma, file, pgoff+pglen,
-					     vm_userfaultfd_ctx)) {
+					     vm_userfaultfd_ctx,
+					     anon_name)) {
 		if (prev && addr < prev->vm_end)	/* case 4 */
 			err = __vma_adjust(prev, prev->vm_start,
 					 addr, prev->vm_pgoff, NULL, next);
@@ -1524,7 +1533,8 @@
 			if (path_noexec(&file->f_path)) {
 				if (vm_flags & VM_EXEC)
 					return -EPERM;
-				vm_flags &= ~VM_MAYEXEC;
+				if (sysctl_mmap_noexec_taint)
+					vm_flags &= ~VM_MAYEXEC;
 			}
 
 			if (!file->f_op->mmap)
@@ -1755,7 +1765,7 @@
 	 * Can we just expand an old mapping?
 	 */
 	vma = vma_merge(mm, prev, addr, addr + len, vm_flags,
-			NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX);
+			NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX, NULL);
 	if (vma)
 		goto out;
 
@@ -1804,7 +1814,8 @@
 		 */
 		if (unlikely(vm_flags != vma->vm_flags && prev)) {
 			merge = vma_merge(mm, prev, vma->vm_start, vma->vm_end, vma->vm_flags,
-				NULL, vma->vm_file, vma->vm_pgoff, NULL, NULL_VM_UFFD_CTX);
+				NULL, vma->vm_file, vma->vm_pgoff, NULL, NULL_VM_UFFD_CTX,
+				vma_get_anon_name(vma));
 			if (merge) {
 				/* ->mmap() can change vma->vm_file and fput the original file. So
 				 * fput the vma->vm_file here or we would add an extra fput for file
@@ -3057,7 +3068,7 @@
 
 	/* Can we just expand an old private anonymous mapping? */
 	vma = vma_merge(mm, prev, addr, addr + len, flags,
-			NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX);
+			NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX, NULL);
 	if (vma)
 		goto out;
 
@@ -3250,7 +3261,7 @@
 		return NULL;	/* should never get here */
 	new_vma = vma_merge(mm, prev, addr, addr + len, vma->vm_flags,
 			    vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
-			    vma->vm_userfaultfd_ctx);
+			    vma->vm_userfaultfd_ctx, vma_get_anon_name(vma));
 	if (new_vma) {
 		/*
 		 * Source vma may have been merged into new_vma
diff -ruN a/mm/mprotect.c b/mm/mprotect.c
--- a/mm/mprotect.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/mprotect.c	2021-12-23 08:35:58.000000000 +0100
@@ -464,7 +464,7 @@
 	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
 	*pprev = vma_merge(mm, *pprev, start, end, newflags,
 			   vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
-			   vma->vm_userfaultfd_ctx);
+			   vma->vm_userfaultfd_ctx, vma_get_anon_name(vma));
 	if (*pprev) {
 		vma = *pprev;
 		VM_WARN_ON((vma->vm_flags ^ newflags) & ~VM_SOFTDIRTY);
diff -ruN a/mm/page_alloc.c b/mm/page_alloc.c
--- a/mm/page_alloc.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/page_alloc.c	2021-12-23 08:35:58.000000000 +0100
@@ -72,6 +72,7 @@
 #include <linux/padata.h>
 #include <linux/khugepaged.h>
 #include <linux/buffer_head.h>
+#include <linux/low-mem-notify.h>
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
 #include <asm/div64.h>
@@ -4768,6 +4769,7 @@
 	 * several times in the row.
 	 */
 	if (*no_progress_loops > MAX_RECLAIM_RETRIES) {
+		low_mem_notify();
 		/* Before OOM, exhaust highatomic_reserve */
 		return unreserve_highatomic_pageblock(ac, true);
 	}
@@ -5371,6 +5373,8 @@
 			&alloc_gfp, &alloc_flags))
 		return NULL;
 
+	low_mem_check();
+
 	/*
 	 * Forbid the first pass from falling back to types that fragment
 	 * memory until all local zones are considered.
diff -ruN a/mm/shmem.c b/mm/shmem.c
--- a/mm/shmem.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/shmem.c	2021-12-23 08:35:58.000000000 +0100
@@ -3174,7 +3174,8 @@
 
 static int shmem_xattr_handler_get(const struct xattr_handler *handler,
 				   struct dentry *unused, struct inode *inode,
-				   const char *name, void *buffer, size_t size)
+				   const char *name, void *buffer, size_t size,
+				   int flags)
 {
 	struct shmem_inode_info *info = SHMEM_I(inode);
 
diff -ruN a/mm/swap.c b/mm/swap.c
--- a/mm/swap.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/swap.c	2021-12-23 08:35:58.000000000 +0100
@@ -553,6 +553,7 @@
 		del_page_from_lru_list(page, lruvec);
 		ClearPageActive(page);
 		ClearPageReferenced(page);
+		test_and_clear_page_young(page);
 		add_page_to_lru_list(page, lruvec);
 
 		__count_vm_events(PGDEACTIVATE, nr_pages);
diff -ruN a/mm/swapfile.c b/mm/swapfile.c
--- a/mm/swapfile.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/swapfile.c	2021-12-23 08:35:58.000000000 +0100
@@ -2527,6 +2527,8 @@
 	struct filename *pathname;
 	int err, found = 0;
 	unsigned int old_block_size;
+	struct path path_holder;
+	struct path *victim_path = NULL;
 
 	if (!capable(CAP_SYS_ADMIN))
 		return -EPERM;
@@ -2539,10 +2541,16 @@
 
 	victim = file_open_name(pathname, O_RDWR|O_LARGEFILE, 0);
 	err = PTR_ERR(victim);
-	if (IS_ERR(victim))
-		goto out;
-
-	mapping = victim->f_mapping;
+	if (IS_ERR(victim)) {
+		/* Fallback to just the inode mapping if possible. */
+		if (kern_path(pathname->name, LOOKUP_FOLLOW, &path_holder))
+			goto out;  /* Propogate the original err. */
+		victim_path = &path_holder;
+		mapping = victim_path->dentry->d_inode->i_mapping;
+		victim = NULL;
+	} else {
+		mapping = victim->f_mapping;
+	}
 	spin_lock(&swap_lock);
 	plist_for_each_entry(p, &swap_active_head, list) {
 		if (p->flags & SWP_WRITEOK) {
@@ -2690,7 +2698,10 @@
 	wake_up_interruptible(&proc_poll_wait);
 
 out_dput:
-	filp_close(victim, NULL);
+	if (victim)
+		filp_close(victim, NULL);
+	if (victim_path)
+		path_put(victim_path);
 out:
 	putname(pathname);
 	return err;
@@ -2893,11 +2904,17 @@
 	return p;
 }
 
+/* This sysctl is only exposed when CONFIG_DISK_BASED_SWAP is enabled. */
+int sysctl_disk_based_swap;
+
 static int claim_swapfile(struct swap_info_struct *p, struct inode *inode)
 {
 	int error;
 
+	/* On Chromium OS, we only support zram swap devices. */
 	if (S_ISBLK(inode->i_mode)) {
+		char name[BDEVNAME_SIZE];
+
 		p->bdev = blkdev_get_by_dev(inode->i_rdev,
 				   FMODE_READ | FMODE_WRITE | FMODE_EXCL, p);
 		if (IS_ERR(p->bdev)) {
@@ -2905,6 +2922,12 @@
 			p->bdev = NULL;
 			return error;
 		}
+		bdevname(p->bdev, name);
+		if (strncmp(name, "zram", strlen("zram"))) {
+			iput(p->bdev->bd_inode);
+			p->bdev = NULL;
+			return -EINVAL;
+		}
 		p->old_block_size = block_size(p->bdev);
 		error = set_blocksize(p->bdev, PAGE_SIZE);
 		if (error < 0)
@@ -2918,6 +2941,8 @@
 			return -EINVAL;
 		p->flags |= SWP_BLKDEV;
 	} else if (S_ISREG(inode->i_mode)) {
+		if (!sysctl_disk_based_swap)
+			return -EINVAL;
 		p->bdev = inode->i_sb->s_bdev;
 	}
 
diff -ruN a/mm/util.c b/mm/util.c
--- a/mm/util.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/util.c	2021-12-23 08:35:58.000000000 +0100
@@ -764,6 +764,7 @@
 int sysctl_overcommit_ratio __read_mostly = 50;
 unsigned long sysctl_overcommit_kbytes __read_mostly;
 int sysctl_max_map_count __read_mostly = DEFAULT_MAX_MAP_COUNT;
+int sysctl_mmap_noexec_taint __read_mostly = CONFIG_MMAP_NOEXEC_TAINT;
 unsigned long sysctl_user_reserve_kbytes __read_mostly = 1UL << 17; /* 128MB */
 unsigned long sysctl_admin_reserve_kbytes __read_mostly = 1UL << 13; /* 8MB */
 
diff -ruN a/mm/vmscan.c b/mm/vmscan.c
--- a/mm/vmscan.c	2021-12-08 09:04:57.000000000 +0100
+++ b/mm/vmscan.c	2021-12-23 08:35:58.000000000 +0100
@@ -2563,6 +2563,28 @@
 };
 
 /*
+ * Low watermark used to prevent fscache thrashing during low memory.
+ */
+int min_filelist_kbytes;
+
+/*
+ * Check low watermark used to prevent fscache thrashing during low memory.
+ */
+static int file_is_low(struct lruvec *lruvec)
+{
+	unsigned long size;
+
+	if (!mem_cgroup_disabled())
+		return false;
+
+	size = node_page_state(lruvec_pgdat(lruvec), NR_ACTIVE_FILE);
+	size += node_page_state(lruvec_pgdat(lruvec), NR_INACTIVE_FILE);
+	size <<= (PAGE_SHIFT - 10);
+
+	return size < min_filelist_kbytes;
+}
+
+/*
  * Determine how aggressively the anon and file LRU lists should be
  * scanned.  The relative value of each set of LRU lists is determined
  * by looking at the fraction of the pages scanned we did rotate back
@@ -2602,6 +2624,15 @@
 		goto out;
 	}
 
+	/*
+	 * Do not scan file pages when swap is allowed by __GFP_IO and
+	 * file page count is low.
+	 */
+	if ((sc->gfp_mask & __GFP_IO) && file_is_low(lruvec)) {
+		scan_balance = SCAN_ANON;
+		goto out;
+	}
+
 	/*
 	 * Do not apply any pressure balancing cleverness when the
 	 * system is close to OOM, scan both anon and file equally
diff -ruN a/net/core/dev_ioctl.c b/net/core/dev_ioctl.c
--- a/net/core/dev_ioctl.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/core/dev_ioctl.c	2021-12-23 08:35:58.000000000 +0100
@@ -8,6 +8,7 @@
 #include <linux/wireless.h>
 #include <linux/if_bridge.h>
 #include <net/dsa.h>
+#include <net/sock.h>
 #include <net/wext.h>
 
 /*
@@ -577,7 +578,7 @@
 	case SIOCBRADDIF:
 	case SIOCBRDELIF:
 	case SIOCSHWTSTAMP:
-		if (!ns_capable(net->user_ns, CAP_NET_ADMIN))
+		if (!android_ns_capable(net, CAP_NET_ADMIN))
 			return -EPERM;
 		fallthrough;
 	case SIOCBONDSLAVEINFOQUERY:
diff -ruN a/net/core/sock.c b/net/core/sock.c
--- a/net/core/sock.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/core/sock.c	2021-12-23 08:35:58.000000000 +0100
@@ -114,6 +114,9 @@
 #include <linux/memcontrol.h>
 #include <linux/prefetch.h>
 #include <linux/compat.h>
+#include <linux/cred.h>
+#include <linux/uidgid.h>
+#include <linux/android_aid.h>
 
 #include <linux/uaccess.h>
 
@@ -194,6 +197,40 @@
 }
 EXPORT_SYMBOL(sk_net_capable);
 
+static bool in_android_group(struct user_namespace *user, gid_t gid)
+{
+	kgid_t kgid = make_kgid(user, gid);
+
+	if (!gid_valid(kgid))
+		return false;
+	return in_egroup_p(kgid);
+}
+
+bool inet_sk_allowed(struct net *net, gid_t gid)
+{
+	if (!net->core.sysctl_android_paranoid ||
+	    ns_capable(net->user_ns, CAP_NET_RAW))
+		return true;
+	return in_android_group(net->user_ns, gid);
+}
+EXPORT_SYMBOL(inet_sk_allowed);
+
+bool android_ns_capable(struct net *net, int cap)
+{
+	if (ns_capable(net->user_ns, cap))
+		return true;
+	if (!net->core.sysctl_android_paranoid)
+		return false;
+	if (cap == CAP_NET_RAW &&
+	    in_android_group(net->user_ns, AID_NET_RAW))
+		return true;
+	if (cap == CAP_NET_ADMIN &&
+	    in_android_group(net->user_ns, AID_NET_ADMIN))
+		return true;
+	return false;
+}
+EXPORT_SYMBOL(android_ns_capable);
+
 /*
  * Each address family might have different locking rules, so we have
  * one slock key per address family and separate keys for internal and
@@ -591,7 +628,7 @@
 
 	/* Sorry... */
 	ret = -EPERM;
-	if (sk->sk_bound_dev_if && !ns_capable(net->user_ns, CAP_NET_RAW))
+	if (sk->sk_bound_dev_if && !android_ns_capable(net, CAP_NET_RAW))
 		goto out;
 
 	ret = -EINVAL;
diff -ruN a/net/core/sysctl_net_core.c b/net/core/sysctl_net_core.c
--- a/net/core/sysctl_net_core.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/core/sysctl_net_core.c	2021-12-23 08:35:58.000000000 +0100
@@ -585,6 +585,15 @@
 
 static struct ctl_table netns_core_table[] = {
 	{
+		.procname	= "android_paranoid",
+		.data		= &init_net.core.sysctl_android_paranoid,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+		.proc_handler	= proc_dointvec_minmax
+	},
+	{
 		.procname	= "somaxconn",
 		.data		= &init_net.core.sysctl_somaxconn,
 		.maxlen		= sizeof(int),
@@ -612,17 +621,22 @@
 {
 	struct ctl_table *tbl;
 
+	net->core.sysctl_android_paranoid = 0;
+
 	tbl = netns_core_table;
 	if (!net_eq(net, &init_net)) {
 		tbl = kmemdup(tbl, sizeof(netns_core_table), GFP_KERNEL);
 		if (tbl == NULL)
 			goto err_dup;
 
-		tbl[0].data = &net->core.sysctl_somaxconn;
+		tbl[0].data = &net->core.sysctl_android_paranoid;
+		tbl[1].data = &net->core.sysctl_somaxconn;
 
-		/* Don't export any sysctls to unprivileged users */
+		/* Don't export sysctls other than android_paranoid
+		 * to unprivileged users
+		 */
 		if (net->user_ns != &init_user_ns) {
-			tbl[0].procname = NULL;
+			tbl[1].procname = NULL;
 		}
 	}
 
diff -ruN a/net/ipv4/af_inet.c b/net/ipv4/af_inet.c
--- a/net/ipv4/af_inet.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/ipv4/af_inet.c	2021-12-23 08:35:59.000000000 +0100
@@ -85,6 +85,8 @@
 #include <linux/netfilter_ipv4.h>
 #include <linux/random.h>
 #include <linux/slab.h>
+#include <linux/netfilter/xt_qtaguid.h>
+#include <linux/android_aid.h>
 
 #include <linux/uaccess.h>
 
@@ -259,6 +260,9 @@
 	if (protocol < 0 || protocol >= IPPROTO_MAX)
 		return -EINVAL;
 
+	if (!inet_sk_allowed(net, AID_INET))
+		return -EACCES;
+
 	sock->state = SS_UNCONNECTED;
 
 	/* Look for the requested type/protocol pair. */
@@ -308,7 +312,7 @@
 
 	err = -EPERM;
 	if (sock->type == SOCK_RAW && !kern &&
-	    !ns_capable(net->user_ns, CAP_NET_RAW))
+	    !android_ns_capable(net, CAP_NET_RAW))
 		goto out_rcu_unlock;
 
 	sock->ops = answer->ops;
@@ -414,6 +419,9 @@
 		if (!sk->sk_kern_sock)
 			BPF_CGROUP_RUN_PROG_INET_SOCK_RELEASE(sk);
 
+#ifdef CONFIG_NETFILTER_XT_MATCH_QTAGUID
+		qtaguid_untag(sock, true);
+#endif
 		/* Applications forget to leave groups before exiting */
 		ip_mc_drop_socket(sk);
 
diff -ruN a/net/ipv4/devinet.c b/net/ipv4/devinet.c
--- a/net/ipv4/devinet.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/ipv4/devinet.c	2021-12-23 08:35:59.000000000 +0100
@@ -61,6 +61,7 @@
 #include <net/rtnetlink.h>
 #include <net/net_namespace.h>
 #include <net/addrconf.h>
+#include <net/sock.h>
 
 #define IPV6ONLY_FLAGS	\
 		(IFA_F_NODAD | IFA_F_OPTIMISTIC | IFA_F_DADFAILED | \
@@ -1044,7 +1045,7 @@
 
 	case SIOCSIFFLAGS:
 		ret = -EPERM;
-		if (!ns_capable(net->user_ns, CAP_NET_ADMIN))
+		if (!android_ns_capable(net, CAP_NET_ADMIN))
 			goto out;
 		break;
 	case SIOCSIFADDR:	/* Set interface address (and family) */
@@ -1052,7 +1053,7 @@
 	case SIOCSIFDSTADDR:	/* Set the destination address */
 	case SIOCSIFNETMASK: 	/* Set the netmask for the interface */
 		ret = -EPERM;
-		if (!ns_capable(net->user_ns, CAP_NET_ADMIN))
+		if (!android_ns_capable(net, CAP_NET_ADMIN))
 			goto out;
 		ret = -EINVAL;
 		if (sin->sin_family != AF_INET)
diff -ruN a/net/ipv4/sysctl_net_ipv4.c b/net/ipv4/sysctl_net_ipv4.c
--- a/net/ipv4/sysctl_net_ipv4.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/ipv4/sysctl_net_ipv4.c	2021-12-23 08:35:59.000000000 +0100
@@ -219,6 +219,13 @@
 	return ret;
 }
 
+/* The current kernel does not rely on this value so we do nothing here */
+static int proc_tcp_default_init_rwnd(struct ctl_table *ctl, int write,
+				      void *buffer, size_t *lenp, loff_t *ppos)
+{
+	return proc_dointvec(ctl, write, buffer, lenp, ppos);
+}
+
 static int proc_tcp_congestion_control(struct ctl_table *ctl, int write,
 				       void *buffer, size_t *lenp, loff_t *ppos)
 {
@@ -1406,6 +1413,13 @@
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= &two,
 	},
+	{
+		.procname       = "tcp_default_init_rwnd",
+		.data           = &init_net.ipv4.sysctl_tcp_default_init_rwnd,
+		.maxlen         = sizeof(int),
+		.mode           = 0644,
+		.proc_handler   = proc_tcp_default_init_rwnd
+	},
 	{ }
 };
 
diff -ruN a/net/ipv6/af_inet6.c b/net/ipv6/af_inet6.c
--- a/net/ipv6/af_inet6.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/ipv6/af_inet6.c	2021-12-23 08:35:59.000000000 +0100
@@ -39,6 +39,7 @@
 #include <linux/netdevice.h>
 #include <linux/icmpv6.h>
 #include <linux/netfilter_ipv6.h>
+#include <linux/android_aid.h>
 
 #include <net/ip.h>
 #include <net/ipv6.h>
@@ -123,6 +124,9 @@
 	if (protocol < 0 || protocol >= IPPROTO_MAX)
 		return -EINVAL;
 
+	if (!inet_sk_allowed(net, AID_INET))
+		return -EACCES;
+
 	/* Look for the requested type/protocol pair. */
 lookup_protocol:
 	err = -ESOCKTNOSUPPORT;
@@ -170,7 +174,7 @@
 
 	err = -EPERM;
 	if (sock->type == SOCK_RAW && !kern &&
-	    !ns_capable(net->user_ns, CAP_NET_RAW))
+	    !android_ns_capable(net, CAP_NET_RAW))
 		goto out_rcu_unlock;
 
 	sock->ops = answer->ops;
diff -ruN a/net/netfilter/Kconfig b/net/netfilter/Kconfig
--- a/net/netfilter/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/net/netfilter/Kconfig	2021-12-23 08:35:59.000000000 +0100
@@ -1511,6 +1511,45 @@
 	  If you want to compile it as a module, say M here and read
 	  <file:Documentation/kbuild/modules.rst>.  If unsure, say `N'.
 
+config NETFILTER_XT_MATCH_QTAGUID
+	bool '"quota, tag, owner" match and stats support'
+        depends on NETFILTER_XT_MATCH_SOCKET
+	depends on NETFILTER_XT_MATCH_OWNER=n
+	help
+	  This option replaces the `owner' match. In addition to matching
+	  on uid, it keeps stats based on a tag assigned to a socket.
+	  The full tag is comprised of a UID and an accounting tag.
+	  The tags are assignable to sockets from user space (e.g. a download
+	  manager can assign the socket to another UID for accounting).
+	  Stats and control are done via /proc/net/xt_qtaguid/.
+	  It replaces owner as it takes the same arguments, but should
+	  really be recognized by the iptables tool.
+
+	  If unsure, say `N'.
+
+config NETFILTER_XT_MATCH_QUOTA2
+	tristate '"quota2" match support'
+	depends on NETFILTER_ADVANCED
+	help
+	  This option adds a `quota2' match, which allows to match on a
+	  byte counter correctly and not per CPU.
+	  It allows naming the quotas.
+	  This is based on http://xtables-addons.git.sourceforge.net
+
+	  If you want to compile it as a module, say M here and read
+	  <file:Documentation/kbuild/modules.txt>.  If unsure, say `N'.
+
+config NETFILTER_XT_MATCH_QUOTA2_LOG
+	bool '"quota2" Netfilter LOG support'
+	depends on NETFILTER_XT_MATCH_QUOTA2
+	default n
+	help
+	  This option allows `quota2' to log ONCE when a quota limit
+	  is passed. It logs via NETLINK using the NETLINK_NFLOG family.
+	  It logs similarly to how ipt_ULOG would without data.
+
+	  If unsure, say `N'.
+
 config NETFILTER_XT_MATCH_RATEEST
 	tristate '"rateest" match support'
 	depends on NETFILTER_ADVANCED
diff -ruN a/net/netfilter/Makefile b/net/netfilter/Makefile
--- a/net/netfilter/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/net/netfilter/Makefile	2021-12-23 08:35:59.000000000 +0100
@@ -195,6 +195,8 @@
 obj-$(CONFIG_NETFILTER_XT_MATCH_PKTTYPE) += xt_pkttype.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_POLICY) += xt_policy.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_QUOTA) += xt_quota.o
+obj-$(CONFIG_NETFILTER_XT_MATCH_QTAGUID) += xt_qtaguid_print.o xt_qtaguid.o
+obj-$(CONFIG_NETFILTER_XT_MATCH_QUOTA2) += xt_quota2.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_RATEEST) += xt_rateest.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_REALM) += xt_realm.o
 obj-$(CONFIG_NETFILTER_XT_MATCH_RECENT) += xt_recent.o
diff -ruN a/net/netfilter/xt_IDLETIMER.c b/net/netfilter/xt_IDLETIMER.c
--- a/net/netfilter/xt_IDLETIMER.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/netfilter/xt_IDLETIMER.c	2021-12-23 08:35:59.000000000 +0100
@@ -28,6 +28,11 @@
 #include <linux/kobject.h>
 #include <linux/workqueue.h>
 #include <linux/sysfs.h>
+#include <linux/suspend.h>
+#include <net/sock.h>
+#include <net/inet_sock.h>
+
+#define NLMSG_MAX_SIZE 64
 
 struct idletimer_tg {
 	struct list_head entry;
@@ -38,15 +43,112 @@
 	struct kobject *kobj;
 	struct device_attribute attr;
 
+	struct timespec64 delayed_timer_trigger;
+	struct timespec64 last_modified_timer;
+	struct timespec64 last_suspend_time;
+	struct notifier_block pm_nb;
+
+	int timeout;
 	unsigned int refcnt;
 	u8 timer_type;
+
+	bool work_pending;
+	bool send_nl_msg;
+	bool active;
+	uid_t uid;
+	bool suspend_time_valid;
 };
 
 static LIST_HEAD(idletimer_tg_list);
 static DEFINE_MUTEX(list_mutex);
+static DEFINE_SPINLOCK(timestamp_lock);
 
 static struct kobject *idletimer_tg_kobj;
 
+static bool check_for_delayed_trigger(struct idletimer_tg *timer,
+				      struct timespec64 *ts)
+{
+	bool state;
+	struct timespec64 temp;
+	spin_lock_bh(&timestamp_lock);
+	timer->work_pending = false;
+	if ((ts->tv_sec - timer->last_modified_timer.tv_sec) > timer->timeout ||
+	    timer->delayed_timer_trigger.tv_sec != 0) {
+		state = false;
+		temp.tv_sec = timer->timeout;
+		temp.tv_nsec = 0;
+		if (timer->delayed_timer_trigger.tv_sec != 0) {
+			temp = timespec64_add(timer->delayed_timer_trigger,
+					      temp);
+			ts->tv_sec = temp.tv_sec;
+			ts->tv_nsec = temp.tv_nsec;
+			timer->delayed_timer_trigger.tv_sec = 0;
+			timer->work_pending = true;
+			schedule_work(&timer->work);
+		} else {
+			temp = timespec64_add(timer->last_modified_timer, temp);
+			ts->tv_sec = temp.tv_sec;
+			ts->tv_nsec = temp.tv_nsec;
+		}
+	} else {
+		state = timer->active;
+	}
+	spin_unlock_bh(&timestamp_lock);
+	return state;
+}
+
+static void notify_netlink_uevent(const char *iface, struct idletimer_tg *timer)
+{
+	char iface_msg[NLMSG_MAX_SIZE];
+	char state_msg[NLMSG_MAX_SIZE];
+	char timestamp_msg[NLMSG_MAX_SIZE];
+	char uid_msg[NLMSG_MAX_SIZE];
+	char *envp[] = { iface_msg, state_msg, timestamp_msg, uid_msg, NULL };
+	int res;
+	struct timespec64 ts;
+	u64 time_ns;
+	bool state;
+
+	res = snprintf(iface_msg, NLMSG_MAX_SIZE, "INTERFACE=%s",
+		       iface);
+	if (NLMSG_MAX_SIZE <= res) {
+		pr_err("message too long (%d)\n", res);
+		return;
+	}
+
+	ts = ktime_to_timespec64(ktime_get_boottime());
+	state = check_for_delayed_trigger(timer, &ts);
+	res = snprintf(state_msg, NLMSG_MAX_SIZE, "STATE=%s",
+		       state ? "active" : "inactive");
+
+	if (NLMSG_MAX_SIZE <= res) {
+		pr_err("message too long (%d)\n", res);
+		return;
+	}
+
+	if (state) {
+		res = snprintf(uid_msg, NLMSG_MAX_SIZE, "UID=%u", timer->uid);
+		if (NLMSG_MAX_SIZE <= res)
+			pr_err("message too long (%d)\n", res);
+	} else {
+		res = snprintf(uid_msg, NLMSG_MAX_SIZE, "UID=");
+		if (NLMSG_MAX_SIZE <= res)
+			pr_err("message too long (%d)\n", res);
+	}
+
+	time_ns = timespec64_to_ns(&ts);
+	res = snprintf(timestamp_msg, NLMSG_MAX_SIZE, "TIME_NS=%llu", time_ns);
+	if (NLMSG_MAX_SIZE <= res) {
+		timestamp_msg[0] = '\0';
+		pr_err("message too long (%d)\n", res);
+	}
+
+	pr_debug("putting nlmsg: <%s> <%s> <%s> <%s>\n", iface_msg, state_msg,
+		 timestamp_msg, uid_msg);
+	kobject_uevent_env(idletimer_tg_kobj, KOBJ_CHANGE, envp);
+	return;
+}
+
 static
 struct idletimer_tg *__idletimer_tg_find_by_label(const char *label)
 {
@@ -67,6 +169,7 @@
 	unsigned long expires = 0;
 	struct timespec64 ktimespec = {};
 	long time_diff = 0;
+	unsigned long now = jiffies;
 
 	mutex_lock(&list_mutex);
 
@@ -78,16 +181,20 @@
 			time_diff = ktimespec.tv_sec;
 		} else {
 			expires = timer->timer.expires;
-			time_diff = jiffies_to_msecs(expires - jiffies) / 1000;
+			time_diff = jiffies_to_msecs(expires - now) / 1000;
 		}
 	}
 
 	mutex_unlock(&list_mutex);
 
-	if (time_after(expires, jiffies) || ktimespec.tv_sec > 0)
-		return snprintf(buf, PAGE_SIZE, "%ld\n", time_diff);
+	if (time_after(expires, now) || ktimespec.tv_sec > 0)
+		return scnprintf(buf, PAGE_SIZE, "%ld\n", time_diff);
 
-	return snprintf(buf, PAGE_SIZE, "0\n");
+	if (timer->send_nl_msg)
+		return scnprintf(buf, PAGE_SIZE, "0 %d\n",
+				 jiffies_to_msecs(now - expires) / 1000);
+
+	return scnprintf(buf, PAGE_SIZE, "0\n");
 }
 
 static void idletimer_tg_work(struct work_struct *work)
@@ -96,6 +203,9 @@
 						  work);
 
 	sysfs_notify(idletimer_tg_kobj, NULL, timer->attr.attr.name);
+
+	if (timer->send_nl_msg)
+		notify_netlink_uevent(timer->attr.attr.name, timer);
 }
 
 static void idletimer_tg_expired(struct timer_list *t)
@@ -104,7 +214,62 @@
 
 	pr_debug("timer %s expired\n", timer->attr.attr.name);
 
+	spin_lock_bh(&timestamp_lock);
+	timer->active = false;
+	timer->work_pending = true;
 	schedule_work(&timer->work);
+	spin_unlock_bh(&timestamp_lock);
+}
+
+static int idletimer_resume(struct notifier_block *notifier,
+			    unsigned long pm_event, void *unused)
+{
+	struct timespec64 ts;
+	unsigned long time_diff, now = jiffies;
+	struct idletimer_tg *timer = container_of(notifier,
+						  struct idletimer_tg, pm_nb);
+	if (!timer)
+		return NOTIFY_DONE;
+
+	switch (pm_event) {
+	case PM_SUSPEND_PREPARE:
+		timer->last_suspend_time =
+			ktime_to_timespec64(ktime_get_boottime());
+		timer->suspend_time_valid = true;
+		break;
+	case PM_POST_SUSPEND:
+		if (!timer->suspend_time_valid)
+			break;
+		timer->suspend_time_valid = false;
+
+		spin_lock_bh(&timestamp_lock);
+		if (!timer->active) {
+			spin_unlock_bh(&timestamp_lock);
+			break;
+		}
+		/* since jiffies are not updated when suspended now represents
+		 * the time it would have suspended */
+		if (time_after(timer->timer.expires, now)) {
+			ts = ktime_to_timespec64(ktime_get_boottime());
+			ts = timespec64_sub(ts, timer->last_suspend_time);
+			time_diff = timespec64_to_jiffies(&ts);
+			if (timer->timer.expires > (time_diff + now)) {
+				mod_timer_pending(&timer->timer,
+						  (timer->timer.expires - time_diff));
+			} else {
+				del_timer(&timer->timer);
+				timer->timer.expires = 0;
+				timer->active = false;
+				timer->work_pending = true;
+				schedule_work(&timer->work);
+			}
+		}
+		spin_unlock_bh(&timestamp_lock);
+		break;
+	default:
+		break;
+	}
+	return NOTIFY_DONE;
 }
 
 static enum alarmtimer_restart idletimer_tg_alarmproc(struct alarm *alarm,
@@ -158,17 +323,34 @@
 
 	ret = sysfs_create_file(idletimer_tg_kobj, &info->timer->attr.attr);
 	if (ret < 0) {
-		pr_debug("couldn't add file to sysfs");
+		pr_debug("couldn't add file to sysfs\n");
 		goto out_free_attr;
 	}
 
 	list_add(&info->timer->entry, &idletimer_tg_list);
-
-	timer_setup(&info->timer->timer, idletimer_tg_expired, 0);
+	pr_debug("timer type value is 0.\n");
+	info->timer->timer_type = 0;
 	info->timer->refcnt = 1;
+	info->timer->send_nl_msg = false;
+	info->timer->active = true;
+	info->timer->timeout = info->timeout;
+
+	info->timer->delayed_timer_trigger.tv_sec = 0;
+	info->timer->delayed_timer_trigger.tv_nsec = 0;
+	info->timer->work_pending = false;
+	info->timer->uid = 0;
+	info->timer->last_modified_timer =
+		ktime_to_timespec64(ktime_get_boottime());
+
+	info->timer->pm_nb.notifier_call = idletimer_resume;
+	ret = register_pm_notifier(&info->timer->pm_nb);
+	if (ret)
+		printk(KERN_WARNING "[%s] Failed to register pm notifier %d\n",
+		       __func__, ret);
 
 	INIT_WORK(&info->timer->work, idletimer_tg_work);
 
+	timer_setup(&info->timer->timer, idletimer_tg_expired, 0);
 	mod_timer(&info->timer->timer,
 		  msecs_to_jiffies(info->timeout * 1000) + jiffies);
 
@@ -186,7 +368,7 @@
 {
 	int ret;
 
-	info->timer = kmalloc(sizeof(*info->timer), GFP_KERNEL);
+	info->timer = kzalloc(sizeof(*info->timer), GFP_KERNEL);
 	if (!info->timer) {
 		ret = -ENOMEM;
 		goto out;
@@ -207,7 +389,7 @@
 
 	ret = sysfs_create_file(idletimer_tg_kobj, &info->timer->attr.attr);
 	if (ret < 0) {
-		pr_debug("couldn't add file to sysfs");
+		pr_debug("couldn't add file to sysfs\n");
 		goto out_free_attr;
 	}
 
@@ -215,9 +397,25 @@
 	kobject_uevent(idletimer_tg_kobj,KOBJ_ADD);
 
 	list_add(&info->timer->entry, &idletimer_tg_list);
-	pr_debug("timer type value is %u", info->timer_type);
+	pr_debug("timer type value is %u\n", info->timer_type);
 	info->timer->timer_type = info->timer_type;
 	info->timer->refcnt = 1;
+	info->timer->send_nl_msg = (info->send_nl_msg != 0);
+	info->timer->active = true;
+	info->timer->timeout = info->timeout;
+
+	info->timer->delayed_timer_trigger.tv_sec = 0;
+	info->timer->delayed_timer_trigger.tv_nsec = 0;
+	info->timer->work_pending = false;
+	info->timer->uid = 0;
+	info->timer->last_modified_timer =
+		ktime_to_timespec64(ktime_get_boottime());
+
+	info->timer->pm_nb.notifier_call = idletimer_resume;
+	ret = register_pm_notifier(&info->timer->pm_nb);
+	if (ret)
+		printk(KERN_WARNING "[%s] Failed to register pm notifier %d\n",
+		       __func__, ret);
 
 	INIT_WORK(&info->timer->work, idletimer_tg_work);
 
@@ -231,7 +429,7 @@
 	} else {
 		timer_setup(&info->timer->timer, idletimer_tg_expired, 0);
 		mod_timer(&info->timer->timer,
-				msecs_to_jiffies(info->timeout * 1000) + jiffies);
+			  msecs_to_jiffies(info->timeout * 1000) + jiffies);
 	}
 
 	return 0;
@@ -244,6 +442,41 @@
 	return ret;
 }
 
+static void reset_timer(struct idletimer_tg * const info_timer,
+			const __u32 info_timeout,
+			struct sk_buff *skb)
+{
+	unsigned long now = jiffies;
+	bool timer_prev;
+
+	spin_lock_bh(&timestamp_lock);
+	timer_prev = info_timer->active;
+	info_timer->active = true;
+	/* timer_prev is used to guard overflow problem in time_before*/
+	if (!timer_prev || time_before(info_timer->timer.expires, now)) {
+		pr_debug("Starting Checkentry timer (Expired, Jiffies): %lu, %lu\n",
+			 info_timer->timer.expires, now);
+
+		/* Stores the uid resposible for waking up the radio */
+		if (skb && (skb->sk)) {
+			info_timer->uid = from_kuid_munged(current_user_ns(),
+							   sock_i_uid(skb_to_full_sk(skb)));
+		}
+
+		/* checks if there is a pending inactive notification*/
+		if (info_timer->work_pending)
+			info_timer->delayed_timer_trigger = info_timer->last_modified_timer;
+		else {
+			info_timer->work_pending = true;
+			schedule_work(&info_timer->work);
+		}
+	}
+
+	info_timer->last_modified_timer = ktime_to_timespec64(ktime_get_boottime());
+	mod_timer(&info_timer->timer, msecs_to_jiffies(info_timeout * 1000) + now);
+	spin_unlock_bh(&timestamp_lock);
+}
+
 /*
  * The actual xt_tables plugin.
  */
@@ -251,12 +484,21 @@
 					 const struct xt_action_param *par)
 {
 	const struct idletimer_tg_info *info = par->targinfo;
+	unsigned long now = jiffies;
 
 	pr_debug("resetting timer %s, timeout period %u\n",
 		 info->label, info->timeout);
 
-	mod_timer(&info->timer->timer,
-		  msecs_to_jiffies(info->timeout * 1000) + jiffies);
+	info->timer->active = true;
+
+	if (time_before(info->timer->timer.expires, now)) {
+		schedule_work(&info->timer->work);
+		pr_debug("Starting timer %s (Expired, Jiffies): %lu, %lu\n",
+			 info->label, info->timer->timer.expires, now);
+	}
+
+	/* TODO: Avoid modifying timers on each packet */
+	reset_timer(info->timer, info->timeout, skb);
 
 	return XT_CONTINUE;
 }
@@ -268,6 +510,7 @@
 					 const struct xt_action_param *par)
 {
 	const struct idletimer_tg_info_v1 *info = par->targinfo;
+	unsigned long now = jiffies;
 
 	pr_debug("resetting timer %s, timeout period %u\n",
 		 info->label, info->timeout);
@@ -276,8 +519,16 @@
 		ktime_t tout = ktime_set(info->timeout, 0);
 		alarm_start_relative(&info->timer->alarm, tout);
 	} else {
-		mod_timer(&info->timer->timer,
-				msecs_to_jiffies(info->timeout * 1000) + jiffies);
+		info->timer->active = true;
+
+		if (time_before(info->timer->timer.expires, now)) {
+			schedule_work(&info->timer->work);
+			pr_debug("Starting timer %s (Expired, Jiffies): %lu, %lu\n",
+				 info->label, info->timer->timer.expires, now);
+		}
+
+		/* TODO: Avoid modifying timers on each packet */
+		reset_timer(info->timer, info->timeout, skb);
 	}
 
 	return XT_CONTINUE;
@@ -321,9 +572,7 @@
 	info->timer = __idletimer_tg_find_by_label(info->label);
 	if (info->timer) {
 		info->timer->refcnt++;
-		mod_timer(&info->timer->timer,
-			  msecs_to_jiffies(info->timeout * 1000) + jiffies);
-
+		reset_timer(info->timer, info->timeout, NULL);
 		pr_debug("increased refcnt of timer %s to %u\n",
 			 info->label, info->timer->refcnt);
 	} else {
@@ -346,9 +595,6 @@
 
 	pr_debug("checkentry targinfo%s\n", info->label);
 
-	if (info->send_nl_msg)
-		return -EOPNOTSUPP;
-
 	ret = idletimer_tg_helper((struct idletimer_tg_info *)info);
 	if(ret < 0)
 	{
@@ -361,6 +607,11 @@
 		return -EINVAL;
 	}
 
+	if (info->send_nl_msg > 1) {
+		pr_debug("invalid value for send_nl_msg\n");
+		return -EINVAL;
+	}
+
 	mutex_lock(&list_mutex);
 
 	info->timer = __idletimer_tg_find_by_label(info->label);
@@ -383,8 +634,7 @@
 				alarm_start_relative(&info->timer->alarm, tout);
 			}
 		} else {
-				mod_timer(&info->timer->timer,
-					msecs_to_jiffies(info->timeout * 1000) + jiffies);
+			reset_timer(info->timer, info->timeout, NULL);
 		}
 		pr_debug("increased refcnt of timer %s to %u\n",
 			 info->label, info->timer->refcnt);
@@ -414,8 +664,9 @@
 
 		list_del(&info->timer->entry);
 		del_timer_sync(&info->timer->timer);
-		cancel_work_sync(&info->timer->work);
 		sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr);
+		unregister_pm_notifier(&info->timer->pm_nb);
+		cancel_work_sync(&info->timer->work);
 		kfree(info->timer->attr.attr.name);
 		kfree(info->timer);
 	} else {
@@ -443,8 +694,9 @@
 		} else {
 			del_timer_sync(&info->timer->timer);
 		}
-		cancel_work_sync(&info->timer->work);
 		sysfs_remove_file(idletimer_tg_kobj, &info->timer->attr.attr);
+		unregister_pm_notifier(&info->timer->pm_nb);
+		cancel_work_sync(&info->timer->work);
 		kfree(info->timer->attr.attr.name);
 		kfree(info->timer);
 	} else {
@@ -540,3 +792,4 @@
 MODULE_LICENSE("GPL v2");
 MODULE_ALIAS("ipt_IDLETIMER");
 MODULE_ALIAS("ip6t_IDLETIMER");
+MODULE_ALIAS("arpt_IDLETIMER");
diff -ruN a/net/netfilter/xt_qtaguid.c b/net/netfilter/xt_qtaguid.c
--- a/net/netfilter/xt_qtaguid.c	1970-01-01 01:00:00.000000000 +0100
+++ b/net/netfilter/xt_qtaguid.c	2021-03-20 09:20:37.000000000 +0100
@@ -0,0 +1,3338 @@
+/*
+ * Kernel iptables module to track stats for packets based on user tags.
+ *
+ * (C) 2011 Google, Inc
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/*
+ * There are run-time debug flags enabled via the debug_mask module param, or
+ * via the DEFAULT_DEBUG_MASK. See xt_qtaguid_internal.h.
+ */
+#define DEBUG
+
+#include <linux/file.h>
+#include <linux/inetdevice.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/miscdevice.h>
+#include <linux/netfilter/x_tables.h>
+#include <linux/netfilter/xt_qtaguid.h>
+#include <linux/ratelimit.h>
+#include <linux/seq_file.h>
+#include <linux/skbuff.h>
+#include <linux/workqueue.h>
+#include <net/addrconf.h>
+#include <net/net_namespace.h>
+#include <net/netns/generic.h>
+#include <net/sock.h>
+#include <net/tcp.h>
+#include <net/udp.h>
+#include <net/netfilter/nf_socket.h>
+
+#if defined(CONFIG_IP6_NF_IPTABLES) || defined(CONFIG_IP6_NF_IPTABLES_MODULE)
+#include <linux/netfilter_ipv6/ip6_tables.h>
+#endif
+
+#include <linux/netfilter/xt_socket.h>
+#include "xt_qtaguid_internal.h"
+#include "xt_qtaguid_print.h"
+#include "../../fs/proc/internal.h"
+
+/*
+ * We only use the xt_socket funcs within a similar context to avoid unexpected
+ * return values.
+ */
+#define XT_SOCKET_SUPPORTED_HOOKS \
+	((1 << NF_INET_PRE_ROUTING) | (1 << NF_INET_LOCAL_IN))
+
+
+static unsigned int proc_iface_perms = S_IRUGO;
+module_param_named(iface_perms, proc_iface_perms, uint, S_IRUGO | S_IWUSR);
+
+static unsigned int proc_stats_perms = S_IRUGO;
+module_param_named(stats_perms, proc_stats_perms, uint, S_IRUGO | S_IWUSR);
+
+/* Everybody can write. But proc_ctrl_write_limited is true by default which
+ * limits what can be controlled. See the can_*() functions.
+ */
+static unsigned int proc_ctrl_perms = S_IRUGO | S_IWUGO;
+module_param_named(ctrl_perms, proc_ctrl_perms, uint, S_IRUGO | S_IWUSR);
+
+/* Limited by default, so the gid of the ctrl and stats proc entries
+ * will limit what can be done. See the can_*() functions.
+ */
+static bool proc_stats_readall_limited = true;
+static bool proc_ctrl_write_limited = true;
+
+module_param_named(stats_readall_limited, proc_stats_readall_limited, bool,
+		   S_IRUGO | S_IWUSR);
+module_param_named(ctrl_write_limited, proc_ctrl_write_limited, bool,
+		   S_IRUGO | S_IWUSR);
+
+/*
+ * Limit the number of active tags (via socket tags) for a given UID.
+ * Multiple processes could share the UID.
+ */
+static int max_sock_tags = DEFAULT_MAX_SOCK_TAGS;
+module_param(max_sock_tags, int, S_IRUGO | S_IWUSR);
+
+/*
+ * After the kernel has initiallized this module, it is still possible
+ * to make it passive.
+ * Setting passive to Y:
+ *  - the iface stats handling will not act on notifications.
+ *  - iptables matches will never match.
+ *  - ctrl commands silently succeed.
+ *  - stats are always empty.
+ * This is mostly usefull when a bug is suspected.
+ */
+static bool module_passive;
+module_param_named(passive, module_passive, bool, S_IRUGO | S_IWUSR);
+
+/*
+ * Control how qtaguid data is tracked per proc/uid.
+ * Setting tag_tracking_passive to Y:
+ *  - don't create proc specific structs to track tags
+ *  - don't check that active tag stats exceed some limits.
+ *  - don't clean up socket tags on process exits.
+ * This is mostly usefull when a bug is suspected.
+ */
+static bool qtu_proc_handling_passive;
+module_param_named(tag_tracking_passive, qtu_proc_handling_passive, bool,
+		   S_IRUGO | S_IWUSR);
+
+#define QTU_DEV_NAME "xt_qtaguid"
+
+struct qtaguid_net {
+	struct proc_dir_entry *procdir;
+	struct proc_dir_entry *ctrl_file;
+	struct proc_dir_entry *stats_file;
+	struct proc_dir_entry *iface_stat_procdir;
+
+	/* iface_stat_all will go away once userspace gets use to the new
+	 * fields that have a format line.
+	 */
+	struct proc_dir_entry *iface_stat_all_procfile;
+	struct proc_dir_entry *iface_stat_fmt_procfile;
+
+	struct list_head iface_stat_list;
+	spinlock_t iface_stat_list_lock;
+
+	struct rb_root sock_tag_tree;
+	spinlock_t sock_tag_list_lock;
+
+	struct rb_root tag_counter_set_tree;
+	spinlock_t tag_counter_set_list_lock;
+
+	struct rb_root uid_tag_data_tree;
+	spinlock_t uid_tag_data_tree_lock;
+
+	struct rb_root proc_qtu_data_tree;
+	/* No proc_qtu_data_tree_lock; use uid_tag_data_tree_lock */
+
+	struct qtaguid_event_counts qtu_events;
+};
+
+static int qtaguid_net_id;
+static inline struct qtaguid_net *qtaguid_pernet(const struct net *net)
+{
+	return net_generic(net, qtaguid_net_id);
+}
+
+uint qtaguid_debug_mask = DEFAULT_DEBUG_MASK;
+module_param_named(debug_mask, qtaguid_debug_mask, uint, S_IRUGO | S_IWUSR);
+
+/*----------------------------------------------*/
+
+static bool can_manipulate_uids(const struct net *net)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+
+	/* root pwnd */
+	return in_egroup_p(qtaguid_net->ctrl_file->gid) ||
+	       unlikely(!from_kuid(net->user_ns, current_fsuid())) ||
+	       unlikely(!proc_ctrl_write_limited) ||
+	       unlikely(uid_eq(current_fsuid(), qtaguid_net->ctrl_file->uid));
+}
+
+static bool can_impersonate_uid(const struct net *net, kuid_t uid)
+{
+	return uid_eq(uid, current_fsuid()) || can_manipulate_uids(net);
+}
+
+static bool can_read_other_uid_stats(const struct net *net, kuid_t uid)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+
+	/* root pwnd */
+	return in_egroup_p(qtaguid_net->stats_file->gid) ||
+	       unlikely(!from_kuid(net->user_ns, current_fsuid())) ||
+	       uid_eq(uid, current_fsuid()) ||
+	       unlikely(!proc_stats_readall_limited) ||
+	       unlikely(uid_eq(current_fsuid(),
+			       qtaguid_net->ctrl_file->uid));
+}
+
+static inline void dc_add_byte_packets(struct data_counters *counters, int set,
+				  enum ifs_tx_rx direction,
+				  enum ifs_proto ifs_proto,
+				  int bytes,
+				  int packets)
+{
+	counters->bpc[set][direction][ifs_proto].bytes += bytes;
+	counters->bpc[set][direction][ifs_proto].packets += packets;
+}
+
+static struct tag_node *tag_node_tree_search(struct rb_root *root, tag_t tag)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct tag_node *data = rb_entry(node, struct tag_node, node);
+		int result;
+		RB_DEBUG("qtaguid: tag_node_tree_search(0x%llx): "
+			 " node=%p data=%p\n", tag, node, data);
+		result = tag_compare(tag, data->tag);
+		RB_DEBUG("qtaguid: tag_node_tree_search(0x%llx): "
+			 " data.tag=0x%llx (uid=%u) res=%d\n",
+			 tag, data->tag, get_uid_from_tag(data->tag), result);
+		if (result < 0)
+			node = node->rb_left;
+		else if (result > 0)
+			node = node->rb_right;
+		else
+			return data;
+	}
+	return NULL;
+}
+
+static void tag_node_tree_insert(struct tag_node *data, struct rb_root *root)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	/* Figure out where to put new node */
+	while (*new) {
+		struct tag_node *this = rb_entry(*new, struct tag_node,
+						 node);
+		int result = tag_compare(data->tag, this->tag);
+		RB_DEBUG("qtaguid: %s(): tag=0x%llx"
+			 " (uid=%u)\n", __func__,
+			 this->tag,
+			 get_uid_from_tag(this->tag));
+		parent = *new;
+		if (result < 0)
+			new = &((*new)->rb_left);
+		else if (result > 0)
+			new = &((*new)->rb_right);
+		else
+			BUG();
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&data->node, parent, new);
+	rb_insert_color(&data->node, root);
+}
+
+static void tag_stat_tree_insert(struct tag_stat *data, struct rb_root *root)
+{
+	tag_node_tree_insert(&data->tn, root);
+}
+
+static struct tag_stat *tag_stat_tree_search(struct rb_root *root, tag_t tag)
+{
+	struct tag_node *node = tag_node_tree_search(root, tag);
+	if (!node)
+		return NULL;
+	return rb_entry(&node->node, struct tag_stat, tn.node);
+}
+
+static void tag_stat_tree_erase(struct rb_root *root)
+{
+	struct rb_node *node;
+
+	for (node = rb_first(root); node; ) {
+		struct tag_stat *entry =
+			rb_entry(node, struct tag_stat, tn.node);
+		node = rb_next(node);
+		rb_erase(&entry->tn.node, root);
+		kfree(entry);
+	}
+}
+
+static void tag_counter_set_tree_insert(struct tag_counter_set *data,
+					struct rb_root *root)
+{
+	tag_node_tree_insert(&data->tn, root);
+}
+
+static struct tag_counter_set *tag_counter_set_tree_search(struct rb_root *root,
+							   tag_t tag)
+{
+	struct tag_node *node = tag_node_tree_search(root, tag);
+	if (!node)
+		return NULL;
+	return rb_entry(&node->node, struct tag_counter_set, tn.node);
+
+}
+
+static void tag_counter_set_tree_erase(struct rb_root *root)
+{
+	struct rb_node *node;
+
+	for (node = rb_first(root); node; ) {
+		struct tag_counter_set *entry =
+			rb_entry(node, struct tag_counter_set, tn.node);
+		node = rb_next(node);
+		rb_erase(&entry->tn.node, root);
+		kfree(entry);
+	}
+}
+
+static void tag_ref_tree_insert(struct tag_ref *data, struct rb_root *root)
+{
+	tag_node_tree_insert(&data->tn, root);
+}
+
+static struct tag_ref *tag_ref_tree_search(struct rb_root *root, tag_t tag)
+{
+	struct tag_node *node = tag_node_tree_search(root, tag);
+	if (!node)
+		return NULL;
+	return rb_entry(&node->node, struct tag_ref, tn.node);
+}
+
+static void tag_ref_set_tree_erase(struct rb_root *root)
+{
+	struct rb_node *node;
+
+	for (node = rb_first(root); node; ) {
+		struct tag_ref *entry =
+			rb_entry(node, struct tag_ref, tn.node);
+		node = rb_next(node);
+		rb_erase(&entry->tn.node, root);
+		kfree(entry);
+	}
+}
+
+static struct sock_tag *sock_tag_tree_search(struct rb_root *root,
+					     const struct sock *sk)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct sock_tag *data = rb_entry(node, struct sock_tag,
+						 sock_node);
+		if (sk < data->sk)
+			node = node->rb_left;
+		else if (sk > data->sk)
+			node = node->rb_right;
+		else
+			return data;
+	}
+	return NULL;
+}
+
+static void sock_tag_tree_insert(struct sock_tag *data, struct rb_root *root)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	/* Figure out where to put new node */
+	while (*new) {
+		struct sock_tag *this = rb_entry(*new, struct sock_tag,
+						 sock_node);
+		parent = *new;
+		if (data->sk < this->sk)
+			new = &((*new)->rb_left);
+		else if (data->sk > this->sk)
+			new = &((*new)->rb_right);
+		else
+			BUG();
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&data->sock_node, parent, new);
+	rb_insert_color(&data->sock_node, root);
+}
+
+static void sock_tag_tree_erase(struct rb_root *st_to_free_tree)
+{
+	struct rb_node *node;
+	struct sock_tag *st_entry;
+
+	node = rb_first(st_to_free_tree);
+	while (node) {
+		st_entry = rb_entry(node, struct sock_tag, sock_node);
+		node = rb_next(node);
+		CT_DEBUG("qtaguid: %s(): "
+			 "erase st: sk=%p tag=0x%llx (uid=%u)\n", __func__,
+			 st_entry->sk,
+			 st_entry->tag,
+			 get_uid_from_tag(st_entry->tag));
+		rb_erase(&st_entry->sock_node, st_to_free_tree);
+		sock_put(st_entry->sk);
+		kfree(st_entry);
+	}
+}
+
+static struct proc_qtu_data *proc_qtu_data_tree_search(struct rb_root *root,
+						       const pid_t pid)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct proc_qtu_data *data = rb_entry(node,
+						      struct proc_qtu_data,
+						      node);
+		if (pid < data->pid)
+			node = node->rb_left;
+		else if (pid > data->pid)
+			node = node->rb_right;
+		else
+			return data;
+	}
+	return NULL;
+}
+
+static void proc_qtu_data_tree_insert(struct proc_qtu_data *data,
+				      struct rb_root *root)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	/* Figure out where to put new node */
+	while (*new) {
+		struct proc_qtu_data *this = rb_entry(*new,
+						      struct proc_qtu_data,
+						      node);
+		parent = *new;
+		if (data->pid < this->pid)
+			new = &((*new)->rb_left);
+		else if (data->pid > this->pid)
+			new = &((*new)->rb_right);
+		else
+			BUG();
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&data->node, parent, new);
+	rb_insert_color(&data->node, root);
+}
+
+static void uid_tag_data_tree_insert(struct uid_tag_data *data,
+				     struct rb_root *root)
+{
+	struct rb_node **new = &(root->rb_node), *parent = NULL;
+
+	/* Figure out where to put new node */
+	while (*new) {
+		struct uid_tag_data *this = rb_entry(*new,
+						     struct uid_tag_data,
+						     node);
+		parent = *new;
+		if (data->uid < this->uid)
+			new = &((*new)->rb_left);
+		else if (data->uid > this->uid)
+			new = &((*new)->rb_right);
+		else
+			BUG();
+	}
+
+	/* Add new node and rebalance tree. */
+	rb_link_node(&data->node, parent, new);
+	rb_insert_color(&data->node, root);
+}
+
+static struct uid_tag_data *uid_tag_data_tree_search(struct rb_root *root,
+						     uid_t uid)
+{
+	struct rb_node *node = root->rb_node;
+
+	while (node) {
+		struct uid_tag_data *data = rb_entry(node,
+						     struct uid_tag_data,
+						     node);
+		if (uid < data->uid)
+			node = node->rb_left;
+		else if (uid > data->uid)
+			node = node->rb_right;
+		else
+			return data;
+	}
+	return NULL;
+}
+
+static void uid_tag_data_tree_erase(struct rb_root *root)
+{
+	struct rb_node *node;
+
+	for (node = rb_first(root); node; ) {
+		struct uid_tag_data *entry =
+			rb_entry(node, struct uid_tag_data, node);
+		node = rb_next(node);
+		tag_ref_set_tree_erase(&entry->tag_ref_tree);
+		rb_erase(&entry->node, root);
+		kfree(entry);
+	}
+}
+
+/*
+ * Allocates a new uid_tag_data struct if needed.
+ * Returns a pointer to the found or allocated uid_tag_data.
+ * Returns a PTR_ERR on failures, and lock is not held.
+ * If found is not NULL:
+ *   sets *found to true if not allocated.
+ *   sets *found to false if allocated.
+ */
+static struct uid_tag_data *get_uid_data(struct qtaguid_net *qtaguid_net,
+					 uid_t uid, bool *found_res)
+{
+	struct uid_tag_data *utd_entry;
+
+	/* Look for top level uid_tag_data for the UID */
+	utd_entry = uid_tag_data_tree_search(&qtaguid_net->uid_tag_data_tree,
+					     uid);
+	DR_DEBUG("qtaguid: get_uid_data(%u) utd=%p\n", uid, utd_entry);
+
+	if (found_res)
+		*found_res = utd_entry;
+	if (utd_entry)
+		return utd_entry;
+
+	utd_entry = kzalloc(sizeof(*utd_entry), GFP_ATOMIC);
+	if (!utd_entry) {
+		pr_err("qtaguid: get_uid_data(%u): "
+		       "tag data alloc failed\n", uid);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	utd_entry->uid = uid;
+	utd_entry->tag_ref_tree = RB_ROOT;
+	uid_tag_data_tree_insert(utd_entry, &qtaguid_net->uid_tag_data_tree);
+	DR_DEBUG("qtaguid: get_uid_data(%u) new utd=%p\n", uid, utd_entry);
+	return utd_entry;
+}
+
+/* Never returns NULL. Either PTR_ERR or a valid ptr. */
+static struct tag_ref *new_tag_ref(tag_t new_tag,
+				   struct uid_tag_data *utd_entry)
+{
+	struct tag_ref *tr_entry;
+	int res;
+
+	if (utd_entry->num_active_tags + 1 > max_sock_tags) {
+		pr_info("qtaguid: new_tag_ref(0x%llx): "
+			"tag ref alloc quota exceeded. max=%d\n",
+			new_tag, max_sock_tags);
+		res = -EMFILE;
+		goto err_res;
+
+	}
+
+	tr_entry = kzalloc(sizeof(*tr_entry), GFP_ATOMIC);
+	if (!tr_entry) {
+		pr_err("qtaguid: new_tag_ref(0x%llx): "
+		       "tag ref alloc failed\n",
+		       new_tag);
+		res = -ENOMEM;
+		goto err_res;
+	}
+	tr_entry->tn.tag = new_tag;
+	/* tr_entry->num_sock_tags  handled by caller */
+	utd_entry->num_active_tags++;
+	tag_ref_tree_insert(tr_entry, &utd_entry->tag_ref_tree);
+	DR_DEBUG("qtaguid: new_tag_ref(0x%llx): "
+		 " inserted new tag ref %p\n",
+		 new_tag, tr_entry);
+	return tr_entry;
+
+err_res:
+	return ERR_PTR(res);
+}
+
+static struct tag_ref *lookup_tag_ref(struct qtaguid_net *qtaguid_net,
+				      tag_t full_tag,
+				      struct uid_tag_data **utd_res)
+{
+	struct uid_tag_data *utd_entry;
+	struct tag_ref *tr_entry;
+	bool found_utd;
+	uid_t uid = get_uid_from_tag(full_tag);
+
+	DR_DEBUG("qtaguid: lookup_tag_ref(tag=0x%llx (uid=%u))\n",
+		 full_tag, uid);
+
+	utd_entry = get_uid_data(qtaguid_net, uid, &found_utd);
+	if (IS_ERR_OR_NULL(utd_entry)) {
+		if (utd_res)
+			*utd_res = utd_entry;
+		return NULL;
+	}
+
+	tr_entry = tag_ref_tree_search(&utd_entry->tag_ref_tree, full_tag);
+	if (utd_res)
+		*utd_res = utd_entry;
+	DR_DEBUG("qtaguid: lookup_tag_ref(0x%llx) utd_entry=%p tr_entry=%p\n",
+		 full_tag, utd_entry, tr_entry);
+	return tr_entry;
+}
+
+/* Never returns NULL. Either PTR_ERR or a valid ptr. */
+static struct tag_ref *get_tag_ref(struct qtaguid_net *qtaguid_net,
+				   tag_t full_tag,
+				   struct uid_tag_data **utd_res)
+{
+	struct uid_tag_data *utd_entry;
+	struct tag_ref *tr_entry;
+
+	DR_DEBUG("qtaguid: get_tag_ref(0x%llx)\n",
+		 full_tag);
+	tr_entry = lookup_tag_ref(qtaguid_net, full_tag, &utd_entry);
+	BUG_ON(IS_ERR_OR_NULL(utd_entry));
+	if (!tr_entry)
+		tr_entry = new_tag_ref(full_tag, utd_entry);
+
+	if (utd_res)
+		*utd_res = utd_entry;
+	DR_DEBUG("qtaguid: get_tag_ref(0x%llx) utd=%p tr=%p\n",
+		 full_tag, utd_entry, tr_entry);
+	return tr_entry;
+}
+
+/* Checks and maybe frees the UID Tag Data entry */
+static void put_utd_entry(const struct net *net,
+			  struct uid_tag_data *utd_entry)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+
+	/* Are we done with the UID tag data entry? */
+	if (RB_EMPTY_ROOT(&utd_entry->tag_ref_tree) &&
+		!utd_entry->num_pqd) {
+		DR_DEBUG("qtaguid: %s(): "
+			 "erase utd_entry=%p uid=%u "
+			 "by pid=%u tgid=%u uid=%u\n", __func__,
+			 utd_entry, utd_entry->uid,
+			 current->pid, current->tgid,
+			 from_kuid(net->user_ns, current_fsuid()));
+		BUG_ON(utd_entry->num_active_tags);
+		rb_erase(&utd_entry->node, &qtaguid_net->uid_tag_data_tree);
+		kfree(utd_entry);
+	} else {
+		DR_DEBUG("qtaguid: %s(): "
+			 "utd_entry=%p still has %d tags %d proc_qtu_data\n",
+			 __func__, utd_entry, utd_entry->num_active_tags,
+			 utd_entry->num_pqd);
+		BUG_ON(!(utd_entry->num_active_tags ||
+			 utd_entry->num_pqd));
+	}
+}
+
+/*
+ * If no sock_tags are using this tag_ref,
+ * decrements refcount of utd_entry, removes tr_entry
+ * from utd_entry->tag_ref_tree and frees.
+ */
+static void free_tag_ref_from_utd_entry(struct tag_ref *tr_entry,
+					struct uid_tag_data *utd_entry)
+{
+	DR_DEBUG("qtaguid: %s(): %p tag=0x%llx (uid=%u)\n", __func__,
+		 tr_entry, tr_entry->tn.tag,
+		 get_uid_from_tag(tr_entry->tn.tag));
+	if (!tr_entry->num_sock_tags) {
+		BUG_ON(!utd_entry->num_active_tags);
+		utd_entry->num_active_tags--;
+		rb_erase(&tr_entry->tn.node, &utd_entry->tag_ref_tree);
+		DR_DEBUG("qtaguid: %s(): erased %p\n", __func__, tr_entry);
+		kfree(tr_entry);
+	}
+}
+
+static void put_tag_ref_tree(tag_t full_tag, struct uid_tag_data *utd_entry)
+{
+	struct rb_node *node;
+	struct tag_ref *tr_entry;
+	tag_t acct_tag;
+
+	DR_DEBUG("qtaguid: %s(tag=0x%llx (uid=%u))\n", __func__,
+		 full_tag, get_uid_from_tag(full_tag));
+	acct_tag = get_atag_from_tag(full_tag);
+	node = rb_first(&utd_entry->tag_ref_tree);
+	while (node) {
+		tr_entry = rb_entry(node, struct tag_ref, tn.node);
+		node = rb_next(node);
+		if (!acct_tag || tr_entry->tn.tag == full_tag)
+			free_tag_ref_from_utd_entry(tr_entry, utd_entry);
+	}
+}
+
+static ssize_t read_proc_u64(struct file *file, char __user *buf,
+			 size_t size, loff_t *ppos)
+{
+	uint64_t *valuep = PDE_DATA(file_inode(file));
+	char tmp[24];
+	size_t tmp_size;
+
+	tmp_size = scnprintf(tmp, sizeof(tmp), "%llu\n", *valuep);
+	return simple_read_from_buffer(buf, size, ppos, tmp, tmp_size);
+}
+
+static ssize_t read_proc_bool(struct file *file, char __user *buf,
+			  size_t size, loff_t *ppos)
+{
+	bool *valuep = PDE_DATA(file_inode(file));
+	char tmp[24];
+	size_t tmp_size;
+
+	tmp_size = scnprintf(tmp, sizeof(tmp), "%u\n", *valuep);
+	return simple_read_from_buffer(buf, size, ppos, tmp, tmp_size);
+}
+
+static int get_active_counter_set(struct qtaguid_net *qtaguid_net, tag_t tag)
+{
+	int active_set = 0;
+	struct tag_counter_set *tcs;
+
+	MT_DEBUG("qtaguid: get_active_counter_set(tag=0x%llx)"
+		 " (uid=%u)\n",
+		 tag, get_uid_from_tag(tag));
+	/* For now we only handle UID tags for active sets */
+	tag = get_utag_from_tag(tag);
+	spin_lock_bh(&qtaguid_net->tag_counter_set_list_lock);
+	tcs = tag_counter_set_tree_search(&qtaguid_net->tag_counter_set_tree,
+					  tag);
+	if (tcs)
+		active_set = tcs->active_set;
+	spin_unlock_bh(&qtaguid_net->tag_counter_set_list_lock);
+	return active_set;
+}
+
+/*
+ * Find the entry for tracking the specified interface.
+ * Caller must hold iface_stat_list_lock
+ */
+static struct iface_stat *get_iface_entry(struct qtaguid_net *qtaguid_net,
+					  const char *ifname)
+{
+	struct iface_stat *iface_entry;
+
+	/* Find the entry for tracking the specified tag within the interface */
+	if (ifname == NULL) {
+		pr_info("qtaguid: iface_stat: get() NULL device name\n");
+		return NULL;
+	}
+
+	/* Iterate over interfaces */
+	list_for_each_entry(iface_entry, &qtaguid_net->iface_stat_list, list) {
+		if (!strcmp(ifname, iface_entry->ifname))
+			goto done;
+	}
+	iface_entry = NULL;
+done:
+	return iface_entry;
+}
+
+/* This is for fmt2 only */
+static void pp_iface_stat_header(struct seq_file *m)
+{
+	seq_puts(m,
+		 "ifname "
+		 "total_skb_rx_bytes total_skb_rx_packets "
+		 "total_skb_tx_bytes total_skb_tx_packets "
+		 "rx_tcp_bytes rx_tcp_packets "
+		 "rx_udp_bytes rx_udp_packets "
+		 "rx_other_bytes rx_other_packets "
+		 "tx_tcp_bytes tx_tcp_packets "
+		 "tx_udp_bytes tx_udp_packets "
+		 "tx_other_bytes tx_other_packets\n"
+	);
+}
+
+static void pp_iface_stat_line(struct seq_file *m,
+			       struct iface_stat *iface_entry)
+{
+	struct data_counters *cnts;
+	int cnt_set = 0;   /* We only use one set for the device */
+	cnts = &iface_entry->totals_via_skb;
+	seq_printf(m, "%s %llu %llu %llu %llu %llu %llu %llu %llu "
+		   "%llu %llu %llu %llu %llu %llu %llu %llu\n",
+		   iface_entry->ifname,
+		   dc_sum_bytes(cnts, cnt_set, IFS_RX),
+		   dc_sum_packets(cnts, cnt_set, IFS_RX),
+		   dc_sum_bytes(cnts, cnt_set, IFS_TX),
+		   dc_sum_packets(cnts, cnt_set, IFS_TX),
+		   cnts->bpc[cnt_set][IFS_RX][IFS_TCP].bytes,
+		   cnts->bpc[cnt_set][IFS_RX][IFS_TCP].packets,
+		   cnts->bpc[cnt_set][IFS_RX][IFS_UDP].bytes,
+		   cnts->bpc[cnt_set][IFS_RX][IFS_UDP].packets,
+		   cnts->bpc[cnt_set][IFS_RX][IFS_PROTO_OTHER].bytes,
+		   cnts->bpc[cnt_set][IFS_RX][IFS_PROTO_OTHER].packets,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_TCP].bytes,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_TCP].packets,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_UDP].bytes,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_UDP].packets,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_PROTO_OTHER].bytes,
+		   cnts->bpc[cnt_set][IFS_TX][IFS_PROTO_OTHER].packets);
+}
+
+struct proc_iface_stat_fmt_info {
+	struct net *net;
+	int fmt;
+};
+
+static void *iface_stat_fmt_proc_start(struct seq_file *m, loff_t *pos)
+{
+	struct proc_iface_stat_fmt_info *p = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(p->net);
+	loff_t n = *pos;
+
+	/*
+	 * This lock will prevent iface_stat_update() from changing active,
+	 * and in turn prevent an interface from unregistering itself.
+	 */
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+
+	if (unlikely(module_passive))
+		return NULL;
+
+	if (!n && p->fmt == 2)
+		pp_iface_stat_header(m);
+
+	return seq_list_start(&qtaguid_net->iface_stat_list, n);
+}
+
+static void *iface_stat_fmt_proc_next(struct seq_file *m, void *p, loff_t *pos)
+{
+	struct proc_iface_stat_fmt_info *fmt_info = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(fmt_info->net);
+
+	return seq_list_next(p, &qtaguid_net->iface_stat_list, pos);
+}
+
+static void iface_stat_fmt_proc_stop(struct seq_file *m, void *p)
+{
+	struct proc_iface_stat_fmt_info *fmt_info = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(fmt_info->net);
+
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+}
+
+static int iface_stat_fmt_proc_show(struct seq_file *m, void *v)
+{
+	struct proc_iface_stat_fmt_info *p = m->private;
+	struct iface_stat *iface_entry;
+	struct rtnl_link_stats64 dev_stats, *stats;
+	struct rtnl_link_stats64 no_dev_stats = {0};
+
+
+	CT_DEBUG("qtaguid:proc iface_stat_fmt pid=%u tgid=%u uid=%u\n",
+		 current->pid, current->tgid,
+		 from_kuid(p->net->user_ns, current_fsuid()));
+
+	iface_entry = list_entry(v, struct iface_stat, list);
+
+	if (iface_entry->active) {
+		stats = dev_get_stats(iface_entry->net_dev,
+				      &dev_stats);
+	} else {
+		stats = &no_dev_stats;
+	}
+	/*
+	 * If the meaning of the data changes, then update the fmtX
+	 * string.
+	 */
+	if (p->fmt == 1) {
+		seq_printf(m, "%s %d %llu %llu %llu %llu %llu %llu %llu %llu\n",
+			   iface_entry->ifname,
+			   iface_entry->active,
+			   iface_entry->totals_via_dev[IFS_RX].bytes,
+			   iface_entry->totals_via_dev[IFS_RX].packets,
+			   iface_entry->totals_via_dev[IFS_TX].bytes,
+			   iface_entry->totals_via_dev[IFS_TX].packets,
+			   stats->rx_bytes, stats->rx_packets,
+			   stats->tx_bytes, stats->tx_packets
+			   );
+	} else {
+		pp_iface_stat_line(m, iface_entry);
+	}
+	return 0;
+}
+
+static const struct proc_ops read_u64_fops = {
+	.proc_read	= read_proc_u64,
+	.proc_lseek	= default_llseek,
+};
+
+static const struct proc_ops read_bool_fops = {
+	.proc_read	= read_proc_bool,
+	.proc_lseek	= default_llseek,
+};
+
+static void iface_create_proc_worker(struct work_struct *work)
+{
+	struct proc_dir_entry *proc_entry;
+	struct iface_stat_work *isw = container_of(work, struct iface_stat_work,
+						   iface_work);
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(dev_net(isw->net_dev));
+	struct iface_stat *new_iface  = isw->iface_entry;
+
+	/* iface_entries are not deleted, so safe to manipulate. */
+	proc_entry = proc_mkdir(new_iface->ifname,
+				qtaguid_net->iface_stat_procdir);
+	if (IS_ERR_OR_NULL(proc_entry)) {
+		pr_err("qtaguid: iface_stat: create_proc(): alloc failed.\n");
+		goto abort;
+	}
+
+	new_iface->proc_ptr = proc_entry;
+
+	proc_create_data("tx_bytes", proc_iface_perms, proc_entry,
+			 &read_u64_fops,
+			 &new_iface->totals_via_dev[IFS_TX].bytes);
+	proc_create_data("rx_bytes", proc_iface_perms, proc_entry,
+			 &read_u64_fops,
+			 &new_iface->totals_via_dev[IFS_RX].bytes);
+	proc_create_data("tx_packets", proc_iface_perms, proc_entry,
+			 &read_u64_fops,
+			 &new_iface->totals_via_dev[IFS_TX].packets);
+	proc_create_data("rx_packets", proc_iface_perms, proc_entry,
+			 &read_u64_fops,
+			 &new_iface->totals_via_dev[IFS_RX].packets);
+	proc_create_data("active", proc_iface_perms, proc_entry,
+			 &read_bool_fops, &new_iface->active);
+
+	IF_DEBUG("qtaguid: iface_stat: create_proc(): done "
+		 "entry=%p dev=%s\n", new_iface, new_iface->ifname);
+abort:
+	dev_put(isw->net_dev);
+	kfree(isw);
+}
+
+static void iface_delete_proc(struct qtaguid_net *qtaguid_net,
+			      struct iface_stat *iface_entry)
+{
+	struct proc_dir_entry *proc_entry = iface_entry->proc_ptr;
+
+	if (!proc_entry)
+		return;
+
+	remove_proc_entry("active", proc_entry);
+	remove_proc_entry("rx_packets", proc_entry);
+	remove_proc_entry("tx_packets", proc_entry);
+	remove_proc_entry("rx_bytes", proc_entry);
+	remove_proc_entry("tx_bytes", proc_entry);
+	remove_proc_entry(iface_entry->ifname, qtaguid_net->iface_stat_procdir);
+}
+
+/*
+ * Will set the entry's active state, and
+ * update the net_dev accordingly also.
+ */
+static void _iface_stat_set_active(struct iface_stat *entry,
+				   struct net_device *net_dev,
+				   bool activate)
+{
+	if (activate) {
+		entry->net_dev = net_dev;
+		entry->active = true;
+		IF_DEBUG("qtaguid: %s(%s): "
+			 "enable tracking. rfcnt=%d\n", __func__,
+			 entry->ifname,
+			 __this_cpu_read(*net_dev->pcpu_refcnt));
+	} else {
+		entry->active = false;
+		entry->net_dev = NULL;
+		IF_DEBUG("qtaguid: %s(%s): "
+			 "disable tracking. rfcnt=%d\n", __func__,
+			 entry->ifname,
+			 __this_cpu_read(*net_dev->pcpu_refcnt));
+	}
+}
+
+/* Caller must hold iface_stat_list_lock */
+static struct iface_stat *iface_alloc(struct net_device *net_dev)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(dev_net(net_dev));
+	struct iface_stat *new_iface;
+	struct iface_stat_work *isw;
+
+	new_iface = kzalloc(sizeof(*new_iface), GFP_ATOMIC);
+	if (new_iface == NULL) {
+		pr_err("qtaguid: iface_stat: create(%s): "
+		       "iface_stat alloc failed\n", net_dev->name);
+		return NULL;
+	}
+	new_iface->ifname = kstrdup(net_dev->name, GFP_ATOMIC);
+	if (new_iface->ifname == NULL) {
+		pr_err("qtaguid: iface_stat: create(%s): "
+		       "ifname alloc failed\n", net_dev->name);
+		kfree(new_iface);
+		return NULL;
+	}
+	spin_lock_init(&new_iface->tag_stat_list_lock);
+	new_iface->tag_stat_tree = RB_ROOT;
+	_iface_stat_set_active(new_iface, net_dev, true);
+
+	/*
+	 * ipv6 notifier chains are atomic :(
+	 * No create_proc_read_entry() for you!
+	 */
+	isw = kmalloc(sizeof(*isw), GFP_ATOMIC);
+	if (!isw) {
+		pr_err("qtaguid: iface_stat: create(%s): "
+		       "work alloc failed\n", new_iface->ifname);
+		_iface_stat_set_active(new_iface, net_dev, false);
+		kfree(new_iface->ifname);
+		kfree(new_iface);
+		return NULL;
+	}
+	isw->iface_entry = new_iface;
+	dev_hold(net_dev);
+	isw->net_dev = net_dev;
+	INIT_WORK(&isw->iface_work, iface_create_proc_worker);
+	schedule_work(&isw->iface_work);
+	list_add(&new_iface->list, &qtaguid_net->iface_stat_list);
+	return new_iface;
+}
+
+static void iface_check_stats_reset_and_adjust(struct net_device *net_dev,
+					       struct iface_stat *iface)
+{
+	struct rtnl_link_stats64 dev_stats, *stats;
+	bool stats_rewound;
+
+	stats = dev_get_stats(net_dev, &dev_stats);
+	/* No empty packets */
+	stats_rewound =
+		(stats->rx_bytes < iface->last_known[IFS_RX].bytes)
+		|| (stats->tx_bytes < iface->last_known[IFS_TX].bytes);
+
+	IF_DEBUG("qtaguid: %s(%s): iface=%p netdev=%p "
+		 "bytes rx/tx=%llu/%llu "
+		 "active=%d last_known=%d "
+		 "stats_rewound=%d\n", __func__,
+		 net_dev ? net_dev->name : "?",
+		 iface, net_dev,
+		 stats->rx_bytes, stats->tx_bytes,
+		 iface->active, iface->last_known_valid, stats_rewound);
+
+	if (iface->active && iface->last_known_valid && stats_rewound) {
+		pr_warn_once("qtaguid: iface_stat: %s(%s): "
+			     "iface reset its stats unexpectedly\n", __func__,
+			     net_dev->name);
+
+		iface->totals_via_dev[IFS_TX].bytes +=
+			iface->last_known[IFS_TX].bytes;
+		iface->totals_via_dev[IFS_TX].packets +=
+			iface->last_known[IFS_TX].packets;
+		iface->totals_via_dev[IFS_RX].bytes +=
+			iface->last_known[IFS_RX].bytes;
+		iface->totals_via_dev[IFS_RX].packets +=
+			iface->last_known[IFS_RX].packets;
+		iface->last_known_valid = false;
+		IF_DEBUG("qtaguid: %s(%s): iface=%p "
+			 "used last known bytes rx/tx=%llu/%llu\n", __func__,
+			 iface->ifname, iface, iface->last_known[IFS_RX].bytes,
+			 iface->last_known[IFS_TX].bytes);
+	}
+}
+
+/*
+ * Create a new entry for tracking the specified interface.
+ * Do nothing if the entry already exists.
+ * Called when an interface is configured with a valid IP address.
+ */
+static void iface_stat_create(struct net_device *net_dev,
+			      struct in_ifaddr *ifa)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(dev_net(net_dev));
+	struct in_device *in_dev = NULL;
+	const char *ifname;
+	struct iface_stat *entry;
+	__be32 ipaddr = 0;
+	struct iface_stat *new_iface;
+
+	IF_DEBUG("qtaguid: iface_stat: create(%s): ifa=%p netdev=%p\n",
+		 net_dev ? net_dev->name : "?",
+		 ifa, net_dev);
+	if (!net_dev) {
+		pr_err("qtaguid: iface_stat: create(): no net dev\n");
+		return;
+	}
+
+	ifname = net_dev->name;
+	if (!ifa) {
+		in_dev = in_dev_get(net_dev);
+		if (!in_dev) {
+			pr_err("qtaguid: iface_stat: create(%s): no inet dev\n",
+			       ifname);
+			return;
+		}
+		IF_DEBUG("qtaguid: iface_stat: create(%s): in_dev=%p\n",
+			 ifname, in_dev);
+		for (ifa = in_dev->ifa_list; ifa; ifa = ifa->ifa_next) {
+			IF_DEBUG("qtaguid: iface_stat: create(%s): "
+				 "ifa=%p ifa_label=%s\n",
+				 ifname, ifa, ifa->ifa_label);
+			if (!strcmp(ifname, ifa->ifa_label))
+				break;
+		}
+	}
+
+	if (!ifa) {
+		IF_DEBUG("qtaguid: iface_stat: create(%s): no matching IP\n",
+			 ifname);
+		goto done_put;
+	}
+	ipaddr = ifa->ifa_local;
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	entry = get_iface_entry(qtaguid_net, ifname);
+	if (entry != NULL) {
+		IF_DEBUG("qtaguid: iface_stat: create(%s): entry=%p\n",
+			 ifname, entry);
+		iface_check_stats_reset_and_adjust(net_dev, entry);
+		_iface_stat_set_active(entry, net_dev, true);
+		IF_DEBUG("qtaguid: %s(%s): "
+			 "tracking now %d on ip=%pI4\n", __func__,
+			 entry->ifname, true, &ipaddr);
+		goto done_unlock_put;
+	}
+
+	new_iface = iface_alloc(net_dev);
+	IF_DEBUG("qtaguid: iface_stat: create(%s): done "
+		 "entry=%p ip=%pI4\n", ifname, new_iface, &ipaddr);
+done_unlock_put:
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+done_put:
+	if (in_dev)
+		in_dev_put(in_dev);
+}
+
+static void iface_stat_create_ipv6(struct net_device *net_dev,
+				   struct inet6_ifaddr *ifa)
+{
+	struct qtaguid_net *qtaguid_net =
+		qtaguid_pernet(dev_net(net_dev));
+	struct in_device *in_dev;
+	const char *ifname;
+	struct iface_stat *entry;
+	struct iface_stat *new_iface;
+	int addr_type;
+
+	IF_DEBUG("qtaguid: iface_stat: create6(): ifa=%p netdev=%p->name=%s\n",
+		 ifa, net_dev, net_dev ? net_dev->name : "");
+	if (!net_dev) {
+		pr_err("qtaguid: iface_stat: create6(): no net dev!\n");
+		return;
+	}
+	ifname = net_dev->name;
+
+	in_dev = in_dev_get(net_dev);
+	if (!in_dev) {
+		pr_err("qtaguid: iface_stat: create6(%s): no inet dev\n",
+		       ifname);
+		return;
+	}
+
+	IF_DEBUG("qtaguid: iface_stat: create6(%s): in_dev=%p\n",
+		 ifname, in_dev);
+
+	if (!ifa) {
+		IF_DEBUG("qtaguid: iface_stat: create6(%s): no matching IP\n",
+			 ifname);
+		goto done_put;
+	}
+	addr_type = ipv6_addr_type(&ifa->addr);
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	entry = get_iface_entry(qtaguid_net, ifname);
+	if (entry != NULL) {
+		IF_DEBUG("qtaguid: %s(%s): entry=%p\n", __func__,
+			 ifname, entry);
+		iface_check_stats_reset_and_adjust(net_dev, entry);
+		_iface_stat_set_active(entry, net_dev, true);
+		IF_DEBUG("qtaguid: %s(%s): "
+			 "tracking now %d on ip=%pI6c\n", __func__,
+			 entry->ifname, true, &ifa->addr);
+		goto done_unlock_put;
+	}
+
+	new_iface = iface_alloc(net_dev);
+	IF_DEBUG("qtaguid: iface_stat: create6(%s): done "
+		 "entry=%p ip=%pI6c\n", ifname, new_iface, &ifa->addr);
+
+done_unlock_put:
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+done_put:
+	in_dev_put(in_dev);
+}
+
+static struct sock_tag *get_sock_stat_nl(struct qtaguid_net *qtaguid_net,
+					 const struct sock *sk)
+{
+	MT_DEBUG("qtaguid: get_sock_stat_nl(sk=%p)\n", sk);
+	return sock_tag_tree_search(&qtaguid_net->sock_tag_tree, sk);
+}
+
+static struct sock_tag *get_sock_stat(struct qtaguid_net *qtaguid_net,
+				      const struct sock *sk)
+{
+	struct sock_tag *sock_tag_entry;
+	MT_DEBUG("qtaguid: get_sock_stat(sk=%p)\n", sk);
+	if (!sk)
+		return NULL;
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	sock_tag_entry = get_sock_stat_nl(qtaguid_net, sk);
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+	return sock_tag_entry;
+}
+
+static int ipx_proto(const struct sk_buff *skb,
+		     struct xt_action_param *par)
+{
+	int thoff = 0, tproto;
+
+	switch (par->state->pf) {
+	case NFPROTO_IPV6:
+		tproto = ipv6_find_hdr(skb, &thoff, -1, NULL, NULL);
+		if (tproto < 0)
+			MT_DEBUG("%s(): transport header not found in ipv6"
+				 " skb=%p\n", __func__, skb);
+		break;
+	case NFPROTO_IPV4:
+		tproto = ip_hdr(skb)->protocol;
+		break;
+	default:
+		tproto = IPPROTO_RAW;
+	}
+	return tproto;
+}
+
+static void
+data_counters_update(struct data_counters *dc, int set,
+		     enum ifs_tx_rx direction, int proto, int bytes)
+{
+	switch (proto) {
+	case IPPROTO_TCP:
+		dc_add_byte_packets(dc, set, direction, IFS_TCP, bytes, 1);
+		break;
+	case IPPROTO_UDP:
+		dc_add_byte_packets(dc, set, direction, IFS_UDP, bytes, 1);
+		break;
+	case IPPROTO_IP:
+	default:
+		dc_add_byte_packets(dc, set, direction, IFS_PROTO_OTHER, bytes,
+				    1);
+		break;
+	}
+}
+
+/*
+ * Update stats for the specified interface. Do nothing if the entry
+ * does not exist (when a device was never configured with an IP address).
+ * Called when an device is being unregistered.
+ */
+static void iface_stat_update(struct net_device *net_dev, bool stash_only)
+{
+	struct qtaguid_net *qtaguid_net =
+		qtaguid_pernet(dev_net(net_dev));
+	struct rtnl_link_stats64 dev_stats, *stats;
+	struct iface_stat *entry;
+
+	stats = dev_get_stats(net_dev, &dev_stats);
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	entry = get_iface_entry(qtaguid_net, net_dev->name);
+	if (entry == NULL) {
+		IF_DEBUG("qtaguid: iface_stat: update(%s): not tracked\n",
+			 net_dev->name);
+		spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+		return;
+	}
+
+	IF_DEBUG("qtaguid: %s(%s): entry=%p\n", __func__,
+		 net_dev->name, entry);
+	if (!entry->active) {
+		IF_DEBUG("qtaguid: %s(%s): already disabled\n", __func__,
+			 net_dev->name);
+		spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+		return;
+	}
+
+	if (stash_only) {
+		entry->last_known[IFS_TX].bytes = stats->tx_bytes;
+		entry->last_known[IFS_TX].packets = stats->tx_packets;
+		entry->last_known[IFS_RX].bytes = stats->rx_bytes;
+		entry->last_known[IFS_RX].packets = stats->rx_packets;
+		entry->last_known_valid = true;
+		IF_DEBUG("qtaguid: %s(%s): "
+			 "dev stats stashed rx/tx=%llu/%llu\n", __func__,
+			 net_dev->name, stats->rx_bytes, stats->tx_bytes);
+		spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+		return;
+	}
+	entry->totals_via_dev[IFS_TX].bytes += stats->tx_bytes;
+	entry->totals_via_dev[IFS_TX].packets += stats->tx_packets;
+	entry->totals_via_dev[IFS_RX].bytes += stats->rx_bytes;
+	entry->totals_via_dev[IFS_RX].packets += stats->rx_packets;
+	/* We don't need the last_known[] anymore */
+	entry->last_known_valid = false;
+	_iface_stat_set_active(entry, net_dev, false);
+	IF_DEBUG("qtaguid: %s(%s): "
+		 "disable tracking. rx/tx=%llu/%llu\n", __func__,
+		 net_dev->name, stats->rx_bytes, stats->tx_bytes);
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+}
+
+/* Guarantied to return a net_device that has a name */
+static void get_dev_and_dir(const struct sk_buff *skb,
+			    struct xt_action_param *par,
+			    enum ifs_tx_rx *direction,
+			    const struct net_device **el_dev)
+{
+	const struct nf_hook_state *parst = par->state;
+
+	BUG_ON(!direction || !el_dev);
+
+	if (parst->in) {
+		*el_dev = parst->in;
+		*direction = IFS_RX;
+	} else if (parst->out) {
+		*el_dev = parst->out;
+		*direction = IFS_TX;
+	} else {
+		pr_err("qtaguid[%d]: %s(): no par->state->in/out?!!\n",
+		       parst->hook, __func__);
+		BUG();
+	}
+	if (skb->dev && *el_dev != skb->dev) {
+		MT_DEBUG("qtaguid[%d]: skb->dev=%p %s vs par->%s=%p %s\n",
+			 parst->hook, skb->dev, skb->dev->name,
+			 *direction == IFS_RX ? "in" : "out",  *el_dev,
+			 (*el_dev)->name);
+	}
+}
+
+/*
+ * Update stats for the specified interface from the skb.
+ * Do nothing if the entry
+ * does not exist (when a device was never configured with an IP address).
+ * Called on each sk.
+ */
+static void iface_stat_update_from_skb(const struct net *net,
+				       const struct sk_buff *skb,
+				       struct xt_action_param *par)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	const struct nf_hook_state *parst = par->state;
+	struct iface_stat *entry;
+	const struct net_device *el_dev;
+	enum ifs_tx_rx direction;
+	int bytes = skb->len;
+	int proto;
+
+	get_dev_and_dir(skb, par, &direction, &el_dev);
+	proto = ipx_proto(skb, par);
+	MT_DEBUG("qtaguid[%d]: iface_stat: %s(%s): "
+		 "type=%d fam=%d proto=%d dir=%d\n",
+		 parst->hook, __func__, el_dev->name, el_dev->type,
+		 parst->pf, proto, direction);
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	entry = get_iface_entry(qtaguid_net, el_dev->name);
+	if (entry == NULL) {
+		IF_DEBUG("qtaguid[%d]: iface_stat: %s(%s): not tracked\n",
+			 parst->hook, __func__, el_dev->name);
+		spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+		return;
+	}
+
+	IF_DEBUG("qtaguid[%d]: %s(%s): entry=%p\n", parst->hook,  __func__,
+		 el_dev->name, entry);
+
+	data_counters_update(&entry->totals_via_skb, 0, direction, proto,
+			     bytes);
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+}
+
+static void tag_stat_update(struct qtaguid_net *qtaguid_net,
+			    struct tag_stat *tag_entry,
+			    enum ifs_tx_rx direction, int proto, int bytes)
+{
+	int active_set;
+	active_set = get_active_counter_set(qtaguid_net, tag_entry->tn.tag);
+	MT_DEBUG("qtaguid: tag_stat_update(tag=0x%llx (uid=%u) set=%d "
+		 "dir=%d proto=%d bytes=%d)\n",
+		 tag_entry->tn.tag, get_uid_from_tag(tag_entry->tn.tag),
+		 active_set, direction, proto, bytes);
+	data_counters_update(&tag_entry->counters, active_set, direction,
+			     proto, bytes);
+	if (tag_entry->parent_counters)
+		data_counters_update(tag_entry->parent_counters, active_set,
+				     direction, proto, bytes);
+}
+
+/*
+ * Create a new entry for tracking the specified {acct_tag,uid_tag} within
+ * the interface.
+ * iface_entry->tag_stat_list_lock should be held.
+ */
+static struct tag_stat *create_if_tag_stat(struct iface_stat *iface_entry,
+					   tag_t tag)
+{
+	struct tag_stat *new_tag_stat_entry = NULL;
+	IF_DEBUG("qtaguid: iface_stat: %s(): ife=%p tag=0x%llx"
+		 " (uid=%u)\n", __func__,
+		 iface_entry, tag, get_uid_from_tag(tag));
+	new_tag_stat_entry = kzalloc(sizeof(*new_tag_stat_entry), GFP_ATOMIC);
+	if (!new_tag_stat_entry) {
+		pr_err("qtaguid: iface_stat: tag stat alloc failed\n");
+		goto done;
+	}
+	new_tag_stat_entry->tn.tag = tag;
+	tag_stat_tree_insert(new_tag_stat_entry, &iface_entry->tag_stat_tree);
+done:
+	return new_tag_stat_entry;
+}
+
+static void if_tag_stat_update(const struct net_device *net_dev, uid_t uid,
+			       const struct sock *sk, enum ifs_tx_rx direction,
+			       int proto, int bytes)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(dev_net(net_dev));
+	struct tag_stat *tag_stat_entry;
+	tag_t tag, acct_tag;
+	tag_t uid_tag;
+	struct data_counters *uid_tag_counters;
+	struct sock_tag *sock_tag_entry;
+	struct iface_stat *iface_entry;
+	struct tag_stat *new_tag_stat = NULL;
+	MT_DEBUG("qtaguid: if_tag_stat_update(ifname=%s "
+		"uid=%u sk=%p dir=%d proto=%d bytes=%d)\n",
+		 net_dev->name, uid, sk, direction, proto, bytes);
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	iface_entry = get_iface_entry(qtaguid_net, net_dev->name);
+	if (!iface_entry) {
+		pr_err_ratelimited("qtaguid: tag_stat: stat_update() "
+				   "%s not found\n", net_dev->name);
+		spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+		return;
+	}
+	/* It is ok to process data when an iface_entry is inactive */
+
+	MT_DEBUG("qtaguid: tag_stat: stat_update() dev=%s entry=%p\n",
+		 net_dev->name, iface_entry);
+
+	/*
+	 * Look for a tagged sock.
+	 * It will have an acct_uid.
+	 */
+	sock_tag_entry = get_sock_stat(qtaguid_net, sk);
+	if (sock_tag_entry) {
+		tag = sock_tag_entry->tag;
+		acct_tag = get_atag_from_tag(tag);
+		uid_tag = get_utag_from_tag(tag);
+	} else {
+		acct_tag = make_atag_from_value(0);
+		tag = combine_atag_with_uid(acct_tag, uid);
+		uid_tag = make_tag_from_uid(uid);
+	}
+	MT_DEBUG("qtaguid: tag_stat: stat_update(): "
+		 " looking for tag=0x%llx (uid=%u) in ife=%p\n",
+		 tag, get_uid_from_tag(tag), iface_entry);
+	/* Loop over tag list under this interface for {acct_tag,uid_tag} */
+	spin_lock_bh(&iface_entry->tag_stat_list_lock);
+
+	tag_stat_entry = tag_stat_tree_search(&iface_entry->tag_stat_tree,
+					      tag);
+	if (tag_stat_entry) {
+		/*
+		 * Updating the {acct_tag, uid_tag} entry handles both stats:
+		 * {0, uid_tag} will also get updated.
+		 */
+		tag_stat_update(qtaguid_net, tag_stat_entry, direction, proto,
+				bytes);
+		goto unlock;
+	}
+
+	/* Loop over tag list under this interface for {0,uid_tag} */
+	tag_stat_entry = tag_stat_tree_search(&iface_entry->tag_stat_tree,
+					      uid_tag);
+	if (!tag_stat_entry) {
+		/* Here: the base uid_tag did not exist */
+		/*
+		 * No parent counters. So
+		 *  - No {0, uid_tag} stats and no {acc_tag, uid_tag} stats.
+		 */
+		new_tag_stat = create_if_tag_stat(iface_entry, uid_tag);
+		if (!new_tag_stat)
+			goto unlock;
+		uid_tag_counters = &new_tag_stat->counters;
+	} else {
+		uid_tag_counters = &tag_stat_entry->counters;
+	}
+
+	if (acct_tag) {
+		/* Create the child {acct_tag, uid_tag} and hook up parent. */
+		new_tag_stat = create_if_tag_stat(iface_entry, tag);
+		if (!new_tag_stat)
+			goto unlock;
+		new_tag_stat->parent_counters = uid_tag_counters;
+	} else {
+		/*
+		 * For new_tag_stat to be still NULL here would require:
+		 *  {0, uid_tag} exists
+		 *  and {acct_tag, uid_tag} doesn't exist
+		 *  AND acct_tag == 0.
+		 * Impossible. This reassures us that new_tag_stat
+		 * below will always be assigned.
+		 */
+		BUG_ON(!new_tag_stat);
+	}
+	tag_stat_update(qtaguid_net, new_tag_stat, direction, proto, bytes);
+unlock:
+	spin_unlock_bh(&iface_entry->tag_stat_list_lock);
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+}
+
+static int iface_netdev_event_handler(struct notifier_block *nb,
+				      unsigned long event, void *ptr) {
+	struct net_device *dev = netdev_notifier_info_to_dev(ptr);
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(dev_net(dev));
+
+	if (unlikely(module_passive))
+		return NOTIFY_DONE;
+
+	IF_DEBUG("qtaguid: iface_stat: netdev_event(): "
+		 "ev=0x%lx/%s netdev=%p->name=%s\n",
+		 event, netdev_evt_str(event), dev, dev ? dev->name : "");
+
+	switch (event) {
+	case NETDEV_UP:
+		iface_stat_create(dev, NULL);
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	case NETDEV_DOWN:
+	case NETDEV_UNREGISTER:
+		iface_stat_update(dev, event == NETDEV_DOWN);
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static int iface_inet6addr_event_handler(struct notifier_block *nb,
+					 unsigned long event, void *ptr)
+{
+	struct inet6_ifaddr *ifa = ptr;
+	struct net_device *dev;
+	struct qtaguid_net *qtaguid_net;
+
+	if (unlikely(module_passive))
+		return NOTIFY_DONE;
+
+	IF_DEBUG("qtaguid: iface_stat: inet6addr_event(): "
+		 "ev=0x%lx/%s ifa=%p\n",
+		 event, netdev_evt_str(event), ifa);
+
+	switch (event) {
+	case NETDEV_UP:
+		BUG_ON(!ifa || !ifa->idev);
+		dev = (struct net_device *)ifa->idev->dev;
+		iface_stat_create_ipv6(dev, ifa);
+		qtaguid_net = qtaguid_pernet(dev_net(dev));
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	case NETDEV_DOWN:
+	case NETDEV_UNREGISTER:
+		BUG_ON(!ifa || !ifa->idev);
+		dev = (struct net_device *)ifa->idev->dev;
+		iface_stat_update(dev, event == NETDEV_DOWN);
+		qtaguid_net = qtaguid_pernet(dev_net(dev));
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static int iface_inetaddr_event_handler(struct notifier_block *nb,
+					unsigned long event, void *ptr)
+{
+	struct in_ifaddr *ifa = ptr;
+	struct net_device *dev;
+	struct qtaguid_net *qtaguid_net;
+
+	if (unlikely(module_passive))
+		return NOTIFY_DONE;
+
+	IF_DEBUG("qtaguid: iface_stat: inetaddr_event(): "
+		 "ev=0x%lx/%s ifa=%p\n",
+		 event, netdev_evt_str(event), ifa);
+
+	switch (event) {
+	case NETDEV_UP:
+		BUG_ON(!ifa || !ifa->ifa_dev);
+		dev = ifa->ifa_dev->dev;
+		iface_stat_create(dev, ifa);
+		qtaguid_net = qtaguid_pernet(dev_net(dev));
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	case NETDEV_DOWN:
+	case NETDEV_UNREGISTER:
+		BUG_ON(!ifa || !ifa->ifa_dev);
+		dev = ifa->ifa_dev->dev;
+		iface_stat_update(dev, event == NETDEV_DOWN);
+		qtaguid_net = qtaguid_pernet(dev_net(dev));
+		atomic64_inc(&qtaguid_net->qtu_events.iface_events);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block iface_netdev_notifier_blk = {
+	.notifier_call = iface_netdev_event_handler,
+};
+
+static struct notifier_block iface_inetaddr_notifier_blk = {
+	.notifier_call = iface_inetaddr_event_handler,
+};
+
+static struct notifier_block iface_inet6addr_notifier_blk = {
+	.notifier_call = iface_inet6addr_event_handler,
+};
+
+static const struct seq_operations iface_stat_fmt_proc_seq_ops = {
+	.start	= iface_stat_fmt_proc_start,
+	.next	= iface_stat_fmt_proc_next,
+	.stop	= iface_stat_fmt_proc_stop,
+	.show	= iface_stat_fmt_proc_show,
+};
+
+static int proc_iface_stat_all_open(struct inode *inode, struct file *file)
+{
+	struct proc_iface_stat_fmt_info *s;
+
+	s = __seq_open_private(file, &iface_stat_fmt_proc_seq_ops, sizeof(*s));
+	if (!s)
+		return -ENOMEM;
+
+	s->fmt = 1;
+	s->net = PDE_DATA(inode);
+	return 0;
+}
+
+static int proc_iface_stat_fmt_open(struct inode *inode, struct file *file)
+{
+	struct proc_iface_stat_fmt_info *s;
+
+	s = __seq_open_private(file, &iface_stat_fmt_proc_seq_ops, sizeof(*s));
+	if (!s)
+		return -ENOMEM;
+
+	s->fmt = 2;
+	s->net = PDE_DATA(inode);
+	return 0;
+}
+
+static const struct proc_ops proc_iface_stat_all_fops = {
+	.proc_open	= proc_iface_stat_all_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= seq_release_private,
+};
+
+static const struct proc_ops proc_iface_stat_fmt_fops = {
+	.proc_open	= proc_iface_stat_fmt_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= seq_release_private,
+};
+
+static struct sock *qtaguid_find_sk(const struct sk_buff *skb,
+				    struct xt_action_param *par)
+{
+	const struct nf_hook_state *parst = par->state;
+	struct sock *sk;
+	unsigned int hook_mask = (1 << parst->hook);
+
+	MT_DEBUG("qtaguid[%d]: find_sk(skb=%p) family=%d\n",
+		 parst->hook, skb, parst->pf);
+
+	/*
+	 * Let's not abuse the the xt_socket_get*_sk(), or else it will
+	 * return garbage SKs.
+	 */
+	if (!(hook_mask & XT_SOCKET_SUPPORTED_HOOKS))
+		return NULL;
+
+	switch (parst->pf) {
+	case NFPROTO_IPV6:
+		sk = nf_sk_lookup_slow_v6(dev_net(skb->dev), skb, parst->in);
+		break;
+	case NFPROTO_IPV4:
+		sk = nf_sk_lookup_slow_v4(dev_net(skb->dev), skb, parst->in);
+		break;
+	default:
+		return NULL;
+	}
+
+	if (sk) {
+		MT_DEBUG("qtaguid[%d]: %p->sk_proto=%u->sk_state=%d\n",
+			 parst->hook, sk, sk->sk_protocol, sk->sk_state);
+	}
+	return sk;
+}
+
+static void account_for_uid(const struct sk_buff *skb,
+			    const struct sock *alternate_sk, uid_t uid,
+			    struct xt_action_param *par)
+{
+	const struct net_device *el_dev;
+	enum ifs_tx_rx direction;
+	int proto;
+
+	get_dev_and_dir(skb, par, &direction, &el_dev);
+	proto = ipx_proto(skb, par);
+	MT_DEBUG("qtaguid[%d]: dev name=%s type=%d fam=%d proto=%d dir=%d\n",
+		 par->state->hook, el_dev->name, el_dev->type,
+		 par->state->pf, proto, direction);
+
+	if_tag_stat_update(el_dev, uid,
+			   skb->sk ? skb->sk : alternate_sk,
+			   direction,
+			   proto, skb->len);
+}
+
+/* This function is based on xt_owner.c:owner_check(). */
+static int qtaguid_check(const struct xt_mtchk_param *par)
+{
+	struct xt_qtaguid_match_info *info = par->matchinfo;
+	struct net *net = par->net;
+
+	/* Only allow the common case where the userns of the writer
+	 * matches the userns of the network namespace.
+	 */
+	if ((info->match & (XT_QTAGUID_UID | XT_QTAGUID_GID)) &&
+	    (current_user_ns() != net->user_ns))
+		return -EINVAL;
+
+	/* Ensure the uids are valid */
+	if (info->match & XT_QTAGUID_UID) {
+		kuid_t uid_min = make_kuid(net->user_ns, info->uid_min);
+		kuid_t uid_max = make_kuid(net->user_ns, info->uid_max);
+
+		if (!uid_valid(uid_min) || !uid_valid(uid_max) ||
+		    (info->uid_max < info->uid_min) ||
+		    uid_lt(uid_max, uid_min)) {
+			return -EINVAL;
+		}
+	}
+
+	/* Ensure the gids are valid */
+	if (info->match & XT_QTAGUID_GID) {
+		kgid_t gid_min = make_kgid(net->user_ns, info->gid_min);
+		kgid_t gid_max = make_kgid(net->user_ns, info->gid_max);
+
+		if (!gid_valid(gid_min) || !gid_valid(gid_max) ||
+		    (info->gid_max < info->gid_min) ||
+		    gid_lt(gid_max, gid_min)) {
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+static bool qtaguid_mt(const struct sk_buff *skb, struct xt_action_param *par)
+{
+	const struct xt_qtaguid_match_info *info = par->matchinfo;
+	const struct nf_hook_state *parst = par->state;
+	const struct file *filp;
+	const struct net *net = dev_net(xt_in(par) ? xt_in(par) : xt_out(par));
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	bool got_sock = false;
+	struct sock *sk;
+	kuid_t sock_uid;
+	bool res;
+	bool set_sk_callback_lock = false;
+	/*
+	 * TODO: unhack how to force just accounting.
+	 * For now we only do tag stats when the uid-owner is not requested
+	 */
+	bool do_tag_stat = !(info->match & XT_QTAGUID_UID);
+
+	if (unlikely(module_passive))
+		return (info->match ^ info->invert) == 0;
+
+	MT_DEBUG("qtaguid[%d]: entered skb=%p par->in=%p/out=%p fam=%d\n",
+		 parst->hook, skb, parst->in, parst->out, parst->pf);
+
+	atomic64_inc(&qtaguid_net->qtu_events.match_calls);
+	if (skb == NULL) {
+		res = (info->match ^ info->invert) == 0;
+		goto ret_res;
+	}
+
+	switch (parst->hook) {
+	case NF_INET_PRE_ROUTING:
+	case NF_INET_POST_ROUTING:
+		atomic64_inc(&qtaguid_net->qtu_events.match_calls_prepost);
+		iface_stat_update_from_skb(net, skb, par);
+		/*
+		 * We are done in pre/post. The skb will get processed
+		 * further alter.
+		 */
+		res = (info->match ^ info->invert);
+		goto ret_res;
+		break;
+	/* default: Fall through and do UID releated work */
+	}
+
+	sk = skb_to_full_sk(skb);
+	/*
+	 * When in TCP_TIME_WAIT the sk is not a "struct sock" but
+	 * "struct inet_timewait_sock" which is missing fields.
+	 * So we ignore it.
+	 */
+	if (sk && sk->sk_state == TCP_TIME_WAIT)
+		sk = NULL;
+	if (sk == NULL) {
+		/*
+		 * A missing sk->sk_socket happens when packets are in-flight
+		 * and the matching socket is already closed and gone.
+		 */
+		sk = qtaguid_find_sk(skb, par);
+		/*
+		 * TCP_NEW_SYN_RECV are not "struct sock" but "struct request_sock"
+		 * where we can get a pointer to a full socket to retrieve uid/gid.
+		 * When in TCP_TIME_WAIT, sk is a struct inet_timewait_sock
+		 * which is missing fields and does not contain any reference
+		 * to a full socket, so just ignore the socket.
+		 */
+		if (sk && sk->sk_state == TCP_NEW_SYN_RECV) {
+			sock_gen_put(sk);
+			sk = sk_to_full_sk(sk);
+		} else if (sk && (!sk_fullsock(sk) || sk->sk_state == TCP_TIME_WAIT)) {
+			sock_gen_put(sk);
+			sk = NULL;
+		} else {
+			/*
+			 * If we got the socket from the find_sk(), we will need to put
+			 * it back, as nf_tproxy_get_sock_v4() got it.
+			 */
+			got_sock = sk;
+		}
+		if (sk)
+			atomic64_inc(&qtaguid_net->qtu_events.match_found_sk_in_ct);
+		else
+			atomic64_inc(&qtaguid_net->qtu_events.match_found_no_sk_in_ct);
+	} else {
+		atomic64_inc(&qtaguid_net->qtu_events.match_found_sk);
+	}
+	MT_DEBUG("qtaguid[%d]: sk=%p got_sock=%d fam=%d proto=%d\n",
+		 parst->hook, sk, got_sock, parst->pf, ipx_proto(skb, par));
+
+	if (!sk) {
+		/*
+		 * Here, the qtaguid_find_sk() using connection tracking
+		 * couldn't find the owner, so for now we just count them
+		 * against the system.
+		 */
+		if (do_tag_stat)
+			account_for_uid(skb, sk, 0, par);
+		MT_DEBUG("qtaguid[%d]: leaving (sk=NULL)\n", parst->hook);
+		res = (info->match ^ info->invert) == 0;
+		atomic64_inc(&qtaguid_net->qtu_events.match_no_sk);
+		goto put_sock_ret_res;
+	} else if (info->match & info->invert & XT_QTAGUID_SOCKET) {
+		res = false;
+		goto put_sock_ret_res;
+	}
+	sock_uid = sk->sk_uid;
+	net = sock_net(sk);
+	if (do_tag_stat)
+		account_for_uid(skb, sk, from_kuid(net->user_ns, sock_uid),
+				par);
+
+	/*
+	 * The following two tests fail the match when:
+	 *    id not in range AND no inverted condition requested
+	 * or id     in range AND    inverted condition requested
+	 * Thus (!a && b) || (a && !b) == a ^ b
+	 */
+	if (info->match & XT_QTAGUID_UID) {
+		kuid_t uid_min = make_kuid(net->user_ns, info->uid_min);
+		kuid_t uid_max = make_kuid(net->user_ns, info->uid_max);
+
+		if ((uid_gte(sock_uid, uid_min) &&
+		     uid_lte(sock_uid, uid_max)) ^
+		    !(info->invert & XT_QTAGUID_UID)) {
+			MT_DEBUG("qtaguid[%d]: leaving uid not matching\n",
+				 parst->hook);
+			res = false;
+			goto put_sock_ret_res;
+		}
+	}
+	if (info->match & XT_QTAGUID_GID) {
+		kgid_t gid_min = make_kgid(net->user_ns, info->gid_min);
+		kgid_t gid_max = make_kgid(net->user_ns, info->gid_max);
+		set_sk_callback_lock = true;
+		read_lock_bh(&sk->sk_callback_lock);
+		MT_DEBUG("qtaguid[%d]: sk=%p->sk_socket=%p->file=%p\n",
+			 parst->hook, sk, sk->sk_socket,
+			 sk->sk_socket ? sk->sk_socket->file : (void *)-1LL);
+		filp = sk->sk_socket ? sk->sk_socket->file : NULL;
+		if (!filp) {
+			res = ((info->match ^ info->invert) &
+			       XT_QTAGUID_GID) == 0;
+			atomic64_inc(&qtaguid_net->qtu_events.match_no_sk_gid);
+			goto put_sock_ret_res;
+		}
+		MT_DEBUG("qtaguid[%d]: filp...uid=%u\n",
+			 parst->hook, filp ?
+			 from_kuid(net->user_ns, filp->f_cred->fsuid) : -1);
+
+		if ((gid_gte(filp->f_cred->fsgid, gid_min) &&
+				gid_lte(filp->f_cred->fsgid, gid_max)) ^
+			!(info->invert & XT_QTAGUID_GID)) {
+			MT_DEBUG("qtaguid[%d]: leaving gid not matching\n",
+				parst->hook);
+			res = false;
+			goto put_sock_ret_res;
+		}
+	}
+	MT_DEBUG("qtaguid[%d]: leaving matched\n", parst->hook);
+	res = true;
+
+put_sock_ret_res:
+	if (got_sock)
+		sock_gen_put(sk);
+	if (set_sk_callback_lock)
+		read_unlock_bh(&sk->sk_callback_lock);
+ret_res:
+	MT_DEBUG("qtaguid[%d]: left %d\n", parst->hook, res);
+	return res;
+}
+
+#ifdef DDEBUG
+/*
+ * This function is not in xt_qtaguid_print.c because of locks visibility.
+ * The lock of sock_tag_list must be aquired before calling this function
+ */
+static void prdebug_full_state_locked(struct qtaguid_net *qtaguid_net,
+				      int indent_level, const char *fmt, ...)
+{
+	va_list args;
+	char *fmt_buff;
+	char *buff;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	fmt_buff = kasprintf(GFP_ATOMIC,
+			     "qtaguid: %s(): %s {\n", __func__, fmt);
+	BUG_ON(!fmt_buff);
+	va_start(args, fmt);
+	buff = kvasprintf(GFP_ATOMIC,
+			  fmt_buff, args);
+	BUG_ON(!buff);
+	pr_debug("%s", buff);
+	kfree(fmt_buff);
+	kfree(buff);
+	va_end(args);
+
+	prdebug_sock_tag_tree(indent_level, &qtaguid_net->sock_tag_tree);
+
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	prdebug_uid_tag_data_tree(indent_level,
+				  &qtaguid_net->uid_tag_data_tree);
+	prdebug_proc_qtu_data_tree(indent_level,
+				   &qtaguid_net->proc_qtu_data_tree);
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	prdebug_iface_stat_list(indent_level, &qtaguid_net->iface_stat_list);
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+
+	pr_debug("qtaguid: %s(): }\n", __func__);
+}
+#else
+static void prdebug_full_state_locked(struct qtaguid_net *qtaguid_net,
+				      int indent_level, const char *fmt, ...) {}
+#endif
+
+struct proc_ctrl_print_info {
+	struct net *net;
+	struct sock *sk; /* socket found by reading to sk_pos */
+	loff_t sk_pos;
+};
+
+static void *qtaguid_ctrl_proc_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	struct proc_ctrl_print_info *pcpi = m->private;
+	struct sock_tag *sock_tag_entry = v;
+	struct rb_node *node;
+
+	(*pos)++;
+
+	if (!v || v  == SEQ_START_TOKEN)
+		return NULL;
+
+	node = rb_next(&sock_tag_entry->sock_node);
+	if (!node) {
+		pcpi->sk = NULL;
+		sock_tag_entry = SEQ_START_TOKEN;
+	} else {
+		sock_tag_entry = rb_entry(node, struct sock_tag, sock_node);
+		pcpi->sk = sock_tag_entry->sk;
+	}
+	pcpi->sk_pos = *pos;
+	return sock_tag_entry;
+}
+
+static void *qtaguid_ctrl_proc_start(struct seq_file *m, loff_t *pos)
+{
+	struct proc_ctrl_print_info *pcpi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(pcpi->net);
+	struct sock_tag *sock_tag_entry;
+	struct rb_node *node;
+
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+
+	if (unlikely(module_passive))
+		return NULL;
+
+	if (*pos == 0) {
+		pcpi->sk_pos = 0;
+		node = rb_first(&qtaguid_net->sock_tag_tree);
+		if (!node) {
+			pcpi->sk = NULL;
+			return SEQ_START_TOKEN;
+		}
+		sock_tag_entry = rb_entry(node, struct sock_tag, sock_node);
+		pcpi->sk = sock_tag_entry->sk;
+	} else {
+		sock_tag_entry = (pcpi->sk ?
+			get_sock_stat_nl(qtaguid_net, pcpi->sk) :
+			NULL) ?: SEQ_START_TOKEN;
+		if (*pos != pcpi->sk_pos) {
+			/* seq_read skipped a next call */
+			*pos = pcpi->sk_pos;
+			return qtaguid_ctrl_proc_next(m, sock_tag_entry, pos);
+		}
+	}
+	return sock_tag_entry;
+}
+
+static void qtaguid_ctrl_proc_stop(struct seq_file *m, void *v)
+{
+	struct proc_ctrl_print_info *pcpi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(pcpi->net);
+
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+}
+
+/*
+ * Procfs reader to get all active socket tags using style "1)" as described in
+ * fs/proc/generic.c
+ */
+static int qtaguid_ctrl_proc_show(struct seq_file *m, void *v)
+{
+	struct proc_ctrl_print_info *pcpi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(pcpi->net);
+	struct sock_tag *sock_tag_entry = v;
+	uid_t uid;
+
+	CT_DEBUG("qtaguid: proc ctrl pid=%u tgid=%u uid=%u\n",
+		 current->pid, current->tgid,
+		 from_kuid(pcpi->net->user_ns, current_fsuid()));
+
+	if (sock_tag_entry != SEQ_START_TOKEN) {
+		int sk_ref_count;
+		uid = get_uid_from_tag(sock_tag_entry->tag);
+		CT_DEBUG("qtaguid: proc_read(): sk=%p tag=0x%llx (uid=%u) "
+			 "pid=%u\n",
+			 sock_tag_entry->sk,
+			 sock_tag_entry->tag,
+			 uid,
+			 sock_tag_entry->pid
+			);
+		sk_ref_count = refcount_read(
+			&sock_tag_entry->sk->sk_refcnt);
+		seq_printf(m, "sock=%pK tag=0x%llx (uid=%u) pid=%u "
+			   "f_count=%d\n",
+			   sock_tag_entry->sk,
+			   sock_tag_entry->tag, uid,
+			   sock_tag_entry->pid, sk_ref_count);
+	} else {
+		seq_printf(m, "events: sockets_tagged=%llu "
+			   "sockets_untagged=%llu "
+			   "counter_set_changes=%llu "
+			   "delete_cmds=%llu "
+			   "iface_events=%llu "
+			   "match_calls=%llu "
+			   "match_calls_prepost=%llu "
+			   "match_found_sk=%llu "
+			   "match_found_sk_in_ct=%llu "
+			   "match_found_no_sk_in_ct=%llu "
+			   "match_no_sk=%llu "
+			   "match_no_sk_gid=%llu\n",
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.sockets_tagged),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.sockets_untagged),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.counter_set_changes),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.delete_cmds),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.iface_events),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_calls),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_calls_prepost),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_found_sk),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_found_sk_in_ct),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_found_no_sk_in_ct),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_no_sk),
+			   (u64)atomic64_read(&qtaguid_net->qtu_events.match_no_sk_gid));
+
+		/* Count the following as part of the last item_index. No need
+		 * to lock the sock_tag_list here since it is already locked when
+		 * starting the seq_file operation
+		 */
+		prdebug_full_state_locked(qtaguid_net, 0, "proc ctrl");
+	}
+
+	return 0;
+}
+
+/*
+ * Delete socket tags, and stat tags associated with a given
+ * accouting tag and uid.
+ */
+static int ctrl_cmd_delete(struct net *net, const char *input)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	char cmd;
+	int uid_int;
+	kuid_t uid;
+	uid_t entry_uid;
+	tag_t acct_tag;
+	tag_t tag;
+	int res, argc;
+	struct iface_stat *iface_entry;
+	struct rb_node *node;
+	struct sock_tag *st_entry;
+	struct rb_root st_to_free_tree = RB_ROOT;
+	struct tag_stat *ts_entry;
+	struct tag_counter_set *tcs_entry;
+	struct tag_ref *tr_entry;
+	struct uid_tag_data *utd_entry;
+
+	argc = sscanf(input, "%c %llu %u", &cmd, &acct_tag, &uid_int);
+	uid = make_kuid(net->user_ns, uid_int);
+	CT_DEBUG("qtaguid: ctrl_delete(%s): argc=%d cmd=%c "
+		 "user_tag=0x%llx uid=%u\n", input, argc, cmd,
+		 acct_tag, uid_int);
+	if (argc < 2) {
+		res = -EINVAL;
+		goto err;
+	}
+	if (!valid_atag(acct_tag)) {
+		pr_info("qtaguid: ctrl_delete(%s): invalid tag\n", input);
+		res = -EINVAL;
+		goto err;
+	}
+	if (argc < 3) {
+		uid = current_fsuid();
+		uid_int = from_kuid(net->user_ns, uid);
+	} else if (!can_impersonate_uid(net, uid)) {
+		pr_info("qtaguid: ctrl_delete(%s): "
+			"insufficient priv from pid=%u tgid=%u uid=%u\n",
+			input, current->pid, current->tgid,
+			from_kuid(net->user_ns, current_fsuid()));
+		res = -EPERM;
+		goto err;
+	}
+
+	tag = combine_atag_with_uid(acct_tag, uid_int);
+	CT_DEBUG("qtaguid: ctrl_delete(%s): "
+		 "looking for tag=0x%llx (uid=%u)\n",
+		 input, tag, uid_int);
+
+	/* Delete socket tags */
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	node = rb_first(&qtaguid_net->sock_tag_tree);
+	while (node) {
+		st_entry = rb_entry(node, struct sock_tag, sock_node);
+		entry_uid = get_uid_from_tag(st_entry->tag);
+		node = rb_next(node);
+		if (entry_uid != uid_int)
+			continue;
+
+		CT_DEBUG("qtaguid: ctrl_delete(%s): st tag=0x%llx (uid=%u)\n",
+			 input, st_entry->tag, entry_uid);
+
+		if (!acct_tag || st_entry->tag == tag) {
+			rb_erase(&st_entry->sock_node,
+				 &qtaguid_net->sock_tag_tree);
+			/* Can't sockfd_put() within spinlock, do it later. */
+			sock_tag_tree_insert(st_entry, &st_to_free_tree);
+			tr_entry = lookup_tag_ref(qtaguid_net, st_entry->tag,
+						  NULL);
+			BUG_ON(tr_entry->num_sock_tags <= 0);
+			tr_entry->num_sock_tags--;
+			/*
+			 * TODO: remove if, and start failing.
+			 * This is a hack to work around the fact that in some
+			 * places we have "if (IS_ERR_OR_NULL(pqd_entry))"
+			 * and are trying to work around apps
+			 * that didn't open the /dev/xt_qtaguid.
+			 */
+			if (st_entry->list.next && st_entry->list.prev)
+				list_del(&st_entry->list);
+		}
+	}
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+
+	sock_tag_tree_erase(&st_to_free_tree);
+
+	/* Delete tag counter-sets */
+	spin_lock_bh(&qtaguid_net->tag_counter_set_list_lock);
+	/* Counter sets are only on the uid tag, not full tag */
+	tcs_entry = tag_counter_set_tree_search(
+		&qtaguid_net->tag_counter_set_tree, tag);
+	if (tcs_entry) {
+		CT_DEBUG("qtaguid: ctrl_delete(%s): "
+			 "erase tcs: tag=0x%llx (uid=%u) set=%d\n",
+			 input,
+			 tcs_entry->tn.tag,
+			 get_uid_from_tag(tcs_entry->tn.tag),
+			 tcs_entry->active_set);
+		rb_erase(&tcs_entry->tn.node,
+			 &qtaguid_net->tag_counter_set_tree);
+		kfree(tcs_entry);
+	}
+	spin_unlock_bh(&qtaguid_net->tag_counter_set_list_lock);
+
+	/*
+	 * If acct_tag is 0, then all entries belonging to uid are
+	 * erased.
+	 */
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+	list_for_each_entry(iface_entry, &qtaguid_net->iface_stat_list, list) {
+		spin_lock_bh(&iface_entry->tag_stat_list_lock);
+		node = rb_first(&iface_entry->tag_stat_tree);
+		while (node) {
+			ts_entry = rb_entry(node, struct tag_stat, tn.node);
+			entry_uid = get_uid_from_tag(ts_entry->tn.tag);
+			node = rb_next(node);
+
+			CT_DEBUG("qtaguid: ctrl_delete(%s): "
+				 "ts tag=0x%llx (uid=%u)\n",
+				 input, ts_entry->tn.tag, entry_uid);
+
+			if (entry_uid != uid_int)
+				continue;
+			if (!acct_tag || ts_entry->tn.tag == tag) {
+				CT_DEBUG("qtaguid: ctrl_delete(%s): "
+					 "erase ts: %s 0x%llx %u\n",
+					 input, iface_entry->ifname,
+					 get_atag_from_tag(ts_entry->tn.tag),
+					 entry_uid);
+				rb_erase(&ts_entry->tn.node,
+					 &iface_entry->tag_stat_tree);
+				kfree(ts_entry);
+			}
+		}
+		spin_unlock_bh(&iface_entry->tag_stat_list_lock);
+	}
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+
+	/* Cleanup the uid_tag_data */
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	node = rb_first(&qtaguid_net->uid_tag_data_tree);
+	while (node) {
+		utd_entry = rb_entry(node, struct uid_tag_data, node);
+		entry_uid = utd_entry->uid;
+		node = rb_next(node);
+
+		CT_DEBUG("qtaguid: ctrl_delete(%s): "
+			 "utd uid=%u\n",
+			 input, entry_uid);
+
+		if (entry_uid != uid_int)
+			continue;
+		/*
+		 * Go over the tag_refs, and those that don't have
+		 * sock_tags using them are freed.
+		 */
+		put_tag_ref_tree(tag, utd_entry);
+		put_utd_entry(net, utd_entry);
+	}
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+
+	atomic64_inc(&qtaguid_net->qtu_events.delete_cmds);
+	res = 0;
+
+err:
+	return res;
+}
+
+static int ctrl_cmd_counter_set(struct net *net, const char *input)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	char cmd;
+	uid_t uid = 0;
+	tag_t tag;
+	int res, argc;
+	struct tag_counter_set *tcs;
+	int counter_set;
+
+	argc = sscanf(input, "%c %d %u", &cmd, &counter_set, &uid);
+	CT_DEBUG("qtaguid: ctrl_counterset(%s): argc=%d cmd=%c "
+		 "set=%d uid=%u\n", input, argc, cmd,
+		 counter_set, uid);
+	if (argc != 3) {
+		res = -EINVAL;
+		goto err;
+	}
+	if (counter_set < 0 || counter_set >= IFS_MAX_COUNTER_SETS) {
+		pr_info("qtaguid: ctrl_counterset(%s): invalid counter_set range\n",
+			input);
+		res = -EINVAL;
+		goto err;
+	}
+	if (!can_manipulate_uids(net)) {
+		pr_info("qtaguid: ctrl_counterset(%s): "
+			"insufficient priv from pid=%u tgid=%u uid=%u\n",
+			input, current->pid, current->tgid,
+			from_kuid(net->user_ns, current_fsuid()));
+		res = -EPERM;
+		goto err;
+	}
+
+	tag = make_tag_from_uid(uid);
+	spin_lock_bh(&qtaguid_net->tag_counter_set_list_lock);
+	tcs = tag_counter_set_tree_search(&qtaguid_net->tag_counter_set_tree,
+					  tag);
+	if (!tcs) {
+		tcs = kzalloc(sizeof(*tcs), GFP_ATOMIC);
+		if (!tcs) {
+			spin_unlock_bh(&qtaguid_net->tag_counter_set_list_lock);
+			pr_err("qtaguid: ctrl_counterset(%s): "
+			       "failed to alloc counter set\n",
+			       input);
+			res = -ENOMEM;
+			goto err;
+		}
+		tcs->tn.tag = tag;
+		tag_counter_set_tree_insert(tcs, &qtaguid_net->
+					    tag_counter_set_tree);
+		CT_DEBUG("qtaguid: ctrl_counterset(%s): added tcs tag=0x%llx "
+			 "(uid=%u) set=%d\n",
+			 input, tag, get_uid_from_tag(tag), counter_set);
+	}
+	tcs->active_set = counter_set;
+	spin_unlock_bh(&qtaguid_net->tag_counter_set_list_lock);
+	atomic64_inc(&qtaguid_net->qtu_events.counter_set_changes);
+	res = 0;
+
+err:
+	return res;
+}
+
+static int ctrl_cmd_tag(struct net *net, const char *input)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	char cmd;
+	int sock_fd = 0;
+	kuid_t uid;
+	unsigned int uid_int = 0;
+	tag_t acct_tag = make_atag_from_value(0);
+	tag_t full_tag;
+	struct socket *el_socket;
+	int res, argc;
+	struct sock_tag *sock_tag_entry;
+	struct tag_ref *tag_ref_entry;
+	struct uid_tag_data *uid_tag_data_entry;
+	struct proc_qtu_data *pqd_entry;
+
+	/* Unassigned args will get defaulted later. */
+	argc = sscanf(input, "%c %d %llu %u", &cmd, &sock_fd, &acct_tag, &uid_int);
+	uid = make_kuid(net->user_ns, uid_int);
+	CT_DEBUG("qtaguid: ctrl_tag(%s): argc=%d cmd=%c sock_fd=%d "
+		 "acct_tag=0x%llx uid=%u\n", input, argc, cmd, sock_fd,
+		 acct_tag, uid_int);
+	if (argc < 2) {
+		res = -EINVAL;
+		goto err;
+	}
+	el_socket = sockfd_lookup(sock_fd, &res);  /* This locks the file */
+	if (!el_socket) {
+		pr_info("qtaguid: ctrl_tag(%s): failed to lookup"
+			" sock_fd=%d err=%d pid=%u tgid=%u uid=%u\n",
+			input, sock_fd, res, current->pid, current->tgid,
+			from_kuid(net->user_ns, current_fsuid()));
+		goto err;
+	}
+	CT_DEBUG("qtaguid: ctrl_tag(%s): socket->...->sk_refcnt=%d ->sk=%p\n",
+		 input, refcount_read(&el_socket->sk->sk_refcnt),
+		 el_socket->sk);
+	if (argc < 3) {
+		acct_tag = make_atag_from_value(0);
+	} else if (!valid_atag(acct_tag)) {
+		pr_info("qtaguid: ctrl_tag(%s): invalid tag\n", input);
+		res = -EINVAL;
+		goto err_put;
+	}
+	CT_DEBUG("qtaguid: ctrl_tag(%s): pid=%u tgid=%u uid=%u euid=%u fsuid=%u ctrl.gid=%u in_group()=%d in_egroup()=%d\n",
+		 input, current->pid, current->tgid,
+		 from_kuid(net->user_ns, current_uid()),
+		 from_kuid(net->user_ns, current_euid()),
+		 from_kuid(net->user_ns, current_fsuid()),
+		 from_kgid(net->user_ns, qtaguid_net->ctrl_file->gid),
+		 in_group_p(qtaguid_net->ctrl_file->gid),
+		 in_egroup_p(qtaguid_net->ctrl_file->gid));
+	if (argc < 4) {
+		uid = current_fsuid();
+		uid_int = from_kuid(net->user_ns, uid);
+	} else if (!can_impersonate_uid(net, uid)) {
+		pr_info("qtaguid: ctrl_tag(%s): insufficient priv from pid=%u tgid=%u uid=%u\n",
+			input, current->pid, current->tgid,
+			from_kuid(net->user_ns, current_fsuid()));
+		res = -EPERM;
+		goto err_put;
+	}
+	full_tag = combine_atag_with_uid(acct_tag, uid_int);
+
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	sock_tag_entry = get_sock_stat_nl(qtaguid_net, el_socket->sk);
+	tag_ref_entry = get_tag_ref(qtaguid_net, full_tag, &uid_tag_data_entry);
+	if (IS_ERR(tag_ref_entry)) {
+		res = PTR_ERR(tag_ref_entry);
+		spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+		spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+		goto err_put;
+	}
+	tag_ref_entry->num_sock_tags++;
+	if (sock_tag_entry) {
+		struct tag_ref *prev_tag_ref_entry;
+
+		CT_DEBUG("qtaguid: ctrl_tag(%s): retag for sk=%p "
+			 "st@%p ...->sk_refcnt=%d\n",
+			 input, el_socket->sk, sock_tag_entry,
+			 refcount_read(&el_socket->sk->sk_refcnt));
+		prev_tag_ref_entry = lookup_tag_ref(qtaguid_net,
+						    sock_tag_entry->tag,
+						    &uid_tag_data_entry);
+		BUG_ON(IS_ERR_OR_NULL(prev_tag_ref_entry));
+		BUG_ON(prev_tag_ref_entry->num_sock_tags <= 0);
+		prev_tag_ref_entry->num_sock_tags--;
+		sock_tag_entry->tag = full_tag;
+	} else {
+		CT_DEBUG("qtaguid: ctrl_tag(%s): newtag for sk=%p\n",
+			 input, el_socket->sk);
+		sock_tag_entry = kzalloc(sizeof(*sock_tag_entry),
+					 GFP_ATOMIC);
+		if (!sock_tag_entry) {
+			pr_err("qtaguid: ctrl_tag(%s): "
+			       "socket tag alloc failed\n",
+			       input);
+			BUG_ON(tag_ref_entry->num_sock_tags <= 0);
+			tag_ref_entry->num_sock_tags--;
+			free_tag_ref_from_utd_entry(tag_ref_entry,
+						    uid_tag_data_entry);
+			spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+			spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+			res = -ENOMEM;
+			goto err_put;
+		}
+		/*
+		 * Hold the sk refcount here to make sure the sk pointer cannot
+		 * be freed and reused
+		 */
+		sock_hold(el_socket->sk);
+		sock_tag_entry->sk = el_socket->sk;
+		sock_tag_entry->pid = current->tgid;
+		sock_tag_entry->tag = combine_atag_with_uid(acct_tag, uid_int);
+		pqd_entry = proc_qtu_data_tree_search(
+			&qtaguid_net->proc_qtu_data_tree, current->tgid);
+		/*
+		 * TODO: remove if, and start failing.
+		 * At first, we want to catch user-space code that is not
+		 * opening the /dev/xt_qtaguid.
+		 */
+		if (IS_ERR_OR_NULL(pqd_entry))
+			pr_warn_once(
+				"qtaguid: %s(): "
+				"User space forgot to open /dev/xt_qtaguid? "
+				"pid=%u tgid=%u uid=%u\n", __func__,
+				current->pid, current->tgid,
+				from_kuid(net->user_ns, current_fsuid()));
+		else
+			list_add(&sock_tag_entry->list,
+				 &pqd_entry->sock_tag_list);
+
+		sock_tag_tree_insert(sock_tag_entry,
+				     &qtaguid_net->sock_tag_tree);
+		atomic64_inc(&qtaguid_net->qtu_events.sockets_tagged);
+	}
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+	/* We keep the ref to the sk until it is untagged */
+	CT_DEBUG("qtaguid: ctrl_tag(%s): done st@%p ...->sk_refcnt=%d\n",
+		 input, sock_tag_entry,
+		 refcount_read(&el_socket->sk->sk_refcnt));
+	sockfd_put(el_socket);
+	return 0;
+
+err_put:
+	CT_DEBUG("qtaguid: ctrl_tag(%s): done. ...->sk_refcnt=%d\n",
+		 input, refcount_read(&el_socket->sk->sk_refcnt) - 1);
+	/* Release the sock_fd that was grabbed by sockfd_lookup(). */
+	sockfd_put(el_socket);
+	return res;
+
+err:
+	CT_DEBUG("qtaguid: ctrl_tag(%s): done.\n", input);
+	return res;
+}
+
+static int ctrl_cmd_untag(struct net *net, const char *input)
+{
+	char cmd;
+	int sock_fd = 0;
+	struct socket *el_socket;
+	int res, argc;
+
+	argc = sscanf(input, "%c %d", &cmd, &sock_fd);
+	CT_DEBUG("qtaguid: ctrl_untag(%s): argc=%d cmd=%c sock_fd=%d\n",
+		 input, argc, cmd, sock_fd);
+	if (argc < 2) {
+		res = -EINVAL;
+		return res;
+	}
+	el_socket = sockfd_lookup(sock_fd, &res);  /* This locks the file */
+	if (!el_socket) {
+		pr_info("qtaguid: ctrl_untag(%s): failed to lookup"
+			" sock_fd=%d err=%d pid=%u tgid=%u uid=%u\n",
+			input, sock_fd, res, current->pid, current->tgid,
+			from_kuid(net->user_ns, current_fsuid()));
+		return res;
+	}
+	CT_DEBUG("qtaguid: ctrl_untag(%s): socket->...->f_count=%ld ->sk=%p\n",
+		 input, atomic_long_read(&el_socket->file->f_count),
+		 el_socket->sk);
+	res = qtaguid_untag(el_socket, false);
+	sockfd_put(el_socket);
+	return res;
+}
+
+int qtaguid_untag(struct socket *el_socket, bool kernel)
+{
+	struct sock *sk = el_socket->sk;
+	struct net *net = sock_net(sk);
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	int res;
+	pid_t pid;
+	struct sock_tag *sock_tag_entry;
+	struct tag_ref *tag_ref_entry;
+	struct uid_tag_data *utd_entry;
+	struct proc_qtu_data *pqd_entry;
+
+	/* qtaguid_untag() may be called from inet_release(), which in turn
+	 * may be called from the error handler in setup_net() if creating
+	 * the network namespace failed. In this case, there is no guarantee
+	 * that qtaguid_net was ever initialized, and qtaguid_net may be NULL.
+	 */
+	if (!qtaguid_net)
+		return -EINVAL;
+
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	sock_tag_entry = get_sock_stat_nl(qtaguid_net, el_socket->sk);
+	if (!sock_tag_entry) {
+		spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+		res = -EINVAL;
+		return res;
+	}
+	/*
+	 * The socket already belongs to the current process
+	 * so it can do whatever it wants to it.
+	 */
+	rb_erase(&sock_tag_entry->sock_node, &qtaguid_net->sock_tag_tree);
+
+	tag_ref_entry = lookup_tag_ref(qtaguid_net, sock_tag_entry->tag,
+				       &utd_entry);
+	BUG_ON(!tag_ref_entry);
+	BUG_ON(tag_ref_entry->num_sock_tags <= 0);
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	if (kernel)
+		pid = sock_tag_entry->pid;
+	else
+		pid = current->tgid;
+	pqd_entry = proc_qtu_data_tree_search(
+		&qtaguid_net->proc_qtu_data_tree, pid);
+	/*
+	 * TODO: remove if, and start failing.
+	 * At first, we want to catch user-space code that is not
+	 * opening the /dev/xt_qtaguid.
+	 */
+	if (IS_ERR_OR_NULL(pqd_entry))
+		pr_warn_once("qtaguid: %s(): "
+			     "User space forgot to open /dev/xt_qtaguid? "
+			     "pid=%u tgid=%u sk_pid=%u, uid=%u\n", __func__,
+			     current->pid, current->tgid, sock_tag_entry->pid,
+			     from_kuid(net->user_ns, current_fsuid()));
+	/*
+	 * This check is needed because tagging from a process that
+	 * didnt open /dev/xt_qtaguid still adds the sock_tag_entry
+	 * to sock_tag_tree.
+	 */
+	if (sock_tag_entry->list.next)
+		list_del(&sock_tag_entry->list);
+
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	/*
+	 * We don't free tag_ref from the utd_entry here,
+	 * only during a cmd_delete().
+	 */
+	tag_ref_entry->num_sock_tags--;
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+	/*
+	 * Release the sock_fd that was grabbed at tag time.
+	 */
+	sock_put(sock_tag_entry->sk);
+	CT_DEBUG("qtaguid: done. st@%p ...->sk_refcnt=%d\n",
+		 sock_tag_entry,
+		 refcount_read(&el_socket->sk->sk_refcnt));
+
+	kfree(sock_tag_entry);
+	atomic64_inc(&qtaguid_net->qtu_events.sockets_untagged);
+
+	return 0;
+}
+
+static ssize_t qtaguid_ctrl_parse(struct net *net,
+				  const char *input,
+				  size_t count)
+{
+	char cmd;
+	ssize_t res;
+
+	CT_DEBUG("qtaguid: ctrl(%s): pid=%u tgid=%u uid=%u\n",
+		 input, current->pid, current->tgid,
+		 from_kuid(net->user_ns, current_fsuid()));
+
+	cmd = input[0];
+	/* Collect params for commands */
+	switch (cmd) {
+	case 'd':
+		res = ctrl_cmd_delete(net, input);
+		break;
+
+	case 's':
+		res = ctrl_cmd_counter_set(net, input);
+		break;
+
+	case 't':
+		res = ctrl_cmd_tag(net, input);
+		break;
+
+	case 'u':
+		res = ctrl_cmd_untag(net, input);
+		break;
+
+	default:
+		res = -EINVAL;
+		goto err;
+	}
+	if (!res)
+		res = count;
+err:
+	CT_DEBUG("qtaguid: ctrl(%s): res=%zd\n", input, res);
+	return res;
+}
+
+#define MAX_QTAGUID_CTRL_INPUT_LEN 255
+static ssize_t qtaguid_ctrl_proc_write(struct file *file, const char __user *buffer,
+				   size_t count, loff_t *offp)
+{
+	struct net *net = PDE_DATA(file_inode(file));
+	char input_buf[MAX_QTAGUID_CTRL_INPUT_LEN];
+
+	if (unlikely(module_passive))
+		return count;
+
+	if (count >= MAX_QTAGUID_CTRL_INPUT_LEN)
+		return -EINVAL;
+
+	if (copy_from_user(input_buf, buffer, count))
+		return -EFAULT;
+
+	input_buf[count] = '\0';
+	return qtaguid_ctrl_parse(net, input_buf, count);
+}
+
+struct proc_print_info {
+	struct net *net;
+	struct iface_stat *iface_entry;
+	int item_index;
+	tag_t tag; /* tag found by reading to tag_pos */
+	off_t tag_pos;
+	int tag_item_index;
+};
+
+static void pp_stats_header(struct seq_file *m)
+{
+	seq_puts(m,
+		 "idx iface acct_tag_hex uid_tag_int cnt_set "
+		 "rx_bytes rx_packets "
+		 "tx_bytes tx_packets "
+		 "rx_tcp_bytes rx_tcp_packets "
+		 "rx_udp_bytes rx_udp_packets "
+		 "rx_other_bytes rx_other_packets "
+		 "tx_tcp_bytes tx_tcp_packets "
+		 "tx_udp_bytes tx_udp_packets "
+		 "tx_other_bytes tx_other_packets\n");
+}
+
+static int pp_stats_line(struct seq_file *m, struct tag_stat *ts_entry,
+			 int cnt_set)
+{
+	struct data_counters *cnts;
+	tag_t tag = ts_entry->tn.tag;
+	uid_t stat_uid = get_uid_from_tag(tag);
+	struct proc_print_info *ppi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(ppi->net);
+
+	/* Detailed tags are not available to everybody */
+	if (!can_read_other_uid_stats(ppi->net,
+				      make_kuid(ppi->net->user_ns, stat_uid))) {
+		CT_DEBUG("qtaguid: stats line: "
+			 "%s 0x%llx %u: insufficient priv "
+			 "from pid=%u tgid=%u uid=%u stats.gid=%u\n",
+			 ppi->iface_entry->ifname,
+			 get_atag_from_tag(tag), stat_uid,
+			 current->pid, current->tgid,
+			 from_kuid(ppi->net->user_ns, current_fsuid()),
+			 from_kgid(ppi->net->user_ns,
+				   qtaguid_net->stats_file->gid));
+		return 0;
+	}
+	ppi->item_index++;
+	cnts = &ts_entry->counters;
+	seq_printf(m, "%d %s 0x%llx %u %u "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu "
+		"%llu %llu\n",
+		ppi->item_index,
+		ppi->iface_entry->ifname,
+		get_atag_from_tag(tag),
+		stat_uid,
+		cnt_set,
+		dc_sum_bytes(cnts, cnt_set, IFS_RX),
+		dc_sum_packets(cnts, cnt_set, IFS_RX),
+		dc_sum_bytes(cnts, cnt_set, IFS_TX),
+		dc_sum_packets(cnts, cnt_set, IFS_TX),
+		cnts->bpc[cnt_set][IFS_RX][IFS_TCP].bytes,
+		cnts->bpc[cnt_set][IFS_RX][IFS_TCP].packets,
+		cnts->bpc[cnt_set][IFS_RX][IFS_UDP].bytes,
+		cnts->bpc[cnt_set][IFS_RX][IFS_UDP].packets,
+		cnts->bpc[cnt_set][IFS_RX][IFS_PROTO_OTHER].bytes,
+		cnts->bpc[cnt_set][IFS_RX][IFS_PROTO_OTHER].packets,
+		cnts->bpc[cnt_set][IFS_TX][IFS_TCP].bytes,
+		cnts->bpc[cnt_set][IFS_TX][IFS_TCP].packets,
+		cnts->bpc[cnt_set][IFS_TX][IFS_UDP].bytes,
+		cnts->bpc[cnt_set][IFS_TX][IFS_UDP].packets,
+		cnts->bpc[cnt_set][IFS_TX][IFS_PROTO_OTHER].bytes,
+		cnts->bpc[cnt_set][IFS_TX][IFS_PROTO_OTHER].packets);
+	return seq_has_overflowed(m) ? -ENOSPC : 1;
+}
+
+static bool pp_sets(struct seq_file *m, struct tag_stat *ts_entry)
+{
+	int ret;
+	int counter_set;
+	for (counter_set = 0; counter_set < IFS_MAX_COUNTER_SETS;
+	     counter_set++) {
+		ret = pp_stats_line(m, ts_entry, counter_set);
+		if (ret < 0)
+			return false;
+	}
+	return true;
+}
+
+static int qtaguid_stats_proc_iface_stat_ptr_valid(
+	struct qtaguid_net *qtaguid_net,
+	struct iface_stat *ptr)
+{
+	struct iface_stat *iface_entry;
+
+	if (!ptr)
+		return false;
+
+	list_for_each_entry(iface_entry, &qtaguid_net->iface_stat_list, list)
+		if (iface_entry == ptr)
+			return true;
+	return false;
+}
+
+static void qtaguid_stats_proc_next_iface_entry(struct qtaguid_net *qtaguid_net,
+						struct proc_print_info *ppi)
+{
+	spin_unlock_bh(&ppi->iface_entry->tag_stat_list_lock);
+	list_for_each_entry_continue(ppi->iface_entry,
+				     &qtaguid_net->iface_stat_list, list) {
+		spin_lock_bh(&ppi->iface_entry->tag_stat_list_lock);
+		return;
+	}
+	ppi->iface_entry = NULL;
+}
+
+static void *qtaguid_stats_proc_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	struct proc_print_info *ppi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(ppi->net);
+	struct tag_stat *ts_entry;
+	struct rb_node *node;
+
+	if (!v) {
+		pr_err("qtaguid: %s(): unexpected v: NULL\n", __func__);
+		return NULL;
+	}
+
+	(*pos)++;
+
+	if (!ppi->iface_entry || unlikely(module_passive))
+		return NULL;
+
+	if (v == SEQ_START_TOKEN)
+		node = rb_first(&ppi->iface_entry->tag_stat_tree);
+	else
+		node = rb_next(&((struct tag_stat *)v)->tn.node);
+
+	while (!node) {
+		qtaguid_stats_proc_next_iface_entry(qtaguid_net, ppi);
+		if (!ppi->iface_entry)
+			return NULL;
+		node = rb_first(&ppi->iface_entry->tag_stat_tree);
+	}
+
+	ts_entry = rb_entry(node, struct tag_stat, tn.node);
+	ppi->tag = ts_entry->tn.tag;
+	ppi->tag_pos = *pos;
+	ppi->tag_item_index = ppi->item_index;
+	return ts_entry;
+}
+
+static void *qtaguid_stats_proc_start(struct seq_file *m, loff_t *pos)
+{
+	struct proc_print_info *ppi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(ppi->net);
+	struct tag_stat *ts_entry = NULL;
+
+	spin_lock_bh(&qtaguid_net->iface_stat_list_lock);
+
+	if (*pos == 0) {
+		ppi->item_index = 1;
+		ppi->tag_pos = 0;
+		if (list_empty(&qtaguid_net->iface_stat_list)) {
+			ppi->iface_entry = NULL;
+		} else {
+			ppi->iface_entry =
+				list_first_entry(&qtaguid_net->iface_stat_list,
+						 struct iface_stat,
+						 list);
+			spin_lock_bh(&ppi->iface_entry->tag_stat_list_lock);
+		}
+		return SEQ_START_TOKEN;
+	}
+	if (!qtaguid_stats_proc_iface_stat_ptr_valid(qtaguid_net,
+						     ppi->iface_entry)) {
+		if (ppi->iface_entry) {
+			pr_err("qtaguid: %s(): iface_entry %p not found\n",
+			       __func__, ppi->iface_entry);
+			ppi->iface_entry = NULL;
+		}
+		return NULL;
+	}
+
+	spin_lock_bh(&ppi->iface_entry->tag_stat_list_lock);
+
+	if (!ppi->tag_pos) {
+		/* seq_read skipped first next call */
+		ts_entry = SEQ_START_TOKEN;
+	} else {
+		ts_entry = tag_stat_tree_search(
+				&ppi->iface_entry->tag_stat_tree, ppi->tag);
+		if (!ts_entry) {
+			pr_info("qtaguid: %s(): tag_stat.tag 0x%llx not found. Abort.\n",
+				__func__, ppi->tag);
+			return NULL;
+		}
+	}
+
+	if (*pos == ppi->tag_pos) { /* normal resume */
+		ppi->item_index = ppi->tag_item_index;
+	} else {
+		/* seq_read skipped a next call */
+		*pos = ppi->tag_pos;
+		ts_entry = qtaguid_stats_proc_next(m, ts_entry, pos);
+	}
+
+	return ts_entry;
+}
+
+static void qtaguid_stats_proc_stop(struct seq_file *m, void *v)
+{
+	struct proc_print_info *ppi = m->private;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(ppi->net);
+	if (ppi->iface_entry)
+		spin_unlock_bh(&ppi->iface_entry->tag_stat_list_lock);
+	spin_unlock_bh(&qtaguid_net->iface_stat_list_lock);
+}
+
+/*
+ * Procfs reader to get all tag stats using style "1)" as described in
+ * fs/proc/generic.c
+ * Groups all protocols tx/rx bytes.
+ */
+static int qtaguid_stats_proc_show(struct seq_file *m, void *v)
+{
+	struct tag_stat *ts_entry = v;
+
+	if (v == SEQ_START_TOKEN)
+		pp_stats_header(m);
+	else
+		pp_sets(m, ts_entry);
+
+	return 0;
+}
+
+/*------------------------------------------*/
+static int qtudev_open(struct inode *inode, struct file *file)
+{
+	struct net *net = current->nsproxy->net_ns;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	struct uid_tag_data *utd_entry;
+	struct proc_qtu_data *pqd_entry;
+	struct proc_qtu_data *new_pqd_entry;
+	int res;
+	bool utd_entry_found;
+
+	if (unlikely(qtu_proc_handling_passive))
+		return 0;
+
+	DR_DEBUG("qtaguid: qtudev_open(): pid=%u tgid=%u uid=%u\n",
+		 current->pid, current->tgid,
+		 from_kuid(net->user_ns, current_fsuid()));
+
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+
+	/* Look for existing uid data, or alloc one. */
+	utd_entry = get_uid_data(qtaguid_net,
+				 from_kuid(net->user_ns, current_fsuid()),
+				 &utd_entry_found);
+	if (IS_ERR_OR_NULL(utd_entry)) {
+		res = PTR_ERR(utd_entry);
+		goto err_unlock;
+	}
+
+	/* Look for existing PID based proc_data */
+	pqd_entry = proc_qtu_data_tree_search(&qtaguid_net->proc_qtu_data_tree,
+					      current->tgid);
+	if (pqd_entry) {
+		pr_err("qtaguid: qtudev_open(): %u/%u %u "
+		       "%s already opened\n",
+		       current->pid, current->tgid,
+		       from_kuid(net->user_ns, current_fsuid()),
+		       QTU_DEV_NAME);
+		res = -EBUSY;
+		goto err_unlock_free_utd;
+	}
+
+	new_pqd_entry = kzalloc(sizeof(*new_pqd_entry), GFP_ATOMIC);
+	if (!new_pqd_entry) {
+		pr_err("qtaguid: qtudev_open(): %u/%u %u: "
+		       "proc data alloc failed\n",
+		       current->pid, current->tgid,
+		       from_kuid(net->user_ns, current_fsuid()));
+		res = -ENOMEM;
+		goto err_unlock_free_utd;
+	}
+	new_pqd_entry->pid = current->tgid;
+	new_pqd_entry->net = get_net(net);
+	INIT_LIST_HEAD(&new_pqd_entry->sock_tag_list);
+	new_pqd_entry->parent_tag_data = utd_entry;
+	utd_entry->num_pqd++;
+
+	proc_qtu_data_tree_insert(new_pqd_entry,
+				  &qtaguid_net->proc_qtu_data_tree);
+
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	DR_DEBUG("qtaguid: tracking data for uid=%u in pqd=%p\n",
+		 from_kuid(net->user_ns, current_fsuid()), new_pqd_entry);
+	file->private_data = new_pqd_entry;
+	return 0;
+
+err_unlock_free_utd:
+	if (!utd_entry_found) {
+		rb_erase(&utd_entry->node, &qtaguid_net->uid_tag_data_tree);
+		kfree(utd_entry);
+	}
+err_unlock:
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	return res;
+}
+
+static int qtudev_release(struct inode *inode, struct file *file)
+{
+	struct proc_qtu_data *pqd_entry = file->private_data;
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(pqd_entry->net);
+	struct uid_tag_data *utd_entry = pqd_entry->parent_tag_data;
+	struct sock_tag *st_entry;
+	struct rb_root st_to_free_tree = RB_ROOT;
+	struct list_head *entry, *next;
+	struct tag_ref *tr;
+
+	if (unlikely(qtu_proc_handling_passive))
+		return 0;
+
+	/*
+	 * Do not trust the current->pid, it might just be a kworker cleaning
+	 * up after a dead proc.
+	 */
+	DR_DEBUG("qtaguid: qtudev_release(): "
+		 "pid=%u tgid=%u uid=%u "
+		 "pqd_entry=%p->pid=%u utd_entry=%p->active_tags=%d\n",
+		 current->pid, current->tgid, pqd_entry->parent_tag_data->uid,
+		 pqd_entry, pqd_entry->pid, utd_entry,
+		 utd_entry->num_active_tags);
+
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	spin_lock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+
+	list_for_each_safe(entry, next, &pqd_entry->sock_tag_list) {
+		st_entry = list_entry(entry, struct sock_tag, list);
+		DR_DEBUG("qtaguid: %s(): "
+			 "erase sock_tag=%p->sk=%p pid=%u tgid=%u uid=%u\n",
+			 __func__,
+			 st_entry, st_entry->sk,
+			 current->pid, current->tgid,
+			 pqd_entry->parent_tag_data->uid);
+
+		utd_entry = uid_tag_data_tree_search(
+			&qtaguid_net->uid_tag_data_tree,
+			get_uid_from_tag(st_entry->tag));
+		BUG_ON(IS_ERR_OR_NULL(utd_entry));
+		DR_DEBUG("qtaguid: %s(): "
+			 "looking for tag=0x%llx in utd_entry=%p\n", __func__,
+			 st_entry->tag, utd_entry);
+		tr = tag_ref_tree_search(&utd_entry->tag_ref_tree,
+					 st_entry->tag);
+		BUG_ON(!tr);
+		BUG_ON(tr->num_sock_tags <= 0);
+		tr->num_sock_tags--;
+		free_tag_ref_from_utd_entry(tr, utd_entry);
+
+		rb_erase(&st_entry->sock_node, &qtaguid_net->sock_tag_tree);
+		list_del(&st_entry->list);
+		/* Can't sockfd_put() within spinlock, do it later. */
+		sock_tag_tree_insert(st_entry, &st_to_free_tree);
+
+		/*
+		 * Try to free the utd_entry if no other proc_qtu_data is
+		 * using it (num_pqd is 0) and it doesn't have active tags
+		 * (num_active_tags is 0).
+		 */
+		put_utd_entry(pqd_entry->net, utd_entry);
+	}
+
+	rb_erase(&pqd_entry->node, &qtaguid_net->proc_qtu_data_tree);
+	BUG_ON(pqd_entry->parent_tag_data->num_pqd < 1);
+	pqd_entry->parent_tag_data->num_pqd--;
+	put_utd_entry(pqd_entry->net, pqd_entry->parent_tag_data);
+	put_net(pqd_entry->net);
+	kfree(pqd_entry);
+	file->private_data = NULL;
+
+	spin_unlock_bh(&qtaguid_net->uid_tag_data_tree_lock);
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+
+	sock_tag_tree_erase(&st_to_free_tree);
+
+	spin_lock_bh(&qtaguid_net->sock_tag_list_lock);
+	prdebug_full_state_locked(qtaguid_net, 0, "%s(): pid=%u tgid=%u",
+				  __func__, current->pid, current->tgid);
+	spin_unlock_bh(&qtaguid_net->sock_tag_list_lock);
+	return 0;
+}
+
+/*------------------------------------------*/
+static const struct file_operations qtudev_fops = {
+	.owner = THIS_MODULE,
+	.open = qtudev_open,
+	.release = qtudev_release,
+};
+
+static struct miscdevice qtu_device = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = QTU_DEV_NAME,
+	.fops = &qtudev_fops,
+	/* How sad it doesn't allow for defaults: .mode = S_IRUGO | S_IWUSR */
+};
+
+static const struct seq_operations proc_qtaguid_ctrl_seqops = {
+	.start = qtaguid_ctrl_proc_start,
+	.next = qtaguid_ctrl_proc_next,
+	.stop = qtaguid_ctrl_proc_stop,
+	.show = qtaguid_ctrl_proc_show,
+};
+
+static int proc_qtaguid_ctrl_open(struct inode *inode, struct file *file)
+{
+	struct proc_ctrl_print_info *pcpi;
+
+	pcpi = __seq_open_private(file, &proc_qtaguid_ctrl_seqops,
+				  sizeof(*pcpi));
+	if (!pcpi)
+		return -ENOMEM;
+
+	pcpi->net = PDE_DATA(inode);
+	return 0;
+}
+
+static const struct proc_ops proc_qtaguid_ctrl_fops = {
+	.proc_open	= proc_qtaguid_ctrl_open,
+	.proc_read	= seq_read,
+	.proc_write	= qtaguid_ctrl_proc_write,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= seq_release_private,
+};
+
+static const struct seq_operations proc_qtaguid_stats_seqops = {
+	.start = qtaguid_stats_proc_start,
+	.next = qtaguid_stats_proc_next,
+	.stop = qtaguid_stats_proc_stop,
+	.show = qtaguid_stats_proc_show,
+};
+
+static int proc_qtaguid_stats_open(struct inode *inode, struct file *file)
+{
+	struct proc_print_info *ppi;
+
+	ppi = __seq_open_private(file, &proc_qtaguid_stats_seqops,
+				 sizeof(*ppi));
+	if (!ppi)
+		return -ENOMEM;
+
+	ppi->net = PDE_DATA(inode);
+	return 0;
+}
+
+static const struct proc_ops proc_qtaguid_stats_fops = {
+	.proc_open	= proc_qtaguid_stats_open,
+	.proc_read	= seq_read,
+	.proc_lseek	= seq_lseek,
+	.proc_release	= seq_release_private,
+};
+
+/*------------------------------------------*/
+static int __net_init qtaguid_proc_register(struct net *net)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+
+	qtaguid_net->procdir = proc_mkdir("xt_qtaguid", net->proc_net);
+	if (!qtaguid_net->procdir)
+		goto out1;
+
+	qtaguid_net->ctrl_file =
+		proc_create_data("ctrl", proc_ctrl_perms,
+				 qtaguid_net->procdir,
+				 &proc_qtaguid_ctrl_fops, net);
+	if (!qtaguid_net->ctrl_file)
+		goto out2;
+
+	qtaguid_net->stats_file =
+		proc_create_data("stats", proc_stats_perms,
+				 qtaguid_net->procdir,
+				 &proc_qtaguid_stats_fops, net);
+	if (!qtaguid_net->stats_file)
+		goto out3;
+
+	qtaguid_net->iface_stat_procdir =
+		proc_mkdir("iface_stat", qtaguid_net->procdir);
+	if (!qtaguid_net->iface_stat_procdir)
+		goto out4;
+
+	qtaguid_net->iface_stat_all_procfile =
+		proc_create_data("iface_stat_all", proc_iface_perms,
+				 qtaguid_net->procdir,
+				 &proc_iface_stat_all_fops, net);
+	if (!qtaguid_net->iface_stat_all_procfile)
+		goto out5;
+
+	qtaguid_net->iface_stat_fmt_procfile =
+		proc_create_data("iface_stat_fmt", proc_iface_perms,
+				 qtaguid_net->procdir,
+				 &proc_iface_stat_fmt_fops, net);
+	if (!qtaguid_net->iface_stat_fmt_procfile)
+		goto out6;
+
+	return 0;
+
+out6:
+	remove_proc_entry("iface_stat_all", qtaguid_net->procdir);
+out5:
+	remove_proc_entry("iface_stat", qtaguid_net->procdir);
+out4:
+	remove_proc_entry("stats", qtaguid_net->procdir);
+out3:
+	remove_proc_entry("ctrl", qtaguid_net->procdir);
+out2:
+	remove_proc_entry("xt_qtaguid", net->proc_net);
+out1:
+	return -ENOMEM;
+}
+
+static int __net_init qtaguid_net_init(struct net *net)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+
+	INIT_LIST_HEAD(&qtaguid_net->iface_stat_list);
+	spin_lock_init(&qtaguid_net->iface_stat_list_lock);
+
+	qtaguid_net->sock_tag_tree = RB_ROOT;
+	spin_lock_init(&qtaguid_net->sock_tag_list_lock);
+
+	qtaguid_net->tag_counter_set_tree = RB_ROOT;
+	spin_lock_init(&qtaguid_net->tag_counter_set_list_lock);
+
+	qtaguid_net->uid_tag_data_tree = RB_ROOT;
+	spin_lock_init(&qtaguid_net->uid_tag_data_tree_lock);
+
+	if (qtaguid_proc_register(net) < 0)
+		return -EACCES;
+
+	return 0;
+}
+
+static void __net_exit qtaguid_net_exit(struct net *net)
+{
+	struct qtaguid_net *qtaguid_net = qtaguid_pernet(net);
+	struct iface_stat *iface_entry, *tmp;
+
+	list_for_each_entry_safe(iface_entry, tmp,
+				 &qtaguid_net->iface_stat_list, list) {
+		iface_delete_proc(qtaguid_net, iface_entry);
+		tag_stat_tree_erase(&iface_entry->tag_stat_tree);
+		kfree(iface_entry->ifname);
+		kfree(iface_entry);
+	}
+
+	remove_proc_entry("iface_stat_fmt", qtaguid_net->procdir);
+	remove_proc_entry("iface_stat_all", qtaguid_net->procdir);
+	remove_proc_entry("iface_stat", qtaguid_net->procdir);
+	remove_proc_entry("stats", qtaguid_net->procdir);
+	remove_proc_entry("ctrl", qtaguid_net->procdir);
+	remove_proc_entry("xt_qtaguid", net->proc_net);
+
+	sock_tag_tree_erase(&qtaguid_net->sock_tag_tree);
+	tag_counter_set_tree_erase(&qtaguid_net->tag_counter_set_tree);
+	uid_tag_data_tree_erase(&qtaguid_net->uid_tag_data_tree);
+	/* proc_qtu_data_tree should be empty already because the
+	 * netns won't be destroyed until all open file descriptors
+	 * for /dev/xt_qtaguid are closed.
+	 */
+}
+
+static struct xt_match qtaguid_mt_reg __read_mostly = {
+	/*
+	 * This module masquerades as the "owner" module so that iptables
+	 * tools can deal with it.
+	 */
+	.name       = "owner",
+	.revision   = 1,
+	.family     = NFPROTO_UNSPEC,
+	.checkentry = qtaguid_check,
+	.match      = qtaguid_mt,
+	.matchsize  = sizeof(struct xt_qtaguid_match_info),
+	.me         = THIS_MODULE,
+};
+
+static struct pernet_operations qtaguid_net_ops = {
+	.init   = qtaguid_net_init,
+	.exit   = qtaguid_net_exit,
+	.id     = &qtaguid_net_id,
+	.size   = sizeof(struct qtaguid_net),
+};
+
+static int __init qtaguid_mt_init(void)
+{
+	int ret;
+
+	ret = register_pernet_subsys(&qtaguid_net_ops);
+	if (ret < 0)
+		goto out1;
+
+	ret = xt_register_match(&qtaguid_mt_reg);
+	if (ret < 0)
+		goto out2;
+
+	ret = register_netdevice_notifier(&iface_netdev_notifier_blk);
+	if (ret < 0) {
+		pr_err("qtaguid: iface_stat: init failed to register dev event handler\n");
+		goto out3;
+	}
+
+	ret = register_inetaddr_notifier(&iface_inetaddr_notifier_blk);
+	if (ret < 0) {
+		pr_err("qtaguid: iface_stat: init failed to register ipv4 dev event handler\n");
+		goto out4;
+	}
+
+	ret = register_inet6addr_notifier(&iface_inet6addr_notifier_blk);
+	if (ret < 0) {
+		pr_err("qtaguid: iface_stat: init failed to register ipv6 dev event handler\n");
+		goto out5;
+	}
+
+	ret = misc_register(&qtu_device);
+	if (ret < 0)
+		goto out6;
+
+	return 0;
+
+out6:
+	unregister_inet6addr_notifier(&iface_inet6addr_notifier_blk);
+out5:
+	unregister_inetaddr_notifier(&iface_inetaddr_notifier_blk);
+out4:
+	unregister_netdevice_notifier(&iface_netdev_notifier_blk);
+out3:
+	xt_unregister_match(&qtaguid_mt_reg);
+out2:
+	unregister_pernet_subsys(&qtaguid_net_ops);
+out1:
+	return ret;
+}
+
+/*
+ * TODO: allow unloading of the module.
+ * For now stats are permanent.
+ * Kconfig forces'y/n' and never an 'm'.
+ */
+
+module_init(qtaguid_mt_init);
+MODULE_AUTHOR("jpa <jpa@google.com>");
+MODULE_DESCRIPTION("Xtables: socket owner+tag matching and associated stats");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("ipt_owner");
+MODULE_ALIAS("ip6t_owner");
+MODULE_ALIAS("ipt_qtaguid");
+MODULE_ALIAS("ip6t_qtaguid");
diff -ruN a/net/netfilter/xt_qtaguid_internal.h b/net/netfilter/xt_qtaguid_internal.h
--- a/net/netfilter/xt_qtaguid_internal.h	1970-01-01 01:00:00.000000000 +0100
+++ b/net/netfilter/xt_qtaguid_internal.h	2021-03-20 09:20:37.000000000 +0100
@@ -0,0 +1,353 @@
+/*
+ * Kernel iptables module to track stats for packets based on user tags.
+ *
+ * (C) 2011 Google, Inc
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef __XT_QTAGUID_INTERNAL_H__
+#define __XT_QTAGUID_INTERNAL_H__
+
+#include <linux/types.h>
+#include <linux/rbtree.h>
+#include <linux/spinlock_types.h>
+#include <linux/workqueue.h>
+#include <net/net_namespace.h>
+
+/* Iface handling */
+#define IDEBUG_MASK (1<<0)
+/* Iptable Matching. Per packet. */
+#define MDEBUG_MASK (1<<1)
+/* Red-black tree handling. Per packet. */
+#define RDEBUG_MASK (1<<2)
+/* procfs ctrl/stats handling */
+#define CDEBUG_MASK (1<<3)
+/* dev and resource tracking */
+#define DDEBUG_MASK (1<<4)
+
+/* E.g (IDEBUG_MASK | CDEBUG_MASK | DDEBUG_MASK) */
+#define DEFAULT_DEBUG_MASK 0
+
+/*
+ * (Un)Define these *DEBUG to compile out/in the pr_debug calls.
+ * All undef: text size ~ 0x3030; all def: ~ 0x4404.
+ */
+#define IDEBUG
+#define MDEBUG
+#define RDEBUG
+#define CDEBUG
+#define DDEBUG
+
+#define MSK_DEBUG(mask, ...) do {                           \
+		if (unlikely(qtaguid_debug_mask & (mask)))  \
+			pr_debug(__VA_ARGS__);              \
+	} while (0)
+#ifdef IDEBUG
+#define IF_DEBUG(...) MSK_DEBUG(IDEBUG_MASK, __VA_ARGS__)
+#else
+#define IF_DEBUG(...) no_printk(__VA_ARGS__)
+#endif
+#ifdef MDEBUG
+#define MT_DEBUG(...) MSK_DEBUG(MDEBUG_MASK, __VA_ARGS__)
+#else
+#define MT_DEBUG(...) no_printk(__VA_ARGS__)
+#endif
+#ifdef RDEBUG
+#define RB_DEBUG(...) MSK_DEBUG(RDEBUG_MASK, __VA_ARGS__)
+#else
+#define RB_DEBUG(...) no_printk(__VA_ARGS__)
+#endif
+#ifdef CDEBUG
+#define CT_DEBUG(...) MSK_DEBUG(CDEBUG_MASK, __VA_ARGS__)
+#else
+#define CT_DEBUG(...) no_printk(__VA_ARGS__)
+#endif
+#ifdef DDEBUG
+#define DR_DEBUG(...) MSK_DEBUG(DDEBUG_MASK, __VA_ARGS__)
+#else
+#define DR_DEBUG(...) no_printk(__VA_ARGS__)
+#endif
+
+extern uint qtaguid_debug_mask;
+
+/*---------------------------------------------------------------------------*/
+/*
+ * Tags:
+ *
+ * They represent what the data usage counters will be tracked against.
+ * By default a tag is just based on the UID.
+ * The UID is used as the base for policing, and can not be ignored.
+ * So a tag will always at least represent a UID (uid_tag).
+ *
+ * A tag can be augmented with an "accounting tag" which is associated
+ * with a UID.
+ * User space can set the acct_tag portion of the tag which is then used
+ * with sockets: all data belonging to that socket will be counted against the
+ * tag. The policing is then based on the tag's uid_tag portion,
+ * and stats are collected for the acct_tag portion separately.
+ *
+ * There could be
+ * a:  {acct_tag=1, uid_tag=10003}
+ * b:  {acct_tag=2, uid_tag=10003}
+ * c:  {acct_tag=3, uid_tag=10003}
+ * d:  {acct_tag=0, uid_tag=10003}
+ * a, b, and c represent tags associated with specific sockets.
+ * d is for the totals for that uid, including all untagged traffic.
+ * Typically d is used with policing/quota rules.
+ *
+ * We want tag_t big enough to distinguish uid_t and acct_tag.
+ * It might become a struct if needed.
+ * Nothing should be using it as an int.
+ */
+typedef uint64_t tag_t;  /* Only used via accessors */
+
+#define TAG_UID_MASK 0xFFFFFFFFULL
+#define TAG_ACCT_MASK (~0xFFFFFFFFULL)
+
+static inline int tag_compare(tag_t t1, tag_t t2)
+{
+	return t1 < t2 ? -1 : t1 == t2 ? 0 : 1;
+}
+
+static inline tag_t combine_atag_with_uid(tag_t acct_tag, uid_t uid)
+{
+	return acct_tag | uid;
+}
+static inline tag_t make_tag_from_uid(uid_t uid)
+{
+	return uid;
+}
+static inline uid_t get_uid_from_tag(tag_t tag)
+{
+	return tag & TAG_UID_MASK;
+}
+static inline tag_t get_utag_from_tag(tag_t tag)
+{
+	return tag & TAG_UID_MASK;
+}
+static inline tag_t get_atag_from_tag(tag_t tag)
+{
+	return tag & TAG_ACCT_MASK;
+}
+
+static inline bool valid_atag(tag_t tag)
+{
+	return !(tag & TAG_UID_MASK);
+}
+static inline tag_t make_atag_from_value(uint32_t value)
+{
+	return (uint64_t)value << 32;
+}
+/*---------------------------------------------------------------------------*/
+
+/*
+ * Maximum number of socket tags that a UID is allowed to have active.
+ * Multiple processes belonging to the same UID contribute towards this limit.
+ * Special UIDs that can impersonate a UID also contribute (e.g. download
+ * manager, ...)
+ */
+#define DEFAULT_MAX_SOCK_TAGS 1024
+
+/*
+ * For now we only track 2 sets of counters.
+ * The default set is 0.
+ * Userspace can activate another set for a given uid being tracked.
+ */
+#define IFS_MAX_COUNTER_SETS 2
+
+enum ifs_tx_rx {
+	IFS_TX,
+	IFS_RX,
+	IFS_MAX_DIRECTIONS
+};
+
+/* For now, TCP, UDP, the rest */
+enum ifs_proto {
+	IFS_TCP,
+	IFS_UDP,
+	IFS_PROTO_OTHER,
+	IFS_MAX_PROTOS
+};
+
+struct byte_packet_counters {
+	uint64_t bytes;
+	uint64_t packets;
+};
+
+struct data_counters {
+	struct byte_packet_counters bpc[IFS_MAX_COUNTER_SETS][IFS_MAX_DIRECTIONS][IFS_MAX_PROTOS];
+};
+
+static inline uint64_t dc_sum_bytes(struct data_counters *counters,
+				    int set,
+				    enum ifs_tx_rx direction)
+{
+	return counters->bpc[set][direction][IFS_TCP].bytes
+		+ counters->bpc[set][direction][IFS_UDP].bytes
+		+ counters->bpc[set][direction][IFS_PROTO_OTHER].bytes;
+}
+
+static inline uint64_t dc_sum_packets(struct data_counters *counters,
+				      int set,
+				      enum ifs_tx_rx direction)
+{
+	return counters->bpc[set][direction][IFS_TCP].packets
+		+ counters->bpc[set][direction][IFS_UDP].packets
+		+ counters->bpc[set][direction][IFS_PROTO_OTHER].packets;
+}
+
+
+/* Generic X based nodes used as a base for rb_tree ops */
+struct tag_node {
+	struct rb_node node;
+	tag_t tag;
+};
+
+struct tag_stat {
+	struct tag_node tn;
+	struct data_counters counters;
+	/*
+	 * If this tag is acct_tag based, we need to count against the
+	 * matching parent uid_tag.
+	 */
+	struct data_counters *parent_counters;
+};
+
+struct iface_stat {
+	struct list_head list;  /* in iface_stat_list */
+	char *ifname;
+	bool active;
+	/* net_dev is only valid for active iface_stat */
+	struct net_device *net_dev;
+
+	struct byte_packet_counters totals_via_dev[IFS_MAX_DIRECTIONS];
+	struct data_counters totals_via_skb;
+	/*
+	 * We keep the last_known, because some devices reset their counters
+	 * just before NETDEV_UP, while some will reset just before
+	 * NETDEV_REGISTER (which is more normal).
+	 * So now, if the device didn't do a NETDEV_UNREGISTER and we see
+	 * its current dev stats smaller that what was previously known, we
+	 * assume an UNREGISTER and just use the last_known.
+	 */
+	struct byte_packet_counters last_known[IFS_MAX_DIRECTIONS];
+	/* last_known is usable when last_known_valid is true */
+	bool last_known_valid;
+
+	struct proc_dir_entry *proc_ptr;
+
+	struct rb_root tag_stat_tree;
+	spinlock_t tag_stat_list_lock;
+};
+
+/* This is needed to create proc_dir_entries from atomic context. */
+struct iface_stat_work {
+	struct work_struct iface_work;
+	struct iface_stat *iface_entry;
+	struct net_device *net_dev;
+};
+
+/*
+ * Track tag that this socket is transferring data for, and not necessarily
+ * the uid that owns the socket.
+ * This is the tag against which tag_stat.counters will be billed.
+ * These structs need to be looked up by sock and pid.
+ */
+struct sock_tag {
+	struct rb_node sock_node;
+	struct sock *sk;  /* Only used as a number, never dereferenced */
+	/* Used to associate with a given pid */
+	struct list_head list;   /* in proc_qtu_data.sock_tag_list */
+	pid_t pid;
+
+	tag_t tag;
+};
+
+struct qtaguid_event_counts {
+	/* Various successful events */
+	atomic64_t sockets_tagged;
+	atomic64_t sockets_untagged;
+	atomic64_t counter_set_changes;
+	atomic64_t delete_cmds;
+	atomic64_t iface_events;  /* Number of NETDEV_* events handled */
+
+	atomic64_t match_calls;   /* Number of times iptables called mt */
+	/* Number of times iptables called mt from pre or post routing hooks */
+	atomic64_t match_calls_prepost;
+	/*
+	 * match_found_sk_*: numbers related to the netfilter matching
+	 * function finding a sock for the sk_buff.
+	 * Total skbs processed is sum(match_found*).
+	 */
+	atomic64_t match_found_sk;   /* An sk was already in the sk_buff. */
+	/* The connection tracker had or didn't have the sk. */
+	atomic64_t match_found_sk_in_ct;
+	atomic64_t match_found_no_sk_in_ct;
+	/*
+	 * No sk could be found. No apparent owner. Could happen with
+	 * unsolicited traffic.
+	 */
+	atomic64_t match_no_sk;
+	/*
+	 * The file ptr in the sk_socket wasn't there and we couldn't get GID.
+	 * This might happen for traffic while the socket is being closed.
+	 */
+	atomic64_t match_no_sk_gid;
+};
+
+/* Track the set active_set for the given tag. */
+struct tag_counter_set {
+	struct tag_node tn;
+	int active_set;
+};
+
+/*----------------------------------------------*/
+/*
+ * The qtu uid data is used to track resources that are created directly or
+ * indirectly by processes (uid tracked).
+ * It is shared by the processes with the same uid.
+ * Some of the resource will be counted to prevent further rogue allocations,
+ * some will need freeing once the owner process (uid) exits.
+ */
+struct uid_tag_data {
+	struct rb_node node;
+	uid_t uid;
+
+	/*
+	 * For the uid, how many accounting tags have been set.
+	 */
+	int num_active_tags;
+	/* Track the number of proc_qtu_data that reference it */
+	int num_pqd;
+	struct rb_root tag_ref_tree;
+	/* No tag_node_tree_lock; use uid_tag_data_tree_lock */
+};
+
+struct tag_ref {
+	struct tag_node tn;
+
+	/*
+	 * This tracks the number of active sockets that have a tag on them
+	 * which matches this tag_ref.tn.tag.
+	 * A tag ref can live on after the sockets are untagged.
+	 * A tag ref can only be removed during a tag delete command.
+	 */
+	int num_sock_tags;
+};
+
+struct proc_qtu_data {
+	struct net *net;
+	struct rb_node node;
+	pid_t pid;
+
+	struct uid_tag_data *parent_tag_data;
+
+	/* Tracks the sock_tags that need freeing upon this proc's death */
+	struct list_head sock_tag_list;
+	/* No spinlock_t sock_tag_list_lock; use the global one. */
+};
+
+/*----------------------------------------------*/
+#endif  /* ifndef __XT_QTAGUID_INTERNAL_H__ */
diff -ruN a/net/netfilter/xt_qtaguid_print.c b/net/netfilter/xt_qtaguid_print.c
--- a/net/netfilter/xt_qtaguid_print.c	1970-01-01 01:00:00.000000000 +0100
+++ b/net/netfilter/xt_qtaguid_print.c	2021-03-20 09:20:37.000000000 +0100
@@ -0,0 +1,565 @@
+/*
+ * Pretty printing Support for iptables xt_qtaguid module.
+ *
+ * (C) 2011 Google, Inc
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/*
+ * Most of the functions in this file just waste time if DEBUG is not defined.
+ * The matching xt_qtaguid_print.h will static inline empty funcs if the needed
+ * debug flags ore not defined.
+ * Those funcs that fail to allocate memory will panic as there is no need to
+ * hobble allong just pretending to do the requested work.
+ */
+
+#define DEBUG
+
+#include <linux/fs.h>
+#include <linux/gfp.h>
+#include <linux/net.h>
+#include <linux/rbtree.h>
+#include <linux/slab.h>
+#include <linux/spinlock_types.h>
+#include <net/sock.h>
+
+#include "xt_qtaguid_internal.h"
+#include "xt_qtaguid_print.h"
+
+#ifdef DDEBUG
+
+static void _bug_on_err_or_null(void *ptr)
+{
+	if (IS_ERR_OR_NULL(ptr)) {
+		pr_err("qtaguid: kmalloc failed\n");
+		BUG();
+	}
+}
+
+char *pp_tag_t(tag_t *tag)
+{
+	char *res;
+
+	if (!tag)
+		res = kasprintf(GFP_ATOMIC, "tag_t@null{}");
+	else
+		res = kasprintf(GFP_ATOMIC,
+				"tag_t@%p{tag=0x%llx, uid=%u}",
+				tag, *tag, get_uid_from_tag(*tag));
+	_bug_on_err_or_null(res);
+	return res;
+}
+
+char *pp_data_counters(struct data_counters *dc, bool showValues)
+{
+	char *res;
+
+	if (!dc)
+		res = kasprintf(GFP_ATOMIC, "data_counters@null{}");
+	else if (showValues)
+		res = kasprintf(
+			GFP_ATOMIC, "data_counters@%p{"
+			"set0{"
+			"rx{"
+			"tcp{b=%llu, p=%llu}, "
+			"udp{b=%llu, p=%llu},"
+			"other{b=%llu, p=%llu}}, "
+			"tx{"
+			"tcp{b=%llu, p=%llu}, "
+			"udp{b=%llu, p=%llu},"
+			"other{b=%llu, p=%llu}}}, "
+			"set1{"
+			"rx{"
+			"tcp{b=%llu, p=%llu}, "
+			"udp{b=%llu, p=%llu},"
+			"other{b=%llu, p=%llu}}, "
+			"tx{"
+			"tcp{b=%llu, p=%llu}, "
+			"udp{b=%llu, p=%llu},"
+			"other{b=%llu, p=%llu}}}}",
+			dc,
+			dc->bpc[0][IFS_RX][IFS_TCP].bytes,
+			dc->bpc[0][IFS_RX][IFS_TCP].packets,
+			dc->bpc[0][IFS_RX][IFS_UDP].bytes,
+			dc->bpc[0][IFS_RX][IFS_UDP].packets,
+			dc->bpc[0][IFS_RX][IFS_PROTO_OTHER].bytes,
+			dc->bpc[0][IFS_RX][IFS_PROTO_OTHER].packets,
+			dc->bpc[0][IFS_TX][IFS_TCP].bytes,
+			dc->bpc[0][IFS_TX][IFS_TCP].packets,
+			dc->bpc[0][IFS_TX][IFS_UDP].bytes,
+			dc->bpc[0][IFS_TX][IFS_UDP].packets,
+			dc->bpc[0][IFS_TX][IFS_PROTO_OTHER].bytes,
+			dc->bpc[0][IFS_TX][IFS_PROTO_OTHER].packets,
+			dc->bpc[1][IFS_RX][IFS_TCP].bytes,
+			dc->bpc[1][IFS_RX][IFS_TCP].packets,
+			dc->bpc[1][IFS_RX][IFS_UDP].bytes,
+			dc->bpc[1][IFS_RX][IFS_UDP].packets,
+			dc->bpc[1][IFS_RX][IFS_PROTO_OTHER].bytes,
+			dc->bpc[1][IFS_RX][IFS_PROTO_OTHER].packets,
+			dc->bpc[1][IFS_TX][IFS_TCP].bytes,
+			dc->bpc[1][IFS_TX][IFS_TCP].packets,
+			dc->bpc[1][IFS_TX][IFS_UDP].bytes,
+			dc->bpc[1][IFS_TX][IFS_UDP].packets,
+			dc->bpc[1][IFS_TX][IFS_PROTO_OTHER].bytes,
+			dc->bpc[1][IFS_TX][IFS_PROTO_OTHER].packets);
+	else
+		res = kasprintf(GFP_ATOMIC, "data_counters@%p{...}", dc);
+	_bug_on_err_or_null(res);
+	return res;
+}
+
+char *pp_tag_node(struct tag_node *tn)
+{
+	char *tag_str;
+	char *res;
+
+	if (!tn) {
+		res = kasprintf(GFP_ATOMIC, "tag_node@null{}");
+		_bug_on_err_or_null(res);
+		return res;
+	}
+	tag_str = pp_tag_t(&tn->tag);
+	res = kasprintf(GFP_ATOMIC,
+			"tag_node@%p{tag=%s}",
+			tn, tag_str);
+	_bug_on_err_or_null(res);
+	kfree(tag_str);
+	return res;
+}
+
+char *pp_tag_ref(struct tag_ref *tr)
+{
+	char *tn_str;
+	char *res;
+
+	if (!tr) {
+		res = kasprintf(GFP_ATOMIC, "tag_ref@null{}");
+		_bug_on_err_or_null(res);
+		return res;
+	}
+	tn_str = pp_tag_node(&tr->tn);
+	res = kasprintf(GFP_ATOMIC,
+			"tag_ref@%p{%s, num_sock_tags=%d}",
+			tr, tn_str, tr->num_sock_tags);
+	_bug_on_err_or_null(res);
+	kfree(tn_str);
+	return res;
+}
+
+char *pp_tag_stat(struct tag_stat *ts)
+{
+	char *tn_str;
+	char *counters_str;
+	char *parent_counters_str;
+	char *res;
+
+	if (!ts) {
+		res = kasprintf(GFP_ATOMIC, "tag_stat@null{}");
+		_bug_on_err_or_null(res);
+		return res;
+	}
+	tn_str = pp_tag_node(&ts->tn);
+	counters_str = pp_data_counters(&ts->counters, true);
+	parent_counters_str = pp_data_counters(ts->parent_counters, false);
+	res = kasprintf(GFP_ATOMIC,
+			"tag_stat@%p{%s, counters=%s, parent_counters=%s}",
+			ts, tn_str, counters_str, parent_counters_str);
+	_bug_on_err_or_null(res);
+	kfree(tn_str);
+	kfree(counters_str);
+	kfree(parent_counters_str);
+	return res;
+}
+
+char *pp_iface_stat(struct iface_stat *is)
+{
+	char *res;
+	if (!is) {
+		res = kasprintf(GFP_ATOMIC, "iface_stat@null{}");
+	} else {
+		struct data_counters *cnts = &is->totals_via_skb;
+		res = kasprintf(GFP_ATOMIC, "iface_stat@%p{"
+				"list=list_head{...}, "
+				"ifname=%s, "
+				"total_dev={rx={bytes=%llu, "
+				"packets=%llu}, "
+				"tx={bytes=%llu, "
+				"packets=%llu}}, "
+				"total_skb={rx={bytes=%llu, "
+				"packets=%llu}, "
+				"tx={bytes=%llu, "
+				"packets=%llu}}, "
+				"last_known_valid=%d, "
+				"last_known={rx={bytes=%llu, "
+				"packets=%llu}, "
+				"tx={bytes=%llu, "
+				"packets=%llu}}, "
+				"active=%d, "
+				"net_dev=%p, "
+				"proc_ptr=%p, "
+				"tag_stat_tree=rb_root{...}}",
+				is,
+				is->ifname,
+				is->totals_via_dev[IFS_RX].bytes,
+				is->totals_via_dev[IFS_RX].packets,
+				is->totals_via_dev[IFS_TX].bytes,
+				is->totals_via_dev[IFS_TX].packets,
+				dc_sum_bytes(cnts, 0, IFS_RX),
+				dc_sum_packets(cnts, 0, IFS_RX),
+				dc_sum_bytes(cnts, 0, IFS_TX),
+				dc_sum_packets(cnts, 0, IFS_TX),
+				is->last_known_valid,
+				is->last_known[IFS_RX].bytes,
+				is->last_known[IFS_RX].packets,
+				is->last_known[IFS_TX].bytes,
+				is->last_known[IFS_TX].packets,
+				is->active,
+				is->net_dev,
+				is->proc_ptr);
+	}
+	_bug_on_err_or_null(res);
+	return res;
+}
+
+char *pp_sock_tag(struct sock_tag *st)
+{
+	char *tag_str;
+	char *res;
+
+	if (!st) {
+		res = kasprintf(GFP_ATOMIC, "sock_tag@null{}");
+		_bug_on_err_or_null(res);
+		return res;
+	}
+	tag_str = pp_tag_t(&st->tag);
+	res = kasprintf(GFP_ATOMIC, "sock_tag@%p{"
+			"sock_node=rb_node{...}, "
+			"sk=%p (f_count=%d), list=list_head{...}, "
+			"pid=%u, tag=%s}",
+			st, st->sk, refcount_read(&st->sk->sk_refcnt),
+			st->pid, tag_str);
+	_bug_on_err_or_null(res);
+	kfree(tag_str);
+	return res;
+}
+
+char *pp_uid_tag_data(struct uid_tag_data *utd)
+{
+	char *res;
+
+	if (!utd)
+		res = kasprintf(GFP_ATOMIC, "uid_tag_data@null{}");
+	else
+		res = kasprintf(GFP_ATOMIC, "uid_tag_data@%p{"
+				"uid=%u, num_active_acct_tags=%d, "
+				"num_pqd=%d, "
+				"tag_node_tree=rb_root{...}, "
+				"proc_qtu_data_tree=rb_root{...}}",
+				utd, utd->uid,
+				utd->num_active_tags, utd->num_pqd);
+	_bug_on_err_or_null(res);
+	return res;
+}
+
+char *pp_proc_qtu_data(struct proc_qtu_data *pqd)
+{
+	char *parent_tag_data_str;
+	char *res;
+
+	if (!pqd) {
+		res = kasprintf(GFP_ATOMIC, "proc_qtu_data@null{}");
+		_bug_on_err_or_null(res);
+		return res;
+	}
+	parent_tag_data_str = pp_uid_tag_data(pqd->parent_tag_data);
+	res = kasprintf(GFP_ATOMIC, "proc_qtu_data@%p{"
+			"node=rb_node{...}, pid=%u, "
+			"parent_tag_data=%s, "
+			"sock_tag_list=list_head{...}}",
+			pqd, pqd->pid, parent_tag_data_str
+		);
+	_bug_on_err_or_null(res);
+	kfree(parent_tag_data_str);
+	return res;
+}
+
+/*------------------------------------------*/
+void prdebug_sock_tag_tree(int indent_level,
+			   struct rb_root *sock_tag_tree)
+{
+	struct rb_node *node;
+	struct sock_tag *sock_tag_entry;
+	char *str;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (RB_EMPTY_ROOT(sock_tag_tree)) {
+		str = "sock_tag_tree=rb_root{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "sock_tag_tree=rb_root{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	for (node = rb_first(sock_tag_tree);
+	     node;
+	     node = rb_next(node)) {
+		sock_tag_entry = rb_entry(node, struct sock_tag, sock_node);
+		str = pp_sock_tag(sock_tag_entry);
+		pr_debug("%*d: %s,\n", indent_level*2, indent_level, str);
+		kfree(str);
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_sock_tag_list(int indent_level,
+			   struct list_head *sock_tag_list)
+{
+	struct sock_tag *sock_tag_entry;
+	char *str;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (list_empty(sock_tag_list)) {
+		str = "sock_tag_list=list_head{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "sock_tag_list=list_head{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	list_for_each_entry(sock_tag_entry, sock_tag_list, list) {
+		str = pp_sock_tag(sock_tag_entry);
+		pr_debug("%*d: %s,\n", indent_level*2, indent_level, str);
+		kfree(str);
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_proc_qtu_data_tree(int indent_level,
+				struct rb_root *proc_qtu_data_tree)
+{
+	char *str;
+	struct rb_node *node;
+	struct proc_qtu_data *proc_qtu_data_entry;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (RB_EMPTY_ROOT(proc_qtu_data_tree)) {
+		str = "proc_qtu_data_tree=rb_root{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "proc_qtu_data_tree=rb_root{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	for (node = rb_first(proc_qtu_data_tree);
+	     node;
+	     node = rb_next(node)) {
+		proc_qtu_data_entry = rb_entry(node,
+					       struct proc_qtu_data,
+					       node);
+		str = pp_proc_qtu_data(proc_qtu_data_entry);
+		pr_debug("%*d: %s,\n", indent_level*2, indent_level,
+			 str);
+		kfree(str);
+		indent_level++;
+		prdebug_sock_tag_list(indent_level,
+				      &proc_qtu_data_entry->sock_tag_list);
+		indent_level--;
+
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_tag_ref_tree(int indent_level, struct rb_root *tag_ref_tree)
+{
+	char *str;
+	struct rb_node *node;
+	struct tag_ref *tag_ref_entry;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (RB_EMPTY_ROOT(tag_ref_tree)) {
+		str = "tag_ref_tree{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "tag_ref_tree{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	for (node = rb_first(tag_ref_tree);
+	     node;
+	     node = rb_next(node)) {
+		tag_ref_entry = rb_entry(node,
+					 struct tag_ref,
+					 tn.node);
+		str = pp_tag_ref(tag_ref_entry);
+		pr_debug("%*d: %s,\n", indent_level*2, indent_level,
+			 str);
+		kfree(str);
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_uid_tag_data_tree(int indent_level,
+			       struct rb_root *uid_tag_data_tree)
+{
+	char *str;
+	struct rb_node *node;
+	struct uid_tag_data *uid_tag_data_entry;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (RB_EMPTY_ROOT(uid_tag_data_tree)) {
+		str = "uid_tag_data_tree=rb_root{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "uid_tag_data_tree=rb_root{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	for (node = rb_first(uid_tag_data_tree);
+	     node;
+	     node = rb_next(node)) {
+		uid_tag_data_entry = rb_entry(node, struct uid_tag_data,
+					      node);
+		str = pp_uid_tag_data(uid_tag_data_entry);
+		pr_debug("%*d: %s,\n", indent_level*2, indent_level, str);
+		kfree(str);
+		if (!RB_EMPTY_ROOT(&uid_tag_data_entry->tag_ref_tree)) {
+			indent_level++;
+			prdebug_tag_ref_tree(indent_level,
+					     &uid_tag_data_entry->tag_ref_tree);
+			indent_level--;
+		}
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_tag_stat_tree(int indent_level,
+				  struct rb_root *tag_stat_tree)
+{
+	char *str;
+	struct rb_node *node;
+	struct tag_stat *ts_entry;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (RB_EMPTY_ROOT(tag_stat_tree)) {
+		str = "tag_stat_tree{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "tag_stat_tree{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	for (node = rb_first(tag_stat_tree);
+	     node;
+	     node = rb_next(node)) {
+		ts_entry = rb_entry(node, struct tag_stat, tn.node);
+		str = pp_tag_stat(ts_entry);
+		pr_debug("%*d: %s\n", indent_level*2, indent_level,
+			 str);
+		kfree(str);
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+void prdebug_iface_stat_list(int indent_level,
+			     struct list_head *iface_stat_list)
+{
+	char *str;
+	struct iface_stat *iface_entry;
+
+	if (!unlikely(qtaguid_debug_mask & DDEBUG_MASK))
+		return;
+
+	if (list_empty(iface_stat_list)) {
+		str = "iface_stat_list=list_head{}";
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		return;
+	}
+
+	str = "iface_stat_list=list_head{";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+	indent_level++;
+	list_for_each_entry(iface_entry, iface_stat_list, list) {
+		str = pp_iface_stat(iface_entry);
+		pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+		kfree(str);
+
+		spin_lock_bh(&iface_entry->tag_stat_list_lock);
+		if (!RB_EMPTY_ROOT(&iface_entry->tag_stat_tree)) {
+			indent_level++;
+			prdebug_tag_stat_tree(indent_level,
+					      &iface_entry->tag_stat_tree);
+			indent_level--;
+		}
+		spin_unlock_bh(&iface_entry->tag_stat_list_lock);
+	}
+	indent_level--;
+	str = "}";
+	pr_debug("%*d: %s\n", indent_level*2, indent_level, str);
+}
+
+#endif  /* ifdef DDEBUG */
+/*------------------------------------------*/
+static const char * const netdev_event_strings[] = {
+	"netdev_unknown",
+	"NETDEV_UP",
+	"NETDEV_DOWN",
+	"NETDEV_REBOOT",
+	"NETDEV_CHANGE",
+	"NETDEV_REGISTER",
+	"NETDEV_UNREGISTER",
+	"NETDEV_CHANGEMTU",
+	"NETDEV_CHANGEADDR",
+	"NETDEV_GOING_DOWN",
+	"NETDEV_CHANGENAME",
+	"NETDEV_FEAT_CHANGE",
+	"NETDEV_BONDING_FAILOVER",
+	"NETDEV_PRE_UP",
+	"NETDEV_PRE_TYPE_CHANGE",
+	"NETDEV_POST_TYPE_CHANGE",
+	"NETDEV_POST_INIT",
+	"NETDEV_UNREGISTER_BATCH",
+	"NETDEV_RELEASE",
+	"NETDEV_NOTIFY_PEERS",
+	"NETDEV_JOIN",
+};
+
+const char *netdev_evt_str(int netdev_event)
+{
+	if (netdev_event < 0
+	    || netdev_event >= ARRAY_SIZE(netdev_event_strings))
+		return "bad event num";
+	return netdev_event_strings[netdev_event];
+}
diff -ruN a/net/netfilter/xt_qtaguid_print.h b/net/netfilter/xt_qtaguid_print.h
--- a/net/netfilter/xt_qtaguid_print.h	1970-01-01 01:00:00.000000000 +0100
+++ b/net/netfilter/xt_qtaguid_print.h	2021-03-20 09:20:37.000000000 +0100
@@ -0,0 +1,120 @@
+/*
+ * Pretty printing Support for iptables xt_qtaguid module.
+ *
+ * (C) 2011 Google, Inc
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#ifndef __XT_QTAGUID_PRINT_H__
+#define __XT_QTAGUID_PRINT_H__
+
+#include "xt_qtaguid_internal.h"
+
+#ifdef DDEBUG
+
+char *pp_tag_t(tag_t *tag);
+char *pp_data_counters(struct data_counters *dc, bool showValues);
+char *pp_tag_node(struct tag_node *tn);
+char *pp_tag_ref(struct tag_ref *tr);
+char *pp_tag_stat(struct tag_stat *ts);
+char *pp_iface_stat(struct iface_stat *is);
+char *pp_sock_tag(struct sock_tag *st);
+char *pp_uid_tag_data(struct uid_tag_data *qtd);
+char *pp_proc_qtu_data(struct proc_qtu_data *pqd);
+
+/*------------------------------------------*/
+void prdebug_sock_tag_list(int indent_level,
+			   struct list_head *sock_tag_list);
+void prdebug_sock_tag_tree(int indent_level,
+			   struct rb_root *sock_tag_tree);
+void prdebug_proc_qtu_data_tree(int indent_level,
+				struct rb_root *proc_qtu_data_tree);
+void prdebug_tag_ref_tree(int indent_level, struct rb_root *tag_ref_tree);
+void prdebug_uid_tag_data_tree(int indent_level,
+			       struct rb_root *uid_tag_data_tree);
+void prdebug_tag_stat_tree(int indent_level,
+			   struct rb_root *tag_stat_tree);
+void prdebug_iface_stat_list(int indent_level,
+			     struct list_head *iface_stat_list);
+
+#else
+
+/*------------------------------------------*/
+static inline char *pp_tag_t(tag_t *tag)
+{
+	return NULL;
+}
+static inline char *pp_data_counters(struct data_counters *dc, bool showValues)
+{
+	return NULL;
+}
+static inline char *pp_tag_node(struct tag_node *tn)
+{
+	return NULL;
+}
+static inline char *pp_tag_ref(struct tag_ref *tr)
+{
+	return NULL;
+}
+static inline char *pp_tag_stat(struct tag_stat *ts)
+{
+	return NULL;
+}
+static inline char *pp_iface_stat(struct iface_stat *is)
+{
+	return NULL;
+}
+static inline char *pp_sock_tag(struct sock_tag *st)
+{
+	return NULL;
+}
+static inline char *pp_uid_tag_data(struct uid_tag_data *qtd)
+{
+	return NULL;
+}
+static inline char *pp_proc_qtu_data(struct proc_qtu_data *pqd)
+{
+	return NULL;
+}
+
+/*------------------------------------------*/
+static inline
+void prdebug_sock_tag_list(int indent_level,
+			   struct list_head *sock_tag_list)
+{
+}
+static inline
+void prdebug_sock_tag_tree(int indent_level,
+			   struct rb_root *sock_tag_tree)
+{
+}
+static inline
+void prdebug_proc_qtu_data_tree(int indent_level,
+				struct rb_root *proc_qtu_data_tree)
+{
+}
+static inline
+void prdebug_tag_ref_tree(int indent_level, struct rb_root *tag_ref_tree)
+{
+}
+static inline
+void prdebug_uid_tag_data_tree(int indent_level,
+			       struct rb_root *uid_tag_data_tree)
+{
+}
+static inline
+void prdebug_tag_stat_tree(int indent_level,
+			   struct rb_root *tag_stat_tree)
+{
+}
+static inline
+void prdebug_iface_stat_list(int indent_level,
+			     struct list_head *iface_stat_list)
+{
+}
+#endif
+/*------------------------------------------*/
+const char *netdev_evt_str(int netdev_event);
+#endif  /* ifndef __XT_QTAGUID_PRINT_H__ */
diff -ruN a/include/linux/netfilter/xt_qtaguid.h b/include/linux/netfilter/xt_qtaguid.h
--- a/include/linux/netfilter/xt_qtaguid.h	1970-01-01 01:00:00.000000000 +0100
+++ b/include/linux/netfilter/xt_qtaguid.h	2021-03-20 09:20:32.000000000 +0100
@@ -0,0 +1,15 @@
+#ifndef _XT_QTAGUID_MATCH_H
+#define _XT_QTAGUID_MATCH_H
+
+/* For now we just replace the xt_owner.
+ * FIXME: make iptables aware of qtaguid. */
+#include <linux/net.h>
+#include <linux/netfilter/xt_owner.h>
+
+#define XT_QTAGUID_UID    XT_OWNER_UID
+#define XT_QTAGUID_GID    XT_OWNER_GID
+#define XT_QTAGUID_SOCKET XT_OWNER_SOCKET
+#define xt_qtaguid_match_info xt_owner_match_info
+
+int qtaguid_untag(struct socket *sock, bool kernel);
+#endif /* _XT_QTAGUID_MATCH_H */
diff -ruN a/net/netfilter/xt_quota2.c b/net/netfilter/xt_quota2.c
--- a/net/netfilter/xt_quota2.c	1970-01-01 01:00:00.000000000 +0100
+++ b/net/netfilter/xt_quota2.c	2021-12-23 08:35:59.000000000 +0100
@@ -0,0 +1,397 @@
+/*
+ * xt_quota2 - enhanced xt_quota that can count upwards and in packets
+ * as a minimal accounting match.
+ * by Jan Engelhardt <jengelh@medozas.de>, 2008
+ *
+ * Originally based on xt_quota.c:
+ * 	netfilter module to enforce network quotas
+ * 	Sam Johnston <samj@samj.net>
+ *
+ *	This program is free software; you can redistribute it and/or modify
+ *	it under the terms of the GNU General Public License; either
+ *	version 2 of the License, as published by the Free Software Foundation.
+ */
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <linux/skbuff.h>
+#include <linux/spinlock.h>
+#include <asm/atomic.h>
+#include <net/netlink.h>
+
+#include <linux/netfilter/x_tables.h>
+#include <linux/netfilter/xt_quota2.h>
+
+#ifdef CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG
+/* For compatibility, these definitions are copied from the
+ * deprecated header file <linux/netfilter_ipv4/ipt_ULOG.h> */
+#define ULOG_MAC_LEN	80
+#define ULOG_PREFIX_LEN	32
+
+/* Format of the ULOG packets passed through netlink */
+typedef struct ulog_packet_msg {
+	unsigned long mark;
+	long timestamp_sec;
+	long timestamp_usec;
+	unsigned int hook;
+	char indev_name[IFNAMSIZ];
+	char outdev_name[IFNAMSIZ];
+	size_t data_len;
+	char prefix[ULOG_PREFIX_LEN];
+	unsigned char mac_len;
+	unsigned char mac[ULOG_MAC_LEN];
+	unsigned char payload[0];
+} ulog_packet_msg_t;
+#endif
+
+/**
+ * @lock:	lock to protect quota writers from each other
+ */
+struct xt_quota_counter {
+	u_int64_t quota;
+	spinlock_t lock;
+	struct list_head list;
+	atomic_t ref;
+	char name[sizeof(((struct xt_quota_mtinfo2 *)NULL)->name)];
+	struct proc_dir_entry *procfs_entry;
+};
+
+#ifdef CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG
+/* Harald's favorite number +1 :D From ipt_ULOG.C */
+static int qlog_nl_event = 112;
+module_param_named(event_num, qlog_nl_event, uint, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(event_num,
+		 "Event number for NETLINK_NFLOG message. 0 disables log."
+		 "111 is what ipt_ULOG uses.");
+static struct sock *nflognl;
+#endif
+
+static LIST_HEAD(counter_list);
+static DEFINE_SPINLOCK(counter_list_lock);
+
+static struct proc_dir_entry *proc_xt_quota;
+static unsigned int quota_list_perms = S_IRUGO | S_IWUSR;
+static kuid_t quota_list_uid = KUIDT_INIT(0);
+static kgid_t quota_list_gid = KGIDT_INIT(0);
+module_param_named(perms, quota_list_perms, uint, S_IRUGO | S_IWUSR);
+
+#ifdef CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG
+static void quota2_log(unsigned int hooknum,
+		       const struct sk_buff *skb,
+		       const struct net_device *in,
+		       const struct net_device *out,
+		       const char *prefix)
+{
+	ulog_packet_msg_t *pm;
+	struct sk_buff *log_skb;
+	size_t size;
+	struct nlmsghdr *nlh;
+
+	if (!qlog_nl_event)
+		return;
+
+	size = NLMSG_SPACE(sizeof(*pm));
+	size = max(size, (size_t)NLMSG_GOODSIZE);
+	log_skb = alloc_skb(size, GFP_ATOMIC);
+	if (!log_skb) {
+		pr_err("xt_quota2: cannot alloc skb for logging\n");
+		return;
+	}
+
+	nlh = nlmsg_put(log_skb, /*pid*/0, /*seq*/0, qlog_nl_event,
+			sizeof(*pm), 0);
+	if (!nlh) {
+		pr_err("xt_quota2: nlmsg_put failed\n");
+		kfree_skb(log_skb);
+		return;
+	}
+	pm = nlmsg_data(nlh);
+	memset(pm, 0, sizeof(*pm));
+	if (skb->tstamp == 0)
+		__net_timestamp((struct sk_buff *)skb);
+	pm->hook = hooknum;
+	if (prefix != NULL)
+		strlcpy(pm->prefix, prefix, sizeof(pm->prefix));
+	if (in)
+		strlcpy(pm->indev_name, in->name, sizeof(pm->indev_name));
+	if (out)
+		strlcpy(pm->outdev_name, out->name, sizeof(pm->outdev_name));
+
+	NETLINK_CB(log_skb).dst_group = 1;
+	pr_debug("throwing 1 packets to netlink group 1\n");
+	netlink_broadcast(nflognl, log_skb, 0, 1, GFP_ATOMIC);
+}
+#else
+static void quota2_log(unsigned int hooknum,
+		       const struct sk_buff *skb,
+		       const struct net_device *in,
+		       const struct net_device *out,
+		       const char *prefix)
+{
+}
+#endif  /* if+else CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG */
+
+static ssize_t quota_proc_read(struct file *file, char __user *buf,
+			   size_t size, loff_t *ppos)
+{
+	struct xt_quota_counter *e = PDE_DATA(file_inode(file));
+	char tmp[24];
+	size_t tmp_size;
+
+	spin_lock_bh(&e->lock);
+	tmp_size = scnprintf(tmp, sizeof(tmp), "%llu\n", e->quota);
+	spin_unlock_bh(&e->lock);
+	return simple_read_from_buffer(buf, size, ppos, tmp, tmp_size);
+}
+
+static ssize_t quota_proc_write(struct file *file, const char __user *input,
+                            size_t size, loff_t *ppos)
+{
+	struct xt_quota_counter *e = PDE_DATA(file_inode(file));
+	char buf[sizeof("18446744073709551616")];
+
+	if (size > sizeof(buf))
+		size = sizeof(buf);
+	if (copy_from_user(buf, input, size) != 0)
+		return -EFAULT;
+	buf[sizeof(buf)-1] = '\0';
+	if (size < sizeof(buf))
+		buf[size] = '\0';
+
+	spin_lock_bh(&e->lock);
+	e->quota = simple_strtoull(buf, NULL, 0);
+	spin_unlock_bh(&e->lock);
+	return size;
+}
+
+static const struct proc_ops q2_counter_fops = {
+	.proc_read	= quota_proc_read,
+	.proc_write	= quota_proc_write,
+	.proc_lseek	= default_llseek,
+};
+
+static struct xt_quota_counter *
+q2_new_counter(const struct xt_quota_mtinfo2 *q, bool anon)
+{
+	struct xt_quota_counter *e;
+	unsigned int size;
+
+	/* Do not need all the procfs things for anonymous counters. */
+	size = anon ? offsetof(typeof(*e), list) : sizeof(*e);
+	e = kmalloc(size, GFP_KERNEL);
+	if (e == NULL)
+		return NULL;
+
+	e->quota = q->quota;
+	spin_lock_init(&e->lock);
+	if (!anon) {
+		INIT_LIST_HEAD(&e->list);
+		atomic_set(&e->ref, 1);
+		strlcpy(e->name, q->name, sizeof(e->name));
+	}
+	return e;
+}
+
+/**
+ * q2_get_counter - get ref to counter or create new
+ * @name:	name of counter
+ */
+static struct xt_quota_counter *
+q2_get_counter(const struct xt_quota_mtinfo2 *q)
+{
+	struct proc_dir_entry *p;
+	struct xt_quota_counter *e = NULL;
+	struct xt_quota_counter *new_e;
+
+	if (*q->name == '\0')
+		return q2_new_counter(q, true);
+
+	/* No need to hold a lock while getting a new counter */
+	new_e = q2_new_counter(q, false);
+	if (new_e == NULL)
+		goto out;
+
+	spin_lock_bh(&counter_list_lock);
+	list_for_each_entry(e, &counter_list, list)
+		if (strcmp(e->name, q->name) == 0) {
+			atomic_inc(&e->ref);
+			spin_unlock_bh(&counter_list_lock);
+			kfree(new_e);
+			pr_debug("xt_quota2: old counter name=%s", e->name);
+			return e;
+		}
+	e = new_e;
+	pr_debug("xt_quota2: new_counter name=%s", e->name);
+	list_add_tail(&e->list, &counter_list);
+	/* The entry having a refcount of 1 is not directly destructible.
+	 * This func has not yet returned the new entry, thus iptables
+	 * has not references for destroying this entry.
+	 * For another rule to try to destroy it, it would 1st need for this
+	 * func* to be re-invoked, acquire a new ref for the same named quota.
+	 * Nobody will access the e->procfs_entry either.
+	 * So release the lock. */
+	spin_unlock_bh(&counter_list_lock);
+
+	/* create_proc_entry() is not spin_lock happy */
+	p = e->procfs_entry = proc_create_data(e->name, quota_list_perms,
+	                      proc_xt_quota, &q2_counter_fops, e);
+
+	if (IS_ERR_OR_NULL(p)) {
+		spin_lock_bh(&counter_list_lock);
+		list_del(&e->list);
+		spin_unlock_bh(&counter_list_lock);
+		goto out;
+	}
+	proc_set_user(p, quota_list_uid, quota_list_gid);
+	return e;
+
+ out:
+	kfree(e);
+	return NULL;
+}
+
+static int quota_mt2_check(const struct xt_mtchk_param *par)
+{
+	struct xt_quota_mtinfo2 *q = par->matchinfo;
+
+	pr_debug("xt_quota2: check() flags=0x%04x", q->flags);
+
+	if (q->flags & ~XT_QUOTA_MASK)
+		return -EINVAL;
+
+	q->name[sizeof(q->name)-1] = '\0';
+	if (*q->name == '.' || strchr(q->name, '/') != NULL) {
+		printk(KERN_ERR "xt_quota.3: illegal name\n");
+		return -EINVAL;
+	}
+
+	q->master = q2_get_counter(q);
+	if (q->master == NULL) {
+		printk(KERN_ERR "xt_quota.3: memory alloc failure\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void quota_mt2_destroy(const struct xt_mtdtor_param *par)
+{
+	struct xt_quota_mtinfo2 *q = par->matchinfo;
+	struct xt_quota_counter *e = q->master;
+
+	if (*q->name == '\0') {
+		kfree(e);
+		return;
+	}
+
+	spin_lock_bh(&counter_list_lock);
+	if (!atomic_dec_and_test(&e->ref)) {
+		spin_unlock_bh(&counter_list_lock);
+		return;
+	}
+
+	list_del(&e->list);
+	spin_unlock_bh(&counter_list_lock);
+	remove_proc_entry(e->name, proc_xt_quota);
+	kfree(e);
+}
+
+static bool
+quota_mt2(const struct sk_buff *skb, struct xt_action_param *par)
+{
+	struct xt_quota_mtinfo2 *q = (void *)par->matchinfo;
+	struct xt_quota_counter *e = q->master;
+	int charge = (q->flags & XT_QUOTA_PACKET) ? 1 : skb->len;
+	bool no_change = q->flags & XT_QUOTA_NO_CHANGE;
+	bool ret = q->flags & XT_QUOTA_INVERT;
+
+	spin_lock_bh(&e->lock);
+	if (q->flags & XT_QUOTA_GROW) {
+		/*
+		 * While no_change is pointless in "grow" mode, we will
+		 * implement it here simply to have a consistent behavior.
+		 */
+		if (!no_change)
+			e->quota += charge;
+		ret = true; /* note: does not respect inversion (bug??) */
+	} else {
+		if (e->quota > charge) {
+			if (!no_change)
+				e->quota -= charge;
+			ret = !ret;
+		} else if (e->quota) {
+			/* We are transitioning, log that fact. */
+			quota2_log(xt_hooknum(par),
+				   skb,
+				   xt_in(par),
+				   xt_out(par),
+				   q->name);
+			/* we do not allow even small packets from now on */
+			e->quota = 0;
+		}
+	}
+	spin_unlock_bh(&e->lock);
+	return ret;
+}
+
+static struct xt_match quota_mt2_reg[] __read_mostly = {
+	{
+		.name       = "quota2",
+		.revision   = 3,
+		.family     = NFPROTO_IPV4,
+		.checkentry = quota_mt2_check,
+		.match      = quota_mt2,
+		.destroy    = quota_mt2_destroy,
+		.matchsize  = sizeof(struct xt_quota_mtinfo2),
+		.usersize   = offsetof(struct xt_quota_mtinfo2, master),
+		.me         = THIS_MODULE,
+	},
+	{
+		.name       = "quota2",
+		.revision   = 3,
+		.family     = NFPROTO_IPV6,
+		.checkentry = quota_mt2_check,
+		.match      = quota_mt2,
+		.destroy    = quota_mt2_destroy,
+		.matchsize  = sizeof(struct xt_quota_mtinfo2),
+		.usersize   = offsetof(struct xt_quota_mtinfo2, master),
+		.me         = THIS_MODULE,
+	},
+};
+
+static int __init quota_mt2_init(void)
+{
+	int ret;
+	pr_debug("xt_quota2: init()");
+
+#ifdef CONFIG_NETFILTER_XT_MATCH_QUOTA2_LOG
+	nflognl = netlink_kernel_create(&init_net, NETLINK_NFLOG, NULL);
+	if (!nflognl)
+		return -ENOMEM;
+#endif
+
+	proc_xt_quota = proc_mkdir("xt_quota", init_net.proc_net);
+	if (proc_xt_quota == NULL)
+		return -EACCES;
+
+	ret = xt_register_matches(quota_mt2_reg, ARRAY_SIZE(quota_mt2_reg));
+	if (ret < 0)
+		remove_proc_entry("xt_quota", init_net.proc_net);
+	pr_debug("xt_quota2: init() %d", ret);
+	return ret;
+}
+
+static void __exit quota_mt2_exit(void)
+{
+	xt_unregister_matches(quota_mt2_reg, ARRAY_SIZE(quota_mt2_reg));
+	remove_proc_entry("xt_quota", init_net.proc_net);
+}
+
+module_init(quota_mt2_init);
+module_exit(quota_mt2_exit);
+MODULE_DESCRIPTION("Xtables: countdown quota match; up counter");
+MODULE_AUTHOR("Sam Johnston <samj@samj.net>");
+MODULE_AUTHOR("Jan Engelhardt <jengelh@medozas.de>");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("ipt_quota2");
+MODULE_ALIAS("ip6t_quota2");
diff -ruN a/net/socket.c b/net/socket.c
--- a/net/socket.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/socket.c	2021-12-23 08:36:00.000000000 +0100
@@ -364,7 +364,8 @@
 
 static int sockfs_xattr_get(const struct xattr_handler *handler,
 			    struct dentry *dentry, struct inode *inode,
-			    const char *suffix, void *value, size_t size)
+			    const char *suffix, void *value, size_t size,
+			    int flags)
 {
 	if (value) {
 		if (dentry->d_name.len + 1 > size)
diff -ruN a/net/unix/af_unix.c b/net/unix/af_unix.c
--- a/net/unix/af_unix.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/unix/af_unix.c	2021-12-23 08:36:00.000000000 +0100
@@ -3327,7 +3327,12 @@
 {
 	int error = -ENOMEM;
 
-	net->unx.sysctl_max_dgram_qlen = 10;
+	/* The value was 10 in the original kernel. It is modified directly
+	 * here to give the larger value for processes inside containers, in
+	 * which the kernel does not provide a way to dynamically customize.
+	 * TODO(crbug/758081): Implement and upstream a safe way to customize.
+	 */
+	net->unx.sysctl_max_dgram_qlen = 60;
 	if (unix_sysctl_register(net))
 		goto out;
 
diff -ruN a/net/xfrm/xfrm_algo.c b/net/xfrm/xfrm_algo.c
--- a/net/xfrm/xfrm_algo.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/xfrm/xfrm_algo.c	2021-12-23 08:36:00.000000000 +0100
@@ -237,7 +237,7 @@
 
 	.uinfo = {
 		.auth = {
-			.icv_truncbits = 96,
+			.icv_truncbits = IS_ENABLED(CONFIG_ANDROID) ? 128 : 96,
 			.icv_fullbits = 256,
 		}
 	},
diff -ruN a/net/xfrm/xfrm_sysctl.c b/net/xfrm/xfrm_sysctl.c
--- a/net/xfrm/xfrm_sysctl.c	2021-12-08 09:04:57.000000000 +0100
+++ b/net/xfrm/xfrm_sysctl.c	2021-12-23 08:36:00.000000000 +0100
@@ -55,9 +55,13 @@
 	table[2].data = &net->xfrm.sysctl_larval_drop;
 	table[3].data = &net->xfrm.sysctl_acq_expires;
 
-	/* Don't export sysctls to unprivileged users */
-	if (net->user_ns != &init_user_ns)
-		table[0].procname = NULL;
+	/* Only export xfrm_acq_expires to unprivileged users. This is required
+	 * By Android Ipsec stack as per CTS.
+	 */
+	if (net->user_ns != &init_user_ns) {
+		table[0] = table[3];
+		table[1].procname = NULL;
+	}
 
 	net->xfrm.sysctl_hdr = register_net_sysctl(net, "net/core", table);
 	if (!net->xfrm.sysctl_hdr)
diff -ruN a/security/chromiumos/alt-syscall.c b/security/chromiumos/alt-syscall.c
--- a/security/chromiumos/alt-syscall.c	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/alt-syscall.c	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,538 @@
+/*
+ * Chromium OS alt-syscall tables
+ *
+ * Copyright (C) 2015 Google, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/alt-syscall.h>
+#include <linux/compat.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/prctl.h>
+#include <linux/sched/types.h>
+#include <linux/slab.h>
+#include <linux/socket.h>
+#include <linux/syscalls.h>
+#include <linux/timex.h>
+#include <uapi/linux/sched/types.h>
+
+#include <asm/unistd.h>
+
+#include "alt-syscall.h"
+#include "android_whitelists.h"
+#include "complete_whitelists.h"
+#include "read_write_test_whitelists.h"
+#include "third_party_whitelists.h"
+
+/* Intercept and log blocked syscalls. */
+static asmlinkage long block_syscall(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+
+	pr_warn_ratelimited("[%d] %s: blocked syscall %d\n", task_pid_nr(task),
+		task->comm, syscall_get_nr(task, regs));
+
+	return -ENOSYS;
+}
+
+/*
+ * In permissive mode, warn that the syscall was blocked, but still allow
+ * it to go through.  Note that since we don't have an easy way to map from
+ * syscall to number of arguments, we pass the maximum (6).
+ */
+static asmlinkage long warn_syscall(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	int nr = syscall_get_nr(task, regs);
+	sys_call_ptr_t fn = (sys_call_ptr_t)default_table.table[nr];
+
+	pr_warn_ratelimited("[%d] %s: syscall %d not whitelisted\n",
+			    task_pid_nr(task), task->comm, nr);
+
+	return fn(regs);
+}
+
+#ifdef CONFIG_COMPAT
+static asmlinkage long warn_compat_syscall(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	int nr = syscall_get_nr(task, regs);
+	sys_call_ptr_t fn = (sys_call_ptr_t)default_table.compat_table[nr];
+
+	pr_warn_ratelimited("[%d] %s: compat syscall %d not whitelisted\n",
+			    task_pid_nr(task), task->comm, nr);
+
+	return fn(regs);
+}
+#endif /* CONFIG_COMPAT */
+
+static asmlinkage long alt_sys_prctl(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	unsigned long args[6];
+
+	syscall_get_arguments(task, regs, args);
+
+	if (args[0] == PR_ALT_SYSCALL &&
+	    args[1] == PR_ALT_SYSCALL_SET_SYSCALL_TABLE)
+		return -EPERM;
+
+	return ksys_prctl(args[0], args[1], args[2], args[3], args[4]);
+}
+
+/* Thread priority used by Android. */
+#define ANDROID_PRIORITY_FOREGROUND     -2
+#define ANDROID_PRIORITY_DISPLAY        -4
+#define ANDROID_PRIORITY_URGENT_DISPLAY -8
+#define ANDROID_PRIORITY_AUDIO         -16
+#define ANDROID_PRIORITY_URGENT_AUDIO  -19
+#define ANDROID_PRIORITY_HIGHEST       -20
+
+/* Reduced priority when running inside container. */
+#define CONTAINER_PRIORITY_FOREGROUND     -1
+#define CONTAINER_PRIORITY_DISPLAY        -2
+#define CONTAINER_PRIORITY_URGENT_DISPLAY -4
+#define CONTAINER_PRIORITY_AUDIO          -8
+#define CONTAINER_PRIORITY_URGENT_AUDIO   -9
+#define CONTAINER_PRIORITY_HIGHEST       -10
+
+/*
+ * TODO(mortonm): Move the implementation of these Android-specific
+ * alt-syscalls (starting with android_*) to their own .c file.
+ */
+static asmlinkage long android_getpriority(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	long prio, nice;
+	unsigned long args[6];
+	int which, who;
+
+	syscall_get_arguments(task, regs, args);
+	which = args[0];
+	who = args[1];
+
+	prio = ksys_getpriority(which, who);
+	if (prio <= 20)
+		return prio;
+
+	nice = -(prio - 20);
+	switch (nice) {
+	case CONTAINER_PRIORITY_FOREGROUND:
+		nice = ANDROID_PRIORITY_FOREGROUND;
+		break;
+	case CONTAINER_PRIORITY_DISPLAY:
+		nice = ANDROID_PRIORITY_DISPLAY;
+		break;
+	case CONTAINER_PRIORITY_URGENT_DISPLAY:
+		nice = ANDROID_PRIORITY_URGENT_DISPLAY;
+		break;
+	case CONTAINER_PRIORITY_AUDIO:
+		nice = ANDROID_PRIORITY_AUDIO;
+		break;
+	case CONTAINER_PRIORITY_URGENT_AUDIO:
+		nice = ANDROID_PRIORITY_URGENT_AUDIO;
+		break;
+	case CONTAINER_PRIORITY_HIGHEST:
+		nice = ANDROID_PRIORITY_HIGHEST;
+		break;
+	}
+
+	return -nice + 20;
+}
+
+static asmlinkage long android_keyctl(struct pt_regs *regs)
+{
+	return -EACCES;
+}
+
+
+static asmlinkage long android_setpriority(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	unsigned long args[6];
+	int which, who, niceval;
+
+	syscall_get_arguments(task, regs, args);
+	which = args[0];
+	who = args[1];
+	niceval = args[2];
+
+	if (niceval < 0) {
+		if (niceval < -20)
+			niceval = -20;
+		niceval = niceval / 2;
+	}
+	return ksys_setpriority(which, who, niceval);
+}
+
+static asmlinkage long
+do_android_sched_setscheduler(pid_t pid, int policy,
+			      struct sched_param __user *param)
+{
+	struct sched_param lparam;
+	struct task_struct *p;
+	long retval;
+
+	if (!param || pid < 0)
+		return -EINVAL;
+	if (copy_from_user(&lparam, param, sizeof(struct sched_param)))
+		return -EFAULT;
+
+	rcu_read_lock();
+	retval = -ESRCH;
+	p = pid ? find_task_by_vpid(pid) : current;
+	if (likely(p))
+		get_task_struct(p);
+	rcu_read_unlock();
+
+	if (likely(p)) {
+		const struct cred *cred = current_cred();
+		kuid_t android_root_uid, android_system_uid;
+
+		/*
+		 * Allow root(0) and system(1000) processes to set RT scheduler.
+		 *
+		 * The system_server process run under system provides
+		 * SchedulingPolicyService which is used by audioflinger and
+		 * other services to boost their threads, so allow it to set RT
+		 * scheduler for other threads.
+		 */
+		android_root_uid = make_kuid(cred->user_ns, 0);
+		android_system_uid = make_kuid(cred->user_ns, 1000);
+		if ((uid_eq(cred->euid, android_root_uid) ||
+		     uid_eq(cred->euid, android_system_uid)) &&
+		    ns_capable(cred->user_ns, CAP_SYS_NICE))
+			retval = sched_setscheduler_nocheck(p, policy, &lparam);
+		else
+			retval = sched_setscheduler(p, policy, &lparam);
+		put_task_struct(p);
+	}
+
+	return retval;
+}
+
+static asmlinkage long
+android_sched_setscheduler(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	unsigned long args[6];
+	pid_t pid;
+	int policy;
+	struct sched_param __user *param;
+
+	syscall_get_arguments(task, regs, args);
+	pid = args[0];
+	policy = args[1];
+	param = (struct sched_param __user *)args[2];
+
+	/* negative values for policy are not valid */
+	if (policy < 0)
+		return -EINVAL;
+	return do_android_sched_setscheduler(pid, policy, param);
+}
+
+/*
+ * sched_setparam() passes in -1 for its policy, to let the functions
+ * it calls know not to change it.
+ */
+#define SETPARAM_POLICY -1
+
+static asmlinkage long android_sched_setparam(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	unsigned long args[6];
+	pid_t pid;
+	struct sched_param __user *param;
+
+	syscall_get_arguments(task, regs, args);
+	pid = args[0];
+	param = (struct sched_param __user *)args[1];
+
+        return do_android_sched_setscheduler(pid, SETPARAM_POLICY, param);
+}
+
+static asmlinkage long __maybe_unused android_socket(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	unsigned long args[6];
+	int domain, type, socket;
+
+	syscall_get_arguments(task, regs, args);
+	domain = args[0];
+	type = args[1];
+	socket = args[2];
+
+	if (domain == AF_VSOCK)
+	       return -EACCES;
+	return __sys_socket(domain, type, socket);
+}
+
+static asmlinkage long android_perf_event_open(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	unsigned long args[6];
+	struct perf_event_attr __user *attr_uptr;
+	pid_t pid;
+	int cpu, group_fd;
+	unsigned long flags;
+
+	if (!allow_devmode_syscalls)
+		return -EACCES;
+
+	syscall_get_arguments(task, regs, args);
+	attr_uptr = (struct perf_event_attr __user *)args[0];
+	pid = args[1];
+	cpu = args[2];
+	group_fd = args[3];
+	flags = args[4];
+
+	return ksys_perf_event_open(attr_uptr, pid, cpu, group_fd, flags);
+}
+
+static asmlinkage long android_adjtimex(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	struct __kernel_timex kbuf;
+	struct __kernel_timex __user *buf;
+	unsigned long args[6];
+
+	syscall_get_arguments(task, regs, args);
+	buf = (struct __kernel_timex __user *)args[0];
+
+	/* adjtimex() is allowed only for read. */
+	if (copy_from_user(&kbuf, buf, sizeof(struct __kernel_timex)))
+		return -EFAULT;
+	if (kbuf.modes != 0)
+		return -EPERM;
+	return ksys_adjtimex(buf);
+}
+
+static asmlinkage long android_clock_adjtime(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	struct __kernel_timex kbuf;
+	unsigned long args[6];
+	clockid_t which_clock;
+	struct __kernel_timex __user *buf;
+
+	syscall_get_arguments(task, regs, args);
+	which_clock = args[0];
+	buf = (struct __kernel_timex __user *)args[1];
+
+	/* clock_adjtime() is allowed only for read. */
+	if (copy_from_user(&kbuf, buf, sizeof(struct __kernel_timex)))
+		return -EFAULT;
+	if (kbuf.modes != 0)
+		return -EPERM;
+	return ksys_clock_adjtime(which_clock, buf);
+}
+
+static asmlinkage long android_getcpu(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	unsigned long args[6];
+	unsigned __user *cpu;
+	unsigned __user *node;
+	struct getcpu_cache __user *tcache;
+
+	syscall_get_arguments(task, regs, args);
+	cpu = (unsigned __user *)args[0];
+	node = (unsigned __user *)args[1];
+	tcache = (struct getcpu_cache __user *)args[2];
+
+	if (node || tcache)
+		return -EPERM;
+	return ksys_getcpu(cpu, node, tcache);
+}
+
+#ifdef CONFIG_COMPAT
+static asmlinkage long android_compat_adjtimex(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	struct old_timex32 kbuf;
+	struct old_timex32 __user *buf;
+	unsigned long args[6];
+
+	syscall_get_arguments(task, regs, args);
+	buf = (struct old_timex32 __user *)args[0];
+
+	/* adjtimex() is allowed only for read. */
+	if (copy_from_user(&kbuf, buf, sizeof(struct old_timex32)))
+		return -EFAULT;
+	if (kbuf.modes != 0)
+		return -EPERM;
+	return ksys_adjtimex_time32(buf);
+}
+
+static asmlinkage long
+android_compat_clock_adjtime(struct pt_regs *regs)
+{
+	struct task_struct *task = current;
+	struct old_timex32 kbuf;
+	unsigned long args[6];
+	clockid_t which_clock;
+	struct old_timex32 __user *buf;
+
+	syscall_get_arguments(task, regs, args);
+	which_clock = args[0];
+	buf = (struct old_timex32 __user *)args[1];
+
+	/* clock_adjtime() is allowed only for read. */
+	if (copy_from_user(&kbuf, buf, sizeof(struct old_timex32)))
+		return -EFAULT;
+	if (kbuf.modes != 0)
+		return -EPERM;
+	return ksys_clock_adjtime32(which_clock, buf);
+}
+#endif /* CONFIG_COMPAT */
+
+static struct syscall_whitelist whitelists[] = {
+	SYSCALL_WHITELIST(read_write_test),
+	SYSCALL_WHITELIST(android),
+	PERMISSIVE_SYSCALL_WHITELIST(android),
+	SYSCALL_WHITELIST(third_party),
+	PERMISSIVE_SYSCALL_WHITELIST(third_party),
+	SYSCALL_WHITELIST(complete),
+	PERMISSIVE_SYSCALL_WHITELIST(complete)
+};
+
+static int alt_syscall_apply_whitelist(const struct syscall_whitelist *wl,
+				       struct alt_sys_call_table *t)
+{
+	unsigned int i;
+	unsigned long *whitelist = kcalloc(BITS_TO_LONGS(t->size),
+					   sizeof(unsigned long), GFP_KERNEL);
+
+	if (!whitelist)
+		return -ENOMEM;
+
+	for (i = 0; i < wl->nr_whitelist; i++) {
+		unsigned int nr = wl->whitelist[i].nr;
+
+		if (nr >= t->size) {
+			kfree(whitelist);
+			return -EINVAL;
+		}
+		bitmap_set(whitelist, nr, 1);
+		if (wl->whitelist[i].alt)
+			t->table[nr] = wl->whitelist[i].alt;
+	}
+
+	for (i = 0; i < t->size; i++) {
+		if (!test_bit(i, whitelist)) {
+			t->table[i] = wl->permissive ?
+				(sys_call_ptr_t)warn_syscall :
+				(sys_call_ptr_t)block_syscall;
+		}
+	}
+
+	kfree(whitelist);
+	return 0;
+}
+
+#ifdef CONFIG_COMPAT
+static int
+alt_syscall_apply_compat_whitelist(const struct syscall_whitelist *wl,
+				   struct alt_sys_call_table *t)
+{
+	unsigned int i;
+	unsigned long *whitelist = kcalloc(BITS_TO_LONGS(t->compat_size),
+					   sizeof(unsigned long), GFP_KERNEL);
+
+	if (!whitelist)
+		return -ENOMEM;
+
+	for (i = 0; i < wl->nr_compat_whitelist; i++) {
+		unsigned int nr = wl->compat_whitelist[i].nr;
+
+		if (nr >= t->compat_size) {
+			kfree(whitelist);
+			return -EINVAL;
+		}
+		bitmap_set(whitelist, nr, 1);
+		if (wl->compat_whitelist[i].alt)
+			t->compat_table[nr] = wl->compat_whitelist[i].alt;
+	}
+
+	for (i = 0; i < t->compat_size; i++) {
+		if (!test_bit(i, whitelist)) {
+			t->compat_table[i] = wl->permissive ?
+				(sys_call_ptr_t)warn_compat_syscall :
+				(sys_call_ptr_t)block_syscall;
+		}
+	}
+
+	kfree(whitelist);
+	return 0;
+}
+#else
+static inline int
+alt_syscall_apply_compat_whitelist(const struct syscall_whitelist *wl,
+				   struct alt_sys_call_table *t)
+{
+	return 0;
+}
+#endif /* CONFIG_COMPAT */
+
+static int alt_syscall_init_one(const struct syscall_whitelist *wl)
+{
+	struct alt_sys_call_table *t;
+	int err;
+
+	t = kzalloc(sizeof(*t), GFP_KERNEL);
+	if (!t)
+		return -ENOMEM;
+	strncpy(t->name, wl->name, sizeof(t->name));
+
+	err = arch_dup_sys_call_table(t);
+	if (err)
+		return err;
+
+	err = alt_syscall_apply_whitelist(wl, t);
+	if (err)
+		return err;
+	err = alt_syscall_apply_compat_whitelist(wl, t);
+	if (err)
+		return err;
+
+	return register_alt_sys_call_table(t);
+}
+
+/*
+ * Register an alternate syscall table for each whitelist.  Note that the
+ * lack of a module_exit() is intentional - once a syscall table is registered
+ * it cannot be unregistered.
+ *
+ * TODO(abrestic) Support unregistering syscall tables?
+ */
+static int chromiumos_alt_syscall_init(void)
+{
+	unsigned int i;
+	int err;
+
+#ifdef CONFIG_SYSCTL
+	if (!register_sysctl_paths(chromiumos_sysctl_path,
+				   chromiumos_sysctl_table))
+		pr_warn("Failed to register sysctl\n");
+#endif
+
+	err = arch_dup_sys_call_table(&default_table);
+	if (err)
+		return err;
+
+	for (i = 0; i < ARRAY_SIZE(whitelists); i++) {
+		err = alt_syscall_init_one(&whitelists[i]);
+		if (err)
+			pr_warn("Failed to register syscall table %s: %d\n",
+				whitelists[i].name, err);
+	}
+
+	return 0;
+}
+module_init(chromiumos_alt_syscall_init);
diff -ruN a/security/chromiumos/alt-syscall.h b/security/chromiumos/alt-syscall.h
--- a/security/chromiumos/alt-syscall.h	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/alt-syscall.h	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,385 @@
+/*
+ * Linux Security Module for Chromium OS
+ *
+ * Copyright 2018 Google LLC. All Rights Reserved
+ *
+ * Authors:
+ *      Micah Morton <mortonm@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef ALT_SYSCALL_H
+#define ALT_SYSCALL_H
+
+/*
+ * NOTE: this file uses the 'static' keyword for variable and function
+ * definitions because alt-syscall.c is the only .c file that is expected to
+ * include this header. Definitions were pulled out from alt-syscall.c into
+ * this header and the *_whitelists.h headers for the sake of readability.
+ */
+
+static int allow_devmode_syscalls;
+
+#ifdef CONFIG_SYSCTL
+static int zero;
+static int one = 1;
+
+static struct ctl_path chromiumos_sysctl_path[] = {
+        { .procname = "kernel", },
+        { .procname = "chromiumos", },
+        { .procname = "alt_syscall", },
+        { }
+};
+
+static struct ctl_table chromiumos_sysctl_table[] = {
+        {
+                .procname       = "allow_devmode_syscalls",
+                .data           = &allow_devmode_syscalls,
+                .maxlen         = sizeof(int),
+                .mode           = 0644,
+                .proc_handler   = proc_dointvec_minmax,
+                .extra1         = &zero,
+                .extra2         = &one,
+        },
+        { }
+};
+#endif
+
+struct syscall_whitelist_entry {
+        unsigned int nr;
+        sys_call_ptr_t alt;
+};
+
+struct syscall_whitelist {
+        const char *name;
+        const struct syscall_whitelist_entry *whitelist;
+        unsigned int nr_whitelist;
+#ifdef CONFIG_COMPAT
+        const struct syscall_whitelist_entry *compat_whitelist;
+        unsigned int nr_compat_whitelist;
+#endif
+        bool permissive;
+};
+
+static struct alt_sys_call_table default_table;
+
+#define SYSCALL_ENTRY_ALT(name, func)                                   \
+        {                                                               \
+                .nr = __NR_ ## name,                                    \
+                .alt = (sys_call_ptr_t)func,                            \
+        }
+#define SYSCALL_ENTRY(name) SYSCALL_ENTRY_ALT(name, NULL)
+#define COMPAT_SYSCALL_ENTRY_ALT(name, func)                            \
+        {                                                               \
+                .nr = __NR_compat_ ## name,                             \
+                .alt = (sys_call_ptr_t)func,                            \
+        }
+#define COMPAT_SYSCALL_ENTRY(name) COMPAT_SYSCALL_ENTRY_ALT(name, NULL)
+
+/*
+ * If an alt_syscall table allows prctl(), override it to prevent a process
+ * from changing its syscall table.
+ */
+static asmlinkage long alt_sys_prctl(struct pt_regs *regs);
+
+#ifdef CONFIG_COMPAT
+#define SYSCALL_WHITELIST_COMPAT(x)                                     \
+        .compat_whitelist = x ## _compat_whitelist,                     \
+        .nr_compat_whitelist = ARRAY_SIZE(x ## _compat_whitelist),
+#else
+#define SYSCALL_WHITELIST_COMPAT(x)
+#endif
+
+#define SYSCALL_WHITELIST(x)                                            \
+        {                                                               \
+                .name = #x,                                             \
+                .whitelist = x ## _whitelist,                           \
+                .nr_whitelist = ARRAY_SIZE(x ## _whitelist),            \
+                SYSCALL_WHITELIST_COMPAT(x)                             \
+        }
+
+#define PERMISSIVE_SYSCALL_WHITELIST(x)                                 \
+        {                                                               \
+                .name = #x "_permissive",                               \
+                .permissive = true,                                     \
+                .whitelist = x ## _whitelist,                           \
+                .nr_whitelist = ARRAY_SIZE(x ## _whitelist),            \
+                SYSCALL_WHITELIST_COMPAT(x)                             \
+        }
+
+#ifdef CONFIG_COMPAT
+#ifdef CONFIG_X86_64
+#define __NR_compat_access      __NR_ia32_access
+#define __NR_compat_adjtimex    __NR_ia32_adjtimex
+#define __NR_compat_brk __NR_ia32_brk
+#define __NR_compat_capget      __NR_ia32_capget
+#define __NR_compat_capset      __NR_ia32_capset
+#define __NR_compat_chdir       __NR_ia32_chdir
+#define __NR_compat_chmod       __NR_ia32_chmod
+#define __NR_compat_clock_adjtime       __NR_ia32_clock_adjtime
+#define __NR_compat_clock_getres        __NR_ia32_clock_getres
+#define __NR_compat_clock_gettime       __NR_ia32_clock_gettime
+#define __NR_compat_clock_nanosleep     __NR_ia32_clock_nanosleep
+#define __NR_compat_clock_settime       __NR_ia32_clock_settime
+#define __NR_compat_clone       __NR_ia32_clone
+#define __NR_compat_close       __NR_ia32_close
+#define __NR_compat_creat       __NR_ia32_creat
+#define __NR_compat_dup __NR_ia32_dup
+#define __NR_compat_dup2        __NR_ia32_dup2
+#define __NR_compat_dup3        __NR_ia32_dup3
+#define __NR_compat_epoll_create        __NR_ia32_epoll_create
+#define __NR_compat_epoll_create1       __NR_ia32_epoll_create1
+#define __NR_compat_epoll_ctl   __NR_ia32_epoll_ctl
+#define __NR_compat_epoll_wait  __NR_ia32_epoll_wait
+#define __NR_compat_epoll_pwait __NR_ia32_epoll_pwait
+#define __NR_compat_eventfd     __NR_ia32_eventfd
+#define __NR_compat_eventfd2    __NR_ia32_eventfd2
+#define __NR_compat_execve      __NR_ia32_execve
+#define __NR_compat_exit        __NR_ia32_exit
+#define __NR_compat_exit_group  __NR_ia32_exit_group
+#define __NR_compat_faccessat   __NR_ia32_faccessat
+#define __NR_compat_fallocate   __NR_ia32_fallocate
+#define __NR_compat_fchdir      __NR_ia32_fchdir
+#define __NR_compat_fchmod      __NR_ia32_fchmod
+#define __NR_compat_fchmodat    __NR_ia32_fchmodat
+#define __NR_compat_fchown      __NR_ia32_fchown
+#define __NR_compat_fchownat    __NR_ia32_fchownat
+#define __NR_compat_fcntl       __NR_ia32_fcntl
+#define __NR_compat_fdatasync   __NR_ia32_fdatasync
+#define __NR_compat_fgetxattr   __NR_ia32_fgetxattr
+#define __NR_compat_flistxattr  __NR_ia32_flistxattr
+#define __NR_compat_flock       __NR_ia32_flock
+#define __NR_compat_fork        __NR_ia32_fork
+#define __NR_compat_fremovexattr        __NR_ia32_fremovexattr
+#define __NR_compat_fsetxattr   __NR_ia32_fsetxattr
+#define __NR_compat_fstat       __NR_ia32_fstat
+#define __NR_compat_fstatfs     __NR_ia32_fstatfs
+#define __NR_compat_fsync       __NR_ia32_fsync
+#define __NR_compat_ftruncate   __NR_ia32_ftruncate
+#define __NR_compat_futex       __NR_ia32_futex
+#define __NR_compat_futimesat   __NR_ia32_futimesat
+#define __NR_compat_getcpu      __NR_ia32_getcpu
+#define __NR_compat_getcwd      __NR_ia32_getcwd
+#define __NR_compat_getdents    __NR_ia32_getdents
+#define __NR_compat_getdents64  __NR_ia32_getdents64
+#define __NR_compat_getegid     __NR_ia32_getegid
+#define __NR_compat_geteuid     __NR_ia32_geteuid
+#define __NR_compat_getgid      __NR_ia32_getgid
+#define __NR_compat_getgroups32 __NR_ia32_getgroups32
+#define __NR_compat_getpgid     __NR_ia32_getpgid
+#define __NR_compat_getpgrp     __NR_ia32_getpgrp
+#define __NR_compat_getpid      __NR_ia32_getpid
+#define __NR_compat_getppid     __NR_ia32_getppid
+#define __NR_compat_getpriority __NR_ia32_getpriority
+#define __NR_compat_getrandom   __NR_ia32_getrandom
+#define __NR_compat_getresgid   __NR_ia32_getresgid
+#define __NR_compat_getresuid   __NR_ia32_getresuid
+#define __NR_compat_getrlimit   __NR_ia32_getrlimit
+#define __NR_compat_getrusage   __NR_ia32_getrusage
+#define __NR_compat_getsid      __NR_ia32_getsid
+#define __NR_compat_gettid      __NR_ia32_gettid
+#define __NR_compat_gettimeofday        __NR_ia32_gettimeofday
+#define __NR_compat_getuid      __NR_ia32_getuid
+#define __NR_compat_getxattr    __NR_ia32_getxattr
+#define __NR_compat_inotify_add_watch   __NR_ia32_inotify_add_watch
+#define __NR_compat_inotify_init        __NR_ia32_inotify_init
+#define __NR_compat_inotify_init1       __NR_ia32_inotify_init1
+#define __NR_compat_inotify_rm_watch    __NR_ia32_inotify_rm_watch
+#define __NR_compat_ioctl       __NR_ia32_ioctl
+#define __NR_compat_io_destroy  __NR_ia32_io_destroy
+#define __NR_compat_io_getevents      __NR_ia32_io_getevents
+#define __NR_compat_io_setup  __NR_ia32_io_setup
+#define __NR_compat_io_submit __NR_ia32_io_submit
+#define __NR_compat_ioprio_set  __NR_ia32_ioprio_set
+#define __NR_compat_kcmp        __NR_ia32_kcmp
+#define __NR_compat_keyctl      __NR_ia32_keyctl
+#define __NR_compat_kill        __NR_ia32_kill
+#define __NR_compat_lgetxattr   __NR_ia32_lgetxattr
+#define __NR_compat_link        __NR_ia32_link
+#define __NR_compat_linkat      __NR_ia32_linkat
+#define __NR_compat_listxattr   __NR_ia32_listxattr
+#define __NR_compat_llistxattr  __NR_ia32_llistxattr
+#define __NR_compat_lremovexattr        __NR_ia32_lremovexattr
+#define __NR_compat_lseek       __NR_ia32_lseek
+#define __NR_compat_lsetxattr   __NR_ia32_lsetxattr
+#define __NR_compat_lstat       __NR_ia32_lstat
+#define __NR_compat_madvise     __NR_ia32_madvise
+#define __NR_compat_memfd_create        __NR_ia32_memfd_create
+#define __NR_compat_mincore     __NR_ia32_mincore
+#define __NR_compat_mkdir       __NR_ia32_mkdir
+#define __NR_compat_mkdirat     __NR_ia32_mkdirat
+#define __NR_compat_mknod       __NR_ia32_mknod
+#define __NR_compat_mknodat     __NR_ia32_mknodat
+#define __NR_compat_mlock       __NR_ia32_mlock
+#define __NR_compat_munlock     __NR_ia32_munlock
+#define __NR_compat_mlockall    __NR_ia32_mlockall
+#define __NR_compat_munlockall  __NR_ia32_munlockall
+#define __NR_compat_modify_ldt  __NR_ia32_modify_ldt
+#define __NR_compat_mount       __NR_ia32_mount
+#define __NR_compat_mprotect    __NR_ia32_mprotect
+#define __NR_compat_mremap      __NR_ia32_mremap
+#define __NR_compat_msync       __NR_ia32_msync
+#define __NR_compat_munmap      __NR_ia32_munmap
+#define __NR_compat_name_to_handle_at   __NR_ia32_name_to_handle_at
+#define __NR_compat_nanosleep   __NR_ia32_nanosleep
+#define __NR_compat_open        __NR_ia32_open
+#define __NR_compat_open_by_handle_at   __NR_ia32_open_by_handle_at
+#define __NR_compat_openat      __NR_ia32_openat
+#define __NR_compat_perf_event_open     __NR_ia32_perf_event_open
+#define __NR_compat_personality __NR_ia32_personality
+#define __NR_compat_pipe        __NR_ia32_pipe
+#define __NR_compat_pipe2       __NR_ia32_pipe2
+#define __NR_compat_poll        __NR_ia32_poll
+#define __NR_compat_ppoll       __NR_ia32_ppoll
+#define __NR_compat_prctl       __NR_ia32_prctl
+#define __NR_compat_pread64     __NR_ia32_pread64
+#define __NR_compat_preadv      __NR_ia32_preadv
+#define __NR_compat_prlimit64   __NR_ia32_prlimit64
+#define __NR_compat_process_vm_readv    __NR_ia32_process_vm_readv
+#define __NR_compat_process_vm_writev   __NR_ia32_process_vm_writev
+#define __NR_compat_pselect6    __NR_ia32_pselect6
+#define __NR_compat_ptrace      __NR_ia32_ptrace
+#define __NR_compat_pwrite64    __NR_ia32_pwrite64
+#define __NR_compat_pwritev     __NR_ia32_pwritev
+#define __NR_compat_read        __NR_ia32_read
+#define __NR_compat_readahead   __NR_ia32_readahead
+#define __NR_compat_readv       __NR_ia32_readv
+#define __NR_compat_readlink    __NR_ia32_readlink
+#define __NR_compat_readlinkat  __NR_ia32_readlinkat
+#define __NR_compat_recvmmsg    __NR_ia32_recvmmsg
+#define __NR_compat_remap_file_pages    __NR_ia32_remap_file_pages
+#define __NR_compat_removexattr __NR_ia32_removexattr
+#define __NR_compat_rename      __NR_ia32_rename
+#define __NR_compat_renameat    __NR_ia32_renameat
+#define __NR_compat_restart_syscall     __NR_ia32_restart_syscall
+#define __NR_compat_rmdir       __NR_ia32_rmdir
+#define __NR_compat_rt_sigaction        __NR_ia32_rt_sigaction
+#define __NR_compat_rt_sigpending       __NR_ia32_rt_sigpending
+#define __NR_compat_rt_sigprocmask      __NR_ia32_rt_sigprocmask
+#define __NR_compat_rt_sigqueueinfo     __NR_ia32_rt_sigqueueinfo
+#define __NR_compat_rt_sigreturn        __NR_ia32_rt_sigreturn
+#define __NR_compat_rt_sigsuspend       __NR_ia32_rt_sigsuspend
+#define __NR_compat_rt_sigtimedwait     __NR_ia32_rt_sigtimedwait
+#define __NR_compat_rt_tgsigqueueinfo   __NR_ia32_rt_tgsigqueueinfo
+#define __NR_compat_sched_get_priority_max      __NR_ia32_sched_get_priority_max
+#define __NR_compat_sched_get_priority_min      __NR_ia32_sched_get_priority_min
+#define __NR_compat_sched_getaffinity   __NR_ia32_sched_getaffinity
+#define __NR_compat_sched_getparam      __NR_ia32_sched_getparam
+#define __NR_compat_sched_getscheduler  __NR_ia32_sched_getscheduler
+#define __NR_compat_sched_setaffinity   __NR_ia32_sched_setaffinity
+#define __NR_compat_sched_setparam      __NR_ia32_sched_setparam
+#define __NR_compat_sched_setscheduler  __NR_ia32_sched_setscheduler
+#define __NR_compat_sched_yield __NR_ia32_sched_yield
+#define __NR_compat_seccomp     __NR_ia32_seccomp
+#define __NR_compat_sendfile    __NR_ia32_sendfile
+#define __NR_compat_sendfile64  __NR_ia32_sendfile64
+#define __NR_compat_sendmmsg    __NR_ia32_sendmmsg
+#define __NR_compat_setdomainname       __NR_ia32_setdomainname
+#define __NR_compat_set_robust_list     __NR_ia32_set_robust_list
+#define __NR_compat_set_tid_address     __NR_ia32_set_tid_address
+#define __NR_compat_set_thread_area     __NR_ia32_set_thread_area
+#define __NR_compat_setgid      __NR_ia32_setgid
+#define __NR_compat_setgroups   __NR_ia32_setgroups
+#define __NR_compat_setitimer   __NR_ia32_setitimer
+#define __NR_compat_setns       __NR_ia32_setns
+#define __NR_compat_setpgid     __NR_ia32_setpgid
+#define __NR_compat_setpriority __NR_ia32_setpriority
+#define __NR_compat_setregid    __NR_ia32_setregid
+#define __NR_compat_setresgid   __NR_ia32_setresgid
+#define __NR_compat_setresuid   __NR_ia32_setresuid
+#define __NR_compat_setrlimit   __NR_ia32_setrlimit
+#define __NR_compat_setsid      __NR_ia32_setsid
+#define __NR_compat_settimeofday        __NR_ia32_settimeofday
+#define __NR_compat_setuid      __NR_ia32_setuid
+#define __NR_compat_setxattr    __NR_ia32_setxattr
+#define __NR_compat_signalfd4   __NR_ia32_signalfd4
+#define __NR_compat_sigaltstack __NR_ia32_sigaltstack
+#define __NR_compat_socketcall  __NR_ia32_socketcall
+#define __NR_compat_splice      __NR_ia32_splice
+#define __NR_compat_stat        __NR_ia32_stat
+#define __NR_compat_statfs      __NR_ia32_statfs
+#define __NR_compat_symlink     __NR_ia32_symlink
+#define __NR_compat_symlinkat   __NR_ia32_symlinkat
+#define __NR_compat_sync        __NR_ia32_sync
+#define __NR_compat_syncfs      __NR_ia32_syncfs
+#define __NR_compat_sync_file_range     __NR_ia32_sync_file_range
+#define __NR_compat_sysinfo     __NR_ia32_sysinfo
+#define __NR_compat_syslog      __NR_ia32_syslog
+#define __NR_compat_tee         __NR_ia32_tee
+#define __NR_compat_tgkill      __NR_ia32_tgkill
+#define __NR_compat_tkill       __NR_ia32_tkill
+#define __NR_compat_time        __NR_ia32_time
+#define __NR_compat_timer_create        __NR_ia32_timer_create
+#define __NR_compat_timer_delete        __NR_ia32_timer_delete
+#define __NR_compat_timer_getoverrun    __NR_ia32_timer_getoverrun
+#define __NR_compat_timer_gettime       __NR_ia32_timer_gettime
+#define __NR_compat_timer_settime       __NR_ia32_timer_settime
+#define __NR_compat_timerfd_create      __NR_ia32_timerfd_create
+#define __NR_compat_timerfd_gettime     __NR_ia32_timerfd_gettime
+#define __NR_compat_timerfd_settime     __NR_ia32_timerfd_settime
+#define __NR_compat_times               __NR_ia32_times
+#define __NR_compat_truncate    __NR_ia32_truncate
+#define __NR_compat_umask       __NR_ia32_umask
+#define __NR_compat_umount2     __NR_ia32_umount2
+#define __NR_compat_uname       __NR_ia32_uname
+#define __NR_compat_unlink      __NR_ia32_unlink
+#define __NR_compat_unlinkat    __NR_ia32_unlinkat
+#define __NR_compat_unshare     __NR_ia32_unshare
+#define __NR_compat_ustat       __NR_ia32_ustat
+#define __NR_compat_utimensat   __NR_ia32_utimensat
+#define __NR_compat_utimes      __NR_ia32_utimes
+#define __NR_compat_vfork       __NR_ia32_vfork
+#define __NR_compat_vmsplice    __NR_ia32_vmsplice
+#define __NR_compat_wait4       __NR_ia32_wait4
+#define __NR_compat_waitid      __NR_ia32_waitid
+#define __NR_compat_waitpid     __NR_ia32_waitpid
+#define __NR_compat_write       __NR_ia32_write
+#define __NR_compat_writev      __NR_ia32_writev
+#define __NR_compat_chown32     __NR_ia32_chown32
+#define __NR_compat_fadvise64   __NR_ia32_fadvise64
+#define __NR_compat_fadvise64_64        __NR_ia32_fadvise64_64
+#define __NR_compat_fchown32    __NR_ia32_fchown32
+#define __NR_compat_fcntl64     __NR_ia32_fcntl64
+#define __NR_compat_fstat64     __NR_ia32_fstat64
+#define __NR_compat_fstatat64   __NR_ia32_fstatat64
+#define __NR_compat_fstatfs64   __NR_ia32_fstatfs64
+#define __NR_compat_ftruncate64 __NR_ia32_ftruncate64
+#define __NR_compat_getegid32   __NR_ia32_getegid32
+#define __NR_compat_geteuid32   __NR_ia32_geteuid32
+#define __NR_compat_getgid32    __NR_ia32_getgid32
+#define __NR_compat_getresgid32 __NR_ia32_getresgid32
+#define __NR_compat_getresuid32 __NR_ia32_getresuid32
+#define __NR_compat_getuid32    __NR_ia32_getuid32
+#define __NR_compat_lchown32    __NR_ia32_lchown32
+#define __NR_compat_lstat64     __NR_ia32_lstat64
+#define __NR_compat_mmap2       __NR_ia32_mmap2
+#define __NR_compat__newselect  __NR_ia32__newselect
+#define __NR_compat__llseek     __NR_ia32__llseek
+#define __NR_compat_sigaction   __NR_ia32_sigaction
+#define __NR_compat_sigpending  __NR_ia32_sigpending
+#define __NR_compat_sigprocmask __NR_ia32_sigprocmask
+#define __NR_compat_sigreturn   __NR_ia32_sigreturn
+#define __NR_compat_sigsuspend  __NR_ia32_sigsuspend
+#define __NR_compat_setgid32    __NR_ia32_setgid32
+#define __NR_compat_setgroups32 __NR_ia32_setgroups32
+#define __NR_compat_setregid32  __NR_ia32_setregid32
+#define __NR_compat_setresgid32 __NR_ia32_setresgid32
+#define __NR_compat_setresuid32 __NR_ia32_setresuid32
+#define __NR_compat_setreuid32  __NR_ia32_setreuid32
+#define __NR_compat_setuid32    __NR_ia32_setuid32
+#define __NR_compat_stat64      __NR_ia32_stat64
+#define __NR_compat_statfs64    __NR_ia32_statfs64
+#define __NR_compat_truncate64  __NR_ia32_truncate64
+#define __NR_compat_ugetrlimit  __NR_ia32_ugetrlimit
+#endif
+#endif
+
+#endif /* ALT_SYSCALL_H */
diff -ruN a/security/chromiumos/android_whitelists.h b/security/chromiumos/android_whitelists.h
--- a/security/chromiumos/android_whitelists.h	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/android_whitelists.h	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,639 @@
+/*
+ * Linux Security Module for Chromium OS
+ *
+ * Copyright 2018 Google LLC. All Rights Reserved
+ *
+ * Authors:
+ *      Micah Morton <mortonm@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef ANDROID_WHITELISTS_H
+#define ANDROID_WHITELISTS_H
+
+/*
+ * NOTE: the purpose of this header is only to pull out the definition of this
+ * array from alt-syscall.c for the purposes of readability. It should not be
+ * included in other .c files.
+ */
+
+#include "alt-syscall.h"
+
+/*
+ * Syscall overrides for android.
+ */
+
+/*
+ * Reflect the priority adjustment done by android_setpriority.
+ * Note that the prio returned by getpriority has been offset by 20.
+ * (returns 40..1 instead of -20..19)
+ */
+static asmlinkage long android_getpriority(struct pt_regs *regs);
+/* Android does not get to call keyctl. */
+static asmlinkage long android_keyctl(struct pt_regs *regs);
+/* Make sure nothing sets a nice value more favorable than -10. */
+static asmlinkage long android_setpriority(struct pt_regs *regs);
+static asmlinkage long android_sched_setscheduler(struct pt_regs *regs);
+static asmlinkage long android_sched_setparam(struct pt_regs *regs);
+static asmlinkage long __maybe_unused android_socket(struct pt_regs *regs);
+static asmlinkage long android_perf_event_open(struct pt_regs *regs);
+static asmlinkage long android_adjtimex(struct pt_regs *regs);
+static asmlinkage long android_clock_adjtime(struct pt_regs *regs);
+static asmlinkage long android_getcpu(struct pt_regs *regs);
+#ifdef CONFIG_COMPAT
+static asmlinkage long android_compat_adjtimex(struct pt_regs *regs);
+static asmlinkage long android_compat_clock_adjtime(struct pt_regs *regs);
+#endif /* CONFIG_COMPAT */
+
+static struct syscall_whitelist_entry android_whitelist[] = {
+	SYSCALL_ENTRY(accept),
+	SYSCALL_ENTRY(accept4),
+	SYSCALL_ENTRY_ALT(adjtimex, android_adjtimex),
+	SYSCALL_ENTRY(bind),
+	SYSCALL_ENTRY(bpf),
+	SYSCALL_ENTRY(brk),
+	SYSCALL_ENTRY(capget),
+	SYSCALL_ENTRY(capset),
+	SYSCALL_ENTRY(chdir),
+	SYSCALL_ENTRY_ALT(clock_adjtime, android_clock_adjtime),
+	SYSCALL_ENTRY(clock_getres),
+	SYSCALL_ENTRY(clock_gettime),
+	SYSCALL_ENTRY(clock_nanosleep),
+	SYSCALL_ENTRY(clock_settime),
+	SYSCALL_ENTRY(clone),
+	SYSCALL_ENTRY(close),
+	SYSCALL_ENTRY(connect),
+	SYSCALL_ENTRY(dup),
+	SYSCALL_ENTRY(dup3),
+	SYSCALL_ENTRY(epoll_create1),
+	SYSCALL_ENTRY(epoll_ctl),
+	SYSCALL_ENTRY(epoll_pwait),
+	SYSCALL_ENTRY(eventfd2),
+	SYSCALL_ENTRY(execve),
+	SYSCALL_ENTRY(exit),
+	SYSCALL_ENTRY(exit_group),
+	SYSCALL_ENTRY(faccessat),
+	SYSCALL_ENTRY(fallocate),
+	SYSCALL_ENTRY(fchdir),
+	SYSCALL_ENTRY(fchmod),
+	SYSCALL_ENTRY(fchmodat),
+	SYSCALL_ENTRY(fchownat),
+	SYSCALL_ENTRY(fcntl),
+	SYSCALL_ENTRY(fdatasync),
+	SYSCALL_ENTRY(fgetxattr),
+	SYSCALL_ENTRY(flistxattr),
+	SYSCALL_ENTRY(flock),
+	SYSCALL_ENTRY(fremovexattr),
+	SYSCALL_ENTRY(fsetxattr),
+	SYSCALL_ENTRY(fstat),
+	SYSCALL_ENTRY(fstatfs),
+	SYSCALL_ENTRY(fsync),
+	SYSCALL_ENTRY(ftruncate),
+	SYSCALL_ENTRY(futex),
+	SYSCALL_ENTRY_ALT(getcpu, android_getcpu),
+	SYSCALL_ENTRY(getcwd),
+	SYSCALL_ENTRY(getdents64),
+	SYSCALL_ENTRY(getpeername),
+	SYSCALL_ENTRY(getpgid),
+	SYSCALL_ENTRY(getpid),
+	SYSCALL_ENTRY(getppid),
+	SYSCALL_ENTRY_ALT(getpriority, android_getpriority),
+        SYSCALL_ENTRY(getrandom),
+	SYSCALL_ENTRY(getrlimit),
+	SYSCALL_ENTRY(getrusage),
+	SYSCALL_ENTRY(getsid),
+	SYSCALL_ENTRY(getsockname),
+	SYSCALL_ENTRY(getsockopt),
+	SYSCALL_ENTRY(gettid),
+	SYSCALL_ENTRY(gettimeofday),
+	SYSCALL_ENTRY(getxattr),
+	SYSCALL_ENTRY(inotify_add_watch),
+	SYSCALL_ENTRY(inotify_init1),
+	SYSCALL_ENTRY(inotify_rm_watch),
+	SYSCALL_ENTRY(ioctl),
+        SYSCALL_ENTRY(io_destroy),
+        SYSCALL_ENTRY(io_getevents),
+        SYSCALL_ENTRY(io_setup),
+        SYSCALL_ENTRY(io_submit),
+	SYSCALL_ENTRY(ioprio_set),
+        SYSCALL_ENTRY_ALT(keyctl, android_keyctl),
+	SYSCALL_ENTRY(kcmp),
+	SYSCALL_ENTRY(kill),
+	SYSCALL_ENTRY(lgetxattr),
+	SYSCALL_ENTRY(linkat),
+	SYSCALL_ENTRY(listxattr),
+	SYSCALL_ENTRY(listen),
+	SYSCALL_ENTRY(llistxattr),
+	SYSCALL_ENTRY(lremovexattr),
+	SYSCALL_ENTRY(lseek),
+	SYSCALL_ENTRY(lsetxattr),
+	SYSCALL_ENTRY(madvise),
+        SYSCALL_ENTRY(memfd_create),
+	SYSCALL_ENTRY(mincore),
+	SYSCALL_ENTRY(mkdirat),
+	SYSCALL_ENTRY(mknodat),
+	SYSCALL_ENTRY(mlock),
+	SYSCALL_ENTRY(mlockall),
+	SYSCALL_ENTRY(munlock),
+	SYSCALL_ENTRY(munlockall),
+	SYSCALL_ENTRY(mount),
+	SYSCALL_ENTRY(mprotect),
+	SYSCALL_ENTRY(mremap),
+	SYSCALL_ENTRY(msync),
+	SYSCALL_ENTRY(munmap),
+	SYSCALL_ENTRY(name_to_handle_at),
+	SYSCALL_ENTRY(nanosleep),
+	SYSCALL_ENTRY(open_by_handle_at),
+	SYSCALL_ENTRY(openat),
+	SYSCALL_ENTRY_ALT(perf_event_open, android_perf_event_open),
+	SYSCALL_ENTRY(personality),
+	SYSCALL_ENTRY(pipe2),
+	SYSCALL_ENTRY(ppoll),
+	SYSCALL_ENTRY_ALT(prctl, alt_sys_prctl),
+	SYSCALL_ENTRY(pread64),
+	SYSCALL_ENTRY(preadv),
+	SYSCALL_ENTRY(prlimit64),
+	SYSCALL_ENTRY(process_vm_readv),
+	SYSCALL_ENTRY(process_vm_writev),
+	SYSCALL_ENTRY(pselect6),
+	SYSCALL_ENTRY(ptrace),
+	SYSCALL_ENTRY(pwrite64),
+	SYSCALL_ENTRY(pwritev),
+	SYSCALL_ENTRY(read),
+	SYSCALL_ENTRY(readahead),
+	SYSCALL_ENTRY(readv),
+	SYSCALL_ENTRY(readlinkat),
+	SYSCALL_ENTRY(recvfrom),
+	SYSCALL_ENTRY(recvmmsg),
+	SYSCALL_ENTRY(recvmsg),
+	SYSCALL_ENTRY(remap_file_pages),
+	SYSCALL_ENTRY(removexattr),
+	SYSCALL_ENTRY(renameat),
+	SYSCALL_ENTRY(restart_syscall),
+	SYSCALL_ENTRY(rt_sigaction),
+	SYSCALL_ENTRY(rt_sigpending),
+	SYSCALL_ENTRY(rt_sigprocmask),
+	SYSCALL_ENTRY(rt_sigqueueinfo),
+	SYSCALL_ENTRY(rt_sigreturn),
+	SYSCALL_ENTRY(rt_sigsuspend),
+	SYSCALL_ENTRY(rt_sigtimedwait),
+	SYSCALL_ENTRY(rt_tgsigqueueinfo),
+	SYSCALL_ENTRY(sched_get_priority_max),
+	SYSCALL_ENTRY(sched_get_priority_min),
+	SYSCALL_ENTRY(sched_getaffinity),
+	SYSCALL_ENTRY(sched_getparam),
+	SYSCALL_ENTRY(sched_getscheduler),
+	SYSCALL_ENTRY(sched_setaffinity),
+        SYSCALL_ENTRY_ALT(sched_setparam, android_sched_setparam),
+	SYSCALL_ENTRY_ALT(sched_setscheduler, android_sched_setscheduler),
+	SYSCALL_ENTRY(sched_yield),
+	SYSCALL_ENTRY(seccomp),
+	SYSCALL_ENTRY(sendfile),
+	SYSCALL_ENTRY(sendmmsg),
+	SYSCALL_ENTRY(sendmsg),
+	SYSCALL_ENTRY(sendto),
+        SYSCALL_ENTRY(setdomainname),
+	SYSCALL_ENTRY(set_robust_list),
+	SYSCALL_ENTRY(set_tid_address),
+	SYSCALL_ENTRY(setitimer),
+	SYSCALL_ENTRY(setns),
+	SYSCALL_ENTRY(setpgid),
+	SYSCALL_ENTRY_ALT(setpriority, android_setpriority),
+	SYSCALL_ENTRY(setrlimit),
+	SYSCALL_ENTRY(setsid),
+	SYSCALL_ENTRY(setsockopt),
+	SYSCALL_ENTRY(settimeofday),
+	SYSCALL_ENTRY(setxattr),
+	SYSCALL_ENTRY(shutdown),
+	SYSCALL_ENTRY(signalfd4),
+	SYSCALL_ENTRY(sigaltstack),
+	SYSCALL_ENTRY_ALT(socket, android_socket),
+	SYSCALL_ENTRY(socketpair),
+	SYSCALL_ENTRY(splice),
+	SYSCALL_ENTRY(statfs),
+	SYSCALL_ENTRY(symlinkat),
+        SYSCALL_ENTRY(sync),
+        SYSCALL_ENTRY(syncfs),
+	SYSCALL_ENTRY(sysinfo),
+	SYSCALL_ENTRY(syslog),
+	SYSCALL_ENTRY(tee),
+	SYSCALL_ENTRY(tgkill),
+	SYSCALL_ENTRY(tkill),
+	SYSCALL_ENTRY(timer_create),
+	SYSCALL_ENTRY(timer_delete),
+	SYSCALL_ENTRY(timer_gettime),
+	SYSCALL_ENTRY(timer_getoverrun),
+	SYSCALL_ENTRY(timer_settime),
+	SYSCALL_ENTRY(timerfd_create),
+	SYSCALL_ENTRY(timerfd_gettime),
+	SYSCALL_ENTRY(timerfd_settime),
+	SYSCALL_ENTRY(times),
+	SYSCALL_ENTRY(truncate),
+	SYSCALL_ENTRY(umask),
+	SYSCALL_ENTRY(umount2),
+	SYSCALL_ENTRY(uname),
+	SYSCALL_ENTRY(unlinkat),
+	SYSCALL_ENTRY(unshare),
+	SYSCALL_ENTRY(utimensat),
+	SYSCALL_ENTRY(vmsplice),
+	SYSCALL_ENTRY(wait4),
+	SYSCALL_ENTRY(waitid),
+	SYSCALL_ENTRY(write),
+	SYSCALL_ENTRY(writev),
+
+	/*
+	 * Deprecated syscalls which are not wired up on new architectures
+	 * such as ARM64.
+	 */
+#ifndef CONFIG_ARM64
+	SYSCALL_ENTRY(access),
+	SYSCALL_ENTRY(chmod),
+	SYSCALL_ENTRY(open),
+	SYSCALL_ENTRY(creat),
+	SYSCALL_ENTRY(dup2),
+	SYSCALL_ENTRY(epoll_create),
+	SYSCALL_ENTRY(epoll_wait),
+	SYSCALL_ENTRY(eventfd),
+	SYSCALL_ENTRY(fork),
+	SYSCALL_ENTRY(futimesat),
+	SYSCALL_ENTRY(getdents),
+	SYSCALL_ENTRY(getpgrp),
+	SYSCALL_ENTRY(inotify_init),
+	SYSCALL_ENTRY(link),
+	SYSCALL_ENTRY(lstat),
+	SYSCALL_ENTRY(mkdir),
+	SYSCALL_ENTRY(mknod),
+	SYSCALL_ENTRY(pipe),
+	SYSCALL_ENTRY(poll),
+	SYSCALL_ENTRY(readlink),
+	SYSCALL_ENTRY(rename),
+	SYSCALL_ENTRY(rmdir),
+	SYSCALL_ENTRY(stat),
+	SYSCALL_ENTRY(symlink),
+	SYSCALL_ENTRY(time),
+	SYSCALL_ENTRY(unlink),
+	SYSCALL_ENTRY(ustat),
+	SYSCALL_ENTRY(utimes),
+	SYSCALL_ENTRY(vfork),
+#endif
+
+	SYSCALL_ENTRY(fadvise64),
+	SYSCALL_ENTRY(sync_file_range),
+
+	/* 64-bit only syscalls. */
+	SYSCALL_ENTRY(fchown),
+	SYSCALL_ENTRY(getegid),
+	SYSCALL_ENTRY(geteuid),
+	SYSCALL_ENTRY(getgid),
+	SYSCALL_ENTRY(getgroups),
+	SYSCALL_ENTRY(getresgid),
+	SYSCALL_ENTRY(getresuid),
+	SYSCALL_ENTRY(getuid),
+	SYSCALL_ENTRY(newfstatat),
+	SYSCALL_ENTRY(mmap),
+	SYSCALL_ENTRY(setgid),
+	SYSCALL_ENTRY(setgroups),
+	SYSCALL_ENTRY(setregid),
+	SYSCALL_ENTRY(setresgid),
+	SYSCALL_ENTRY(setresuid),
+	SYSCALL_ENTRY(setreuid),
+	SYSCALL_ENTRY(setuid),
+	/*
+	 * chown(2), lchown(2), and select(2) are deprecated and not wired up
+	 * on ARM64.
+	 */
+#ifndef CONFIG_ARM64
+	SYSCALL_ENTRY(chown),
+	SYSCALL_ENTRY(lchown),
+	SYSCALL_ENTRY(select),
+#endif
+
+	/* X86_64-specific syscalls. */
+#ifdef CONFIG_X86_64
+	SYSCALL_ENTRY(arch_prctl),
+	SYSCALL_ENTRY(modify_ldt),
+	SYSCALL_ENTRY(set_thread_area),
+#endif
+
+}; /* end android_whitelist */
+
+#ifdef CONFIG_COMPAT
+static struct syscall_whitelist_entry android_compat_whitelist[] = {
+	COMPAT_SYSCALL_ENTRY(access),
+	COMPAT_SYSCALL_ENTRY_ALT(adjtimex, android_compat_adjtimex),
+	COMPAT_SYSCALL_ENTRY(brk),
+	COMPAT_SYSCALL_ENTRY(capget),
+	COMPAT_SYSCALL_ENTRY(capset),
+	COMPAT_SYSCALL_ENTRY(chdir),
+	COMPAT_SYSCALL_ENTRY(chmod),
+	COMPAT_SYSCALL_ENTRY_ALT(clock_adjtime, android_compat_clock_adjtime),
+	COMPAT_SYSCALL_ENTRY(clock_getres),
+	COMPAT_SYSCALL_ENTRY(clock_gettime),
+	COMPAT_SYSCALL_ENTRY(clock_nanosleep),
+	COMPAT_SYSCALL_ENTRY(clock_settime),
+	COMPAT_SYSCALL_ENTRY(clone),
+	COMPAT_SYSCALL_ENTRY(close),
+	COMPAT_SYSCALL_ENTRY(creat),
+	COMPAT_SYSCALL_ENTRY(dup),
+	COMPAT_SYSCALL_ENTRY(dup2),
+	COMPAT_SYSCALL_ENTRY(dup3),
+	COMPAT_SYSCALL_ENTRY(epoll_create),
+	COMPAT_SYSCALL_ENTRY(epoll_create1),
+	COMPAT_SYSCALL_ENTRY(epoll_ctl),
+	COMPAT_SYSCALL_ENTRY(epoll_wait),
+	COMPAT_SYSCALL_ENTRY(epoll_pwait),
+	COMPAT_SYSCALL_ENTRY(eventfd),
+	COMPAT_SYSCALL_ENTRY(eventfd2),
+	COMPAT_SYSCALL_ENTRY(execve),
+	COMPAT_SYSCALL_ENTRY(exit),
+	COMPAT_SYSCALL_ENTRY(exit_group),
+	COMPAT_SYSCALL_ENTRY(faccessat),
+	COMPAT_SYSCALL_ENTRY(fallocate),
+	COMPAT_SYSCALL_ENTRY(fchdir),
+	COMPAT_SYSCALL_ENTRY(fchmod),
+	COMPAT_SYSCALL_ENTRY(fchmodat),
+	COMPAT_SYSCALL_ENTRY(fchownat),
+	COMPAT_SYSCALL_ENTRY(fcntl),
+	COMPAT_SYSCALL_ENTRY(fdatasync),
+	COMPAT_SYSCALL_ENTRY(fgetxattr),
+	COMPAT_SYSCALL_ENTRY(flistxattr),
+	COMPAT_SYSCALL_ENTRY(flock),
+	COMPAT_SYSCALL_ENTRY(fork),
+	COMPAT_SYSCALL_ENTRY(fremovexattr),
+	COMPAT_SYSCALL_ENTRY(fsetxattr),
+	COMPAT_SYSCALL_ENTRY(fstat),
+	COMPAT_SYSCALL_ENTRY(fstatfs),
+	COMPAT_SYSCALL_ENTRY(fsync),
+	COMPAT_SYSCALL_ENTRY(ftruncate),
+	COMPAT_SYSCALL_ENTRY(futex),
+	COMPAT_SYSCALL_ENTRY(futimesat),
+	COMPAT_SYSCALL_ENTRY_ALT(getcpu, android_getcpu),
+	COMPAT_SYSCALL_ENTRY(getcwd),
+	COMPAT_SYSCALL_ENTRY(getdents),
+	COMPAT_SYSCALL_ENTRY(getdents64),
+	COMPAT_SYSCALL_ENTRY(getpgid),
+	COMPAT_SYSCALL_ENTRY(getpgrp),
+	COMPAT_SYSCALL_ENTRY(getpid),
+	COMPAT_SYSCALL_ENTRY(getppid),
+	COMPAT_SYSCALL_ENTRY_ALT(getpriority, android_getpriority),
+        COMPAT_SYSCALL_ENTRY(getrandom),
+	COMPAT_SYSCALL_ENTRY(getrusage),
+	COMPAT_SYSCALL_ENTRY(getsid),
+	COMPAT_SYSCALL_ENTRY(gettid),
+	COMPAT_SYSCALL_ENTRY(gettimeofday),
+	COMPAT_SYSCALL_ENTRY(getxattr),
+	COMPAT_SYSCALL_ENTRY(inotify_add_watch),
+	COMPAT_SYSCALL_ENTRY(inotify_init),
+	COMPAT_SYSCALL_ENTRY(inotify_init1),
+	COMPAT_SYSCALL_ENTRY(inotify_rm_watch),
+	COMPAT_SYSCALL_ENTRY(ioctl),
+        COMPAT_SYSCALL_ENTRY(io_destroy),
+        COMPAT_SYSCALL_ENTRY(io_getevents),
+        COMPAT_SYSCALL_ENTRY(io_setup),
+        COMPAT_SYSCALL_ENTRY(io_submit),
+	COMPAT_SYSCALL_ENTRY(ioprio_set),
+        COMPAT_SYSCALL_ENTRY_ALT(keyctl, android_keyctl),
+	COMPAT_SYSCALL_ENTRY(kcmp),
+	COMPAT_SYSCALL_ENTRY(kill),
+	COMPAT_SYSCALL_ENTRY(lgetxattr),
+	COMPAT_SYSCALL_ENTRY(link),
+	COMPAT_SYSCALL_ENTRY(linkat),
+	COMPAT_SYSCALL_ENTRY(listxattr),
+	COMPAT_SYSCALL_ENTRY(llistxattr),
+	COMPAT_SYSCALL_ENTRY(lremovexattr),
+	COMPAT_SYSCALL_ENTRY(lseek),
+	COMPAT_SYSCALL_ENTRY(lsetxattr),
+	COMPAT_SYSCALL_ENTRY(lstat),
+	COMPAT_SYSCALL_ENTRY(madvise),
+        COMPAT_SYSCALL_ENTRY(memfd_create),
+	COMPAT_SYSCALL_ENTRY(mincore),
+	COMPAT_SYSCALL_ENTRY(mkdir),
+	COMPAT_SYSCALL_ENTRY(mkdirat),
+	COMPAT_SYSCALL_ENTRY(mknod),
+	COMPAT_SYSCALL_ENTRY(mknodat),
+	COMPAT_SYSCALL_ENTRY(mlock),
+	COMPAT_SYSCALL_ENTRY(mlockall),
+	COMPAT_SYSCALL_ENTRY(munlock),
+	COMPAT_SYSCALL_ENTRY(munlockall),
+	COMPAT_SYSCALL_ENTRY(mount),
+	COMPAT_SYSCALL_ENTRY(mprotect),
+	COMPAT_SYSCALL_ENTRY(mremap),
+	COMPAT_SYSCALL_ENTRY(msync),
+	COMPAT_SYSCALL_ENTRY(munmap),
+	COMPAT_SYSCALL_ENTRY(name_to_handle_at),
+	COMPAT_SYSCALL_ENTRY(nanosleep),
+	COMPAT_SYSCALL_ENTRY(open),
+	COMPAT_SYSCALL_ENTRY(open_by_handle_at),
+	COMPAT_SYSCALL_ENTRY(openat),
+	COMPAT_SYSCALL_ENTRY_ALT(perf_event_open, android_perf_event_open),
+	COMPAT_SYSCALL_ENTRY(personality),
+	COMPAT_SYSCALL_ENTRY(pipe),
+	COMPAT_SYSCALL_ENTRY(pipe2),
+	COMPAT_SYSCALL_ENTRY(poll),
+	COMPAT_SYSCALL_ENTRY(ppoll),
+	COMPAT_SYSCALL_ENTRY_ALT(prctl, alt_sys_prctl),
+	COMPAT_SYSCALL_ENTRY(pread64),
+	COMPAT_SYSCALL_ENTRY(preadv),
+	COMPAT_SYSCALL_ENTRY(prlimit64),
+	COMPAT_SYSCALL_ENTRY(process_vm_readv),
+	COMPAT_SYSCALL_ENTRY(process_vm_writev),
+	COMPAT_SYSCALL_ENTRY(pselect6),
+	COMPAT_SYSCALL_ENTRY(ptrace),
+	COMPAT_SYSCALL_ENTRY(pwrite64),
+	COMPAT_SYSCALL_ENTRY(pwritev),
+	COMPAT_SYSCALL_ENTRY(read),
+	COMPAT_SYSCALL_ENTRY(readahead),
+	COMPAT_SYSCALL_ENTRY(readv),
+	COMPAT_SYSCALL_ENTRY(readlink),
+	COMPAT_SYSCALL_ENTRY(readlinkat),
+	COMPAT_SYSCALL_ENTRY(recvmmsg),
+	COMPAT_SYSCALL_ENTRY(remap_file_pages),
+	COMPAT_SYSCALL_ENTRY(removexattr),
+	COMPAT_SYSCALL_ENTRY(rename),
+	COMPAT_SYSCALL_ENTRY(renameat),
+	COMPAT_SYSCALL_ENTRY(restart_syscall),
+	COMPAT_SYSCALL_ENTRY(rmdir),
+	COMPAT_SYSCALL_ENTRY(rt_sigaction),
+	COMPAT_SYSCALL_ENTRY(rt_sigpending),
+	COMPAT_SYSCALL_ENTRY(rt_sigprocmask),
+	COMPAT_SYSCALL_ENTRY(rt_sigqueueinfo),
+	COMPAT_SYSCALL_ENTRY(rt_sigreturn),
+	COMPAT_SYSCALL_ENTRY(rt_sigsuspend),
+	COMPAT_SYSCALL_ENTRY(rt_sigtimedwait),
+	COMPAT_SYSCALL_ENTRY(rt_tgsigqueueinfo),
+	COMPAT_SYSCALL_ENTRY(sched_get_priority_max),
+	COMPAT_SYSCALL_ENTRY(sched_get_priority_min),
+	COMPAT_SYSCALL_ENTRY(sched_getaffinity),
+	COMPAT_SYSCALL_ENTRY(sched_getparam),
+	COMPAT_SYSCALL_ENTRY(sched_getscheduler),
+	COMPAT_SYSCALL_ENTRY(sched_setaffinity),
+        COMPAT_SYSCALL_ENTRY_ALT(sched_setparam,
+                                 android_sched_setparam),
+	COMPAT_SYSCALL_ENTRY_ALT(sched_setscheduler,
+				 android_sched_setscheduler),
+	COMPAT_SYSCALL_ENTRY(sched_yield),
+	COMPAT_SYSCALL_ENTRY(seccomp),
+	COMPAT_SYSCALL_ENTRY(sendfile),
+	COMPAT_SYSCALL_ENTRY(sendfile64),
+	COMPAT_SYSCALL_ENTRY(sendmmsg),
+        COMPAT_SYSCALL_ENTRY(setdomainname),
+	COMPAT_SYSCALL_ENTRY(set_robust_list),
+	COMPAT_SYSCALL_ENTRY(set_tid_address),
+	COMPAT_SYSCALL_ENTRY(setitimer),
+	COMPAT_SYSCALL_ENTRY(setns),
+	COMPAT_SYSCALL_ENTRY(setpgid),
+	COMPAT_SYSCALL_ENTRY_ALT(setpriority, android_setpriority),
+	COMPAT_SYSCALL_ENTRY(setrlimit),
+	COMPAT_SYSCALL_ENTRY(setsid),
+	COMPAT_SYSCALL_ENTRY(settimeofday),
+	COMPAT_SYSCALL_ENTRY(setxattr),
+	COMPAT_SYSCALL_ENTRY(signalfd4),
+	COMPAT_SYSCALL_ENTRY(sigaltstack),
+	COMPAT_SYSCALL_ENTRY(splice),
+	COMPAT_SYSCALL_ENTRY(stat),
+	COMPAT_SYSCALL_ENTRY(statfs),
+	COMPAT_SYSCALL_ENTRY(symlink),
+	COMPAT_SYSCALL_ENTRY(symlinkat),
+        COMPAT_SYSCALL_ENTRY(sync),
+        COMPAT_SYSCALL_ENTRY(syncfs),
+	COMPAT_SYSCALL_ENTRY(sysinfo),
+	COMPAT_SYSCALL_ENTRY(syslog),
+	COMPAT_SYSCALL_ENTRY(tgkill),
+	COMPAT_SYSCALL_ENTRY(tee),
+	COMPAT_SYSCALL_ENTRY(tkill),
+	COMPAT_SYSCALL_ENTRY(timer_create),
+	COMPAT_SYSCALL_ENTRY(timer_delete),
+	COMPAT_SYSCALL_ENTRY(timer_gettime),
+	COMPAT_SYSCALL_ENTRY(timer_getoverrun),
+	COMPAT_SYSCALL_ENTRY(timer_settime),
+	COMPAT_SYSCALL_ENTRY(timerfd_create),
+	COMPAT_SYSCALL_ENTRY(timerfd_gettime),
+	COMPAT_SYSCALL_ENTRY(timerfd_settime),
+	COMPAT_SYSCALL_ENTRY(times),
+	COMPAT_SYSCALL_ENTRY(truncate),
+	COMPAT_SYSCALL_ENTRY(umask),
+	COMPAT_SYSCALL_ENTRY(umount2),
+	COMPAT_SYSCALL_ENTRY(uname),
+	COMPAT_SYSCALL_ENTRY(unlink),
+	COMPAT_SYSCALL_ENTRY(unlinkat),
+	COMPAT_SYSCALL_ENTRY(unshare),
+	COMPAT_SYSCALL_ENTRY(ustat),
+	COMPAT_SYSCALL_ENTRY(utimensat),
+	COMPAT_SYSCALL_ENTRY(utimes),
+	COMPAT_SYSCALL_ENTRY(vfork),
+	COMPAT_SYSCALL_ENTRY(vmsplice),
+	COMPAT_SYSCALL_ENTRY(wait4),
+	COMPAT_SYSCALL_ENTRY(waitid),
+	COMPAT_SYSCALL_ENTRY(write),
+	COMPAT_SYSCALL_ENTRY(writev),
+	COMPAT_SYSCALL_ENTRY(chown32),
+	COMPAT_SYSCALL_ENTRY(fchown32),
+	COMPAT_SYSCALL_ENTRY(fcntl64),
+	COMPAT_SYSCALL_ENTRY(fstat64),
+	COMPAT_SYSCALL_ENTRY(fstatat64),
+	COMPAT_SYSCALL_ENTRY(fstatfs64),
+	COMPAT_SYSCALL_ENTRY(ftruncate64),
+	COMPAT_SYSCALL_ENTRY(getegid),
+	COMPAT_SYSCALL_ENTRY(getegid32),
+	COMPAT_SYSCALL_ENTRY(geteuid),
+	COMPAT_SYSCALL_ENTRY(geteuid32),
+	COMPAT_SYSCALL_ENTRY(getgid),
+	COMPAT_SYSCALL_ENTRY(getgid32),
+	COMPAT_SYSCALL_ENTRY(getgroups32),
+	COMPAT_SYSCALL_ENTRY(getresgid32),
+	COMPAT_SYSCALL_ENTRY(getresuid32),
+	COMPAT_SYSCALL_ENTRY(getuid),
+	COMPAT_SYSCALL_ENTRY(getuid32),
+	COMPAT_SYSCALL_ENTRY(lchown32),
+	COMPAT_SYSCALL_ENTRY(lstat64),
+	COMPAT_SYSCALL_ENTRY(mmap2),
+	COMPAT_SYSCALL_ENTRY(_newselect),
+	COMPAT_SYSCALL_ENTRY(_llseek),
+	COMPAT_SYSCALL_ENTRY(sigaction),
+	COMPAT_SYSCALL_ENTRY(sigpending),
+	COMPAT_SYSCALL_ENTRY(sigprocmask),
+	COMPAT_SYSCALL_ENTRY(sigreturn),
+	COMPAT_SYSCALL_ENTRY(sigsuspend),
+	COMPAT_SYSCALL_ENTRY(setgid32),
+	COMPAT_SYSCALL_ENTRY(setgroups32),
+	COMPAT_SYSCALL_ENTRY(setregid32),
+	COMPAT_SYSCALL_ENTRY(setresgid32),
+	COMPAT_SYSCALL_ENTRY(setresuid32),
+	COMPAT_SYSCALL_ENTRY(setreuid32),
+	COMPAT_SYSCALL_ENTRY(setuid32),
+	COMPAT_SYSCALL_ENTRY(stat64),
+	COMPAT_SYSCALL_ENTRY(statfs64),
+	COMPAT_SYSCALL_ENTRY(truncate64),
+	COMPAT_SYSCALL_ENTRY(ugetrlimit),
+
+#ifdef CONFIG_X86_64
+	/*
+	 * waitpid(2) is deprecated on most architectures, but still exists
+	 * on IA32.
+	 */
+	COMPAT_SYSCALL_ENTRY(waitpid),
+
+	/* IA32 uses the common socketcall(2) entrypoint for socket calls. */
+	COMPAT_SYSCALL_ENTRY(socketcall),
+#endif
+
+#ifdef CONFIG_ARM64
+	COMPAT_SYSCALL_ENTRY(accept),
+	COMPAT_SYSCALL_ENTRY(accept4),
+	COMPAT_SYSCALL_ENTRY(bind),
+	COMPAT_SYSCALL_ENTRY(connect),
+	COMPAT_SYSCALL_ENTRY(getpeername),
+	COMPAT_SYSCALL_ENTRY(getsockname),
+	COMPAT_SYSCALL_ENTRY(getsockopt),
+	COMPAT_SYSCALL_ENTRY(listen),
+	COMPAT_SYSCALL_ENTRY(recvfrom),
+	COMPAT_SYSCALL_ENTRY(recvmsg),
+	COMPAT_SYSCALL_ENTRY(sendmsg),
+	COMPAT_SYSCALL_ENTRY(sendto),
+	COMPAT_SYSCALL_ENTRY(setsockopt),
+	COMPAT_SYSCALL_ENTRY(shutdown),
+	COMPAT_SYSCALL_ENTRY(socket),
+	COMPAT_SYSCALL_ENTRY(socketpair),
+	COMPAT_SYSCALL_ENTRY(recv),
+	COMPAT_SYSCALL_ENTRY(send),
+#endif
+
+	/*
+	 * posix_fadvise(2) and sync_file_range(2) have ARM-specific wrappers
+	 * to deal with register alignment.
+	 */
+#ifdef CONFIG_ARM64
+	COMPAT_SYSCALL_ENTRY(arm_fadvise64_64),
+	COMPAT_SYSCALL_ENTRY(sync_file_range2),
+#else
+	COMPAT_SYSCALL_ENTRY(fadvise64_64),
+	COMPAT_SYSCALL_ENTRY(fadvise64),
+	COMPAT_SYSCALL_ENTRY(sync_file_range),
+#endif
+
+	/*
+	 * getrlimit(2) and time(2) are deprecated and not wired in the ARM
+         * compat table on ARM64.
+	 */
+#ifndef CONFIG_ARM64
+	COMPAT_SYSCALL_ENTRY(getrlimit),
+        COMPAT_SYSCALL_ENTRY(time),
+#endif
+
+	/* x86-specific syscalls. */
+#ifdef CONFIG_X86_64
+	COMPAT_SYSCALL_ENTRY(modify_ldt),
+	COMPAT_SYSCALL_ENTRY(set_thread_area),
+#endif
+}; /* end android_compat_whitelist */
+#endif /* CONFIG_COMPAT */
+
+#endif /* ANDROID_WHITELISTS_H */
diff -ruN a/security/chromiumos/complete_whitelists.h b/security/chromiumos/complete_whitelists.h
--- a/security/chromiumos/complete_whitelists.h	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/complete_whitelists.h	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,373 @@
+/*
+ * Linux Security Module for Chromium OS
+ *
+ * Copyright 2018 Google LLC. All Rights Reserved
+ *
+ * Authors:
+ *      Micah Morton <mortonm@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef COMPLETE_WHITELISTS_H
+#define COMPLETE_WHITELISTS_H
+
+/*
+ * NOTE: the purpose of this header is only to pull out the definition of this
+ * array from alt-syscall.c for the purposes of readability. It should not be
+ * included in other .c files.
+ */
+
+#include "alt-syscall.h"
+
+static struct syscall_whitelist_entry complete_whitelist[] = {
+	/* Syscalls wired up on ARM32/ARM64 and x86_64. */
+	SYSCALL_ENTRY(accept),
+	SYSCALL_ENTRY(accept4),
+	SYSCALL_ENTRY(acct),
+	SYSCALL_ENTRY(add_key),
+	SYSCALL_ENTRY(adjtimex),
+	SYSCALL_ENTRY(bind),
+	SYSCALL_ENTRY(brk),
+	SYSCALL_ENTRY(capget),
+	SYSCALL_ENTRY(capset),
+	SYSCALL_ENTRY(chdir),
+	SYSCALL_ENTRY(chroot),
+	SYSCALL_ENTRY(clock_adjtime),
+	SYSCALL_ENTRY(clock_getres),
+	SYSCALL_ENTRY(clock_gettime),
+	SYSCALL_ENTRY(clock_nanosleep),
+	SYSCALL_ENTRY(clock_settime),
+	SYSCALL_ENTRY(clone),
+	SYSCALL_ENTRY(close),
+	SYSCALL_ENTRY(connect),
+	SYSCALL_ENTRY(copy_file_range),
+	SYSCALL_ENTRY(delete_module),
+	SYSCALL_ENTRY(dup),
+	SYSCALL_ENTRY(dup3),
+	SYSCALL_ENTRY(epoll_create1),
+	SYSCALL_ENTRY(epoll_ctl),
+	SYSCALL_ENTRY(epoll_pwait),
+	SYSCALL_ENTRY(eventfd2),
+	SYSCALL_ENTRY(execve),
+	SYSCALL_ENTRY(exit),
+	SYSCALL_ENTRY(exit_group),
+	SYSCALL_ENTRY(faccessat),
+	SYSCALL_ENTRY(fallocate),
+	SYSCALL_ENTRY(fanotify_init),
+	SYSCALL_ENTRY(fanotify_mark),
+	SYSCALL_ENTRY(fchdir),
+	SYSCALL_ENTRY(fchmod),
+	SYSCALL_ENTRY(fchmodat),
+	SYSCALL_ENTRY(fchown),
+	SYSCALL_ENTRY(fchownat),
+	SYSCALL_ENTRY(fcntl),
+	SYSCALL_ENTRY(fdatasync),
+	SYSCALL_ENTRY(fgetxattr),
+	SYSCALL_ENTRY(finit_module),
+	SYSCALL_ENTRY(flistxattr),
+	SYSCALL_ENTRY(flock),
+	SYSCALL_ENTRY(fremovexattr),
+	SYSCALL_ENTRY(fsetxattr),
+	SYSCALL_ENTRY(fstatfs),
+	SYSCALL_ENTRY(fsync),
+	SYSCALL_ENTRY(ftruncate),
+	SYSCALL_ENTRY(futex),
+	SYSCALL_ENTRY(getcpu),
+	SYSCALL_ENTRY(getcwd),
+	SYSCALL_ENTRY(getdents64),
+	SYSCALL_ENTRY(getegid),
+	SYSCALL_ENTRY(geteuid),
+	SYSCALL_ENTRY(getgid),
+	SYSCALL_ENTRY(getgroups),
+	SYSCALL_ENTRY(getitimer),
+	SYSCALL_ENTRY(get_mempolicy),
+	SYSCALL_ENTRY(getpeername),
+	SYSCALL_ENTRY(getpgid),
+	SYSCALL_ENTRY(getpid),
+	SYSCALL_ENTRY(getppid),
+	SYSCALL_ENTRY(getpriority),
+	SYSCALL_ENTRY(getrandom),
+	SYSCALL_ENTRY(getresgid),
+	SYSCALL_ENTRY(getresuid),
+	SYSCALL_ENTRY(getrlimit),
+	SYSCALL_ENTRY(get_robust_list),
+	SYSCALL_ENTRY(getrusage),
+	SYSCALL_ENTRY(getsid),
+	SYSCALL_ENTRY(getsockname),
+	SYSCALL_ENTRY(getsockopt),
+	SYSCALL_ENTRY(gettid),
+	SYSCALL_ENTRY(gettimeofday),
+	SYSCALL_ENTRY(getuid),
+	SYSCALL_ENTRY(getxattr),
+	SYSCALL_ENTRY(init_module),
+	SYSCALL_ENTRY(inotify_add_watch),
+	SYSCALL_ENTRY(inotify_init1),
+	SYSCALL_ENTRY(inotify_rm_watch),
+	SYSCALL_ENTRY(io_cancel),
+	SYSCALL_ENTRY(ioctl),
+	SYSCALL_ENTRY(io_destroy),
+	SYSCALL_ENTRY(io_getevents),
+	SYSCALL_ENTRY(ioprio_get),
+	SYSCALL_ENTRY(ioprio_set),
+	SYSCALL_ENTRY(io_setup),
+	SYSCALL_ENTRY(io_submit),
+	SYSCALL_ENTRY(kcmp),
+	SYSCALL_ENTRY(kexec_load),
+	SYSCALL_ENTRY(keyctl),
+	SYSCALL_ENTRY(kill),
+	SYSCALL_ENTRY(lgetxattr),
+	SYSCALL_ENTRY(linkat),
+	SYSCALL_ENTRY(listen),
+	SYSCALL_ENTRY(listxattr),
+	SYSCALL_ENTRY(llistxattr),
+	SYSCALL_ENTRY(lookup_dcookie),
+	SYSCALL_ENTRY(lremovexattr),
+	SYSCALL_ENTRY(lseek),
+	SYSCALL_ENTRY(lsetxattr),
+	SYSCALL_ENTRY(madvise),
+	SYSCALL_ENTRY(mbind),
+	SYSCALL_ENTRY(memfd_create),
+	SYSCALL_ENTRY(mincore),
+	SYSCALL_ENTRY(mkdirat),
+	SYSCALL_ENTRY(mknodat),
+	SYSCALL_ENTRY(mlock),
+	SYSCALL_ENTRY(mlockall),
+	SYSCALL_ENTRY(mount),
+	SYSCALL_ENTRY(move_pages),
+	SYSCALL_ENTRY(mprotect),
+	SYSCALL_ENTRY(mq_getsetattr),
+	SYSCALL_ENTRY(mq_notify),
+	SYSCALL_ENTRY(mq_open),
+	SYSCALL_ENTRY(mq_timedreceive),
+	SYSCALL_ENTRY(mq_timedsend),
+	SYSCALL_ENTRY(mq_unlink),
+	SYSCALL_ENTRY(mremap),
+	SYSCALL_ENTRY(msgctl),
+	SYSCALL_ENTRY(msgget),
+	SYSCALL_ENTRY(msgrcv),
+	SYSCALL_ENTRY(msgsnd),
+	SYSCALL_ENTRY(msync),
+	SYSCALL_ENTRY(munlock),
+	SYSCALL_ENTRY(munlockall),
+	SYSCALL_ENTRY(munmap),
+	SYSCALL_ENTRY(name_to_handle_at),
+	SYSCALL_ENTRY(nanosleep),
+	SYSCALL_ENTRY(openat),
+	SYSCALL_ENTRY(open_by_handle_at),
+	SYSCALL_ENTRY(perf_event_open),
+	SYSCALL_ENTRY(personality),
+	SYSCALL_ENTRY(pipe2),
+	SYSCALL_ENTRY(pivot_root),
+	SYSCALL_ENTRY(pkey_alloc),
+	SYSCALL_ENTRY(pkey_free),
+	SYSCALL_ENTRY(pkey_mprotect),
+	SYSCALL_ENTRY(ppoll),
+	SYSCALL_ENTRY_ALT(prctl, alt_sys_prctl),
+	SYSCALL_ENTRY(pread64),
+	SYSCALL_ENTRY(preadv),
+	SYSCALL_ENTRY(preadv2),
+	SYSCALL_ENTRY(pwritev2),
+	SYSCALL_ENTRY(prlimit64),
+	SYSCALL_ENTRY(process_vm_readv),
+	SYSCALL_ENTRY(process_vm_writev),
+	SYSCALL_ENTRY(pselect6),
+	SYSCALL_ENTRY(ptrace),
+	SYSCALL_ENTRY(pwrite64),
+	SYSCALL_ENTRY(pwritev),
+	SYSCALL_ENTRY(quotactl),
+	SYSCALL_ENTRY(read),
+	SYSCALL_ENTRY(readahead),
+	SYSCALL_ENTRY(readlinkat),
+	SYSCALL_ENTRY(readv),
+	SYSCALL_ENTRY(reboot),
+	SYSCALL_ENTRY(recvfrom),
+	SYSCALL_ENTRY(recvmmsg),
+	SYSCALL_ENTRY(recvmsg),
+	SYSCALL_ENTRY(remap_file_pages),
+	SYSCALL_ENTRY(removexattr),
+	SYSCALL_ENTRY(renameat),
+	SYSCALL_ENTRY(request_key),
+	SYSCALL_ENTRY(restart_syscall),
+	SYSCALL_ENTRY(rt_sigaction),
+	SYSCALL_ENTRY(rt_sigpending),
+	SYSCALL_ENTRY(rt_sigprocmask),
+	SYSCALL_ENTRY(rt_sigqueueinfo),
+	SYSCALL_ENTRY(rt_sigsuspend),
+	SYSCALL_ENTRY(rt_sigtimedwait),
+	SYSCALL_ENTRY(rt_tgsigqueueinfo),
+	SYSCALL_ENTRY(sched_getaffinity),
+	SYSCALL_ENTRY(sched_getattr),
+	SYSCALL_ENTRY(sched_getparam),
+	SYSCALL_ENTRY(sched_get_priority_max),
+	SYSCALL_ENTRY(sched_get_priority_min),
+	SYSCALL_ENTRY(sched_getscheduler),
+	SYSCALL_ENTRY(sched_rr_get_interval),
+	SYSCALL_ENTRY(sched_setaffinity),
+	SYSCALL_ENTRY(sched_setattr),
+	SYSCALL_ENTRY(sched_setparam),
+	SYSCALL_ENTRY(sched_setscheduler),
+	SYSCALL_ENTRY(sched_yield),
+	SYSCALL_ENTRY(seccomp),
+	SYSCALL_ENTRY(semctl),
+	SYSCALL_ENTRY(semget),
+	SYSCALL_ENTRY(semop),
+	SYSCALL_ENTRY(semtimedop),
+	SYSCALL_ENTRY(sendfile),
+	SYSCALL_ENTRY(sendmmsg),
+	SYSCALL_ENTRY(sendmsg),
+	SYSCALL_ENTRY(sendto),
+	SYSCALL_ENTRY(setdomainname),
+	SYSCALL_ENTRY(setfsgid),
+	SYSCALL_ENTRY(setfsuid),
+	SYSCALL_ENTRY(setgid),
+	SYSCALL_ENTRY(setgroups),
+	SYSCALL_ENTRY(sethostname),
+	SYSCALL_ENTRY(setitimer),
+	SYSCALL_ENTRY(set_mempolicy),
+	SYSCALL_ENTRY(setns),
+	SYSCALL_ENTRY(setpgid),
+	SYSCALL_ENTRY(setpriority),
+	SYSCALL_ENTRY(setregid),
+	SYSCALL_ENTRY(setresgid),
+	SYSCALL_ENTRY(setresuid),
+	SYSCALL_ENTRY(setreuid),
+	SYSCALL_ENTRY(setrlimit),
+	SYSCALL_ENTRY(set_robust_list),
+	SYSCALL_ENTRY(setsid),
+	SYSCALL_ENTRY(setsockopt),
+	SYSCALL_ENTRY(set_tid_address),
+	SYSCALL_ENTRY(settimeofday),
+	SYSCALL_ENTRY(setuid),
+	SYSCALL_ENTRY(setxattr),
+	SYSCALL_ENTRY(shmat),
+	SYSCALL_ENTRY(shmctl),
+	SYSCALL_ENTRY(shmdt),
+	SYSCALL_ENTRY(shmget),
+	SYSCALL_ENTRY(shutdown),
+	SYSCALL_ENTRY(sigaltstack),
+	SYSCALL_ENTRY(signalfd4),
+	SYSCALL_ENTRY(socket),
+	SYSCALL_ENTRY(socketpair),
+	SYSCALL_ENTRY(splice),
+	SYSCALL_ENTRY(statfs),
+	SYSCALL_ENTRY(statx),
+	SYSCALL_ENTRY(swapoff),
+	SYSCALL_ENTRY(swapon),
+	SYSCALL_ENTRY(symlinkat),
+	SYSCALL_ENTRY(sync),
+	SYSCALL_ENTRY(syncfs),
+	SYSCALL_ENTRY(sysinfo),
+	SYSCALL_ENTRY(syslog),
+	SYSCALL_ENTRY(tee),
+	SYSCALL_ENTRY(tgkill),
+	SYSCALL_ENTRY(timer_create),
+	SYSCALL_ENTRY(timer_delete),
+	SYSCALL_ENTRY(timerfd_create),
+	SYSCALL_ENTRY(timerfd_gettime),
+	SYSCALL_ENTRY(timerfd_settime),
+	SYSCALL_ENTRY(timer_getoverrun),
+	SYSCALL_ENTRY(timer_gettime),
+	SYSCALL_ENTRY(timer_settime),
+	SYSCALL_ENTRY(times),
+	SYSCALL_ENTRY(tkill),
+	SYSCALL_ENTRY(truncate),
+	SYSCALL_ENTRY(umask),
+	SYSCALL_ENTRY(unlinkat),
+	SYSCALL_ENTRY(unshare),
+	SYSCALL_ENTRY(utimensat),
+	SYSCALL_ENTRY(vhangup),
+	SYSCALL_ENTRY(vmsplice),
+	SYSCALL_ENTRY(wait4),
+	SYSCALL_ENTRY(waitid),
+	SYSCALL_ENTRY(write),
+	SYSCALL_ENTRY(writev),
+
+	/* Exist for x86_64 and ARM32 but not ARM64. */
+#ifndef CONFIG_ARM64
+	SYSCALL_ENTRY(access),
+	SYSCALL_ENTRY(alarm),
+	SYSCALL_ENTRY(chmod),
+	SYSCALL_ENTRY(chown),
+	SYSCALL_ENTRY(creat),
+	SYSCALL_ENTRY(dup2),
+	SYSCALL_ENTRY(epoll_create),
+	SYSCALL_ENTRY(epoll_wait),
+	SYSCALL_ENTRY(eventfd),
+	SYSCALL_ENTRY(fork),
+	SYSCALL_ENTRY(futimesat),
+	SYSCALL_ENTRY(getdents),
+	SYSCALL_ENTRY(getpgrp),
+	SYSCALL_ENTRY(inotify_init),
+	SYSCALL_ENTRY(lchown),
+	SYSCALL_ENTRY(link),
+	SYSCALL_ENTRY(mkdir),
+	SYSCALL_ENTRY(mknod),
+	SYSCALL_ENTRY(open),
+	SYSCALL_ENTRY(pause),
+	SYSCALL_ENTRY(pipe),
+	SYSCALL_ENTRY(poll),
+	SYSCALL_ENTRY(readlink),
+	SYSCALL_ENTRY(rename),
+	SYSCALL_ENTRY(rmdir),
+	SYSCALL_ENTRY(select),
+	SYSCALL_ENTRY(signalfd),
+	SYSCALL_ENTRY(symlink),
+	SYSCALL_ENTRY(sysfs),
+	SYSCALL_ENTRY(time),
+	SYSCALL_ENTRY(unlink),
+	SYSCALL_ENTRY(ustat),
+	SYSCALL_ENTRY(utime),
+	SYSCALL_ENTRY(utimes),
+	SYSCALL_ENTRY(vfork),
+#endif
+
+	/* Exist for x86_64 and ARM64 */
+	SYSCALL_ENTRY(fadvise64),
+	SYSCALL_ENTRY(fstat),
+	SYSCALL_ENTRY(migrate_pages),
+	SYSCALL_ENTRY(mmap),
+	SYSCALL_ENTRY(rt_sigreturn),
+	SYSCALL_ENTRY(sync_file_range),
+	SYSCALL_ENTRY(umount2),
+	SYSCALL_ENTRY(uname),
+
+	/* Unique to x86_64. */
+#ifdef CONFIG_X86_64
+	SYSCALL_ENTRY(arch_prctl),
+	SYSCALL_ENTRY(ioperm),
+	SYSCALL_ENTRY(iopl),
+	SYSCALL_ENTRY(kexec_file_load),
+	SYSCALL_ENTRY(lstat),
+	SYSCALL_ENTRY(modify_ldt),
+	SYSCALL_ENTRY(newfstatat),
+	SYSCALL_ENTRY(stat),
+	SYSCALL_ENTRY(_sysctl),
+#endif
+
+	/* Unique to ARM64. */
+#ifdef CONFIG_ARM64
+	SYSCALL_ENTRY(nfsservctl),
+	SYSCALL_ENTRY(renameat2),
+#endif
+}; /* end complete_whitelist */
+
+#ifdef CONFIG_COMPAT
+/*
+ * For now not adding a 32-bit-compatible version of the complete whitelist.
+ * Since we are not whitelisting any compat syscalls here, a call into the
+ * compat section of this "complete" alt syscall table will be redirected to
+ * block_syscall() (unless the permissive mode is used in which case the call
+ * will be redirected to warn_compat_syscall()).
+ */
+static struct syscall_whitelist_entry complete_compat_whitelist[] = {};
+#endif /* CONFIG_COMPAT */
+
+#endif /* COMPLETE_WHITELISTS_H */
diff -ruN a/security/chromiumos/inode_mark.c b/security/chromiumos/inode_mark.c
--- a/security/chromiumos/inode_mark.c	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/inode_mark.c	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,353 @@
+/*
+ * Linux Security Module for Chromium OS
+ *
+ * Copyright 2016 Google Inc. All Rights Reserved
+ *
+ * Authors:
+ *      Mattias Nissler <mnissler@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/atomic.h>
+#include <linux/compiler.h>
+#include <linux/dcache.h>
+#include <linux/fs.h>
+#include <linux/fsnotify_backend.h>
+#include <linux/hash.h>
+#include <linux/mutex.h>
+#include <linux/rculist.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+
+#include "inode_mark.h"
+
+/*
+ * This file implements facilities to pin inodes in core and attach some
+ * meta data to them. We use fsnotify inode marks as a vehicle to attach the
+ * meta data.
+ */
+struct chromiumos_inode_mark {
+	struct fsnotify_mark mark;
+	struct inode *inode;
+	enum chromiumos_inode_security_policy
+		policies[CHROMIUMOS_NUMBER_OF_POLICIES];
+};
+
+static inline struct chromiumos_inode_mark *
+chromiumos_to_inode_mark(struct fsnotify_mark *mark)
+{
+	return container_of(mark, struct chromiumos_inode_mark, mark);
+}
+
+/*
+ * Hashtable entry that contains tracking information specific to the file
+ * system identified by the corresponding super_block. This contains the
+ * fsnotify group that holds all the marks for inodes belonging to the
+ * super_block.
+ */
+struct chromiumos_super_block_mark {
+	atomic_t refcnt;
+	struct hlist_node node;
+	struct super_block *sb;
+	struct fsnotify_group *fsn_group;
+};
+
+#define CHROMIUMOS_SUPER_BLOCK_HASH_BITS 8
+#define CHROMIUMOS_SUPER_BLOCK_HASH_SIZE (1 << CHROMIUMOS_SUPER_BLOCK_HASH_BITS)
+
+static struct hlist_head chromiumos_super_block_hash_table
+	[CHROMIUMOS_SUPER_BLOCK_HASH_SIZE] __read_mostly;
+static DEFINE_MUTEX(chromiumos_super_block_hash_lock);
+
+static struct hlist_head *chromiumos_super_block_hlist(struct super_block *sb)
+{
+	return &chromiumos_super_block_hash_table[hash_ptr(
+		sb, CHROMIUMOS_SUPER_BLOCK_HASH_BITS)];
+}
+
+static void chromiumos_super_block_put(struct chromiumos_super_block_mark *sbm)
+{
+	if (atomic_dec_and_test(&sbm->refcnt)) {
+		mutex_lock(&chromiumos_super_block_hash_lock);
+		hlist_del_rcu(&sbm->node);
+		mutex_unlock(&chromiumos_super_block_hash_lock);
+
+		synchronize_rcu();
+
+		fsnotify_destroy_group(sbm->fsn_group);
+		kfree(sbm);
+	}
+}
+
+static struct chromiumos_super_block_mark *
+chromiumos_super_block_lookup(struct super_block *sb)
+{
+	struct hlist_head *hlist = chromiumos_super_block_hlist(sb);
+	struct chromiumos_super_block_mark *sbm;
+	struct chromiumos_super_block_mark *matching_sbm = NULL;
+
+	rcu_read_lock();
+	hlist_for_each_entry_rcu(sbm, hlist, node) {
+		if (sbm->sb == sb && atomic_inc_not_zero(&sbm->refcnt)) {
+			matching_sbm = sbm;
+			break;
+		}
+	}
+	rcu_read_unlock();
+
+	return matching_sbm;
+}
+
+static int chromiumos_handle_fsnotify_event(struct fsnotify_group *group,
+					    u32 mask, const void *data,
+					    int data_type, struct inode *dir,
+					    const struct qstr *file_name,
+					    u32 cookie,
+					    struct fsnotify_iter_info *iter_info)
+{
+	/*
+	 * This should never get called because a zero mask is set on the inode
+	 * marks. All cases of marks going away (inode deletion, unmount,
+	 * explicit removal) are handled in chromiumos_freeing_mark.
+	 */
+	WARN_ON_ONCE(1);
+	return 0;
+}
+
+static void chromiumos_freeing_mark(struct fsnotify_mark *mark,
+				    struct fsnotify_group *group)
+{
+	struct chromiumos_inode_mark *inode_mark =
+		chromiumos_to_inode_mark(mark);
+
+	iput(inode_mark->inode);
+	inode_mark->inode = NULL;
+	chromiumos_super_block_put(group->private);
+}
+
+static void chromiumos_free_mark(struct fsnotify_mark *mark)
+{
+	iput(chromiumos_to_inode_mark(mark)->inode);
+	kfree(mark);
+}
+
+static const struct fsnotify_ops chromiumos_fsn_ops = {
+	.handle_event = chromiumos_handle_fsnotify_event,
+	.freeing_mark = chromiumos_freeing_mark,
+	.free_mark = chromiumos_free_mark,
+};
+
+static struct chromiumos_super_block_mark *
+chromiumos_super_block_create(struct super_block *sb)
+{
+	struct hlist_head *hlist = chromiumos_super_block_hlist(sb);
+	struct chromiumos_super_block_mark *sbm = NULL;
+
+	WARN_ON(!mutex_is_locked(&chromiumos_super_block_hash_lock));
+
+	/* No match found, create a new entry. */
+	sbm = kzalloc(sizeof(*sbm), GFP_KERNEL);
+	if (!sbm)
+		return ERR_PTR(-ENOMEM);
+
+	atomic_set(&sbm->refcnt, 1);
+	sbm->sb = sb;
+	sbm->fsn_group = fsnotify_alloc_group(&chromiumos_fsn_ops);
+	if (IS_ERR(sbm->fsn_group)) {
+		int ret = PTR_ERR(sbm->fsn_group);
+
+		kfree(sbm);
+		return ERR_PTR(ret);
+	}
+	sbm->fsn_group->private = sbm;
+	hlist_add_head_rcu(&sbm->node, hlist);
+
+	return sbm;
+}
+
+static struct chromiumos_super_block_mark *
+chromiumos_super_block_get(struct super_block *sb)
+{
+	struct chromiumos_super_block_mark *sbm;
+
+	mutex_lock(&chromiumos_super_block_hash_lock);
+	sbm = chromiumos_super_block_lookup(sb);
+	if (!sbm)
+		sbm = chromiumos_super_block_create(sb);
+
+	mutex_unlock(&chromiumos_super_block_hash_lock);
+	return sbm;
+}
+
+/*
+ * This will only ever get called if the metadata does not already exist for
+ * an inode, so no need to worry about freeing an existing mark.
+ */
+static int
+chromiumos_inode_mark_create(
+	struct chromiumos_super_block_mark *sbm,
+	struct inode *inode,
+	enum chromiumos_inode_security_policy_type type,
+	enum chromiumos_inode_security_policy policy)
+{
+	struct chromiumos_inode_mark *inode_mark;
+	int ret;
+	size_t i;
+
+	WARN_ON(!mutex_is_locked(&sbm->fsn_group->mark_mutex));
+
+	inode_mark = kzalloc(sizeof(*inode_mark), GFP_KERNEL);
+	if (!inode_mark)
+		return -ENOMEM;
+
+	fsnotify_init_mark(&inode_mark->mark, sbm->fsn_group);
+	inode_mark->inode = igrab(inode);
+	if (!inode_mark->inode) {
+		ret = -ENOENT;
+		goto out;
+	}
+
+	/* Initialize all policies to inherit. */
+	for (i = 0; i < CHROMIUMOS_NUMBER_OF_POLICIES; i++)
+		inode_mark->policies[i] = CHROMIUMOS_INODE_POLICY_INHERIT;
+
+	inode_mark->policies[type] = policy;
+	ret = fsnotify_add_mark_locked(&inode_mark->mark, &inode->i_fsnotify_marks,
+				       type, false, NULL);
+	if (ret)
+		goto out;
+
+	/* Take an sbm reference so the created mark is accounted for. */
+	atomic_inc(&sbm->refcnt);
+
+out:
+	fsnotify_put_mark(&inode_mark->mark);
+	return ret;
+}
+
+int chromiumos_update_inode_security_policy(
+	struct inode *inode,
+	enum chromiumos_inode_security_policy_type type,
+	enum chromiumos_inode_security_policy policy)
+{
+	struct chromiumos_super_block_mark *sbm;
+	struct fsnotify_mark *mark;
+	bool free_mark = false;
+	int ret;
+	size_t i;
+
+	sbm = chromiumos_super_block_get(inode->i_sb);
+	if (IS_ERR(sbm))
+		return PTR_ERR(sbm);
+
+	mutex_lock(&sbm->fsn_group->mark_mutex);
+
+	mark = fsnotify_find_mark(&inode->i_fsnotify_marks, sbm->fsn_group);
+	if (mark) {
+		WRITE_ONCE(chromiumos_to_inode_mark(mark)->policies[type],
+				   policy);
+		/*
+		 * Frees mark if all policies are
+		 * CHROMIUM_INODE_POLICY_INHERIT.
+		 */
+		free_mark = true;
+		for (i = 0; i < CHROMIUMOS_NUMBER_OF_POLICIES; i++) {
+			if (chromiumos_to_inode_mark(mark)->policies[i]
+				!= CHROMIUMOS_INODE_POLICY_INHERIT) {
+				free_mark = false;
+				break;
+			}
+		}
+		if (free_mark)
+			fsnotify_detach_mark(mark);
+		ret = 0;
+	} else {
+		ret = chromiumos_inode_mark_create(sbm, inode, type, policy);
+	}
+
+	mutex_unlock(&sbm->fsn_group->mark_mutex);
+	chromiumos_super_block_put(sbm);
+
+	/* This must happen after dropping the mark mutex. */
+	if (free_mark)
+		fsnotify_free_mark(mark);
+	if (mark)
+		fsnotify_put_mark(mark);
+
+	return ret;
+}
+
+/* Flushes all inode security policies. */
+int chromiumos_flush_inode_security_policies(struct super_block *sb)
+{
+	struct chromiumos_super_block_mark *sbm;
+
+	sbm = chromiumos_super_block_lookup(sb);
+	if (sbm) {
+		fsnotify_clear_marks_by_group(sbm->fsn_group,
+					      FSNOTIFY_OBJ_ALL_TYPES_MASK);
+		chromiumos_super_block_put(sbm);
+	}
+
+	return 0;
+}
+
+enum chromiumos_inode_security_policy chromiumos_get_inode_security_policy(
+	struct dentry *dentry, struct inode *inode,
+	enum chromiumos_inode_security_policy_type type)
+{
+	struct chromiumos_super_block_mark *sbm;
+	/*
+	 * Initializes policy to CHROMIUM_INODE_POLICY_INHERIT, which is
+	 * the value that will be returned if neither |dentry| nor any
+	 * directory in its path has been asigned an inode security policy
+	 * value for the given type.
+	 */
+	enum chromiumos_inode_security_policy policy =
+		CHROMIUMOS_INODE_POLICY_INHERIT;
+
+	if (!dentry || !inode || type >= CHROMIUMOS_NUMBER_OF_POLICIES)
+		return policy;
+
+	sbm = chromiumos_super_block_lookup(inode->i_sb);
+	if (!sbm)
+		return policy;
+
+	/* Walk the dentry path and look for a traversal policy. */
+	rcu_read_lock();
+	while (1) {
+		struct fsnotify_mark *mark = fsnotify_find_mark(
+			&inode->i_fsnotify_marks, sbm->fsn_group);
+		if (mark) {
+			struct chromiumos_inode_mark *inode_mark =
+				chromiumos_to_inode_mark(mark);
+			policy = READ_ONCE(inode_mark->policies[type]);
+			fsnotify_put_mark(mark);
+
+			if (policy != CHROMIUMOS_INODE_POLICY_INHERIT)
+				break;
+		}
+
+		if (IS_ROOT(dentry))
+			break;
+		dentry = READ_ONCE(dentry->d_parent);
+		if (!dentry)
+			break;
+		inode = d_inode_rcu(dentry);
+		if (!inode)
+			break;
+	}
+	rcu_read_unlock();
+
+	chromiumos_super_block_put(sbm);
+
+	return policy;
+}
diff -ruN a/security/chromiumos/inode_mark.h b/security/chromiumos/inode_mark.h
--- a/security/chromiumos/inode_mark.h	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/inode_mark.h	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,47 @@
+/*
+ * Linux Security Module for Chromium OS
+ *
+ * Copyright 2016 Google Inc. All Rights Reserved
+ *
+ * Authors:
+ *      Mattias Nissler <mnissler@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+/* FS feature availability policy for inode. */
+enum chromiumos_inode_security_policy {
+	CHROMIUMOS_INODE_POLICY_INHERIT, /* Inherit policy from parent dir */
+	CHROMIUMOS_INODE_POLICY_ALLOW,
+	CHROMIUMOS_INODE_POLICY_BLOCK,
+};
+
+/*
+ * Inode security policy types available for use. To add an additional
+ * security policy, simply add a new member here, add the corresponding policy
+ * files in securityfs.c, and associate the files being added with the new enum
+ * member.
+ */
+enum chromiumos_inode_security_policy_type {
+	CHROMIUMOS_SYMLINK_TRAVERSAL = 0,
+	CHROMIUMOS_FIFO_ACCESS,
+	CHROMIUMOS_NUMBER_OF_POLICIES, /* Do not add entries after this line. */
+};
+
+extern int chromiumos_update_inode_security_policy(
+	struct inode *inode,
+	enum chromiumos_inode_security_policy_type type,
+	enum chromiumos_inode_security_policy policy);
+int chromiumos_flush_inode_security_policies(struct super_block *sb);
+
+extern enum chromiumos_inode_security_policy
+chromiumos_get_inode_security_policy(
+	struct dentry *dentry, struct inode *inode,
+	enum chromiumos_inode_security_policy_type type);
diff -ruN a/security/chromiumos/Kconfig b/security/chromiumos/Kconfig
--- a/security/chromiumos/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/Kconfig	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,45 @@
+config SECURITY_CHROMIUMOS
+	bool "Chromium OS Security Module"
+	depends on SECURITY
+	depends on X86_64 || ARM64
+	help
+	  The purpose of the Chromium OS security module is to reduce attacking
+	  surface by preventing access to general purpose access modes not
+	  required by Chromium OS. Currently: the mount operation is
+	  restricted by requiring a mount point path without symbolic links,
+	  and loading modules is limited to only the root filesystem. This
+	  LSM is stacked ahead of any primary "full" LSM.
+
+config SECURITY_CHROMIUMOS_NO_SYMLINK_MOUNT
+	bool "Chromium OS Security: prohibit mount to symlinked target"
+	depends on SECURITY_CHROMIUMOS
+	default y
+	help
+	  When enabled mount() syscall will return ELOOP whenever target path
+	  contains any symlinks.
+
+config SECURITY_CHROMIUMOS_NO_UNPRIVILEGED_UNSAFE_MOUNTS
+	bool "Chromium OS Security: prohibit unsafe mounts in unprivileged user namespaces"
+	depends on SECURITY_CHROMIUMOS
+	default y
+	help
+	  When enabled, mount() syscall will return EPERM whenever a new mount
+	  is attempted that would cause the filesystem to have the exec, suid,
+	  or dev flags if the caller does not have the CAP_SYS_ADMIN capability
+	  in the init namespace.
+
+config ALT_SYSCALL_CHROMIUMOS
+	bool "Chromium OS Alt-Syscall Tables"
+	depends on ALT_SYSCALL
+	help
+	  Register restricted, alternate syscall tables used by Chromium OS
+	  using the alt-syscall infrastructure.  Alternate syscall tables
+	  can be selected with prctl(PR_ALT_SYSCALL).
+
+config SECURITY_CHROMIUMOS_READONLY_PROC_SELF_MEM
+	bool "Force /proc/<pid>/mem paths to be read-only"
+	default y
+	help
+	  When enabled, attempts to open /proc/self/mem for write access
+	  will always fail.  Write access to this file allows bypassing
+	  of memory map permissions (such as modifying read-only code).
diff -ruN a/security/chromiumos/lsm.c b/security/chromiumos/lsm.c
--- a/security/chromiumos/lsm.c	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/lsm.c	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,274 @@
+/*
+ * Linux Security Module for Chromium OS
+ *
+ * Copyright 2011 Google Inc. All Rights Reserved
+ *
+ * Authors:
+ *      Stephan Uphoff  <ups@google.com>
+ *      Kees Cook       <keescook@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#define pr_fmt(fmt) "Chromium OS LSM: " fmt
+
+#include <asm/syscall.h>
+#include <linux/cred.h>
+#include <linux/fs.h>
+#include <linux/fs_parser.h>
+#include <linux/fs_struct.h>
+#include <linux/lsm_hooks.h>
+#include <linux/module.h>
+#include <linux/mount.h>
+#include <linux/namei.h>	/* for nameidata_get_total_link_count */
+#include <linux/path.h>
+#include <linux/ptrace.h>
+#include <linux/sched/task_stack.h>
+#include <linux/sched.h>	/* current and other task related stuff */
+#include <linux/security.h>
+#include <uapi/linux/mount.h>
+
+#include "inode_mark.h"
+#include "utils.h"
+
+#if defined(CONFIG_SECURITY_CHROMIUMOS_NO_UNPRIVILEGED_UNSAFE_MOUNTS) || \
+	defined(CONFIG_SECURITY_CHROMIUMOS_NO_SYMLINK_MOUNT)
+static void report(const char *origin, const struct path *path, char *operation)
+{
+	char *alloced = NULL, *cmdline;
+	char *pathname; /* Pointer to either static string or "alloced". */
+
+	if (!path)
+		pathname = "<unknown>";
+	else {
+		/* We will allow 11 spaces for ' (deleted)' to be appended */
+		alloced = pathname = kmalloc(PATH_MAX+11, GFP_KERNEL);
+		if (!pathname)
+			pathname = "<no_memory>";
+		else {
+			pathname = d_path(path, pathname, PATH_MAX+11);
+			if (IS_ERR(pathname))
+				pathname = "<too_long>";
+			else {
+				pathname = printable(pathname, PATH_MAX+11);
+				kfree(alloced);
+				alloced = pathname;
+			}
+		}
+	}
+
+	cmdline = printable_cmdline(current);
+
+	pr_notice("%s %s obj=%s pid=%d cmdline=%s\n", origin,
+		  operation, pathname, task_pid_nr(current), cmdline);
+
+	kfree(cmdline);
+	kfree(alloced);
+}
+#endif
+
+static int chromiumos_security_sb_mount(const char *dev_name,
+					const struct path *path,
+					const char *type, unsigned long flags,
+					void *data)
+{
+#ifdef CONFIG_SECURITY_CHROMIUMOS_NO_SYMLINK_MOUNT
+	if (!(path->link_count & PATH_LINK_COUNT_VALID)) {
+		WARN(1, "No link count available");
+		return -ELOOP;
+	} else if (path->link_count & ~PATH_LINK_COUNT_VALID) {
+		report("sb_mount", path, "Mount path with symlinks prohibited");
+		pr_notice("sb_mount dev=%s type=%s flags=%#lx\n",
+			  dev_name, type, flags);
+		return -ELOOP;
+	}
+#endif
+
+#ifdef CONFIG_SECURITY_CHROMIUMOS_NO_UNPRIVILEGED_UNSAFE_MOUNTS
+	if ((!(flags & (MS_BIND | MS_MOVE | MS_SHARED | MS_PRIVATE | MS_SLAVE |
+			MS_UNBINDABLE)) ||
+	     ((flags & MS_REMOUNT) && (flags & MS_BIND))) &&
+	    !capable(CAP_SYS_ADMIN)) {
+		int required_mnt_flags = MNT_NOEXEC | MNT_NOSUID | MNT_NODEV;
+
+		if (flags & MS_REMOUNT) {
+			/*
+			 * If this is a remount, we only require that the
+			 * requested flags are a superset of the original mount
+			 * flags. In addition, using nosymfollow is not
+			 * initially required, but remount is not allowed to
+			 * remove it.
+			 */
+			required_mnt_flags |= MNT_NOSYMFOLLOW;
+			required_mnt_flags &= path->mnt->mnt_flags;
+		}
+		/*
+		 * The three flags we are interested in disallowing in
+		 * unprivileged user namespaces (MS_NOEXEC, MS_NOSUID, MS_NODEV)
+		 * cannot be modified when doing a bind-mount. The kernel
+		 * attempts to dispatch calls to do_mount() within
+		 * fs/namespace.c in the following order:
+		 *
+		 * * If the MS_REMOUNT flag is present, it calls do_remount().
+		 *   When MS_BIND is also present, it only allows to modify the
+		 *   per-mount flags, which are copied into
+		 *   |required_mnt_flags|.  Otherwise it bails in the absence of
+		 *   the CAP_SYS_ADMIN in the init ns.
+		 * * If the MS_BIND flag is present, the only other flag checked
+		 *   is MS_REC.
+		 * * If any of the mount propagation flags are present
+		 *   (MS_SHARED, MS_PRIVATE, MS_SLAVE, MS_UNBINDABLE),
+		 *   flags_to_propagation_type() filters out any additional
+		 *   flags.
+		 * * If MS_MOVE flag is present, all other flags are ignored.
+		 */
+		if ((required_mnt_flags & MNT_NOEXEC) && !(flags & MS_NOEXEC)) {
+			report("sb_mount", path,
+			       "Mounting a filesystem with 'exec' flag requires CAP_SYS_ADMIN in init ns");
+			pr_notice("sb_mount dev=%s type=%s flags=%#lx\n",
+				  dev_name, type, flags);
+			return -EPERM;
+		}
+		if ((required_mnt_flags & MNT_NOSUID) && !(flags & MS_NOSUID)) {
+			report("sb_mount", path,
+			       "Mounting a filesystem with 'suid' flag requires CAP_SYS_ADMIN in init ns");
+			pr_notice("sb_mount dev=%s type=%s flags=%#lx\n",
+				  dev_name, type, flags);
+			return -EPERM;
+		}
+		if ((required_mnt_flags & MNT_NODEV) && !(flags & MS_NODEV) &&
+		    strcmp(type, "devpts")) {
+			report("sb_mount", path,
+			       "Mounting a filesystem with 'dev' flag requires CAP_SYS_ADMIN in init ns");
+			pr_notice("sb_mount dev=%s type=%s flags=%#lx\n",
+				  dev_name, type, flags);
+			return -EPERM;
+		}
+	}
+#endif
+
+	return 0;
+}
+
+/*
+ * NOTE: The WARN() calls will emit a warning in cases of blocked symlink
+ * traversal attempts. These will show up in kernel warning reports
+ * collected by the crash reporter, so we have some insight on spurious
+ * failures that need addressing.
+ */
+static int chromiumos_security_inode_follow_link(struct dentry *dentry,
+						 struct inode *inode, bool rcu)
+{
+	static char accessed_path[PATH_MAX];
+	enum chromiumos_inode_security_policy policy;
+
+	policy = chromiumos_get_inode_security_policy(
+		dentry, inode,
+		CHROMIUMOS_SYMLINK_TRAVERSAL);
+
+	WARN(policy == CHROMIUMOS_INODE_POLICY_BLOCK,
+	     "Blocked symlink traversal for path %x:%x:%s (see https://goo.gl/8xICW6 for context and rationale)\n",
+	     MAJOR(dentry->d_sb->s_dev), MINOR(dentry->d_sb->s_dev),
+	     dentry_path(dentry, accessed_path, PATH_MAX));
+
+	return policy == CHROMIUMOS_INODE_POLICY_BLOCK ? -EACCES : 0;
+}
+
+static int chromiumos_security_file_open(struct file *file)
+{
+	static char accessed_path[PATH_MAX];
+	enum chromiumos_inode_security_policy policy;
+	struct dentry *dentry = file->f_path.dentry;
+
+	/* Returns 0 if file is not a FIFO */
+	if (!S_ISFIFO(file->f_inode->i_mode))
+		return 0;
+
+	policy = chromiumos_get_inode_security_policy(
+		dentry, dentry->d_inode,
+		CHROMIUMOS_FIFO_ACCESS);
+
+	/*
+	 * Emit a warning in cases of blocked fifo access attempts. These will
+	 * show up in kernel warning reports collected by the crash reporter,
+	 * so we have some insight on spurious failures that need addressing.
+	 */
+	WARN(policy == CHROMIUMOS_INODE_POLICY_BLOCK,
+	     "Blocked fifo access for path %x:%x:%s\n (see https://goo.gl/8xICW6 for context and rationale)\n",
+	     MAJOR(dentry->d_sb->s_dev), MINOR(dentry->d_sb->s_dev),
+	     dentry_path(dentry, accessed_path, PATH_MAX));
+
+	return policy == CHROMIUMOS_INODE_POLICY_BLOCK ? -EACCES : 0;
+}
+
+int chromiumos_sb_eat_lsm_opts(char *options, void **mnt_opts)
+{
+	char *from = options, *to = options;
+	bool found = false;
+	bool first = true;
+
+	while (1) {
+		char *next = strchr(from, ',');
+		int len;
+
+		if (next)
+			len = next - from;
+		else
+			len = strlen(from);
+
+		/*
+		 * Remove the option so that filesystems won't see it.
+		 * do_mount() has already forced the MS_NOSYMFOLLOW flag on
+		 * if it found this option, so no other action is needed.
+		 */
+		if (len == strlen("nosymfollow") && !strncmp(from, "nosymfollow", len)) {
+			found = true;
+		} else {
+			if (!first) {   /* copy with preceding comma */
+				from--;
+				len++;
+			}
+			if (to != from)
+				memmove(to, from, len);
+			to += len;
+			first = false;
+		}
+		if (!next)
+			break;
+		from += len + 1;
+	}
+	*to = '\0';
+
+	if (found)
+		pr_notice("nosymfollow option should be changed to MS_NOSYMFOLLOW flag.");
+
+	return 0;
+}
+
+static struct security_hook_list chromiumos_security_hooks[] = {
+	LSM_HOOK_INIT(sb_mount, chromiumos_security_sb_mount),
+	LSM_HOOK_INIT(inode_follow_link, chromiumos_security_inode_follow_link),
+	LSM_HOOK_INIT(file_open, chromiumos_security_file_open),
+	LSM_HOOK_INIT(sb_eat_lsm_opts, chromiumos_sb_eat_lsm_opts),
+};
+
+static int __init chromiumos_security_init(void)
+{
+	security_add_hooks(chromiumos_security_hooks,
+			   ARRAY_SIZE(chromiumos_security_hooks), "chromiumos");
+
+	pr_info("enabled");
+
+	return 0;
+}
+DEFINE_LSM(chromiumos) = {
+	.name = "chromiumos",
+	.init = chromiumos_security_init
+};
diff -ruN a/security/chromiumos/Makefile b/security/chromiumos/Makefile
--- a/security/chromiumos/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/Makefile	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,5 @@
+obj-$(CONFIG_SECURITY_CHROMIUMOS) := chromiumos_lsm.o
+
+chromiumos_lsm-y := inode_mark.o lsm.o securityfs.o utils.o
+
+obj-$(CONFIG_ALT_SYSCALL_CHROMIUMOS) += alt-syscall.o
diff -ruN a/security/chromiumos/read_write_test_whitelists.h b/security/chromiumos/read_write_test_whitelists.h
--- a/security/chromiumos/read_write_test_whitelists.h	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/read_write_test_whitelists.h	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,56 @@
+/*
+ * Linux Security Module for Chromium OS
+ *
+ * Copyright 2018 Google LLC. All Rights Reserved
+ *
+ * Authors:
+ *      Micah Morton <mortonm@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef READ_WRITE_TESTS_WHITELISTS_H
+#define READ_WRITE_TESTS_WHITELISTS_H
+
+/*
+ * NOTE: the purpose of this header is only to pull out the definition of this
+ * array from alt-syscall.c for the purposes of readability. It should not be
+ * included in other .c files.
+ */
+
+#include "alt-syscall.h"
+
+static struct syscall_whitelist_entry read_write_test_whitelist[] = {
+	SYSCALL_ENTRY(exit),
+	SYSCALL_ENTRY(openat),
+	SYSCALL_ENTRY(close),
+	SYSCALL_ENTRY(read),
+	SYSCALL_ENTRY(write),
+	SYSCALL_ENTRY_ALT(prctl, alt_sys_prctl),
+
+	/* open(2) is deprecated and not wired up on ARM64. */
+#ifndef CONFIG_ARM64
+	SYSCALL_ENTRY(open),
+#endif
+}; /* end read_write_test_whitelist */
+
+#ifdef CONFIG_COMPAT
+static struct syscall_whitelist_entry read_write_test_compat_whitelist[] = {
+	COMPAT_SYSCALL_ENTRY(exit),
+	COMPAT_SYSCALL_ENTRY(open),
+	COMPAT_SYSCALL_ENTRY(openat),
+	COMPAT_SYSCALL_ENTRY(close),
+	COMPAT_SYSCALL_ENTRY(read),
+	COMPAT_SYSCALL_ENTRY(write),
+	COMPAT_SYSCALL_ENTRY_ALT(prctl, alt_sys_prctl),
+}; /* end read_write_test_compat_whitelist */
+#endif /* CONFIG_COMPAT */
+
+#endif /* READ_WRITE_TESTS_WHITELISTS_H */
diff -ruN a/security/chromiumos/securityfs.c b/security/chromiumos/securityfs.c
--- a/security/chromiumos/securityfs.c	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/securityfs.c	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,241 @@
+/*
+ * Linux Security Module for Chromium OS
+ *
+ * Copyright 2016 Google Inc. All Rights Reserved
+ *
+ * Authors:
+ *      Mattias Nissler <mnissler@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/capability.h>
+#include <linux/cred.h>
+#include <linux/dcache.h>
+#include <linux/fs.h>
+#include <linux/namei.h>
+#include <linux/sched.h>
+#include <linux/security.h>
+#include <linux/string.h>
+#include <linux/uaccess.h>
+
+#include "inode_mark.h"
+
+static struct dentry *chromiumos_dir;
+static struct dentry *chromiumos_inode_policy_dir;
+
+struct chromiumos_inode_policy_file_entry {
+	const char *name;
+	int (*handle_write)(struct chromiumos_inode_policy_file_entry *,
+			    struct dentry *);
+	enum chromiumos_inode_security_policy_type type;
+	enum chromiumos_inode_security_policy policy;
+	struct dentry *dentry;
+};
+
+static int chromiumos_inode_policy_file_write(
+	struct chromiumos_inode_policy_file_entry *file_entry,
+	struct dentry *dentry)
+{
+	return chromiumos_update_inode_security_policy(dentry->d_inode,
+		file_entry->type, file_entry->policy);
+}
+
+/*
+ * Causes all marks to be removed from inodes thus removing all inode security
+ * policies.
+ */
+static int chromiumos_inode_policy_file_flush_write(
+	struct chromiumos_inode_policy_file_entry *file_entry,
+	struct dentry *dentry)
+{
+	return chromiumos_flush_inode_security_policies(dentry->d_sb);
+}
+
+static struct chromiumos_inode_policy_file_entry
+		chromiumos_inode_policy_files[] = {
+	{.name = "block_symlink",
+	 .handle_write = chromiumos_inode_policy_file_write,
+	 .type = CHROMIUMOS_SYMLINK_TRAVERSAL,
+	 .policy = CHROMIUMOS_INODE_POLICY_BLOCK},
+	{.name = "allow_symlink",
+	 .handle_write = chromiumos_inode_policy_file_write,
+	 .type = CHROMIUMOS_SYMLINK_TRAVERSAL,
+	 .policy = CHROMIUMOS_INODE_POLICY_ALLOW},
+	{.name = "reset_symlink",
+	 .handle_write = chromiumos_inode_policy_file_write,
+	 .type = CHROMIUMOS_SYMLINK_TRAVERSAL,
+	 .policy = CHROMIUMOS_INODE_POLICY_INHERIT},
+	{.name = "block_fifo",
+	 .handle_write = chromiumos_inode_policy_file_write,
+	 .type = CHROMIUMOS_FIFO_ACCESS,
+	 .policy = CHROMIUMOS_INODE_POLICY_BLOCK},
+	{.name = "allow_fifo",
+	 .handle_write = chromiumos_inode_policy_file_write,
+	 .type = CHROMIUMOS_FIFO_ACCESS,
+	 .policy = CHROMIUMOS_INODE_POLICY_ALLOW},
+	{.name = "reset_fifo",
+	 .handle_write = chromiumos_inode_policy_file_write,
+	 .type = CHROMIUMOS_FIFO_ACCESS,
+	 .policy = CHROMIUMOS_INODE_POLICY_INHERIT},
+	{.name = "flush_policies",
+	 .handle_write = &chromiumos_inode_policy_file_flush_write},
+};
+
+static int chromiumos_resolve_path(const char __user *buf, size_t len,
+				   struct path *path)
+{
+	char *filename = NULL;
+	char *canonical_buf = NULL;
+	char *canonical;
+	int ret;
+
+	if (len + 1 > PATH_MAX)
+		return -EINVAL;
+
+	/*
+	 * Copy the path to a kernel buffer. We can't use user_path_at()
+	 * since it expects a zero-terminated path, which we generally don't
+	 * have here.
+	 */
+	filename = kzalloc(len + 1, GFP_KERNEL);
+	if (!filename)
+		return -ENOMEM;
+
+	if (copy_from_user(filename, buf, len)) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	ret = kern_path(filename, 0, path);
+	if (ret)
+		goto out;
+
+	/*
+	 * Make sure the path is canonical, i.e. it didn't contain symlinks. To
+	 * check this we convert |path| back to an absolute path (within the
+	 * global root) and compare the resulting path name with the passed-in
+	 * |filename|. This is stricter than needed (i.e. consecutive slashes
+	 * don't get ignored), but that's fine for our purposes.
+	 */
+	canonical_buf = kzalloc(len + 1, GFP_KERNEL);
+	if (!canonical_buf) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	canonical = d_absolute_path(path, canonical_buf, len + 1);
+	if (IS_ERR(canonical)) {
+		ret = PTR_ERR(canonical);
+
+		/* Buffer too short implies |filename| wasn't canonical. */
+		if (ret == -ENAMETOOLONG)
+			ret = -EMLINK;
+
+		goto out;
+	}
+
+	ret = strcmp(filename, canonical) ? -EMLINK : 0;
+
+out:
+	kfree(canonical_buf);
+	if (ret < 0)
+		path_put(path);
+	kfree(filename);
+	return ret;
+}
+
+static ssize_t chromiumos_inode_file_write(
+	struct file *file,
+	const char __user *buf,
+	size_t len,
+	loff_t *ppos)
+{
+	struct chromiumos_inode_policy_file_entry *file_entry =
+		file->f_inode->i_private;
+	struct path path = {};
+	int ret;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (*ppos != 0)
+		return -EINVAL;
+
+	ret = chromiumos_resolve_path(buf, len, &path);
+	if (ret)
+		return ret;
+
+	ret = file_entry->handle_write(file_entry, path.dentry);
+	path_put(&path);
+	return ret < 0 ? ret : len;
+}
+
+static const struct file_operations chromiumos_inode_policy_file_fops = {
+	.write = chromiumos_inode_file_write,
+};
+
+static void chromiumos_shutdown_securityfs(void)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(chromiumos_inode_policy_files); ++i) {
+		struct chromiumos_inode_policy_file_entry *entry =
+			&chromiumos_inode_policy_files[i];
+		securityfs_remove(entry->dentry);
+		entry->dentry = NULL;
+	}
+
+	securityfs_remove(chromiumos_inode_policy_dir);
+	chromiumos_inode_policy_dir = NULL;
+
+	securityfs_remove(chromiumos_dir);
+	chromiumos_dir = NULL;
+}
+
+static int chromiumos_init_securityfs(void)
+{
+	int i;
+	int ret;
+
+	chromiumos_dir = securityfs_create_dir("chromiumos", NULL);
+	if (!chromiumos_dir) {
+		ret = PTR_ERR(chromiumos_dir);
+		goto error;
+	}
+
+	chromiumos_inode_policy_dir =
+		securityfs_create_dir(
+			"inode_security_policies",
+			chromiumos_dir);
+	if (!chromiumos_inode_policy_dir) {
+		ret = PTR_ERR(chromiumos_inode_policy_dir);
+		goto error;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(chromiumos_inode_policy_files); ++i) {
+		struct chromiumos_inode_policy_file_entry *entry =
+			&chromiumos_inode_policy_files[i];
+		entry->dentry = securityfs_create_file(
+			entry->name, 0200, chromiumos_inode_policy_dir,
+			entry, &chromiumos_inode_policy_file_fops);
+		if (IS_ERR(entry->dentry)) {
+			ret = PTR_ERR(entry->dentry);
+			goto error;
+		}
+	}
+
+	return 0;
+
+error:
+	chromiumos_shutdown_securityfs();
+	return ret;
+}
+fs_initcall(chromiumos_init_securityfs);
diff -ruN a/security/chromiumos/third_party_whitelists.h b/security/chromiumos/third_party_whitelists.h
--- a/security/chromiumos/third_party_whitelists.h	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/third_party_whitelists.h	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,252 @@
+/*
+ * Linux Security Module for Chromium OS
+ *
+ * Copyright 2018 Google LLC. All Rights Reserved
+ *
+ * Authors:
+ *      Micah Morton <mortonm@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef THIRD_PARTY_WHITELISTS_H
+#define THIRD_PARTY_WHITELISTS_H
+
+/*
+ * NOTE: the purpose of this header is only to pull out the definition of this
+ * array from alt-syscall.c for the purposes of readability. It should not be
+ * included in other .c files.
+ */
+
+#include "alt-syscall.h"
+
+static struct syscall_whitelist_entry third_party_whitelist[] = {
+	SYSCALL_ENTRY(accept),
+	SYSCALL_ENTRY(bind),
+	SYSCALL_ENTRY(brk),
+	SYSCALL_ENTRY(chdir),
+	SYSCALL_ENTRY(clock_gettime),
+	SYSCALL_ENTRY(clone),
+	SYSCALL_ENTRY(close),
+	SYSCALL_ENTRY(connect),
+	SYSCALL_ENTRY(dup),
+	SYSCALL_ENTRY(execve),
+	SYSCALL_ENTRY(exit),
+	SYSCALL_ENTRY(exit_group),
+	SYSCALL_ENTRY(fcntl),
+	SYSCALL_ENTRY(fstat),
+	SYSCALL_ENTRY(futex),
+	SYSCALL_ENTRY(getcwd),
+	SYSCALL_ENTRY(getdents64),
+	SYSCALL_ENTRY(getpid),
+	SYSCALL_ENTRY(getpgid),
+	SYSCALL_ENTRY(getppid),
+	SYSCALL_ENTRY(getpriority),
+	SYSCALL_ENTRY(getrlimit),
+	SYSCALL_ENTRY(getsid),
+	SYSCALL_ENTRY(gettimeofday),
+	SYSCALL_ENTRY(ioctl),
+	SYSCALL_ENTRY(listen),
+	SYSCALL_ENTRY(lseek),
+	SYSCALL_ENTRY(madvise),
+        SYSCALL_ENTRY(memfd_create),
+	SYSCALL_ENTRY(mprotect),
+	SYSCALL_ENTRY(munmap),
+	SYSCALL_ENTRY(nanosleep),
+	SYSCALL_ENTRY(openat),
+	SYSCALL_ENTRY(prlimit64),
+	SYSCALL_ENTRY(read),
+	SYSCALL_ENTRY(recvfrom),
+	SYSCALL_ENTRY(recvmsg),
+	SYSCALL_ENTRY(rt_sigaction),
+	SYSCALL_ENTRY(rt_sigprocmask),
+	SYSCALL_ENTRY(rt_sigreturn),
+	SYSCALL_ENTRY(sendfile),
+	SYSCALL_ENTRY(sendmsg),
+	SYSCALL_ENTRY(sendto),
+	SYSCALL_ENTRY(set_robust_list),
+	SYSCALL_ENTRY(set_tid_address),
+	SYSCALL_ENTRY(setpgid),
+	SYSCALL_ENTRY(setpriority),
+	SYSCALL_ENTRY(setsid),
+	SYSCALL_ENTRY(setsockopt),
+	SYSCALL_ENTRY(socket),
+	SYSCALL_ENTRY(socketpair),
+	SYSCALL_ENTRY(syslog),
+	SYSCALL_ENTRY(statfs),
+	SYSCALL_ENTRY(umask),
+	SYSCALL_ENTRY(uname),
+	SYSCALL_ENTRY(wait4),
+	SYSCALL_ENTRY(write),
+	SYSCALL_ENTRY(writev),
+
+	/*
+	 * Deprecated syscalls which are not wired up on new architectures
+	 * such as ARM64.
+	 */
+#ifndef CONFIG_ARM64
+	SYSCALL_ENTRY(access),
+	SYSCALL_ENTRY(creat),
+	SYSCALL_ENTRY(dup2),
+	SYSCALL_ENTRY(getdents),
+	SYSCALL_ENTRY(getpgrp),
+	SYSCALL_ENTRY(lstat),
+	SYSCALL_ENTRY(mkdir),
+	SYSCALL_ENTRY(open),
+	SYSCALL_ENTRY(pipe),
+	SYSCALL_ENTRY(poll),
+	SYSCALL_ENTRY(readlink),
+	SYSCALL_ENTRY(stat),
+	SYSCALL_ENTRY(unlink),
+#endif
+
+	SYSCALL_ENTRY(accept),
+	SYSCALL_ENTRY(bind),
+	SYSCALL_ENTRY(connect),
+	SYSCALL_ENTRY(listen),
+	SYSCALL_ENTRY(recvfrom),
+	SYSCALL_ENTRY(recvmsg),
+	SYSCALL_ENTRY(sendmsg),
+	SYSCALL_ENTRY(sendto),
+	SYSCALL_ENTRY(setsockopt),
+	SYSCALL_ENTRY(socket),
+	SYSCALL_ENTRY(socketpair),
+
+	/* 64-bit only syscalls. */
+	SYSCALL_ENTRY(getegid),
+	SYSCALL_ENTRY(geteuid),
+	SYSCALL_ENTRY(getgid),
+	SYSCALL_ENTRY(getuid),
+	SYSCALL_ENTRY(mmap),
+	SYSCALL_ENTRY(setgid),
+	SYSCALL_ENTRY(setuid),
+	/*
+	 * chown(2), lchown(2), and select(2) are deprecated and not wired up
+	 * on ARM64.
+	 */
+#ifndef CONFIG_ARM64
+	SYSCALL_ENTRY(select),
+#endif
+
+	/* X86_64-specific syscalls. */
+#ifdef CONFIG_X86_64
+	SYSCALL_ENTRY(arch_prctl),
+#endif
+}; /* end third_party_whitelist */
+
+#ifdef CONFIG_COMPAT
+static struct syscall_whitelist_entry third_party_compat_whitelist[] = {
+	COMPAT_SYSCALL_ENTRY(access),
+	COMPAT_SYSCALL_ENTRY(brk),
+	COMPAT_SYSCALL_ENTRY(chdir),
+	COMPAT_SYSCALL_ENTRY(clock_gettime),
+	COMPAT_SYSCALL_ENTRY(clone),
+	COMPAT_SYSCALL_ENTRY(close),
+	COMPAT_SYSCALL_ENTRY(creat),
+	COMPAT_SYSCALL_ENTRY(dup),
+	COMPAT_SYSCALL_ENTRY(dup2),
+	COMPAT_SYSCALL_ENTRY(execve),
+	COMPAT_SYSCALL_ENTRY(exit),
+	COMPAT_SYSCALL_ENTRY(exit_group),
+	COMPAT_SYSCALL_ENTRY(fcntl),
+	COMPAT_SYSCALL_ENTRY(fcntl64),
+	COMPAT_SYSCALL_ENTRY(fstat),
+	COMPAT_SYSCALL_ENTRY(fstat64),
+	COMPAT_SYSCALL_ENTRY(futex),
+	COMPAT_SYSCALL_ENTRY(getcwd),
+	COMPAT_SYSCALL_ENTRY(getdents),
+	COMPAT_SYSCALL_ENTRY(getdents64),
+	COMPAT_SYSCALL_ENTRY(getegid),
+	COMPAT_SYSCALL_ENTRY(geteuid),
+	COMPAT_SYSCALL_ENTRY(geteuid32),
+	COMPAT_SYSCALL_ENTRY(getgid),
+	COMPAT_SYSCALL_ENTRY(getpgid),
+	COMPAT_SYSCALL_ENTRY(getpgrp),
+	COMPAT_SYSCALL_ENTRY(getpid),
+	COMPAT_SYSCALL_ENTRY(getpriority),
+	COMPAT_SYSCALL_ENTRY(getppid),
+	COMPAT_SYSCALL_ENTRY(getsid),
+	COMPAT_SYSCALL_ENTRY(gettimeofday),
+	COMPAT_SYSCALL_ENTRY(getuid),
+	COMPAT_SYSCALL_ENTRY(getuid32),
+	COMPAT_SYSCALL_ENTRY(ioctl),
+	COMPAT_SYSCALL_ENTRY(_llseek),
+	COMPAT_SYSCALL_ENTRY(lseek),
+	COMPAT_SYSCALL_ENTRY(lstat),
+	COMPAT_SYSCALL_ENTRY(lstat64),
+	COMPAT_SYSCALL_ENTRY(madvise),
+        COMPAT_SYSCALL_ENTRY(memfd_create),
+	COMPAT_SYSCALL_ENTRY(mkdir),
+	COMPAT_SYSCALL_ENTRY(mmap2),
+	COMPAT_SYSCALL_ENTRY(mprotect),
+	COMPAT_SYSCALL_ENTRY(munmap),
+	COMPAT_SYSCALL_ENTRY(nanosleep),
+	COMPAT_SYSCALL_ENTRY(_newselect),
+	COMPAT_SYSCALL_ENTRY(open),
+	COMPAT_SYSCALL_ENTRY(openat),
+	COMPAT_SYSCALL_ENTRY(pipe),
+	COMPAT_SYSCALL_ENTRY(poll),
+	COMPAT_SYSCALL_ENTRY(prlimit64),
+	COMPAT_SYSCALL_ENTRY(read),
+	COMPAT_SYSCALL_ENTRY(readlink),
+	COMPAT_SYSCALL_ENTRY(rt_sigaction),
+	COMPAT_SYSCALL_ENTRY(rt_sigprocmask),
+	COMPAT_SYSCALL_ENTRY(rt_sigreturn),
+	COMPAT_SYSCALL_ENTRY(sendfile),
+	COMPAT_SYSCALL_ENTRY(set_robust_list),
+	COMPAT_SYSCALL_ENTRY(set_tid_address),
+	COMPAT_SYSCALL_ENTRY(setgid32),
+	COMPAT_SYSCALL_ENTRY(setuid32),
+	COMPAT_SYSCALL_ENTRY(setpgid),
+	COMPAT_SYSCALL_ENTRY(setpriority),
+	COMPAT_SYSCALL_ENTRY(setsid),
+	COMPAT_SYSCALL_ENTRY(stat),
+	COMPAT_SYSCALL_ENTRY(stat64),
+	COMPAT_SYSCALL_ENTRY(statfs),
+	COMPAT_SYSCALL_ENTRY(syslog),
+	COMPAT_SYSCALL_ENTRY(ugetrlimit),
+	COMPAT_SYSCALL_ENTRY(umask),
+	COMPAT_SYSCALL_ENTRY(uname),
+	COMPAT_SYSCALL_ENTRY(unlink),
+	COMPAT_SYSCALL_ENTRY(wait4),
+	COMPAT_SYSCALL_ENTRY(write),
+	COMPAT_SYSCALL_ENTRY(writev),
+
+	/* IA32 uses the common socketcall(2) entrypoint for socket calls. */
+#ifdef CONFIG_X86_64
+	COMPAT_SYSCALL_ENTRY(socketcall),
+#endif
+
+#ifdef CONFIG_ARM64
+	COMPAT_SYSCALL_ENTRY(accept),
+	COMPAT_SYSCALL_ENTRY(bind),
+	COMPAT_SYSCALL_ENTRY(connect),
+	COMPAT_SYSCALL_ENTRY(listen),
+	COMPAT_SYSCALL_ENTRY(recvfrom),
+	COMPAT_SYSCALL_ENTRY(recvmsg),
+	COMPAT_SYSCALL_ENTRY(sendmsg),
+	COMPAT_SYSCALL_ENTRY(sendto),
+	COMPAT_SYSCALL_ENTRY(setsockopt),
+	COMPAT_SYSCALL_ENTRY(socket),
+	COMPAT_SYSCALL_ENTRY(socketpair),
+#endif
+
+	/*
+	 * getrlimit(2) is deprecated and not wired in the ARM compat table
+	 * on ARM64.
+	 */
+#ifndef CONFIG_ARM64
+	COMPAT_SYSCALL_ENTRY(getrlimit),
+#endif
+
+}; /* end third_party_compat_whitelist */
+#endif /* CONFIG_COMPAT */
+
+#endif /* THIRD_PARTY_WHITELISTS_H */
diff -ruN a/security/chromiumos/utils.c b/security/chromiumos/utils.c
--- a/security/chromiumos/utils.c	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/utils.c	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,157 @@
+/*
+ * Utilities for the Linux Security Module for Chromium OS
+ * (Since CONFIG_AUDIT is disabled for Chrome OS, we must repurpose
+ * a bunch of the audit string handling logic here instead.)
+ *
+ * Copyright 2012 Google Inc. All Rights Reserved
+ *
+ * Author:
+ *      Kees Cook       <keescook@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/module.h>
+#include <linux/sched/mm.h>
+#include <linux/security.h>
+
+#include "utils.h"
+
+/* Disallow double-quote and control characters other than space. */
+static int contains_unprintable(const char *source, size_t len)
+{
+	const unsigned char *p;
+	for (p = source; p < (const unsigned char *)source + len; p++) {
+		if (*p == '"' || *p < 0x20 || *p > 0x7e)
+			return 1;
+	}
+	return 0;
+}
+
+static char *hex_printable(const char *source, size_t len)
+{
+	size_t i;
+	char *dest, *ptr;
+	const char *hex = "0123456789ABCDEF";
+
+	/* Need to double the length of the string, plus a NULL. */
+	if (len > (INT_MAX - 1) / 2)
+		return NULL;
+	dest = kmalloc((len * 2) + 1, GFP_KERNEL);
+	if (!dest)
+		return NULL;
+
+	for (ptr = dest, i = 0; i < len; i++) {
+		*ptr++ = hex[(source[i] & 0xF0) >> 4];
+		*ptr++ = hex[source[i] & 0x0F];
+	}
+	*ptr = '\0';
+
+	return dest;
+}
+
+static char *quoted_printable(const char *source, size_t len)
+{
+	char *dest;
+
+	/* Need to add 2 double quotes and a NULL. */
+	if (len > INT_MAX - 3)
+		return NULL;
+	dest = kmalloc(len + 3, GFP_KERNEL);
+	if (!dest)
+		return NULL;
+
+	dest[0] = '"';
+	strncpy(dest + 1, source, len);
+	dest[len + 1] = '"';
+	dest[len + 2] = '\0';
+	return dest;
+}
+
+/* Return a string that has been sanitized and is safe to log. It is either
+ * in double-quotes, or is a series of hex digits.
+ */
+char *printable(char *source, size_t max_len)
+{
+	size_t len;
+
+	if (!source)
+		return NULL;
+
+	len = strnlen(source, max_len);
+	if (contains_unprintable(source, len))
+		return hex_printable(source, len);
+	else
+		return quoted_printable(source, len);
+}
+
+/* Repurposed from fs/proc/base.c, with NULL-replacement for saner printing.
+ * Allocates the buffer itself.
+ */
+char *printable_cmdline(struct task_struct *task)
+{
+	char *buffer = NULL, *sanitized;
+	int res, i;
+	unsigned int len;
+	struct mm_struct *mm;
+
+	mm = get_task_mm(task);
+	if (!mm)
+		goto out;
+
+	if (!mm->arg_end)
+		goto out_mm;	/* Shh! No looking before we're done */
+
+	buffer = kmalloc(PAGE_SIZE, GFP_KERNEL);
+	if (!buffer)
+		goto out_mm;
+
+	len = mm->arg_end - mm->arg_start;
+
+	if (len > PAGE_SIZE)
+		len = PAGE_SIZE;
+
+	res = access_process_vm(task, mm->arg_start, buffer, len, 0);
+
+	/* Space-fill NULLs. */
+	if (res > 1)
+		for (i = 0; i < res - 2; ++i)
+			if (buffer[i] == '\0')
+				buffer[i] = ' ';
+
+	/* If the NULL at the end of args has been overwritten, then
+	 * assume application is using setproctitle(3).
+	 */
+	if (res > 0 && buffer[res-1] != '\0' && len < PAGE_SIZE) {
+		len = strnlen(buffer, res);
+		if (len < res) {
+			res = len;
+		} else {
+			len = mm->env_end - mm->env_start;
+			if (len > PAGE_SIZE - res)
+				len = PAGE_SIZE - res;
+			res += access_process_vm(task, mm->env_start,
+						 buffer+res, len, 0);
+		}
+	}
+
+	/* Make sure the buffer is always NULL-terminated. */
+	buffer[PAGE_SIZE-1] = 0;
+
+	/* Make sure result is printable. */
+	sanitized = printable(buffer, res);
+	kfree(buffer);
+	buffer = sanitized;
+
+out_mm:
+	mmput(mm);
+out:
+	return buffer;
+}
diff -ruN a/security/chromiumos/utils.h b/security/chromiumos/utils.h
--- a/security/chromiumos/utils.h	1970-01-01 01:00:00.000000000 +0100
+++ b/security/chromiumos/utils.h	2021-12-23 08:36:01.000000000 +0100
@@ -0,0 +1,30 @@
+/*
+ * Utilities for the Linux Security Module for Chromium OS
+ * (Since CONFIG_AUDIT is disabled for Chrome OS, we must repurpose
+ * a bunch of the audit string handling logic here instead.)
+ *
+ * Copyright 2012 Google Inc. All Rights Reserved
+ *
+ * Author:
+ *      Kees Cook       <keescook@chromium.org>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _SECURITY_CHROMIUMOS_UTILS_H
+#define _SECURITY_CHROMIUMOS_UTILS_H
+
+#include <linux/sched.h>
+#include <linux/mm.h>
+
+char *printable(char *source, size_t max_len);
+char *printable_cmdline(struct task_struct *task);
+
+#endif /* _SECURITY_CHROMIUMOS_UTILS_H */
diff -ruN a/security/commoncap.c b/security/commoncap.c
--- a/security/commoncap.c	2021-12-08 09:04:57.000000000 +0100
+++ b/security/commoncap.c	2021-12-23 08:36:01.000000000 +0100
@@ -297,7 +297,8 @@
 	struct inode *inode = d_backing_inode(dentry);
 	int error;
 
-	error = __vfs_getxattr(dentry, inode, XATTR_NAME_CAPS, NULL, 0);
+	error = __vfs_getxattr(&init_user_ns, dentry, inode, XATTR_NAME_CAPS,
+			       NULL, 0, XATTR_NOSECURITY);
 	return error > 0;
 }
 
@@ -660,8 +661,9 @@
 		return -ENODATA;
 
 	fs_ns = inode->i_sb->s_user_ns;
-	size = __vfs_getxattr((struct dentry *)dentry, inode,
-			      XATTR_NAME_CAPS, &data, XATTR_CAPS_SZ);
+	size = __vfs_getxattr(&init_user_ns, (struct dentry *)dentry, inode,
+			      XATTR_NAME_CAPS, &data, XATTR_CAPS_SZ,
+			      XATTR_NOSECURITY);
 	if (size == -ENODATA || size == -EOPNOTSUPP)
 		/* no data, that's ok */
 		return -ENODATA;
diff -ruN a/security/integrity/evm/evm_main.c b/security/integrity/evm/evm_main.c
--- a/security/integrity/evm/evm_main.c	2021-12-08 09:04:57.000000000 +0100
+++ b/security/integrity/evm/evm_main.c	2021-12-23 08:36:01.000000000 +0100
@@ -145,7 +145,9 @@
 		return -EOPNOTSUPP;
 
 	list_for_each_entry_lockless(xattr, &evm_config_xattrnames, list) {
-		error = __vfs_getxattr(dentry, inode, xattr->name, NULL, 0);
+		error = __vfs_getxattr(&init_user_ns, dentry, inode,
+				       xattr->name, NULL, 0,
+				       XATTR_NOSECURITY);
 		if (error < 0) {
 			if (error == -ENODATA)
 				continue;
@@ -343,8 +345,9 @@
 	int rc, size, total_size = 0;
 
 	list_for_each_entry_lockless(xattr, &evm_config_xattrnames, list) {
-		rc = __vfs_getxattr(dentry, d_backing_inode(dentry),
-				    xattr->name, NULL, 0);
+		rc = __vfs_getxattr(&init_user_ns, dentry,
+				    d_backing_inode(dentry),
+				    xattr->name, NULL, 0, 0);
 		if (rc < 0 && rc == -ENODATA)
 			continue;
 		else if (rc < 0)
@@ -372,10 +375,10 @@
 		case 'v':
 			size = rc;
 			if (buffer) {
-				rc = __vfs_getxattr(dentry,
+				rc = __vfs_getxattr(&init_user_ns, dentry,
 					d_backing_inode(dentry), xattr->name,
 					buffer + total_size,
-					buffer_size - total_size);
+					buffer_size - total_size, 0);
 				if (rc < 0)
 					return rc;
 			}
diff -ruN a/security/Kconfig b/security/Kconfig
--- a/security/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/security/Kconfig	2021-12-23 08:36:01.000000000 +0100
@@ -191,9 +191,6 @@
 config FORTIFY_SOURCE
 	bool "Harden common str/mem functions against buffer overflows"
 	depends on ARCH_HAS_FORTIFY_SOURCE
-	# https://bugs.llvm.org/show_bug.cgi?id=50322
-	# https://bugs.llvm.org/show_bug.cgi?id=41459
-	depends on !CC_IS_CLANG
 	help
 	  Detect overflows of buffers in common string and memory functions
 	  where the compiler can determine and validate the buffer sizes.
@@ -242,11 +239,13 @@
 source "security/safesetid/Kconfig"
 source "security/lockdown/Kconfig"
 source "security/landlock/Kconfig"
+source "security/chromiumos/Kconfig"
 
 source "security/integrity/Kconfig"
 
 choice
 	prompt "First legacy 'major LSM' to be initialized"
+	default DEFAULT_SECURITY_CHROMIUMOS if SECURITY_CHROMIUMOS
 	default DEFAULT_SECURITY_SELINUX if SECURITY_SELINUX
 	default DEFAULT_SECURITY_SMACK if SECURITY_SMACK
 	default DEFAULT_SECURITY_TOMOYO if SECURITY_TOMOYO
@@ -262,6 +261,9 @@
 	  Selects the legacy "major security module" that will be
 	  initialized first. Overridden by non-default CONFIG_LSM.
 
+	config DEFAULT_SECURITY_CHROMIUMOS
+		bool "Chromium OS" if SECURITY_CHROMIUMOS=y
+
 	config DEFAULT_SECURITY_SELINUX
 		bool "SELinux" if SECURITY_SELINUX=y
 
@@ -285,6 +287,7 @@
 	default "landlock,lockdown,yama,loadpin,safesetid,integrity,apparmor,selinux,smack,tomoyo,bpf" if DEFAULT_SECURITY_APPARMOR
 	default "landlock,lockdown,yama,loadpin,safesetid,integrity,tomoyo,bpf" if DEFAULT_SECURITY_TOMOYO
 	default "landlock,lockdown,yama,loadpin,safesetid,integrity,bpf" if DEFAULT_SECURITY_DAC
+	default "landlock,lockdown,yama,loadpin,safesetid,integrity,chromiumos,selinux,bpf" if DEFAULT_SECURITY_CHROMIUMOS
 	default "landlock,lockdown,yama,loadpin,safesetid,integrity,selinux,smack,tomoyo,apparmor,bpf"
 	help
 	  A comma-separated list of LSMs, in initialization order.
@@ -295,5 +298,13 @@
 
 source "security/Kconfig.hardening"
 
-endmenu
+config ARCH_HAS_ALT_SYSCALL
+	def_bool n
 
+config ALT_SYSCALL
+	bool "Alternate syscall table support"
+	depends on ARCH_HAS_ALT_SYSCALL
+	help
+	  Allow syscall table to be swapped on a running process.
+
+endmenu
diff -ruN a/security/lsm_audit.c b/security/lsm_audit.c
--- a/security/lsm_audit.c	2021-12-08 09:04:57.000000000 +0100
+++ b/security/lsm_audit.c	2021-12-23 08:36:01.000000000 +0100
@@ -215,7 +215,7 @@
 	 * start making this union too large!  See struct lsm_network_audit
 	 * as an example of how to deal with large data.
 	 */
-	BUILD_BUG_ON(sizeof(a->u) > sizeof(void *)*2);
+	BUILD_BUG_ON(sizeof(a->u) > sizeof(void *)*3);
 
 	audit_log_format(ab, " pid=%d comm=", task_tgid_nr(current));
 	audit_log_untrustedstring(ab, memcpy(comm, current->comm, sizeof(comm)));
diff -ruN a/security/Makefile b/security/Makefile
--- a/security/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/security/Makefile	2021-12-23 08:36:01.000000000 +0100
@@ -12,6 +12,7 @@
 # Object file lists
 obj-$(CONFIG_SECURITY)			+= security.o
 obj-$(CONFIG_SECURITYFS)		+= inode.o
+obj-$(CONFIG_SECURITY_CHROMIUMOS)	+= chromiumos/
 obj-$(CONFIG_SECURITY_SELINUX)		+= selinux/
 obj-$(CONFIG_SECURITY_SMACK)		+= smack/
 obj-$(CONFIG_SECURITY)			+= lsm_audit.o
diff -ruN a/security/security.c b/security/security.c
--- a/security/security.c	2021-12-08 09:04:57.000000000 +0100
+++ b/security/security.c	2021-12-23 08:36:01.000000000 +0100
@@ -1212,6 +1212,7 @@
 		return 0;
 	return call_int_hook(path_chown, 0, path, uid, gid);
 }
+EXPORT_SYMBOL(security_path_chown);
 
 int security_path_chroot(const struct path *path)
 {
diff -ruN a/security/selinux/hooks.c b/security/selinux/hooks.c
--- a/security/selinux/hooks.c	2021-12-08 09:04:57.000000000 +0100
+++ b/security/selinux/hooks.c	2021-12-23 08:36:01.000000000 +0100
@@ -511,7 +511,8 @@
 		goto fallback;
 	}
 
-	rc = __vfs_getxattr(root, root_inode, XATTR_NAME_SELINUX, NULL, 0);
+	rc = __vfs_getxattr(&init_user_ns, root, root_inode,
+			    XATTR_NAME_SELINUX, NULL, 0, XATTR_NOSECURITY);
 	if (rc < 0 && rc != -ENODATA) {
 		if (rc == -EOPNOTSUPP) {
 			pr_warn("SELinux: (dev %s, type %s) has no security xattr handler\n",
@@ -1370,12 +1371,16 @@
 		return -ENOMEM;
 
 	context[len] = '\0';
-	rc = __vfs_getxattr(dentry, inode, XATTR_NAME_SELINUX, context, len);
+	rc = __vfs_getxattr(&init_user_ns, dentry, inode,
+			    XATTR_NAME_SELINUX, context, len,
+			    XATTR_NOSECURITY);
 	if (rc == -ERANGE) {
 		kfree(context);
 
 		/* Need a larger buffer.  Query for the right size. */
-		rc = __vfs_getxattr(dentry, inode, XATTR_NAME_SELINUX, NULL, 0);
+		rc = __vfs_getxattr(&init_user_ns, dentry, inode,
+				    XATTR_NAME_SELINUX, NULL, 0,
+				    XATTR_NOSECURITY);
 		if (rc < 0)
 			return rc;
 
@@ -1385,8 +1390,9 @@
 			return -ENOMEM;
 
 		context[len] = '\0';
-		rc = __vfs_getxattr(dentry, inode, XATTR_NAME_SELINUX,
-				    context, len);
+		rc = __vfs_getxattr(&init_user_ns, dentry, inode,
+				    XATTR_NAME_SELINUX, context, len,
+				    XATTR_NOSECURITY);
 	}
 	if (rc < 0) {
 		kfree(context);
diff -ruN a/security/selinux/include/classmap.h b/security/selinux/include/classmap.h
--- a/security/selinux/include/classmap.h	2021-12-08 09:04:57.000000000 +0100
+++ b/security/selinux/include/classmap.h	2021-12-23 08:36:01.000000000 +0100
@@ -117,7 +117,8 @@
 	  { COMMON_IPC_PERMS, NULL } },
 	{ "netlink_route_socket",
 	  { COMMON_SOCK_PERMS,
-	    "nlmsg_read", "nlmsg_write", NULL } },
+	    "nlmsg_read", "nlmsg_write", "nlmsg_readpriv", "nlmsg_getneigh",
+	    NULL } },
 	{ "netlink_tcpdiag_socket",
 	  { COMMON_SOCK_PERMS,
 	    "nlmsg_read", "nlmsg_write", NULL } },
diff -ruN a/security/selinux/include/security.h b/security/selinux/include/security.h
--- a/security/selinux/include/security.h	2021-12-08 09:04:57.000000000 +0100
+++ b/security/selinux/include/security.h	2021-12-23 08:36:01.000000000 +0100
@@ -97,6 +97,8 @@
 	bool checkreqprot;
 	bool initialized;
 	bool policycap[__POLICYDB_CAPABILITY_MAX];
+	bool android_netlink_route;
+	bool android_netlink_getneigh;
 
 	struct page *status_page;
 	struct mutex status_lock;
@@ -219,6 +221,20 @@
 	return READ_ONCE(state->policycap[POLICYDB_CAPABILITY_GENFS_SECLABEL_SYMLINKS]);
 }
 
+static inline bool selinux_android_nlroute_getlink(void)
+{
+	struct selinux_state *state = &selinux_state;
+
+	return state->android_netlink_route;
+}
+
+static inline bool selinux_android_nlroute_getneigh(void)
+{
+	struct selinux_state *state = &selinux_state;
+
+	return state->android_netlink_getneigh;
+}
+
 struct selinux_policy_convert_data;
 
 struct selinux_load_state {
@@ -452,5 +468,6 @@
 extern void ebitmap_cache_init(void);
 extern void hashtab_cache_init(void);
 extern int security_sidtab_hash_stats(struct selinux_state *state, char *page);
+extern void selinux_nlmsg_init(void);
 
 #endif /* _SELINUX_SECURITY_H_ */
diff -ruN a/security/selinux/nlmsgtab.c b/security/selinux/nlmsgtab.c
--- a/security/selinux/nlmsgtab.c	2021-12-08 09:04:57.000000000 +0100
+++ b/security/selinux/nlmsgtab.c	2021-12-23 08:36:01.000000000 +0100
@@ -25,7 +25,7 @@
 	u32	perm;
 };
 
-static const struct nlmsg_perm nlmsg_route_perms[] =
+static struct nlmsg_perm nlmsg_route_perms[] =
 {
 	{ RTM_NEWLINK,		NETLINK_ROUTE_SOCKET__NLMSG_WRITE },
 	{ RTM_DELLINK,		NETLINK_ROUTE_SOCKET__NLMSG_WRITE },
@@ -216,3 +216,43 @@
 
 	return err;
 }
+
+static void nlmsg_set_perm_for_type(u32 perm, u16 type)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(nlmsg_route_perms); i++) {
+		if (nlmsg_route_perms[i].nlmsg_type == type) {
+			nlmsg_route_perms[i].perm = perm;
+			break;
+		}
+	}
+}
+
+/**
+ * Use nlmsg_readpriv as the permission for RTM_GETLINK messages if the
+ * netlink_route_getlink policy capability is set. Otherwise use nlmsg_read.
+ * Similarly, use nlmsg_getneigh for RTM_GETNEIGH and RTM_GETNEIGHTBL if the
+ * netlink_route_getneigh policy capability is set. Otherwise use nlmsg_read.
+ */
+void selinux_nlmsg_init(void)
+{
+	if (selinux_android_nlroute_getlink())
+		nlmsg_set_perm_for_type(NETLINK_ROUTE_SOCKET__NLMSG_READPRIV,
+					RTM_GETLINK);
+	else
+		nlmsg_set_perm_for_type(NETLINK_ROUTE_SOCKET__NLMSG_READ,
+					RTM_GETLINK);
+
+	if (selinux_android_nlroute_getneigh()) {
+		nlmsg_set_perm_for_type(NETLINK_ROUTE_SOCKET__NLMSG_GETNEIGH,
+					RTM_GETNEIGH);
+		nlmsg_set_perm_for_type(NETLINK_ROUTE_SOCKET__NLMSG_GETNEIGH,
+					RTM_GETNEIGHTBL);
+	} else {
+		nlmsg_set_perm_for_type(NETLINK_ROUTE_SOCKET__NLMSG_READ,
+					RTM_GETNEIGH);
+		nlmsg_set_perm_for_type(NETLINK_ROUTE_SOCKET__NLMSG_READ,
+					RTM_GETNEIGHTBL);
+	}
+}
diff -ruN a/security/selinux/ss/policydb.c b/security/selinux/ss/policydb.c
--- a/security/selinux/ss/policydb.c	2021-12-08 09:04:57.000000000 +0100
+++ b/security/selinux/ss/policydb.c	2021-12-23 08:36:01.000000000 +0100
@@ -2491,6 +2491,14 @@
 	p->reject_unknown = !!(le32_to_cpu(buf[1]) & REJECT_UNKNOWN);
 	p->allow_unknown = !!(le32_to_cpu(buf[1]) & ALLOW_UNKNOWN);
 
+	if ((le32_to_cpu(buf[1]) & POLICYDB_CONFIG_ANDROID_NETLINK_ROUTE)) {
+		p->android_netlink_route = 1;
+	}
+
+	if ((le32_to_cpu(buf[1]) & POLICYDB_CONFIG_ANDROID_NETLINK_GETNEIGH)) {
+		p->android_netlink_getneigh = 1;
+	}
+
 	if (p->policyvers >= POLICYDB_VERSION_POLCAP) {
 		rc = ebitmap_read(&p->policycaps, fp);
 		if (rc)
diff -ruN a/security/selinux/ss/policydb.h b/security/selinux/ss/policydb.h
--- a/security/selinux/ss/policydb.h	2021-12-08 09:04:57.000000000 +0100
+++ b/security/selinux/ss/policydb.h	2021-12-23 08:36:01.000000000 +0100
@@ -238,6 +238,8 @@
 /* The policy database */
 struct policydb {
 	int mls_enabled;
+	int android_netlink_route;
+	int android_netlink_getneigh;
 
 	/* symbol tables */
 	struct symtab symtab[SYM_NUM];
@@ -334,6 +336,8 @@
 	struct policydb *p, struct role_trans_key *key);
 
 #define POLICYDB_CONFIG_MLS    1
+#define POLICYDB_CONFIG_ANDROID_NETLINK_ROUTE    (1 << 31)
+#define POLICYDB_CONFIG_ANDROID_NETLINK_GETNEIGH (1 << 30)
 
 /* the config flags related to unknown classes/perms are bits 2 and 3 */
 #define REJECT_UNKNOWN	0x00000002
diff -ruN a/security/selinux/ss/services.c b/security/selinux/ss/services.c
--- a/security/selinux/ss/services.c	2021-12-08 09:04:57.000000000 +0100
+++ b/security/selinux/ss/services.c	2021-12-23 08:36:01.000000000 +0100
@@ -2164,6 +2164,10 @@
 			pr_info("SELinux:  unknown policy capability %u\n",
 				i);
 	}
+
+	state->android_netlink_route = p->android_netlink_route;
+	state->android_netlink_getneigh = p->android_netlink_getneigh;
+	selinux_nlmsg_init();
 }
 
 static int security_preserve_bools(struct selinux_policy *oldpolicy,
diff -ruN a/security/smack/smack_lsm.c b/security/smack/smack_lsm.c
--- a/security/smack/smack_lsm.c	2021-12-08 09:04:57.000000000 +0100
+++ b/security/smack/smack_lsm.c	2021-12-23 08:36:01.000000000 +0100
@@ -289,7 +289,8 @@
 	if (buffer == NULL)
 		return ERR_PTR(-ENOMEM);
 
-	rc = __vfs_getxattr(dp, ip, name, buffer, SMK_LONGLABEL);
+	rc = __vfs_getxattr(&init_user_ns, dp, ip, name, buffer, SMK_LONGLABEL,
+			    XATTR_NOSECURITY);
 	if (rc < 0)
 		skp = ERR_PTR(rc);
 	else if (rc == 0)
@@ -3429,9 +3430,9 @@
 					TRANS_TRUE, TRANS_TRUE_SIZE,
 					0);
 			} else {
-				rc = __vfs_getxattr(dp, inode,
+				rc = __vfs_getxattr(&init_user_ns, dp, inode,
 					XATTR_NAME_SMACKTRANSMUTE, trattr,
-					TRANS_TRUE_SIZE);
+					TRANS_TRUE_SIZE, XATTR_NOSECURITY);
 				if (rc >= 0 && strncmp(trattr, TRANS_TRUE,
 						       TRANS_TRUE_SIZE) != 0)
 					rc = -EINVAL;
diff -ruN a/sound/soc/codecs/cs42l42.c b/sound/soc/codecs/cs42l42.c
--- a/sound/soc/codecs/cs42l42.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/cs42l42.c	2021-12-23 08:36:02.000000000 +0100
@@ -851,7 +851,7 @@
 	if (params_width(params) == 24)
 		cs42l42->bclk = (cs42l42->bclk / 3) * 4;
 
-	switch(substream->stream) {
+	switch (substream->stream) {
 	case SNDRV_PCM_STREAM_CAPTURE:
 		/* channel 2 on high LRCLK */
 		val = CS42L42_ASP_TX_CH2_AP_MASK |
@@ -933,7 +933,7 @@
 						      CS42L42_HP_ANA_BMUTE_MASK);
 
 		cs42l42->stream_use &= ~(1 << stream);
-		if(!cs42l42->stream_use) {
+		if (!cs42l42->stream_use) {
 			/*
 			 * Switch to the internal oscillator.
 			 * SCLK must remain running until after this clock switch.
@@ -1004,7 +1004,7 @@
 
 #define CS42L42_FORMATS (SNDRV_PCM_FMTBIT_S16_LE |\
 			 SNDRV_PCM_FMTBIT_S24_LE |\
-			 SNDRV_PCM_FMTBIT_S32_LE )
+			 SNDRV_PCM_FMTBIT_S32_LE)
 
 static const struct snd_soc_dai_ops cs42l42_ops = {
 	.startup	= cs42l42_dai_startup,
@@ -1035,11 +1035,121 @@
 		.ops = &cs42l42_ops,
 };
 
+static void cs42l42_manual_hs_type_detect(struct cs42l42_private *cs42l42)
+{
+	unsigned int hs_det_status;
+	unsigned int hs_det_comp1;
+	unsigned int hs_det_comp2;
+	unsigned int hs_det_sw;
+
+	/* Set hs detect to manual, active mode */
+	regmap_update_bits(cs42l42->regmap,
+		CS42L42_HSDET_CTL2,
+		CS42L42_HSDET_CTRL_MASK |
+		CS42L42_HSDET_SET_MASK |
+		CS42L42_HSBIAS_REF_MASK |
+		CS42L42_HSDET_AUTO_TIME_MASK,
+		(1 << CS42L42_HSDET_CTRL_SHIFT) |
+		(0 << CS42L42_HSDET_SET_SHIFT) |
+		(0 << CS42L42_HSBIAS_REF_SHIFT) |
+		(0 << CS42L42_HSDET_AUTO_TIME_SHIFT));
+
+	/* Configure HS DET comparator reference levels. */
+	regmap_update_bits(cs42l42->regmap,
+				CS42L42_HSDET_CTL1,
+				CS42L42_HSDET_COMP1_LVL_MASK |
+				CS42L42_HSDET_COMP2_LVL_MASK,
+				(CS42L42_HSDET_COMP1_LVL_VAL << CS42L42_HSDET_COMP1_LVL_SHIFT) |
+				(CS42L42_HSDET_COMP2_LVL_VAL << CS42L42_HSDET_COMP2_LVL_SHIFT));
+
+	/* Open the SW_HSB_HS3 switch and close SW_HSB_HS4 for a Type 1 headset. */
+	regmap_write(cs42l42->regmap, CS42L42_HS_SWITCH_CTL, CS42L42_HSDET_SW_COMP1);
+
+	msleep(100);
+
+	regmap_read(cs42l42->regmap, CS42L42_HS_DET_STATUS, &hs_det_status);
+
+	hs_det_comp1 = (hs_det_status & CS42L42_HSDET_COMP1_OUT_MASK) >>
+			CS42L42_HSDET_COMP1_OUT_SHIFT;
+	hs_det_comp2 = (hs_det_status & CS42L42_HSDET_COMP2_OUT_MASK) >>
+			CS42L42_HSDET_COMP2_OUT_SHIFT;
+
+	/* Close the SW_HSB_HS3 switch for a Type 2 headset. */
+	regmap_write(cs42l42->regmap, CS42L42_HS_SWITCH_CTL, CS42L42_HSDET_SW_COMP2);
+
+	msleep(100);
+
+	regmap_read(cs42l42->regmap, CS42L42_HS_DET_STATUS, &hs_det_status);
+
+	hs_det_comp1 |= ((hs_det_status & CS42L42_HSDET_COMP1_OUT_MASK) >>
+			CS42L42_HSDET_COMP1_OUT_SHIFT) << 1;
+	hs_det_comp2 |= ((hs_det_status & CS42L42_HSDET_COMP2_OUT_MASK) >>
+			CS42L42_HSDET_COMP2_OUT_SHIFT) << 1;
+
+	/* Use Comparator 1 with 1.25V Threshold. */
+	switch (hs_det_comp1) {
+	case CS42L42_HSDET_COMP_TYPE1:
+		cs42l42->hs_type = CS42L42_PLUG_CTIA;
+		hs_det_sw = CS42L42_HSDET_SW_TYPE1;
+		break;
+	case CS42L42_HSDET_COMP_TYPE2:
+		cs42l42->hs_type = CS42L42_PLUG_OMTP;
+		hs_det_sw = CS42L42_HSDET_SW_TYPE2;
+		break;
+	default:
+		/* Fallback to Comparator 2 with 1.75V Threshold. */
+		switch (hs_det_comp2) {
+		case CS42L42_HSDET_COMP_TYPE1:
+			cs42l42->hs_type = CS42L42_PLUG_CTIA;
+			hs_det_sw = CS42L42_HSDET_SW_TYPE1;
+			break;
+		case CS42L42_HSDET_COMP_TYPE2:
+			cs42l42->hs_type = CS42L42_PLUG_OMTP;
+			hs_det_sw = CS42L42_HSDET_SW_TYPE2;
+			break;
+		case CS42L42_HSDET_COMP_TYPE3:
+			cs42l42->hs_type = CS42L42_PLUG_HEADPHONE;
+			hs_det_sw = CS42L42_HSDET_SW_TYPE3;
+			break;
+		default:
+			cs42l42->hs_type = CS42L42_PLUG_INVALID;
+			hs_det_sw = CS42L42_HSDET_SW_TYPE4;
+			break;
+		}
+	}
+
+	/* Set Switches */
+	regmap_write(cs42l42->regmap, CS42L42_HS_SWITCH_CTL, hs_det_sw);
+
+	/* Set HSDET mode to ManualDisabled */
+	regmap_update_bits(cs42l42->regmap,
+		CS42L42_HSDET_CTL2,
+		CS42L42_HSDET_CTRL_MASK |
+		CS42L42_HSDET_SET_MASK |
+		CS42L42_HSBIAS_REF_MASK |
+		CS42L42_HSDET_AUTO_TIME_MASK,
+		(0 << CS42L42_HSDET_CTRL_SHIFT) |
+		(0 << CS42L42_HSDET_SET_SHIFT) |
+		(0 << CS42L42_HSBIAS_REF_SHIFT) |
+		(0 << CS42L42_HSDET_AUTO_TIME_SHIFT));
+
+	/* Configure HS DET comparator reference levels. */
+	regmap_update_bits(cs42l42->regmap,
+				CS42L42_HSDET_CTL1,
+				CS42L42_HSDET_COMP1_LVL_MASK |
+				CS42L42_HSDET_COMP2_LVL_MASK,
+				(CS42L42_HSDET_COMP1_LVL_DEFAULT << CS42L42_HSDET_COMP1_LVL_SHIFT) |
+				(CS42L42_HSDET_COMP2_LVL_DEFAULT << CS42L42_HSDET_COMP2_LVL_SHIFT));
+}
+
 static void cs42l42_process_hs_type_detect(struct cs42l42_private *cs42l42)
 {
 	unsigned int hs_det_status;
 	unsigned int int_status;
 
+	/* Read and save the hs detection result */
+	regmap_read(cs42l42->regmap, CS42L42_HS_DET_STATUS, &hs_det_status);
+
 	/* Mask the auto detect interrupt */
 	regmap_update_bits(cs42l42->regmap,
 		CS42L42_CODEC_INT_MASK,
@@ -1048,6 +1158,10 @@
 		(1 << CS42L42_PDN_DONE_SHIFT) |
 		(1 << CS42L42_HSDET_AUTO_DONE_SHIFT));
 
+
+	cs42l42->hs_type = (hs_det_status & CS42L42_HSDET_TYPE_MASK) >>
+				CS42L42_HSDET_TYPE_SHIFT;
+
 	/* Set hs detect to automatic, disabled mode */
 	regmap_update_bits(cs42l42->regmap,
 		CS42L42_HSDET_CTL2,
@@ -1060,11 +1174,15 @@
 		(0 << CS42L42_HSBIAS_REF_SHIFT) |
 		(3 << CS42L42_HSDET_AUTO_TIME_SHIFT));
 
-	/* Read and save the hs detection result */
-	regmap_read(cs42l42->regmap, CS42L42_HS_DET_STATUS, &hs_det_status);
-
-	cs42l42->hs_type = (hs_det_status & CS42L42_HSDET_TYPE_MASK) >>
-				CS42L42_HSDET_TYPE_SHIFT;
+	/* Run Manual detection if auto detect has not found a headset.
+	 * We Re-Run with Manual Detection if the original detection was invalid or headphones,
+	 * to ensure that a headset mic is detected in all cases.
+	 */
+	if (cs42l42->hs_type == CS42L42_PLUG_INVALID ||
+		cs42l42->hs_type == CS42L42_PLUG_HEADPHONE) {
+		dev_dbg(cs42l42->component->dev, "Running Manual Detection Fallback\n");
+		cs42l42_manual_hs_type_detect(cs42l42);
+	}
 
 	/* Set up button detection */
 	if ((cs42l42->hs_type == CS42L42_PLUG_CTIA) ||
@@ -1481,7 +1599,7 @@
 	if ((~masks[5]) & irq_params_table[5].mask) {
 		if (stickies[5] & CS42L42_HSDET_AUTO_DONE_MASK) {
 			cs42l42_process_hs_type_detect(cs42l42);
-			switch(cs42l42->hs_type){
+			switch (cs42l42->hs_type) {
 			case CS42L42_PLUG_CTIA:
 			case CS42L42_PLUG_OMTP:
 				snd_soc_jack_report(cs42l42->jack, SND_JACK_HEADSET,
@@ -1513,7 +1631,7 @@
 				cs42l42->plug_state = CS42L42_TS_UNPLUG;
 				cs42l42_cancel_hs_type_detect(cs42l42);
 
-				switch(cs42l42->hs_type){
+				switch (cs42l42->hs_type) {
 				case CS42L42_PLUG_CTIA:
 				case CS42L42_PLUG_OMTP:
 					snd_soc_jack_report(cs42l42->jack, 0, SND_JACK_HEADSET);
diff -ruN a/sound/soc/codecs/cs42l42.h b/sound/soc/codecs/cs42l42.h
--- a/sound/soc/codecs/cs42l42.h	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/cs42l42.h	2021-12-23 08:36:02.000000000 +0100
@@ -188,6 +188,11 @@
 #define CS42L42_HSDET_COMP2_LVL_SHIFT	4
 #define CS42L42_HSDET_COMP2_LVL_MASK	(15 << CS42L42_HSDET_COMP2_LVL_SHIFT)
 
+#define CS42L42_HSDET_COMP1_LVL_VAL	12 /* 1.25V Comparator */
+#define CS42L42_HSDET_COMP2_LVL_VAL	2  /* 1.75V Comparator */
+#define CS42L42_HSDET_COMP1_LVL_DEFAULT	7  /* 1V Comparator */
+#define CS42L42_HSDET_COMP2_LVL_DEFAULT	7  /* 2V Comparator */
+
 #define CS42L42_HSDET_CTL2		(CS42L42_PAGE_11 + 0x20)
 #define CS42L42_HSDET_AUTO_TIME_SHIFT	0
 #define CS42L42_HSDET_AUTO_TIME_MASK	(3 << CS42L42_HSDET_AUTO_TIME_SHIFT)
@@ -228,6 +233,60 @@
 #define CS42L42_PLUG_HEADPHONE		2
 #define CS42L42_PLUG_INVALID		3
 
+#define CS42L42_HSDET_SW_COMP1		((0 << CS42L42_SW_GNDHS_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_GNDHS_HS3_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_HS4_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_HS3_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_FILT_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_FILT_HS3_SHIFT) | \
+					 (0 << CS42L42_SW_REF_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_REF_HS3_SHIFT))
+#define CS42L42_HSDET_SW_COMP2		((1 << CS42L42_SW_GNDHS_HS4_SHIFT) | \
+					 (0 << CS42L42_SW_GNDHS_HS3_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_HS3_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_FILT_HS4_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_FILT_HS3_SHIFT) | \
+					 (1 << CS42L42_SW_REF_HS4_SHIFT) | \
+					 (0 << CS42L42_SW_REF_HS3_SHIFT))
+#define CS42L42_HSDET_SW_TYPE1		((0 << CS42L42_SW_GNDHS_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_GNDHS_HS3_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_HS4_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_HS3_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_FILT_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_FILT_HS3_SHIFT) | \
+					 (0 << CS42L42_SW_REF_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_REF_HS3_SHIFT))
+#define CS42L42_HSDET_SW_TYPE2		((1 << CS42L42_SW_GNDHS_HS4_SHIFT) | \
+					 (0 << CS42L42_SW_GNDHS_HS3_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_HS3_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_FILT_HS4_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_FILT_HS3_SHIFT) | \
+					 (1 << CS42L42_SW_REF_HS4_SHIFT) | \
+					 (0 << CS42L42_SW_REF_HS3_SHIFT))
+#define CS42L42_HSDET_SW_TYPE3		((1 << CS42L42_SW_GNDHS_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_GNDHS_HS3_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_HS4_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_HS3_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_FILT_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_FILT_HS3_SHIFT) | \
+					 (1 << CS42L42_SW_REF_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_REF_HS3_SHIFT))
+#define CS42L42_HSDET_SW_TYPE4		((0 << CS42L42_SW_GNDHS_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_GNDHS_HS3_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_HS4_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_HS3_SHIFT) | \
+					 (0 << CS42L42_SW_HSB_FILT_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_HSB_FILT_HS3_SHIFT) | \
+					 (0 << CS42L42_SW_REF_HS4_SHIFT) | \
+					 (1 << CS42L42_SW_REF_HS3_SHIFT))
+
+#define CS42L42_HSDET_COMP_TYPE1	1
+#define CS42L42_HSDET_COMP_TYPE2	2
+#define CS42L42_HSDET_COMP_TYPE3	0
+#define CS42L42_HSDET_COMP_TYPE4	3
+
 #define CS42L42_HS_CLAMP_DISABLE	(CS42L42_PAGE_11 + 0x29)
 #define CS42L42_HS_CLAMP_DISABLE_SHIFT	0
 #define CS42L42_HS_CLAMP_DISABLE_MASK	(1 << CS42L42_HS_CLAMP_DISABLE_SHIFT)
diff -ruN a/sound/soc/codecs/da7219.c b/sound/soc/codecs/da7219.c
--- a/sound/soc/codecs/da7219.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/da7219.c	2021-12-23 08:36:02.000000000 +0100
@@ -1776,6 +1776,8 @@
 			 pdata->dai_clk_names[DA7219_DAI_WCLK_IDX],
 			 pdata->dai_clk_names[DA7219_DAI_BCLK_IDX]);
 
+	device_property_read_string(dev, "dlg,mclk-name", &pdata->mclk_name);
+
 	if (device_property_read_u32(dev, "dlg,micbias-lvl", &of_val32) >= 0)
 		pdata->micbias_lvl = da7219_fw_micbias_lvl(dev, of_val32);
 	else
@@ -2514,7 +2516,11 @@
 	da7219_handle_pdata(component);
 
 	/* Check if MCLK provided */
-	da7219->mclk = clk_get(component->dev, "mclk");
+	if (da7219->pdata->mclk_name)
+		da7219->mclk = clk_get(NULL, da7219->pdata->mclk_name);
+	if (!da7219->mclk)
+		da7219->mclk = clk_get(component->dev, "mclk");
+
 	if (IS_ERR(da7219->mclk)) {
 		if (PTR_ERR(da7219->mclk) != -ENOENT) {
 			ret = PTR_ERR(da7219->mclk);
@@ -2590,6 +2596,9 @@
 	da7219_free_dai_clks(component);
 	clk_put(da7219->mclk);
 
+	if (da7219->pdata->mclk_name)
+		clk_put(da7219->mclk);
+
 	/* Supplies */
 	regulator_bulk_disable(DA7219_NUM_SUPPLIES, da7219->supplies);
 	regulator_bulk_free(DA7219_NUM_SUPPLIES, da7219->supplies);
diff -ruN a/sound/soc/codecs/hdmi-codec.c b/sound/soc/codecs/hdmi-codec.c
--- a/sound/soc/codecs/hdmi-codec.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/hdmi-codec.c	2021-12-23 08:36:02.000000000 +0100
@@ -685,6 +685,35 @@
 	return -ENOTSUPP;
 }
 
+static int hdmi_codec_trigger(struct snd_pcm_substream *substream, int cmd,
+			      struct snd_soc_dai *dai)
+{
+	struct hdmi_codec_priv *hcp = snd_soc_dai_get_drvdata(dai);
+	int event;
+
+	if (!hcp->hcd.ops->trigger)
+		return 0;
+
+	switch (cmd) {
+	case SNDRV_PCM_TRIGGER_STOP:
+		event = HDMI_CODEC_TRIGGER_EVENT_STOP;
+		break;
+	case SNDRV_PCM_TRIGGER_START:
+		event = HDMI_CODEC_TRIGGER_EVENT_START;
+		break;
+	case SNDRV_PCM_TRIGGER_SUSPEND:
+		event = HDMI_CODEC_TRIGGER_EVENT_SUSPEND;
+		break;
+	case SNDRV_PCM_TRIGGER_RESUME:
+		event = HDMI_CODEC_TRIGGER_EVENT_RESUME;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return hcp->hcd.ops->trigger(dai->dev->parent, event);
+}
+
 /*
  * This driver can select all SND_SOC_DAIFMT_CBx_CFx,
  * but need to be selected from Sound Card, not be auto selected.
@@ -711,6 +740,7 @@
 	.prepare	= hdmi_codec_prepare,
 	.set_fmt	= hdmi_codec_i2s_set_fmt,
 	.mute_stream	= hdmi_codec_mute,
+	.trigger	= hdmi_codec_trigger,
 	.auto_selectable_formats	= &hdmi_codec_formats,
 	.num_auto_selectable_formats	= 1,
 };
@@ -720,6 +750,7 @@
 	.shutdown	= hdmi_codec_shutdown,
 	.hw_params	= hdmi_codec_hw_params,
 	.mute_stream	= hdmi_codec_mute,
+	.trigger	= hdmi_codec_trigger,
 };
 
 #define HDMI_RATES	(SNDRV_PCM_RATE_32000 | SNDRV_PCM_RATE_44100 |\
diff -ruN a/sound/soc/codecs/Kconfig b/sound/soc/codecs/Kconfig
--- a/sound/soc/codecs/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/Kconfig	2021-12-23 08:36:02.000000000 +0100
@@ -180,6 +180,7 @@
 	imply SND_SOC_RT5677
 	imply SND_SOC_RT5682_I2C
 	imply SND_SOC_RT5682_SDW
+	imply SND_SOC_RT5682S
 	imply SND_SOC_RT700_SDW
 	imply SND_SOC_RT711_SDW
 	imply SND_SOC_RT711_SDCA_SDW
@@ -1249,6 +1250,10 @@
 	select SND_SOC_RT5682
 	select REGMAP_SOUNDWIRE
 
+config SND_SOC_RT5682S
+	tristate
+	depends on I2C
+
 config SND_SOC_RT700
 	tristate
 
diff -ruN a/sound/soc/codecs/Makefile b/sound/soc/codecs/Makefile
--- a/sound/soc/codecs/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/Makefile	2021-12-23 08:36:02.000000000 +0100
@@ -198,6 +198,7 @@
 snd-soc-rt5682-objs := rt5682.o
 snd-soc-rt5682-sdw-objs := rt5682-sdw.o
 snd-soc-rt5682-i2c-objs := rt5682-i2c.o
+snd-soc-rt5682s-objs := rt5682s.o
 snd-soc-rt700-objs := rt700.o rt700-sdw.o
 snd-soc-rt711-objs := rt711.o rt711-sdw.o
 snd-soc-rt711-sdca-objs := rt711-sdca.o rt711-sdca-sdw.o
@@ -526,6 +527,7 @@
 obj-$(CONFIG_SND_SOC_RT5682)	+= snd-soc-rt5682.o
 obj-$(CONFIG_SND_SOC_RT5682_I2C)	+= snd-soc-rt5682-i2c.o
 obj-$(CONFIG_SND_SOC_RT5682_SDW)	+= snd-soc-rt5682-sdw.o
+obj-$(CONFIG_SND_SOC_RT5682S)	+= snd-soc-rt5682s.o
 obj-$(CONFIG_SND_SOC_RT700)     += snd-soc-rt700.o
 obj-$(CONFIG_SND_SOC_RT711)     += snd-soc-rt711.o
 obj-$(CONFIG_SND_SOC_RT711_SDCA_SDW)     += snd-soc-rt711-sdca.o
diff -ruN a/sound/soc/codecs/max98927.c b/sound/soc/codecs/max98927.c
--- a/sound/soc/codecs/max98927.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/max98927.c	2021-12-23 08:36:02.000000000 +0100
@@ -620,6 +620,18 @@
 		MAX98927_R0042_BOOST_CTRL1, 1,
 		max98927_current_limit_text);
 
+static const char * const max98927_env_track_headroom_text[] = {
+	"0.000V", "0.125V", "0.250V", "0.375V", "0.500V", "0.625V",
+	"0.750V", "0.875V", "1.000V", "1.125V", "1.250V", "1.375V",
+	"1.500V", "1.625V", "1.750V", "1.875V", "2.000V", "2.125V",
+	"2.250V", "2.375V", "2.500V", "2.625V", "2.750V", "2.875V",
+	"3.000V", "3.125V", "3.250V", "3.375V", "3.500V"
+};
+
+static SOC_ENUM_SINGLE_DECL(max98927_env_track_headroom,
+		MAX98927_R0082_ENV_TRACK_VOUT_HEADROOM, 0,
+		max98927_env_track_headroom_text);
+
 static const struct snd_kcontrol_new max98927_snd_controls[] = {
 	SOC_SINGLE_TLV("Speaker Volume", MAX98927_R003C_SPK_GAIN,
 		0, 6, 0,
@@ -637,6 +649,9 @@
 		MAX98927_AMP_VOL_SEL_SHIFT, 1, 0),
 	SOC_ENUM("Boost Output Voltage", max98927_boost_voltage),
 	SOC_ENUM("Current Limit", max98927_current_limit),
+	SOC_SINGLE("EnvTrack Switch", MAX98927_R0086_ENV_TRACK_CTRL,
+		MAX98927_ENV_TRACKER_EN_SHIFT, 1, 0),
+	SOC_ENUM("EnvTrack Headroom", max98927_env_track_headroom),
 };
 
 static const struct snd_soc_dapm_route max98927_audio_map[] = {
@@ -735,13 +750,10 @@
 	/* Envelope Tracking configuration */
 	regmap_write(max98927->regmap,
 		MAX98927_R0082_ENV_TRACK_VOUT_HEADROOM,
-		0x08);
+		0x0A);
 	regmap_write(max98927->regmap,
 		MAX98927_R0086_ENV_TRACK_CTRL,
 		0x01);
-	regmap_write(max98927->regmap,
-		MAX98927_R0087_ENV_TRACK_BOOST_VOUT_READ,
-		0x10);
 
 	/* voltage, current slot configuration */
 	regmap_write(max98927->regmap,
diff -ruN a/sound/soc/codecs/max98927.h b/sound/soc/codecs/max98927.h
--- a/sound/soc/codecs/max98927.h	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/max98927.h	2021-12-23 08:36:02.000000000 +0100
@@ -245,6 +245,10 @@
 #define MAX98927_BROWNOUT_DSP_EN (0x1 << 2)
 #define MAX98927_BROWNOUT_DSP_SHIFT (2)
 
+/* MAX98927_R0086_ENV_TRACK_CTRL */
+#define MAX98927_ENV_TRACKER_EN (0x1 << 0)
+#define MAX98927_ENV_TRACKER_EN_SHIFT (0)
+
 /* MAX98927_R0100_SOFT_RESET */
 #define MAX98927_SOFT_RESET (0x1 << 0)
 
diff -ruN a/sound/soc/codecs/mt6359.c b/sound/soc/codecs/mt6359.c
--- a/sound/soc/codecs/mt6359.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/mt6359.c	2021-12-23 08:36:02.000000000 +0100
@@ -2697,7 +2697,7 @@
 
 static void mt6359_codec_remove(struct snd_soc_component *cmpnt)
 {
-	snd_soc_component_exit_regmap(cmpnt);
+	cmpnt->regmap = NULL;
 }
 
 static const DECLARE_TLV_DB_SCALE(hp_playback_tlv, -2200, 100, 0);
diff -ruN a/sound/soc/codecs/nau8825.c b/sound/soc/codecs/nau8825.c
--- a/sound/soc/codecs/nau8825.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/nau8825.c	2021-12-23 08:36:02.000000000 +0100
@@ -47,6 +47,7 @@
 
 static int nau8825_configure_sysclk(struct nau8825 *nau8825,
 		int clk_id, unsigned int freq);
+static bool nau8825_is_jack_inserted(struct regmap *regmap);
 
 struct nau8825_fll {
 	int mclk_src;
@@ -981,6 +982,31 @@
 	return 0;
 }
 
+static int system_clock_control(struct snd_soc_dapm_widget *w,
+				struct snd_kcontrol *k, int  event)
+{
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+	struct nau8825 *nau8825 = snd_soc_component_get_drvdata(component);
+	struct regmap *regmap = nau8825->regmap;
+
+	if (SND_SOC_DAPM_EVENT_OFF(event)) {
+		dev_dbg(nau8825->dev, "system clock control : POWER OFF\n");
+		/* Set clock source to disable or internal clock before the
+		 * playback or capture end. Codec needs clock for Jack
+		 * detection and button press if jack inserted; otherwise,
+		 * the clock should be closed.
+		 */
+		if (nau8825_is_jack_inserted(regmap)) {
+			nau8825_configure_sysclk(nau8825,
+						 NAU8825_CLK_INTERNAL, 0);
+		} else {
+			nau8825_configure_sysclk(nau8825, NAU8825_CLK_DIS, 0);
+		}
+	}
+
+	return 0;
+}
+
 static int nau8825_biq_coeff_get(struct snd_kcontrol *kcontrol,
 				     struct snd_ctl_elem_value *ucontrol)
 {
@@ -1094,6 +1120,9 @@
 static const struct snd_soc_dapm_widget nau8825_dapm_widgets[] = {
 	SND_SOC_DAPM_AIF_OUT("AIFTX", "Capture", 0, NAU8825_REG_I2S_PCM_CTRL2,
 		15, 1),
+	SND_SOC_DAPM_AIF_IN("AIFRX", "Playback", 0, SND_SOC_NOPM, 0, 0),
+	SND_SOC_DAPM_SUPPLY("System Clock", SND_SOC_NOPM, 0, 0,
+			    system_clock_control, SND_SOC_DAPM_POST_PMD),
 
 	SND_SOC_DAPM_INPUT("MIC"),
 	SND_SOC_DAPM_MICBIAS("MICBIAS", NAU8825_REG_MIC_BIAS, 8, 0),
@@ -1182,9 +1211,11 @@
 	{"ADC", NULL, "ADC Clock"},
 	{"ADC", NULL, "ADC Power"},
 	{"AIFTX", NULL, "ADC"},
+	{"AIFTX", NULL, "System Clock"},
 
-	{"DDACL", NULL, "Playback"},
-	{"DDACR", NULL, "Playback"},
+	{"AIFRX", NULL, "System Clock"},
+	{"DDACL", NULL, "AIFRX"},
+	{"DDACR", NULL, "AIFRX"},
 	{"DDACL", NULL, "DDAC Clock"},
 	{"DDACR", NULL, "DDAC Clock"},
 	{"DACL Mux", "DACL", "DDACL"},
@@ -1434,6 +1465,12 @@
 
 	nau8825->jack = jack;
 
+	if (!nau8825->jack) {
+		regmap_update_bits(regmap, NAU8825_REG_HSD_CTRL,
+				   NAU8825_HSD_AUTO_MODE | NAU8825_SPKR_DWN1R |
+				   NAU8825_SPKR_DWN1L, 0);
+		return 0;
+	}
 	/* Ground HP Outputs[1:0], needed for headset auto detection
 	 * Enable Automatic Mic/Gnd switching reading on insert interrupt[6]
 	 */
@@ -2416,6 +2453,12 @@
 	return 0;
 }
 
+static int nau8825_set_jack(struct snd_soc_component *component,
+			    struct snd_soc_jack *jack, void *data)
+{
+	return nau8825_enable_jack_detect(component, jack);
+}
+
 static const struct snd_soc_component_driver nau8825_component_driver = {
 	.probe			= nau8825_component_probe,
 	.remove			= nau8825_component_remove,
@@ -2430,6 +2473,7 @@
 	.num_dapm_widgets	= ARRAY_SIZE(nau8825_dapm_widgets),
 	.dapm_routes		= nau8825_dapm_routes,
 	.num_dapm_routes	= ARRAY_SIZE(nau8825_dapm_routes),
+	.set_jack		= nau8825_set_jack,
 	.suspend_bias_off	= 1,
 	.idle_bias_on		= 1,
 	.use_pmdown_time	= 1,
diff -ruN a/sound/soc/codecs/rt1011.c b/sound/soc/codecs/rt1011.c
--- a/sound/soc/codecs/rt1011.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt1011.c	2021-12-23 08:36:02.000000000 +0100
@@ -1311,6 +1311,14 @@
 	.put = rt1011_r0_load_mode_put \
 }
 
+static const char * const rt1011_i2s_ref_texts[] = {
+	"Left Channel", "Right Channel"
+};
+
+static SOC_ENUM_SINGLE_DECL(rt1011_i2s_ref_enum,
+			    RT1011_TDM1_SET_1, 7,
+			    rt1011_i2s_ref_texts);
+
 static const struct snd_kcontrol_new rt1011_snd_controls[] = {
 	/* I2S Data In Selection */
 	SOC_ENUM("DIN Source", rt1011_din_source_enum),
@@ -1349,6 +1357,8 @@
 	/* R0 temperature */
 	SOC_SINGLE("R0 Temperature", RT1011_STP_INITIAL_RESISTANCE_TEMP,
 		2, 255, 0),
+	/* I2S Reference */
+	SOC_ENUM("I2S Reference", rt1011_i2s_ref_enum),
 };
 
 static int rt1011_is_sys_clk_from_pll(struct snd_soc_dapm_widget *source,
diff -ruN a/sound/soc/codecs/rt1015.c b/sound/soc/codecs/rt1015.c
--- a/sound/soc/codecs/rt1015.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt1015.c	2021-12-23 08:36:02.000000000 +0100
@@ -864,7 +864,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt1016.c b/sound/soc/codecs/rt1016.c
--- a/sound/soc/codecs/rt1016.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt1016.c	2021-12-23 08:36:02.000000000 +0100
@@ -490,7 +490,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out * 4, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt1019.c b/sound/soc/codecs/rt1019.c
--- a/sound/soc/codecs/rt1019.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt1019.c	2021-12-23 08:36:02.000000000 +0100
@@ -359,7 +359,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt1305.c b/sound/soc/codecs/rt1305.c
--- a/sound/soc/codecs/rt1305.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt1305.c	2021-12-23 08:36:02.000000000 +0100
@@ -841,7 +841,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt1308.c b/sound/soc/codecs/rt1308.c
--- a/sound/soc/codecs/rt1308.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt1308.c	2021-12-23 08:36:02.000000000 +0100
@@ -664,7 +664,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5514.c b/sound/soc/codecs/rt5514.c
--- a/sound/soc/codecs/rt5514.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5514.c	2021-12-23 08:36:02.000000000 +0100
@@ -936,7 +936,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5616.c b/sound/soc/codecs/rt5616.c
--- a/sound/soc/codecs/rt5616.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5616.c	2021-12-23 08:36:02.000000000 +0100
@@ -1133,7 +1133,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5640.c b/sound/soc/codecs/rt5640.c
--- a/sound/soc/codecs/rt5640.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5640.c	2021-12-23 08:36:02.000000000 +0100
@@ -1909,7 +1909,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5645.c b/sound/soc/codecs/rt5645.c
--- a/sound/soc/codecs/rt5645.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5645.c	2021-12-23 08:36:02.000000000 +0100
@@ -2969,7 +2969,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5651.c b/sound/soc/codecs/rt5651.c
--- a/sound/soc/codecs/rt5651.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5651.c	2021-12-23 08:36:02.000000000 +0100
@@ -1487,7 +1487,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5659.c b/sound/soc/codecs/rt5659.c
--- a/sound/soc/codecs/rt5659.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5659.c	2021-12-23 08:36:02.000000000 +0100
@@ -3509,7 +3509,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5660.c b/sound/soc/codecs/rt5660.c
--- a/sound/soc/codecs/rt5660.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5660.c	2021-12-23 08:36:02.000000000 +0100
@@ -1046,7 +1046,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5663.c b/sound/soc/codecs/rt5663.c
--- a/sound/soc/codecs/rt5663.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5663.c	2021-12-23 08:36:02.000000000 +0100
@@ -2941,7 +2941,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5665.c b/sound/soc/codecs/rt5665.c
--- a/sound/soc/codecs/rt5665.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5665.c	2021-12-23 08:36:02.000000000 +0100
@@ -4374,7 +4374,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5668.c b/sound/soc/codecs/rt5668.c
--- a/sound/soc/codecs/rt5668.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5668.c	2021-12-23 08:36:02.000000000 +0100
@@ -2171,7 +2171,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5670.c b/sound/soc/codecs/rt5670.c
--- a/sound/soc/codecs/rt5670.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5670.c	2021-12-23 08:36:02.000000000 +0100
@@ -2577,7 +2577,7 @@
 
 	ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5677.c b/sound/soc/codecs/rt5677.c
--- a/sound/soc/codecs/rt5677.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5677.c	2021-12-23 08:36:02.000000000 +0100
@@ -4557,7 +4557,7 @@
 
 	ret = rt5677_pll_calc(freq_in, freq_out, &pll_code);
 	if (ret < 0) {
-		dev_err(component->dev, "Unsupport input clock %d\n", freq_in);
+		dev_err(component->dev, "Unsupported input clock %d\n", freq_in);
 		return ret;
 	}
 
diff -ruN a/sound/soc/codecs/rt5682.c b/sound/soc/codecs/rt5682.c
--- a/sound/soc/codecs/rt5682.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/codecs/rt5682.c	2021-12-23 08:36:02.000000000 +0100
@@ -2367,7 +2367,7 @@
 		pll2_fout1 = 3840000;
 		ret = rl6231_pll_calc(freq_in, pll2_fout1, &pll2f_code);
 		if (ret < 0) {
-			dev_err(component->dev, "Unsupport input clock %d\n",
+			dev_err(component->dev, "Unsupported input clock %d\n",
 				freq_in);
 			return ret;
 		}
@@ -2379,7 +2379,7 @@
 
 		ret = rl6231_pll_calc(pll2_fout1, freq_out, &pll2b_code);
 		if (ret < 0) {
-			dev_err(component->dev, "Unsupport input clock %d\n",
+			dev_err(component->dev, "Unsupported input clock %d\n",
 				pll2_fout1);
 			return ret;
 		}
@@ -2430,7 +2430,7 @@
 
 		ret = rl6231_pll_calc(freq_in, freq_out, &pll_code);
 		if (ret < 0) {
-			dev_err(component->dev, "Unsupport input clock %d\n",
+			dev_err(component->dev, "Unsupported input clock %d\n",
 				freq_in);
 			return ret;
 		}
diff -ruN a/sound/soc/codecs/rt5682s.c b/sound/soc/codecs/rt5682s.c
--- a/sound/soc/codecs/rt5682s.c	1970-01-01 01:00:00.000000000 +0100
+++ b/sound/soc/codecs/rt5682s.c	2021-12-23 08:36:02.000000000 +0100
@@ -0,0 +1,3188 @@
+// SPDX-License-Identifier: GPL-2.0-only
+//
+// rt5682s.c  --  RT5682I-VS ALSA SoC audio component driver
+//
+// Copyright 2021 Realtek Semiconductor Corp.
+// Author: Derek Fang <derek.fang@realtek.com>
+//
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/pm.h>
+#include <linux/pm_runtime.h>
+#include <linux/i2c.h>
+#include <linux/platform_device.h>
+#include <linux/spi/spi.h>
+#include <linux/acpi.h>
+#include <linux/gpio.h>
+#include <linux/of_gpio.h>
+#include <linux/mutex.h>
+#include <sound/core.h>
+#include <sound/pcm.h>
+#include <sound/pcm_params.h>
+#include <sound/jack.h>
+#include <sound/soc.h>
+#include <sound/soc-dapm.h>
+#include <sound/initval.h>
+#include <sound/tlv.h>
+#include <sound/rt5682s.h>
+
+#include "rt5682s.h"
+
+#define DEVICE_ID 0x6749
+
+static const struct rt5682s_platform_data i2s_default_platform_data = {
+	.dmic1_data_pin = RT5682S_DMIC1_DATA_GPIO2,
+	.dmic1_clk_pin = RT5682S_DMIC1_CLK_GPIO3,
+	.jd_src = RT5682S_JD1,
+	.dai_clk_names[RT5682S_DAI_WCLK_IDX] = "rt5682-dai-wclk",
+	.dai_clk_names[RT5682S_DAI_BCLK_IDX] = "rt5682-dai-bclk",
+};
+
+const char *rt5682s_supply_names[RT5682S_NUM_SUPPLIES] = {
+	"AVDD",
+	"MICVDD",
+};
+
+static const struct reg_sequence patch_list[] = {
+	{RT5682S_I2C_CTRL,			0x0007},
+	{RT5682S_DIG_IN_CTRL_1,			0x0000},
+	{RT5682S_CHOP_DAC_2,			0x2020},
+	{RT5682S_VREF_REC_OP_FB_CAP_CTRL_2,	0x0101},
+	{RT5682S_VREF_REC_OP_FB_CAP_CTRL_1,	0x80c0},
+	{RT5682S_HP_CALIB_CTRL_9,		0x0002},
+	{RT5682S_DEPOP_1,			0x0000},
+	{RT5682S_HP_CHARGE_PUMP_2,		0x3c15},
+	{RT5682S_DAC1_DIG_VOL,			0xfefe},
+	{RT5682S_SAR_IL_CMD_2,			0xac00},
+	{RT5682S_SAR_IL_CMD_3,			0x024c},
+	{RT5682S_CBJ_CTRL_6,			0x0804},
+};
+
+static void rt5682s_apply_patch_list(struct rt5682s_priv *rt5682s,
+		struct device *dev)
+{
+	int ret;
+
+	ret = regmap_multi_reg_write(rt5682s->regmap, patch_list, ARRAY_SIZE(patch_list));
+	if (ret)
+		dev_warn(dev, "Failed to apply regmap patch: %d\n", ret);
+}
+
+const struct reg_default rt5682s_reg[] = {
+	{0x0000, 0x0001},
+	{0x0002, 0x8080},
+	{0x0003, 0x0001},
+	{0x0005, 0x0000},
+	{0x0006, 0x0000},
+	{0x0008, 0x8007},
+	{0x000b, 0x0000},
+	{0x000f, 0x4000},
+	{0x0010, 0x4040},
+	{0x0011, 0x0000},
+	{0x0012, 0x0000},
+	{0x0013, 0x1200},
+	{0x0014, 0x200a},
+	{0x0015, 0x0404},
+	{0x0016, 0x0404},
+	{0x0017, 0x05a4},
+	{0x0019, 0xffff},
+	{0x001c, 0x2f2f},
+	{0x001f, 0x0000},
+	{0x0022, 0x5757},
+	{0x0023, 0x0039},
+	{0x0024, 0x000b},
+	{0x0026, 0xc0c4},
+	{0x0029, 0x8080},
+	{0x002a, 0xa0a0},
+	{0x002b, 0x0300},
+	{0x0030, 0x0000},
+	{0x003c, 0x08c0},
+	{0x0044, 0x1818},
+	{0x004b, 0x00c0},
+	{0x004c, 0x0000},
+	{0x004d, 0x0000},
+	{0x0061, 0x00c0},
+	{0x0062, 0x008a},
+	{0x0063, 0x0800},
+	{0x0064, 0x0000},
+	{0x0065, 0x0000},
+	{0x0066, 0x0030},
+	{0x0067, 0x000c},
+	{0x0068, 0x0000},
+	{0x0069, 0x0000},
+	{0x006a, 0x0000},
+	{0x006b, 0x0000},
+	{0x006c, 0x0000},
+	{0x006d, 0x2200},
+	{0x006e, 0x0810},
+	{0x006f, 0xe4de},
+	{0x0070, 0x3320},
+	{0x0071, 0x0000},
+	{0x0073, 0x0000},
+	{0x0074, 0x0000},
+	{0x0075, 0x0002},
+	{0x0076, 0x0001},
+	{0x0079, 0x0000},
+	{0x007a, 0x0000},
+	{0x007b, 0x0000},
+	{0x007c, 0x0100},
+	{0x007e, 0x0000},
+	{0x007f, 0x0000},
+	{0x0080, 0x0000},
+	{0x0083, 0x0000},
+	{0x0084, 0x0000},
+	{0x0085, 0x0000},
+	{0x0086, 0x0005},
+	{0x0087, 0x0000},
+	{0x0088, 0x0000},
+	{0x008c, 0x0003},
+	{0x008e, 0x0060},
+	{0x008f, 0x4da1},
+	{0x0091, 0x1c15},
+	{0x0092, 0x0425},
+	{0x0093, 0x0000},
+	{0x0094, 0x0080},
+	{0x0095, 0x008f},
+	{0x0096, 0x0000},
+	{0x0097, 0x0000},
+	{0x0098, 0x0000},
+	{0x0099, 0x0000},
+	{0x009a, 0x0000},
+	{0x009b, 0x0000},
+	{0x009c, 0x0000},
+	{0x009d, 0x0000},
+	{0x009e, 0x0000},
+	{0x009f, 0x0009},
+	{0x00a0, 0x0000},
+	{0x00a3, 0x0002},
+	{0x00a4, 0x0001},
+	{0x00b6, 0x0000},
+	{0x00b7, 0x0000},
+	{0x00b8, 0x0000},
+	{0x00b9, 0x0002},
+	{0x00be, 0x0000},
+	{0x00c0, 0x0160},
+	{0x00c1, 0x82a0},
+	{0x00c2, 0x0000},
+	{0x00d0, 0x0000},
+	{0x00d2, 0x3300},
+	{0x00d3, 0x2200},
+	{0x00d4, 0x0000},
+	{0x00d9, 0x0000},
+	{0x00da, 0x0000},
+	{0x00db, 0x0000},
+	{0x00dc, 0x00c0},
+	{0x00dd, 0x2220},
+	{0x00de, 0x3131},
+	{0x00df, 0x3131},
+	{0x00e0, 0x3131},
+	{0x00e2, 0x0000},
+	{0x00e3, 0x4000},
+	{0x00e4, 0x0aa0},
+	{0x00e5, 0x3131},
+	{0x00e6, 0x3131},
+	{0x00e7, 0x3131},
+	{0x00e8, 0x3131},
+	{0x00ea, 0xb320},
+	{0x00eb, 0x0000},
+	{0x00f0, 0x0000},
+	{0x00f6, 0x0000},
+	{0x00fa, 0x0000},
+	{0x00fb, 0x0000},
+	{0x00fc, 0x0000},
+	{0x00fd, 0x0000},
+	{0x00fe, 0x10ec},
+	{0x00ff, 0x6749},
+	{0x0100, 0xa000},
+	{0x010b, 0x0066},
+	{0x010c, 0x6666},
+	{0x010d, 0x2202},
+	{0x010e, 0x6666},
+	{0x010f, 0xa800},
+	{0x0110, 0x0006},
+	{0x0111, 0x0460},
+	{0x0112, 0x2000},
+	{0x0113, 0x0200},
+	{0x0117, 0x8000},
+	{0x0118, 0x0303},
+	{0x0125, 0x0020},
+	{0x0132, 0x5026},
+	{0x0136, 0x8000},
+	{0x0139, 0x0005},
+	{0x013a, 0x3030},
+	{0x013b, 0xa000},
+	{0x013c, 0x4110},
+	{0x013f, 0x0000},
+	{0x0145, 0x0022},
+	{0x0146, 0x0000},
+	{0x0147, 0x0000},
+	{0x0148, 0x0000},
+	{0x0156, 0x0022},
+	{0x0157, 0x0303},
+	{0x0158, 0x2222},
+	{0x0159, 0x0000},
+	{0x0160, 0x4ec0},
+	{0x0161, 0x0080},
+	{0x0162, 0x0200},
+	{0x0163, 0x0800},
+	{0x0164, 0x0000},
+	{0x0165, 0x0000},
+	{0x0166, 0x0000},
+	{0x0167, 0x000f},
+	{0x0168, 0x000f},
+	{0x0169, 0x0001},
+	{0x0190, 0x4131},
+	{0x0194, 0x0000},
+	{0x0195, 0x0000},
+	{0x0197, 0x0022},
+	{0x0198, 0x0000},
+	{0x0199, 0x0000},
+	{0x01ac, 0x0000},
+	{0x01ad, 0x0000},
+	{0x01ae, 0x0000},
+	{0x01af, 0x2000},
+	{0x01b0, 0x0000},
+	{0x01b1, 0x0000},
+	{0x01b2, 0x0000},
+	{0x01b3, 0x0017},
+	{0x01b4, 0x004b},
+	{0x01b5, 0x0000},
+	{0x01b6, 0x03e8},
+	{0x01b7, 0x0000},
+	{0x01b8, 0x0000},
+	{0x01b9, 0x0400},
+	{0x01ba, 0xb5b6},
+	{0x01bb, 0x9124},
+	{0x01bc, 0x4924},
+	{0x01bd, 0x0009},
+	{0x01be, 0x0018},
+	{0x01bf, 0x002a},
+	{0x01c0, 0x004c},
+	{0x01c1, 0x0097},
+	{0x01c2, 0x01c3},
+	{0x01c3, 0x03e9},
+	{0x01c4, 0x1389},
+	{0x01c5, 0xc351},
+	{0x01c6, 0x02a0},
+	{0x01c7, 0x0b0f},
+	{0x01c8, 0x402f},
+	{0x01c9, 0x0702},
+	{0x01ca, 0x0000},
+	{0x01cb, 0x0000},
+	{0x01cc, 0x5757},
+	{0x01cd, 0x5757},
+	{0x01ce, 0x5757},
+	{0x01cf, 0x5757},
+	{0x01d0, 0x5757},
+	{0x01d1, 0x5757},
+	{0x01d2, 0x5757},
+	{0x01d3, 0x5757},
+	{0x01d4, 0x5757},
+	{0x01d5, 0x5757},
+	{0x01d6, 0x0000},
+	{0x01d7, 0x0000},
+	{0x01d8, 0x0162},
+	{0x01d9, 0x0007},
+	{0x01da, 0x0000},
+	{0x01db, 0x0004},
+	{0x01dc, 0x0000},
+	{0x01de, 0x7c00},
+	{0x01df, 0x0020},
+	{0x01e0, 0x04c1},
+	{0x01e1, 0x0000},
+	{0x01e2, 0x0000},
+	{0x01e3, 0x0000},
+	{0x01e4, 0x0000},
+	{0x01e5, 0x0000},
+	{0x01e6, 0x0001},
+	{0x01e7, 0x0000},
+	{0x01e8, 0x0000},
+	{0x01eb, 0x0000},
+	{0x01ec, 0x0000},
+	{0x01ed, 0x0000},
+	{0x01ee, 0x0000},
+	{0x01ef, 0x0000},
+	{0x01f0, 0x0000},
+	{0x01f1, 0x0000},
+	{0x01f2, 0x0000},
+	{0x01f3, 0x0000},
+	{0x01f4, 0x0000},
+	{0x0210, 0x6297},
+	{0x0211, 0xa004},
+	{0x0212, 0x0365},
+	{0x0213, 0xf7ff},
+	{0x0214, 0xf24c},
+	{0x0215, 0x0102},
+	{0x0216, 0x00a3},
+	{0x0217, 0x0048},
+	{0x0218, 0xa2c0},
+	{0x0219, 0x0400},
+	{0x021a, 0x00c8},
+	{0x021b, 0x00c0},
+	{0x021c, 0x0000},
+	{0x021d, 0x024c},
+	{0x02fa, 0x0000},
+	{0x02fb, 0x0000},
+	{0x02fc, 0x0000},
+	{0x03fe, 0x0000},
+	{0x03ff, 0x0000},
+	{0x0500, 0x0000},
+	{0x0600, 0x0000},
+	{0x0610, 0x6666},
+	{0x0611, 0xa9aa},
+	{0x0620, 0x6666},
+	{0x0621, 0xa9aa},
+	{0x0630, 0x6666},
+	{0x0631, 0xa9aa},
+	{0x0640, 0x6666},
+	{0x0641, 0xa9aa},
+	{0x07fa, 0x0000},
+	{0x08fa, 0x0000},
+	{0x08fb, 0x0000},
+	{0x0d00, 0x0000},
+	{0x1100, 0x0000},
+	{0x1101, 0x0000},
+	{0x1102, 0x0000},
+	{0x1103, 0x0000},
+	{0x1104, 0x0000},
+	{0x1105, 0x0000},
+	{0x1106, 0x0000},
+	{0x1107, 0x0000},
+	{0x1108, 0x0000},
+	{0x1109, 0x0000},
+	{0x110a, 0x0000},
+	{0x110b, 0x0000},
+	{0x110c, 0x0000},
+	{0x1111, 0x0000},
+	{0x1112, 0x0000},
+	{0x1113, 0x0000},
+	{0x1114, 0x0000},
+	{0x1115, 0x0000},
+	{0x1116, 0x0000},
+	{0x1117, 0x0000},
+	{0x1118, 0x0000},
+	{0x1119, 0x0000},
+	{0x111a, 0x0000},
+	{0x111b, 0x0000},
+	{0x111c, 0x0000},
+	{0x1401, 0x0404},
+	{0x1402, 0x0007},
+	{0x1403, 0x0365},
+	{0x1404, 0x0210},
+	{0x1405, 0x0365},
+	{0x1406, 0x0210},
+	{0x1407, 0x0000},
+	{0x1408, 0x0000},
+	{0x1409, 0x0000},
+	{0x140a, 0x0000},
+	{0x140b, 0x0000},
+	{0x140c, 0x0000},
+	{0x140d, 0x0000},
+	{0x140e, 0x0000},
+	{0x140f, 0x0000},
+	{0x1410, 0x0000},
+	{0x1411, 0x0000},
+	{0x1801, 0x0004},
+	{0x1802, 0x0000},
+	{0x1803, 0x0000},
+	{0x1804, 0x0000},
+	{0x1805, 0x00ff},
+	{0x2c00, 0x0000},
+	{0x3400, 0x0200},
+	{0x3404, 0x0000},
+	{0x3405, 0x0000},
+	{0x3406, 0x0000},
+	{0x3407, 0x0000},
+	{0x3408, 0x0000},
+	{0x3409, 0x0000},
+	{0x340a, 0x0000},
+	{0x340b, 0x0000},
+	{0x340c, 0x0000},
+	{0x340d, 0x0000},
+	{0x340e, 0x0000},
+	{0x340f, 0x0000},
+	{0x3410, 0x0000},
+	{0x3411, 0x0000},
+	{0x3412, 0x0000},
+	{0x3413, 0x0000},
+	{0x3414, 0x0000},
+	{0x3415, 0x0000},
+	{0x3424, 0x0000},
+	{0x3425, 0x0000},
+	{0x3426, 0x0000},
+	{0x3427, 0x0000},
+	{0x3428, 0x0000},
+	{0x3429, 0x0000},
+	{0x342a, 0x0000},
+	{0x342b, 0x0000},
+	{0x342c, 0x0000},
+	{0x342d, 0x0000},
+	{0x342e, 0x0000},
+	{0x342f, 0x0000},
+	{0x3430, 0x0000},
+	{0x3431, 0x0000},
+	{0x3432, 0x0000},
+	{0x3433, 0x0000},
+	{0x3434, 0x0000},
+	{0x3435, 0x0000},
+	{0x3440, 0x6319},
+	{0x3441, 0x3771},
+	{0x3500, 0x0002},
+	{0x3501, 0x5728},
+	{0x3b00, 0x3010},
+	{0x3b01, 0x3300},
+	{0x3b02, 0x2200},
+	{0x3b03, 0x0100},
+};
+
+static bool rt5682s_volatile_register(struct device *dev, unsigned int reg)
+{
+	switch (reg) {
+	case RT5682S_RESET:
+	case RT5682S_CBJ_CTRL_2:
+	case RT5682S_I2S1_F_DIV_CTRL_2:
+	case RT5682S_I2S2_F_DIV_CTRL_2:
+	case RT5682S_INT_ST_1:
+	case RT5682S_GPIO_ST:
+	case RT5682S_IL_CMD_1:
+	case RT5682S_4BTN_IL_CMD_1:
+	case RT5682S_AJD1_CTRL:
+	case RT5682S_VERSION_ID...RT5682S_DEVICE_ID:
+	case RT5682S_STO_NG2_CTRL_1:
+	case RT5682S_STO_NG2_CTRL_5...RT5682S_STO_NG2_CTRL_7:
+	case RT5682S_STO1_DAC_SIL_DET:
+	case RT5682S_HP_IMP_SENS_CTRL_1...RT5682S_HP_IMP_SENS_CTRL_4:
+	case RT5682S_HP_IMP_SENS_CTRL_13:
+	case RT5682S_HP_IMP_SENS_CTRL_14:
+	case RT5682S_HP_IMP_SENS_CTRL_43...RT5682S_HP_IMP_SENS_CTRL_46:
+	case RT5682S_HP_CALIB_CTRL_1:
+	case RT5682S_HP_CALIB_CTRL_10:
+	case RT5682S_HP_CALIB_ST_1...RT5682S_HP_CALIB_ST_11:
+	case RT5682S_SAR_IL_CMD_2...RT5682S_SAR_IL_CMD_5:
+	case RT5682S_SAR_IL_CMD_10:
+	case RT5682S_SAR_IL_CMD_11:
+	case RT5682S_VERSION_ID_HIDE:
+	case RT5682S_VERSION_ID_CUS:
+	case RT5682S_I2C_TRANS_CTRL:
+	case RT5682S_DMIC_FLOAT_DET:
+	case RT5682S_HA_CMP_OP_1:
+	case RT5682S_NEW_CBJ_DET_CTL_10...RT5682S_NEW_CBJ_DET_CTL_16:
+	case RT5682S_CLK_SW_TEST_1:
+	case RT5682S_CLK_SW_TEST_2:
+	case RT5682S_EFUSE_READ_1...RT5682S_EFUSE_READ_18:
+	case RT5682S_PILOT_DIG_CTL_1:
+		return true;
+	default:
+		return false;
+	}
+}
+
+static bool rt5682s_readable_register(struct device *dev, unsigned int reg)
+{
+	switch (reg) {
+	case RT5682S_RESET:
+	case RT5682S_VERSION_ID:
+	case RT5682S_VENDOR_ID:
+	case RT5682S_DEVICE_ID:
+	case RT5682S_HP_CTRL_1:
+	case RT5682S_HP_CTRL_2:
+	case RT5682S_HPL_GAIN:
+	case RT5682S_HPR_GAIN:
+	case RT5682S_I2C_CTRL:
+	case RT5682S_CBJ_BST_CTRL:
+	case RT5682S_CBJ_DET_CTRL:
+	case RT5682S_CBJ_CTRL_1...RT5682S_CBJ_CTRL_8:
+	case RT5682S_DAC1_DIG_VOL:
+	case RT5682S_STO1_ADC_DIG_VOL:
+	case RT5682S_STO1_ADC_BOOST:
+	case RT5682S_HP_IMP_GAIN_1:
+	case RT5682S_HP_IMP_GAIN_2:
+	case RT5682S_SIDETONE_CTRL:
+	case RT5682S_STO1_ADC_MIXER:
+	case RT5682S_AD_DA_MIXER:
+	case RT5682S_STO1_DAC_MIXER:
+	case RT5682S_A_DAC1_MUX:
+	case RT5682S_DIG_INF2_DATA:
+	case RT5682S_REC_MIXER:
+	case RT5682S_CAL_REC:
+	case RT5682S_HP_ANA_OST_CTRL_1...RT5682S_HP_ANA_OST_CTRL_3:
+	case RT5682S_PWR_DIG_1...RT5682S_PWR_MIXER:
+	case RT5682S_MB_CTRL:
+	case RT5682S_CLK_GATE_TCON_1...RT5682S_CLK_GATE_TCON_3:
+	case RT5682S_CLK_DET...RT5682S_LPF_AD_DMIC:
+	case RT5682S_I2S1_SDP:
+	case RT5682S_I2S2_SDP:
+	case RT5682S_ADDA_CLK_1:
+	case RT5682S_ADDA_CLK_2:
+	case RT5682S_I2S1_F_DIV_CTRL_1:
+	case RT5682S_I2S1_F_DIV_CTRL_2:
+	case RT5682S_TDM_CTRL:
+	case RT5682S_TDM_ADDA_CTRL_1:
+	case RT5682S_TDM_ADDA_CTRL_2:
+	case RT5682S_DATA_SEL_CTRL_1:
+	case RT5682S_TDM_TCON_CTRL_1:
+	case RT5682S_TDM_TCON_CTRL_2:
+	case RT5682S_GLB_CLK:
+	case RT5682S_PLL_TRACK_1...RT5682S_PLL_TRACK_6:
+	case RT5682S_PLL_TRACK_11:
+	case RT5682S_DEPOP_1:
+	case RT5682S_HP_CHARGE_PUMP_1:
+	case RT5682S_HP_CHARGE_PUMP_2:
+	case RT5682S_HP_CHARGE_PUMP_3:
+	case RT5682S_MICBIAS_1...RT5682S_MICBIAS_3:
+	case RT5682S_PLL_TRACK_12...RT5682S_PLL_CTRL_7:
+	case RT5682S_RC_CLK_CTRL:
+	case RT5682S_I2S2_M_CLK_CTRL_1:
+	case RT5682S_I2S2_F_DIV_CTRL_1:
+	case RT5682S_I2S2_F_DIV_CTRL_2:
+	case RT5682S_IRQ_CTRL_1...RT5682S_IRQ_CTRL_4:
+	case RT5682S_INT_ST_1:
+	case RT5682S_GPIO_CTRL_1:
+	case RT5682S_GPIO_CTRL_2:
+	case RT5682S_GPIO_ST:
+	case RT5682S_HP_AMP_DET_CTRL_1:
+	case RT5682S_MID_HP_AMP_DET:
+	case RT5682S_LOW_HP_AMP_DET:
+	case RT5682S_DELAY_BUF_CTRL:
+	case RT5682S_SV_ZCD_1:
+	case RT5682S_SV_ZCD_2:
+	case RT5682S_IL_CMD_1...RT5682S_IL_CMD_6:
+	case RT5682S_4BTN_IL_CMD_1...RT5682S_4BTN_IL_CMD_7:
+	case RT5682S_ADC_STO1_HP_CTRL_1:
+	case RT5682S_ADC_STO1_HP_CTRL_2:
+	case RT5682S_AJD1_CTRL:
+	case RT5682S_JD_CTRL_1:
+	case RT5682S_DUMMY_1...RT5682S_DUMMY_3:
+	case RT5682S_DAC_ADC_DIG_VOL1:
+	case RT5682S_BIAS_CUR_CTRL_2...RT5682S_BIAS_CUR_CTRL_10:
+	case RT5682S_VREF_REC_OP_FB_CAP_CTRL_1:
+	case RT5682S_VREF_REC_OP_FB_CAP_CTRL_2:
+	case RT5682S_CHARGE_PUMP_1:
+	case RT5682S_DIG_IN_CTRL_1:
+	case RT5682S_PAD_DRIVING_CTRL:
+	case RT5682S_CHOP_DAC_1:
+	case RT5682S_CHOP_DAC_2:
+	case RT5682S_CHOP_ADC:
+	case RT5682S_CALIB_ADC_CTRL:
+	case RT5682S_VOL_TEST:
+	case RT5682S_SPKVDD_DET_ST:
+	case RT5682S_TEST_MODE_CTRL_1...RT5682S_TEST_MODE_CTRL_4:
+	case RT5682S_PLL_INTERNAL_1...RT5682S_PLL_INTERNAL_4:
+	case RT5682S_STO_NG2_CTRL_1...RT5682S_STO_NG2_CTRL_10:
+	case RT5682S_STO1_DAC_SIL_DET:
+	case RT5682S_SIL_PSV_CTRL1:
+	case RT5682S_SIL_PSV_CTRL2:
+	case RT5682S_SIL_PSV_CTRL3:
+	case RT5682S_SIL_PSV_CTRL4:
+	case RT5682S_SIL_PSV_CTRL5:
+	case RT5682S_HP_IMP_SENS_CTRL_1...RT5682S_HP_IMP_SENS_CTRL_46:
+	case RT5682S_HP_LOGIC_CTRL_1...RT5682S_HP_LOGIC_CTRL_3:
+	case RT5682S_HP_CALIB_CTRL_1...RT5682S_HP_CALIB_CTRL_11:
+	case RT5682S_HP_CALIB_ST_1...RT5682S_HP_CALIB_ST_11:
+	case RT5682S_SAR_IL_CMD_1...RT5682S_SAR_IL_CMD_14:
+	case RT5682S_DUMMY_4...RT5682S_DUMMY_6:
+	case RT5682S_VERSION_ID_HIDE:
+	case RT5682S_VERSION_ID_CUS:
+	case RT5682S_SCAN_CTL:
+	case RT5682S_HP_AMP_DET:
+	case RT5682S_BIAS_CUR_CTRL_11:
+	case RT5682S_BIAS_CUR_CTRL_12:
+	case RT5682S_BIAS_CUR_CTRL_13:
+	case RT5682S_BIAS_CUR_CTRL_14:
+	case RT5682S_BIAS_CUR_CTRL_15:
+	case RT5682S_BIAS_CUR_CTRL_16:
+	case RT5682S_BIAS_CUR_CTRL_17:
+	case RT5682S_BIAS_CUR_CTRL_18:
+	case RT5682S_I2C_TRANS_CTRL:
+	case RT5682S_DUMMY_7:
+	case RT5682S_DUMMY_8:
+	case RT5682S_DMIC_FLOAT_DET:
+	case RT5682S_HA_CMP_OP_1...RT5682S_HA_CMP_OP_13:
+	case RT5682S_HA_CMP_OP_14...RT5682S_HA_CMP_OP_25:
+	case RT5682S_NEW_CBJ_DET_CTL_1...RT5682S_NEW_CBJ_DET_CTL_16:
+	case RT5682S_DA_FILTER_1...RT5682S_DA_FILTER_5:
+	case RT5682S_CLK_SW_TEST_1:
+	case RT5682S_CLK_SW_TEST_2:
+	case RT5682S_CLK_SW_TEST_3...RT5682S_CLK_SW_TEST_14:
+	case RT5682S_EFUSE_MANU_WRITE_1...RT5682S_EFUSE_MANU_WRITE_6:
+	case RT5682S_EFUSE_READ_1...RT5682S_EFUSE_READ_18:
+	case RT5682S_EFUSE_TIMING_CTL_1:
+	case RT5682S_EFUSE_TIMING_CTL_2:
+	case RT5682S_PILOT_DIG_CTL_1:
+	case RT5682S_PILOT_DIG_CTL_2:
+	case RT5682S_HP_AMP_DET_CTL_1...RT5682S_HP_AMP_DET_CTL_4:
+		return true;
+	default:
+		return false;
+	}
+}
+
+static void rt5682s_reset(struct rt5682s_priv *rt5682s)
+{
+	regmap_write(rt5682s->regmap, RT5682S_RESET, 0);
+}
+
+static int rt5682s_button_detect(struct snd_soc_component *component)
+{
+	int btn_type, val;
+
+	val = snd_soc_component_read(component, RT5682S_4BTN_IL_CMD_1);
+	btn_type = val & 0xfff0;
+	snd_soc_component_write(component, RT5682S_4BTN_IL_CMD_1, val);
+	dev_dbg(component->dev, "%s btn_type=%x\n", __func__, btn_type);
+	snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_2,
+		RT5682S_SAR_ADC_PSV_MASK, RT5682S_SAR_ADC_PSV_ENTRY);
+
+	return btn_type;
+}
+
+enum {
+	SAR_PWR_OFF,
+	SAR_PWR_NORMAL,
+	SAR_PWR_SAVING,
+};
+
+static void rt5682s_sar_power_mode(struct snd_soc_component *component,
+				int mode, int jd_step)
+{
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+
+	mutex_lock(&rt5682s->sar_mutex);
+
+	switch (mode) {
+	case SAR_PWR_SAVING:
+		snd_soc_component_update_bits(component, RT5682S_CBJ_CTRL_3,
+			RT5682S_CBJ_IN_BUF_MASK, RT5682S_CBJ_IN_BUF_DIS);
+		snd_soc_component_update_bits(component, RT5682S_CBJ_CTRL_1,
+			RT5682S_MB1_PATH_MASK | RT5682S_MB2_PATH_MASK,
+			RT5682S_CTRL_MB1_REG | RT5682S_CTRL_MB2_REG);
+		snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_1,
+			RT5682S_SAR_BUTDET_MASK | RT5682S_SAR_BUTDET_POW_MASK |
+			RT5682S_SAR_SEL_MB1_2_CTL_MASK, RT5682S_SAR_BUTDET_DIS |
+			RT5682S_SAR_BUTDET_POW_SAV | RT5682S_SAR_SEL_MB1_2_MANU);
+		usleep_range(5000, 5500);
+		snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_1,
+			RT5682S_SAR_BUTDET_MASK, RT5682S_SAR_BUTDET_EN);
+		usleep_range(5000, 5500);
+		snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_2,
+			RT5682S_SAR_ADC_PSV_MASK, RT5682S_SAR_ADC_PSV_ENTRY);
+		break;
+	case SAR_PWR_NORMAL:
+		snd_soc_component_update_bits(component, RT5682S_CBJ_CTRL_3,
+			RT5682S_CBJ_IN_BUF_MASK, RT5682S_CBJ_IN_BUF_EN);
+		snd_soc_component_update_bits(component, RT5682S_CBJ_CTRL_1,
+			RT5682S_MB1_PATH_MASK | RT5682S_MB2_PATH_MASK,
+			RT5682S_CTRL_MB1_FSM | RT5682S_CTRL_MB2_FSM);
+		if (!jd_step) {
+			snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_1,
+				RT5682S_SAR_SEL_MB1_2_CTL_MASK, RT5682S_SAR_SEL_MB1_2_AUTO);
+			usleep_range(5000, 5500);
+			snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_1,
+				RT5682S_SAR_BUTDET_MASK | RT5682S_SAR_BUTDET_POW_MASK,
+				RT5682S_SAR_BUTDET_EN | RT5682S_SAR_BUTDET_POW_NORM);
+		}
+		break;
+	case SAR_PWR_OFF:
+		snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_1,
+			RT5682S_SAR_BUTDET_MASK | RT5682S_SAR_BUTDET_POW_MASK |
+			RT5682S_SAR_SEL_MB1_2_CTL_MASK, RT5682S_SAR_BUTDET_DIS |
+			RT5682S_SAR_BUTDET_POW_SAV | RT5682S_SAR_SEL_MB1_2_MANU);
+		break;
+	default:
+		dev_err(component->dev, "Invalid SAR Power mode: %d\n", mode);
+		break;
+	}
+
+	mutex_unlock(&rt5682s->sar_mutex);
+}
+
+static void rt5682s_enable_push_button_irq(struct snd_soc_component *component)
+{
+	snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_13,
+		RT5682S_SAR_SOUR_MASK, RT5682S_SAR_SOUR_BTN);
+	snd_soc_component_write(component, RT5682S_IL_CMD_1, 0x0040);
+	snd_soc_component_update_bits(component, RT5682S_4BTN_IL_CMD_2,
+		RT5682S_4BTN_IL_MASK | RT5682S_4BTN_IL_RST_MASK,
+		RT5682S_4BTN_IL_EN | RT5682S_4BTN_IL_NOR);
+	snd_soc_component_update_bits(component, RT5682S_IRQ_CTRL_3,
+		RT5682S_IL_IRQ_MASK, RT5682S_IL_IRQ_EN);
+}
+
+static void rt5682s_disable_push_button_irq(struct snd_soc_component *component)
+{
+	snd_soc_component_update_bits(component, RT5682S_IRQ_CTRL_3,
+		RT5682S_IL_IRQ_MASK, RT5682S_IL_IRQ_DIS);
+	snd_soc_component_update_bits(component, RT5682S_4BTN_IL_CMD_2,
+		RT5682S_4BTN_IL_MASK, RT5682S_4BTN_IL_DIS);
+	snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_13,
+		RT5682S_SAR_SOUR_MASK, RT5682S_SAR_SOUR_TYPE);
+}
+
+/**
+ * rt5682s_headset_detect - Detect headset.
+ * @component: SoC audio component device.
+ * @jack_insert: Jack insert or not.
+ *
+ * Detect whether is headset or not when jack inserted.
+ *
+ * Returns detect status.
+ */
+static int rt5682s_headset_detect(struct snd_soc_component *component, int jack_insert)
+{
+	unsigned int val, count;
+	int jack_type = 0;
+
+	if (jack_insert) {
+		rt5682s_disable_push_button_irq(component);
+		snd_soc_component_update_bits(component, RT5682S_PWR_ANLG_1,
+			RT5682S_PWR_VREF1 | RT5682S_PWR_VREF2 | RT5682S_PWR_MB,
+			RT5682S_PWR_VREF1 | RT5682S_PWR_VREF2 | RT5682S_PWR_MB);
+		snd_soc_component_update_bits(component, RT5682S_PWR_ANLG_1,
+			RT5682S_PWR_FV1 | RT5682S_PWR_FV2, 0);
+		usleep_range(15000, 20000);
+		snd_soc_component_update_bits(component, RT5682S_PWR_ANLG_1,
+			RT5682S_PWR_FV1 | RT5682S_PWR_FV2,
+			RT5682S_PWR_FV1 | RT5682S_PWR_FV2);
+		snd_soc_component_update_bits(component, RT5682S_PWR_ANLG_3,
+			RT5682S_PWR_CBJ, RT5682S_PWR_CBJ);
+		snd_soc_component_write(component, RT5682S_SAR_IL_CMD_3, 0x0365);
+		snd_soc_component_update_bits(component, RT5682S_HP_CHARGE_PUMP_2,
+			RT5682S_OSW_L_MASK | RT5682S_OSW_R_MASK,
+			RT5682S_OSW_L_DIS | RT5682S_OSW_R_DIS);
+		snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_13,
+			RT5682S_SAR_SOUR_MASK, RT5682S_SAR_SOUR_TYPE);
+		rt5682s_sar_power_mode(component, SAR_PWR_NORMAL, 1);
+		snd_soc_component_update_bits(component, RT5682S_CBJ_CTRL_1,
+			RT5682S_TRIG_JD_MASK, RT5682S_TRIG_JD_LOW);
+		usleep_range(45000, 50000);
+		snd_soc_component_update_bits(component, RT5682S_CBJ_CTRL_1,
+			RT5682S_TRIG_JD_MASK, RT5682S_TRIG_JD_HIGH);
+
+		count = 0;
+		do {
+			usleep_range(10000, 15000);
+			val = snd_soc_component_read(component, RT5682S_CBJ_CTRL_2)
+				& RT5682S_JACK_TYPE_MASK;
+			count++;
+		} while (val == 0 && count < 50);
+
+		pr_debug("%s, val=%d, count=%d\n", __func__, val, count);
+
+		switch (val) {
+		case 0x1:
+		case 0x2:
+			jack_type = SND_JACK_HEADSET;
+			snd_soc_component_write(component, RT5682S_SAR_IL_CMD_3, 0x024c);
+			snd_soc_component_update_bits(component, RT5682S_CBJ_CTRL_1,
+				RT5682S_FAST_OFF_MASK, RT5682S_FAST_OFF_EN);
+			snd_soc_component_update_bits(component, RT5682S_SAR_IL_CMD_1,
+				RT5682S_SAR_SEL_MB1_2_MASK, val << RT5682S_SAR_SEL_MB1_2_SFT);
+			rt5682s_sar_power_mode(component, SAR_PWR_SAVING, 1);
+			rt5682s_enable_push_button_irq(component);
+			break;
+		default:
+			jack_type = SND_JACK_HEADPHONE;
+			break;
+		}
+		snd_soc_component_update_bits(component, RT5682S_HP_CHARGE_PUMP_2,
+			RT5682S_OSW_L_MASK | RT5682S_OSW_R_MASK,
+			RT5682S_OSW_L_EN | RT5682S_OSW_R_EN);
+	} else {
+		rt5682s_sar_power_mode(component, SAR_PWR_OFF, 1);
+		rt5682s_disable_push_button_irq(component);
+		snd_soc_component_update_bits(component, RT5682S_CBJ_CTRL_1,
+			RT5682S_TRIG_JD_MASK, RT5682S_TRIG_JD_LOW);
+
+		if (!snd_soc_dapm_get_pin_status(&component->dapm, "MICBIAS"))
+			snd_soc_component_update_bits(component,
+				RT5682S_PWR_ANLG_1, RT5682S_PWR_MB, 0);
+		if (!snd_soc_dapm_get_pin_status(&component->dapm, "Vref2"))
+			snd_soc_component_update_bits(component,
+				RT5682S_PWR_ANLG_1, RT5682S_PWR_VREF2, 0);
+
+		snd_soc_component_update_bits(component, RT5682S_PWR_ANLG_3,
+			RT5682S_PWR_CBJ, 0);
+		snd_soc_component_update_bits(component, RT5682S_CBJ_CTRL_1,
+			RT5682S_FAST_OFF_MASK, RT5682S_FAST_OFF_DIS);
+		snd_soc_component_update_bits(component, RT5682S_CBJ_CTRL_3,
+			RT5682S_CBJ_IN_BUF_MASK, RT5682S_CBJ_IN_BUF_DIS);
+		jack_type = 0;
+	}
+
+	dev_dbg(component->dev, "jack_type = %d\n", jack_type);
+
+	return jack_type;
+}
+
+static void rt5682s_jack_detect_handler(struct work_struct *work)
+{
+	struct rt5682s_priv *rt5682s =
+		container_of(work, struct rt5682s_priv, jack_detect_work.work);
+	int val, btn_type;
+
+	while (!rt5682s->component)
+		usleep_range(10000, 15000);
+
+	while (!rt5682s->component->card->instantiated)
+		usleep_range(10000, 15000);
+
+	mutex_lock(&rt5682s->calibrate_mutex);
+
+	val = snd_soc_component_read(rt5682s->component, RT5682S_AJD1_CTRL)
+		& RT5682S_JDH_RS_MASK;
+	if (!val) {
+		/* jack in */
+		if (rt5682s->jack_type == 0) {
+			/* jack was out, report jack type */
+			rt5682s->jack_type = rt5682s_headset_detect(rt5682s->component, 1);
+			rt5682s->irq_work_delay_time = 0;
+		} else if ((rt5682s->jack_type & SND_JACK_HEADSET) == SND_JACK_HEADSET) {
+			/* jack is already in, report button event */
+			rt5682s->jack_type = SND_JACK_HEADSET;
+			btn_type = rt5682s_button_detect(rt5682s->component);
+			/**
+			 * rt5682s can report three kinds of button behavior,
+			 * one click, double click and hold. However,
+			 * currently we will report button pressed/released
+			 * event. So all the three button behaviors are
+			 * treated as button pressed.
+			 */
+			switch (btn_type) {
+			case 0x8000:
+			case 0x4000:
+			case 0x2000:
+				rt5682s->jack_type |= SND_JACK_BTN_0;
+				break;
+			case 0x1000:
+			case 0x0800:
+			case 0x0400:
+				rt5682s->jack_type |= SND_JACK_BTN_1;
+				break;
+			case 0x0200:
+			case 0x0100:
+			case 0x0080:
+				rt5682s->jack_type |= SND_JACK_BTN_2;
+				break;
+			case 0x0040:
+			case 0x0020:
+			case 0x0010:
+				rt5682s->jack_type |= SND_JACK_BTN_3;
+				break;
+			case 0x0000: /* unpressed */
+				break;
+			default:
+				dev_err(rt5682s->component->dev,
+					"Unexpected button code 0x%04x\n", btn_type);
+				break;
+			}
+		}
+	} else {
+		/* jack out */
+		rt5682s->jack_type = rt5682s_headset_detect(rt5682s->component, 0);
+		rt5682s->irq_work_delay_time = 50;
+	}
+
+	snd_soc_jack_report(rt5682s->hs_jack, rt5682s->jack_type,
+		SND_JACK_HEADSET | SND_JACK_BTN_0 | SND_JACK_BTN_1 |
+		SND_JACK_BTN_2 | SND_JACK_BTN_3);
+
+	if (rt5682s->jack_type & (SND_JACK_BTN_0 | SND_JACK_BTN_1 |
+		SND_JACK_BTN_2 | SND_JACK_BTN_3))
+		schedule_delayed_work(&rt5682s->jd_check_work, 0);
+	else
+		cancel_delayed_work_sync(&rt5682s->jd_check_work);
+
+	mutex_unlock(&rt5682s->calibrate_mutex);
+}
+
+static void rt5682s_jd_check_handler(struct work_struct *work)
+{
+	struct rt5682s_priv *rt5682s =
+		container_of(work, struct rt5682s_priv, jd_check_work.work);
+
+	if (snd_soc_component_read(rt5682s->component, RT5682S_AJD1_CTRL)
+		& RT5682S_JDH_RS_MASK) {
+		/* jack out */
+		rt5682s->jack_type = rt5682s_headset_detect(rt5682s->component, 0);
+
+		snd_soc_jack_report(rt5682s->hs_jack, rt5682s->jack_type,
+			SND_JACK_HEADSET | SND_JACK_BTN_0 | SND_JACK_BTN_1 |
+			SND_JACK_BTN_2 | SND_JACK_BTN_3);
+	} else {
+		schedule_delayed_work(&rt5682s->jd_check_work, 500);
+	}
+}
+
+static irqreturn_t rt5682s_irq(int irq, void *data)
+{
+	struct rt5682s_priv *rt5682s = data;
+
+	mod_delayed_work(system_power_efficient_wq, &rt5682s->jack_detect_work,
+		msecs_to_jiffies(rt5682s->irq_work_delay_time));
+
+	return IRQ_HANDLED;
+}
+
+static int rt5682s_set_jack_detect(struct snd_soc_component *component,
+		struct snd_soc_jack *hs_jack, void *data)
+{
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	int btndet_delay = 16;
+
+	rt5682s->hs_jack = hs_jack;
+
+	if (!hs_jack) {
+		regmap_update_bits(rt5682s->regmap, RT5682S_IRQ_CTRL_2,
+			RT5682S_JD1_EN_MASK, RT5682S_JD1_DIS);
+		regmap_update_bits(rt5682s->regmap, RT5682S_RC_CLK_CTRL,
+			RT5682S_POW_JDH, 0);
+		cancel_delayed_work_sync(&rt5682s->jack_detect_work);
+
+		return 0;
+	}
+
+	switch (rt5682s->pdata.jd_src) {
+	case RT5682S_JD1:
+		regmap_update_bits(rt5682s->regmap, RT5682S_CBJ_CTRL_5,
+			RT5682S_JD_FAST_OFF_SRC_MASK, RT5682S_JD_FAST_OFF_SRC_JDH);
+		regmap_update_bits(rt5682s->regmap, RT5682S_CBJ_CTRL_2,
+			RT5682S_EXT_JD_SRC, RT5682S_EXT_JD_SRC_MANUAL);
+		regmap_update_bits(rt5682s->regmap, RT5682S_CBJ_CTRL_1,
+			RT5682S_EMB_JD_MASK | RT5682S_DET_TYPE |
+			RT5682S_POL_FAST_OFF_MASK | RT5682S_MIC_CAP_MASK,
+			RT5682S_EMB_JD_EN | RT5682S_DET_TYPE |
+			RT5682S_POL_FAST_OFF_HIGH | RT5682S_MIC_CAP_HS);
+		regmap_update_bits(rt5682s->regmap, RT5682S_SAR_IL_CMD_1,
+			RT5682S_SAR_POW_MASK, RT5682S_SAR_POW_EN);
+		regmap_update_bits(rt5682s->regmap, RT5682S_GPIO_CTRL_1,
+			RT5682S_GP1_PIN_MASK, RT5682S_GP1_PIN_IRQ);
+		regmap_update_bits(rt5682s->regmap, RT5682S_PWR_ANLG_3,
+			RT5682S_PWR_BGLDO, RT5682S_PWR_BGLDO);
+		regmap_update_bits(rt5682s->regmap, RT5682S_PWR_ANLG_2,
+			RT5682S_PWR_JD_MASK, RT5682S_PWR_JD_ENABLE);
+		regmap_update_bits(rt5682s->regmap, RT5682S_RC_CLK_CTRL,
+			RT5682S_POW_IRQ | RT5682S_POW_JDH, RT5682S_POW_IRQ | RT5682S_POW_JDH);
+		regmap_update_bits(rt5682s->regmap, RT5682S_IRQ_CTRL_2,
+			RT5682S_JD1_EN_MASK | RT5682S_JD1_POL_MASK,
+			RT5682S_JD1_EN | RT5682S_JD1_POL_NOR);
+		regmap_update_bits(rt5682s->regmap, RT5682S_4BTN_IL_CMD_4,
+			RT5682S_4BTN_IL_HOLD_WIN_MASK | RT5682S_4BTN_IL_CLICK_WIN_MASK,
+			(btndet_delay << RT5682S_4BTN_IL_HOLD_WIN_SFT | btndet_delay));
+		regmap_update_bits(rt5682s->regmap, RT5682S_4BTN_IL_CMD_5,
+			RT5682S_4BTN_IL_HOLD_WIN_MASK | RT5682S_4BTN_IL_CLICK_WIN_MASK,
+			(btndet_delay << RT5682S_4BTN_IL_HOLD_WIN_SFT | btndet_delay));
+		regmap_update_bits(rt5682s->regmap, RT5682S_4BTN_IL_CMD_6,
+			RT5682S_4BTN_IL_HOLD_WIN_MASK | RT5682S_4BTN_IL_CLICK_WIN_MASK,
+			(btndet_delay << RT5682S_4BTN_IL_HOLD_WIN_SFT | btndet_delay));
+		regmap_update_bits(rt5682s->regmap, RT5682S_4BTN_IL_CMD_7,
+			RT5682S_4BTN_IL_HOLD_WIN_MASK | RT5682S_4BTN_IL_CLICK_WIN_MASK,
+			(btndet_delay << RT5682S_4BTN_IL_HOLD_WIN_SFT | btndet_delay));
+
+		mod_delayed_work(system_power_efficient_wq,
+			&rt5682s->jack_detect_work, msecs_to_jiffies(250));
+		break;
+
+	case RT5682S_JD_NULL:
+		regmap_update_bits(rt5682s->regmap, RT5682S_IRQ_CTRL_2,
+			RT5682S_JD1_EN_MASK, RT5682S_JD1_DIS);
+		regmap_update_bits(rt5682s->regmap, RT5682S_RC_CLK_CTRL,
+			RT5682S_POW_JDH, 0);
+		break;
+
+	default:
+		dev_warn(component->dev, "Wrong JD source\n");
+		break;
+	}
+
+	return 0;
+}
+
+static const DECLARE_TLV_DB_SCALE(dac_vol_tlv, -9450, 150, 0);
+static const DECLARE_TLV_DB_SCALE(adc_vol_tlv, -1725, 75, 0);
+static const DECLARE_TLV_DB_SCALE(adc_bst_tlv, 0, 1200, 0);
+static const DECLARE_TLV_DB_SCALE(cbj_bst_tlv, -1200, 150, 0);
+
+static const struct snd_kcontrol_new rt5682s_snd_controls[] = {
+	/* DAC Digital Volume */
+	SOC_DOUBLE_TLV("DAC1 Playback Volume", RT5682S_DAC1_DIG_VOL,
+		RT5682S_L_VOL_SFT + 2, RT5682S_R_VOL_SFT + 2, 63, 0, dac_vol_tlv),
+
+	/* CBJ Boost Volume */
+	SOC_SINGLE_TLV("CBJ Boost Volume", RT5682S_REC_MIXER,
+		RT5682S_BST_CBJ_SFT, 35, 0,  cbj_bst_tlv),
+
+	/* ADC Digital Volume Control */
+	SOC_DOUBLE("STO1 ADC Capture Switch", RT5682S_STO1_ADC_DIG_VOL,
+		RT5682S_L_MUTE_SFT, RT5682S_R_MUTE_SFT, 1, 1),
+	SOC_DOUBLE_TLV("STO1 ADC Capture Volume", RT5682S_STO1_ADC_DIG_VOL,
+		RT5682S_L_VOL_SFT + 1, RT5682S_R_VOL_SFT + 1, 63, 0, adc_vol_tlv),
+
+	/* ADC Boost Volume Control */
+	SOC_DOUBLE_TLV("STO1 ADC Boost Gain Volume", RT5682S_STO1_ADC_BOOST,
+		RT5682S_STO1_ADC_L_BST_SFT, RT5682S_STO1_ADC_R_BST_SFT, 3, 0, adc_bst_tlv),
+};
+
+/**
+ * rt5682s_sel_asrc_clk_src - select ASRC clock source for a set of filters
+ * @component: SoC audio component device.
+ * @filter_mask: mask of filters.
+ * @clk_src: clock source
+ *
+ * The ASRC function is for asynchronous MCLK and LRCK. Also, since RT5682S can
+ * only support standard 32fs or 64fs i2s format, ASRC should be enabled to
+ * support special i2s clock format such as Intel's 100fs(100 * sampling rate).
+ * ASRC function will track i2s clock and generate a corresponding system clock
+ * for codec. This function provides an API to select the clock source for a
+ * set of filters specified by the mask. And the component driver will turn on
+ * ASRC for these filters if ASRC is selected as their clock source.
+ */
+int rt5682s_sel_asrc_clk_src(struct snd_soc_component *component,
+		unsigned int filter_mask, unsigned int clk_src)
+{
+	switch (clk_src) {
+	case RT5682S_CLK_SEL_SYS:
+	case RT5682S_CLK_SEL_I2S1_ASRC:
+	case RT5682S_CLK_SEL_I2S2_ASRC:
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	if (filter_mask & RT5682S_DA_STEREO1_FILTER) {
+		snd_soc_component_update_bits(component, RT5682S_PLL_TRACK_2,
+			RT5682S_FILTER_CLK_SEL_MASK, clk_src << RT5682S_FILTER_CLK_SEL_SFT);
+	}
+
+	if (filter_mask & RT5682S_AD_STEREO1_FILTER) {
+		snd_soc_component_update_bits(component, RT5682S_PLL_TRACK_3,
+			RT5682S_FILTER_CLK_SEL_MASK, clk_src << RT5682S_FILTER_CLK_SEL_SFT);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(rt5682s_sel_asrc_clk_src);
+
+static int rt5682s_div_sel(struct rt5682s_priv *rt5682s,
+		int target, const int div[], int size)
+{
+	int i;
+
+	if (rt5682s->sysclk < target) {
+		dev_err(rt5682s->component->dev,
+			"sysclk rate %d is too low\n", rt5682s->sysclk);
+		return 0;
+	}
+
+	for (i = 0; i < size - 1; i++) {
+		dev_dbg(rt5682s->component->dev, "div[%d]=%d\n", i, div[i]);
+		if (target * div[i] == rt5682s->sysclk)
+			return i;
+		if (target * div[i + 1] > rt5682s->sysclk) {
+			dev_dbg(rt5682s->component->dev,
+				"can't find div for sysclk %d\n", rt5682s->sysclk);
+			return i;
+		}
+	}
+
+	if (target * div[i] < rt5682s->sysclk)
+		dev_err(rt5682s->component->dev,
+			"sysclk rate %d is too high\n", rt5682s->sysclk);
+
+	return size - 1;
+}
+
+static int get_clk_info(int sclk, int rate)
+{
+	int i;
+	static const int pd[] = {1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48};
+
+	if (sclk <= 0 || rate <= 0)
+		return -EINVAL;
+
+	rate = rate << 8;
+	for (i = 0; i < ARRAY_SIZE(pd); i++)
+		if (sclk == rate * pd[i])
+			return i;
+
+	return -EINVAL;
+}
+
+/**
+ * set_dmic_clk - Set parameter of dmic.
+ *
+ * @w: DAPM widget.
+ * @kcontrol: The kcontrol of this widget.
+ * @event: Event id.
+ *
+ * Choose dmic clock between 1MHz and 3MHz.
+ * It is better for clock to approximate 3MHz.
+ */
+static int set_dmic_clk(struct snd_soc_dapm_widget *w,
+		struct snd_kcontrol *kcontrol, int event)
+{
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	int idx, dmic_clk_rate = 3072000;
+	static const int div[] = {2, 4, 6, 8, 12, 16, 24, 32, 48, 64, 96, 128};
+
+	if (rt5682s->pdata.dmic_clk_rate)
+		dmic_clk_rate = rt5682s->pdata.dmic_clk_rate;
+
+	idx = rt5682s_div_sel(rt5682s, dmic_clk_rate, div, ARRAY_SIZE(div));
+
+	snd_soc_component_update_bits(component, RT5682S_DMIC_CTRL_1,
+		RT5682S_DMIC_CLK_MASK, idx << RT5682S_DMIC_CLK_SFT);
+
+	return 0;
+}
+
+static int set_filter_clk(struct snd_soc_dapm_widget *w,
+		struct snd_kcontrol *kcontrol, int event)
+{
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	int ref, val, reg, idx;
+	static const int div_f[] = {1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48};
+	static const int div_o[] = {1, 2, 4, 6, 8, 12, 16, 24, 32, 48};
+
+	val = snd_soc_component_read(component, RT5682S_GPIO_CTRL_1)
+			& RT5682S_GP4_PIN_MASK;
+
+	if (w->shift == RT5682S_PWR_ADC_S1F_BIT && val == RT5682S_GP4_PIN_ADCDAT2)
+		ref = 256 * rt5682s->lrck[RT5682S_AIF2];
+	else
+		ref = 256 * rt5682s->lrck[RT5682S_AIF1];
+
+	idx = rt5682s_div_sel(rt5682s, ref, div_f, ARRAY_SIZE(div_f));
+
+	if (w->shift == RT5682S_PWR_ADC_S1F_BIT)
+		reg = RT5682S_PLL_TRACK_3;
+	else
+		reg = RT5682S_PLL_TRACK_2;
+
+	snd_soc_component_update_bits(component, reg,
+		RT5682S_FILTER_CLK_DIV_MASK, idx << RT5682S_FILTER_CLK_DIV_SFT);
+
+	/* select over sample rate */
+	for (idx = 0; idx < ARRAY_SIZE(div_o); idx++) {
+		if (rt5682s->sysclk <= 12288000 * div_o[idx])
+			break;
+	}
+
+	snd_soc_component_update_bits(component, RT5682S_ADDA_CLK_1,
+		RT5682S_ADC_OSR_MASK | RT5682S_DAC_OSR_MASK,
+		(idx << RT5682S_ADC_OSR_SFT) | (idx << RT5682S_DAC_OSR_SFT));
+
+	return 0;
+}
+
+static int set_dmic_power(struct snd_soc_dapm_widget *w,
+		struct snd_kcontrol *kcontrol, int event)
+{
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	unsigned int delay = 50, val;
+
+	if (rt5682s->pdata.dmic_delay)
+		delay = rt5682s->pdata.dmic_delay;
+
+	switch (event) {
+	case SND_SOC_DAPM_POST_PMU:
+		val = (snd_soc_component_read(component, RT5682S_GLB_CLK)
+			& RT5682S_SCLK_SRC_MASK) >> RT5682S_SCLK_SRC_SFT;
+		if (val == RT5682S_CLK_SRC_PLL1 || val == RT5682S_CLK_SRC_PLL2)
+			snd_soc_component_update_bits(component, RT5682S_PWR_ANLG_1,
+				RT5682S_PWR_VREF2 | RT5682S_PWR_MB,
+				RT5682S_PWR_VREF2 | RT5682S_PWR_MB);
+
+		/*Add delay to avoid pop noise*/
+		msleep(delay);
+		break;
+
+	case SND_SOC_DAPM_POST_PMD:
+		if (!rt5682s->jack_type) {
+			if (!snd_soc_dapm_get_pin_status(w->dapm, "MICBIAS"))
+				snd_soc_component_update_bits(component,
+					RT5682S_PWR_ANLG_1, RT5682S_PWR_MB, 0);
+			if (!snd_soc_dapm_get_pin_status(w->dapm, "Vref2"))
+				snd_soc_component_update_bits(component,
+					RT5682S_PWR_ANLG_1, RT5682S_PWR_VREF2, 0);
+		}
+		break;
+	}
+
+	return 0;
+}
+
+static int set_i2s_clk(struct snd_soc_dapm_widget *w,
+		struct snd_kcontrol *kcontrol, int event)
+{
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	int pre_div, id;
+	unsigned int reg, mask, sft;
+
+	if (event != SND_SOC_DAPM_PRE_PMU)
+		return 0;
+
+	if (w->shift == RT5682S_PWR_I2S2_BIT) {
+		id = RT5682S_AIF2;
+		reg = RT5682S_I2S2_M_CLK_CTRL_1;
+		mask = RT5682S_I2S2_M_D_MASK;
+		sft = RT5682S_I2S2_M_D_SFT;
+	} else {
+		id = RT5682S_AIF1;
+		reg = RT5682S_ADDA_CLK_1;
+		mask = RT5682S_I2S_M_D_MASK;
+		sft = RT5682S_I2S_M_D_SFT;
+	}
+
+	if (!rt5682s->master[id])
+		return 0;
+
+	pre_div = get_clk_info(rt5682s->sysclk, rt5682s->lrck[id]);
+	if (pre_div < 0) {
+		dev_err(component->dev, "get pre_div failed\n");
+		return -EINVAL;
+	}
+
+	dev_dbg(component->dev, "lrck is %dHz and pre_div is %d for iis %d master\n",
+		rt5682s->lrck[id], pre_div, id);
+	snd_soc_component_update_bits(component, reg, mask, pre_div << sft);
+
+	return 0;
+}
+
+static int is_sys_clk_from_plla(struct snd_soc_dapm_widget *w,
+		struct snd_soc_dapm_widget *sink)
+{
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+
+	if ((rt5682s->sysclk_src == RT5682S_CLK_SRC_PLL1) ||
+	    (rt5682s->sysclk_src == RT5682S_CLK_SRC_PLL2 && rt5682s->pll_comb == USE_PLLAB))
+		return 1;
+
+	return 0;
+}
+
+static int is_sys_clk_from_pllb(struct snd_soc_dapm_widget *w,
+		struct snd_soc_dapm_widget *sink)
+{
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+
+	if (rt5682s->sysclk_src == RT5682S_CLK_SRC_PLL2)
+		return 1;
+
+	return 0;
+}
+
+static int is_using_asrc(struct snd_soc_dapm_widget *w,
+		struct snd_soc_dapm_widget *sink)
+{
+	unsigned int reg, sft, val;
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+
+	switch (w->shift) {
+	case RT5682S_ADC_STO1_ASRC_SFT:
+		reg = RT5682S_PLL_TRACK_3;
+		sft = RT5682S_FILTER_CLK_SEL_SFT;
+		break;
+	case RT5682S_DAC_STO1_ASRC_SFT:
+		reg = RT5682S_PLL_TRACK_2;
+		sft = RT5682S_FILTER_CLK_SEL_SFT;
+		break;
+	default:
+		return 0;
+	}
+
+	val = (snd_soc_component_read(component, reg) >> sft) & 0xf;
+	switch (val) {
+	case RT5682S_CLK_SEL_I2S1_ASRC:
+	case RT5682S_CLK_SEL_I2S2_ASRC:
+		return 1;
+	default:
+		return 0;
+	}
+}
+
+static int is_headset_type(struct snd_soc_dapm_widget *w,
+		struct snd_soc_dapm_widget *sink)
+{
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+
+	if ((rt5682s->jack_type & SND_JACK_HEADSET) == SND_JACK_HEADSET)
+		return 1;
+
+	return 0;
+}
+
+static int rt5682s_hp_amp_event(struct snd_soc_dapm_widget *w,
+		struct snd_kcontrol *kcontrol, int event)
+{
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+
+	switch (event) {
+	case SND_SOC_DAPM_PRE_PMU:
+		snd_soc_component_update_bits(component, RT5682S_DEPOP_1,
+			RT5682S_OUT_HP_L_EN | RT5682S_OUT_HP_R_EN,
+			RT5682S_OUT_HP_L_EN | RT5682S_OUT_HP_R_EN);
+		snd_soc_component_update_bits(component, RT5682S_DEPOP_1,
+			RT5682S_LDO_PUMP_EN | RT5682S_PUMP_EN |
+			RT5682S_CAPLESS_L_EN | RT5682S_CAPLESS_R_EN,
+			RT5682S_LDO_PUMP_EN | RT5682S_PUMP_EN |
+			RT5682S_CAPLESS_L_EN | RT5682S_CAPLESS_R_EN);
+		break;
+
+	case SND_SOC_DAPM_POST_PMU:
+		usleep_range(30000, 35000);
+		snd_soc_component_write(component, RT5682S_BIAS_CUR_CTRL_11, 0x6666);
+		snd_soc_component_write(component, RT5682S_BIAS_CUR_CTRL_12, 0xa82a);
+		snd_soc_component_update_bits(component, RT5682S_HP_CTRL_2,
+			RT5682S_HPO_L_PATH_MASK | RT5682S_HPO_R_PATH_MASK |
+			RT5682S_HPO_SEL_IP_EN_SW, RT5682S_HPO_L_PATH_EN |
+			RT5682S_HPO_R_PATH_EN | RT5682S_HPO_IP_EN_GATING);
+		snd_soc_component_write(component, RT5682S_HP_AMP_DET_CTL_1, 0x3050);
+		break;
+
+	case SND_SOC_DAPM_POST_PMD:
+		snd_soc_component_update_bits(component, RT5682S_HP_CTRL_2,
+			RT5682S_HPO_L_PATH_MASK | RT5682S_HPO_R_PATH_MASK |
+			RT5682S_HPO_SEL_IP_EN_SW, 0);
+		snd_soc_component_update_bits(component, RT5682S_DEPOP_1,
+			RT5682S_LDO_PUMP_EN | RT5682S_PUMP_EN |
+			RT5682S_CAPLESS_L_EN | RT5682S_CAPLESS_R_EN, 0);
+		snd_soc_component_update_bits(component, RT5682S_DEPOP_1,
+			RT5682S_OUT_HP_L_EN | RT5682S_OUT_HP_R_EN, 0);
+		break;
+	}
+
+	return 0;
+}
+
+static int sar_power_event(struct snd_soc_dapm_widget *w,
+		struct snd_kcontrol *kcontrol, int event)
+{
+	struct snd_soc_component *component = snd_soc_dapm_to_component(w->dapm);
+
+	switch (event) {
+	case SND_SOC_DAPM_PRE_PMU:
+		rt5682s_sar_power_mode(component, SAR_PWR_NORMAL, 0);
+		break;
+	case SND_SOC_DAPM_POST_PMD:
+		rt5682s_sar_power_mode(component, SAR_PWR_SAVING, 0);
+		break;
+	}
+
+	return 0;
+}
+
+/* Interface data select */
+static const char * const rt5682s_data_select[] = {
+	"L/R", "R/L", "L/L", "R/R"
+};
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_if2_adc_enum, RT5682S_DIG_INF2_DATA,
+	RT5682S_IF2_ADC_SEL_SFT, rt5682s_data_select);
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_if1_01_adc_enum, RT5682S_TDM_ADDA_CTRL_1,
+	RT5682S_IF1_ADC1_SEL_SFT, rt5682s_data_select);
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_if1_23_adc_enum, RT5682S_TDM_ADDA_CTRL_1,
+	RT5682S_IF1_ADC2_SEL_SFT, rt5682s_data_select);
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_if1_45_adc_enum, RT5682S_TDM_ADDA_CTRL_1,
+	RT5682S_IF1_ADC3_SEL_SFT, rt5682s_data_select);
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_if1_67_adc_enum, RT5682S_TDM_ADDA_CTRL_1,
+	RT5682S_IF1_ADC4_SEL_SFT, rt5682s_data_select);
+
+static const struct snd_kcontrol_new rt5682s_if2_adc_swap_mux =
+	SOC_DAPM_ENUM("IF2 ADC Swap Mux", rt5682s_if2_adc_enum);
+
+static const struct snd_kcontrol_new rt5682s_if1_01_adc_swap_mux =
+	SOC_DAPM_ENUM("IF1 01 ADC Swap Mux", rt5682s_if1_01_adc_enum);
+
+static const struct snd_kcontrol_new rt5682s_if1_23_adc_swap_mux =
+	SOC_DAPM_ENUM("IF1 23 ADC Swap Mux", rt5682s_if1_23_adc_enum);
+
+static const struct snd_kcontrol_new rt5682s_if1_45_adc_swap_mux =
+	SOC_DAPM_ENUM("IF1 45 ADC Swap Mux", rt5682s_if1_45_adc_enum);
+
+static const struct snd_kcontrol_new rt5682s_if1_67_adc_swap_mux =
+	SOC_DAPM_ENUM("IF1 67 ADC Swap Mux", rt5682s_if1_67_adc_enum);
+
+/* Digital Mixer */
+static const struct snd_kcontrol_new rt5682s_sto1_adc_l_mix[] = {
+	SOC_DAPM_SINGLE("ADC1 Switch", RT5682S_STO1_ADC_MIXER,
+			RT5682S_M_STO1_ADC_L1_SFT, 1, 1),
+	SOC_DAPM_SINGLE("ADC2 Switch", RT5682S_STO1_ADC_MIXER,
+			RT5682S_M_STO1_ADC_L2_SFT, 1, 1),
+};
+
+static const struct snd_kcontrol_new rt5682s_sto1_adc_r_mix[] = {
+	SOC_DAPM_SINGLE("ADC1 Switch", RT5682S_STO1_ADC_MIXER,
+			RT5682S_M_STO1_ADC_R1_SFT, 1, 1),
+	SOC_DAPM_SINGLE("ADC2 Switch", RT5682S_STO1_ADC_MIXER,
+			RT5682S_M_STO1_ADC_R2_SFT, 1, 1),
+};
+
+static const struct snd_kcontrol_new rt5682s_dac_l_mix[] = {
+	SOC_DAPM_SINGLE("Stereo ADC Switch", RT5682S_AD_DA_MIXER,
+			RT5682S_M_ADCMIX_L_SFT, 1, 1),
+	SOC_DAPM_SINGLE("DAC1 Switch", RT5682S_AD_DA_MIXER,
+			RT5682S_M_DAC1_L_SFT, 1, 1),
+};
+
+static const struct snd_kcontrol_new rt5682s_dac_r_mix[] = {
+	SOC_DAPM_SINGLE("Stereo ADC Switch", RT5682S_AD_DA_MIXER,
+			RT5682S_M_ADCMIX_R_SFT, 1, 1),
+	SOC_DAPM_SINGLE("DAC1 Switch", RT5682S_AD_DA_MIXER,
+			RT5682S_M_DAC1_R_SFT, 1, 1),
+};
+
+static const struct snd_kcontrol_new rt5682s_sto1_dac_l_mix[] = {
+	SOC_DAPM_SINGLE("DAC L1 Switch", RT5682S_STO1_DAC_MIXER,
+			RT5682S_M_DAC_L1_STO_L_SFT, 1, 1),
+	SOC_DAPM_SINGLE("DAC R1 Switch", RT5682S_STO1_DAC_MIXER,
+			RT5682S_M_DAC_R1_STO_L_SFT, 1, 1),
+};
+
+static const struct snd_kcontrol_new rt5682s_sto1_dac_r_mix[] = {
+	SOC_DAPM_SINGLE("DAC L1 Switch", RT5682S_STO1_DAC_MIXER,
+			RT5682S_M_DAC_L1_STO_R_SFT, 1, 1),
+	SOC_DAPM_SINGLE("DAC R1 Switch", RT5682S_STO1_DAC_MIXER,
+			RT5682S_M_DAC_R1_STO_R_SFT, 1, 1),
+};
+
+/* Analog Input Mixer */
+static const struct snd_kcontrol_new rt5682s_rec1_l_mix[] = {
+	SOC_DAPM_SINGLE("CBJ Switch", RT5682S_REC_MIXER,
+			RT5682S_M_CBJ_RM1_L_SFT, 1, 1),
+};
+
+static const struct snd_kcontrol_new rt5682s_rec1_r_mix[] = {
+	SOC_DAPM_SINGLE("CBJ Switch", RT5682S_REC_MIXER,
+			RT5682S_M_CBJ_RM1_R_SFT, 1, 1),
+};
+
+/* STO1 ADC1 Source */
+/* MX-26 [13] [5] */
+static const char * const rt5682s_sto1_adc1_src[] = {
+	"DAC MIX", "ADC"
+};
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_sto1_adc1l_enum, RT5682S_STO1_ADC_MIXER,
+	RT5682S_STO1_ADC1L_SRC_SFT, rt5682s_sto1_adc1_src);
+
+static const struct snd_kcontrol_new rt5682s_sto1_adc1l_mux =
+	SOC_DAPM_ENUM("Stereo1 ADC1L Source", rt5682s_sto1_adc1l_enum);
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_sto1_adc1r_enum, RT5682S_STO1_ADC_MIXER,
+	RT5682S_STO1_ADC1R_SRC_SFT, rt5682s_sto1_adc1_src);
+
+static const struct snd_kcontrol_new rt5682s_sto1_adc1r_mux =
+	SOC_DAPM_ENUM("Stereo1 ADC1L Source", rt5682s_sto1_adc1r_enum);
+
+/* STO1 ADC Source */
+/* MX-26 [11:10] [3:2] */
+static const char * const rt5682s_sto1_adc_src[] = {
+	"ADC1 L", "ADC1 R"
+};
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_sto1_adcl_enum, RT5682S_STO1_ADC_MIXER,
+	RT5682S_STO1_ADCL_SRC_SFT, rt5682s_sto1_adc_src);
+
+static const struct snd_kcontrol_new rt5682s_sto1_adcl_mux =
+	SOC_DAPM_ENUM("Stereo1 ADCL Source", rt5682s_sto1_adcl_enum);
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_sto1_adcr_enum, RT5682S_STO1_ADC_MIXER,
+	RT5682S_STO1_ADCR_SRC_SFT, rt5682s_sto1_adc_src);
+
+static const struct snd_kcontrol_new rt5682s_sto1_adcr_mux =
+	SOC_DAPM_ENUM("Stereo1 ADCR Source", rt5682s_sto1_adcr_enum);
+
+/* STO1 ADC2 Source */
+/* MX-26 [12] [4] */
+static const char * const rt5682s_sto1_adc2_src[] = {
+	"DAC MIX", "DMIC"
+};
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_sto1_adc2l_enum, RT5682S_STO1_ADC_MIXER,
+	RT5682S_STO1_ADC2L_SRC_SFT, rt5682s_sto1_adc2_src);
+
+static const struct snd_kcontrol_new rt5682s_sto1_adc2l_mux =
+	SOC_DAPM_ENUM("Stereo1 ADC2L Source", rt5682s_sto1_adc2l_enum);
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_sto1_adc2r_enum, RT5682S_STO1_ADC_MIXER,
+	RT5682S_STO1_ADC2R_SRC_SFT, rt5682s_sto1_adc2_src);
+
+static const struct snd_kcontrol_new rt5682s_sto1_adc2r_mux =
+	SOC_DAPM_ENUM("Stereo1 ADC2R Source", rt5682s_sto1_adc2r_enum);
+
+/* MX-79 [6:4] I2S1 ADC data location */
+static const unsigned int rt5682s_if1_adc_slot_values[] = {
+	0, 2, 4, 6,
+};
+
+static const char * const rt5682s_if1_adc_slot_src[] = {
+	"Slot 0", "Slot 2", "Slot 4", "Slot 6"
+};
+
+static SOC_VALUE_ENUM_SINGLE_DECL(rt5682s_if1_adc_slot_enum,
+	RT5682S_TDM_CTRL, RT5682S_TDM_ADC_LCA_SFT, RT5682S_TDM_ADC_LCA_MASK,
+	rt5682s_if1_adc_slot_src, rt5682s_if1_adc_slot_values);
+
+static const struct snd_kcontrol_new rt5682s_if1_adc_slot_mux =
+	SOC_DAPM_ENUM("IF1 ADC Slot location", rt5682s_if1_adc_slot_enum);
+
+/* Analog DAC L1 Source, Analog DAC R1 Source*/
+/* MX-2B [4], MX-2B [0]*/
+static const char * const rt5682s_alg_dac1_src[] = {
+	"Stereo1 DAC Mixer", "DAC1"
+};
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_alg_dac_l1_enum, RT5682S_A_DAC1_MUX,
+	RT5682S_A_DACL1_SFT, rt5682s_alg_dac1_src);
+
+static const struct snd_kcontrol_new rt5682s_alg_dac_l1_mux =
+	SOC_DAPM_ENUM("Analog DAC L1 Source", rt5682s_alg_dac_l1_enum);
+
+static SOC_ENUM_SINGLE_DECL(rt5682s_alg_dac_r1_enum, RT5682S_A_DAC1_MUX,
+	RT5682S_A_DACR1_SFT, rt5682s_alg_dac1_src);
+
+static const struct snd_kcontrol_new rt5682s_alg_dac_r1_mux =
+	SOC_DAPM_ENUM("Analog DAC R1 Source", rt5682s_alg_dac_r1_enum);
+
+static const unsigned int rt5682s_adcdat_pin_values[] = {
+	1, 3,
+};
+
+static const char * const rt5682s_adcdat_pin_select[] = {
+	"ADCDAT1", "ADCDAT2",
+};
+
+static SOC_VALUE_ENUM_SINGLE_DECL(rt5682s_adcdat_pin_enum,
+	RT5682S_GPIO_CTRL_1, RT5682S_GP4_PIN_SFT, RT5682S_GP4_PIN_MASK,
+	rt5682s_adcdat_pin_select, rt5682s_adcdat_pin_values);
+
+static const struct snd_kcontrol_new rt5682s_adcdat_pin_ctrl =
+	SOC_DAPM_ENUM("ADCDAT", rt5682s_adcdat_pin_enum);
+
+static const struct snd_soc_dapm_widget rt5682s_dapm_widgets[] = {
+	SND_SOC_DAPM_SUPPLY("LDO MB1", RT5682S_PWR_ANLG_3,
+		RT5682S_PWR_LDO_MB1_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY("LDO MB2", RT5682S_PWR_ANLG_3,
+		RT5682S_PWR_LDO_MB2_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY("LDO", RT5682S_PWR_ANLG_3,
+		RT5682S_PWR_LDO_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY("Vref2", SND_SOC_NOPM, 0, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY("MICBIAS", SND_SOC_NOPM, 0, 0, NULL, 0),
+
+	/* PLL Powers */
+	SND_SOC_DAPM_SUPPLY_S("PLLA_LDO", 0, RT5682S_PWR_ANLG_3,
+		RT5682S_PWR_LDO_PLLA_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY_S("PLLB_LDO", 0, RT5682S_PWR_ANLG_3,
+		RT5682S_PWR_LDO_PLLB_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY_S("PLLA_BIAS", 0, RT5682S_PWR_ANLG_3,
+		RT5682S_PWR_BIAS_PLLA_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY_S("PLLB_BIAS", 0, RT5682S_PWR_ANLG_3,
+		RT5682S_PWR_BIAS_PLLB_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY_S("PLLA", 0, RT5682S_PWR_ANLG_3,
+		RT5682S_PWR_PLLA_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY_S("PLLB", 0, RT5682S_PWR_ANLG_3,
+		RT5682S_PWR_PLLB_BIT, 0, set_filter_clk, SND_SOC_DAPM_PRE_PMU),
+	SND_SOC_DAPM_SUPPLY_S("PLLA_RST", 1, RT5682S_PWR_ANLG_3,
+		RT5682S_RSTB_PLLA_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY_S("PLLB_RST", 1, RT5682S_PWR_ANLG_3,
+		RT5682S_RSTB_PLLB_BIT, 0, NULL, 0),
+
+	/* ASRC */
+	SND_SOC_DAPM_SUPPLY_S("DAC STO1 ASRC", 1, RT5682S_PLL_TRACK_1,
+		RT5682S_DAC_STO1_ASRC_SFT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY_S("ADC STO1 ASRC", 1, RT5682S_PLL_TRACK_1,
+		RT5682S_ADC_STO1_ASRC_SFT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY_S("AD ASRC", 1, RT5682S_PLL_TRACK_1,
+		RT5682S_AD_ASRC_SFT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY_S("DA ASRC", 1, RT5682S_PLL_TRACK_1,
+		RT5682S_DA_ASRC_SFT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY_S("DMIC ASRC", 1, RT5682S_PLL_TRACK_1,
+		RT5682S_DMIC_ASRC_SFT, 0, NULL, 0),
+
+	/* Input Side */
+	SND_SOC_DAPM_SUPPLY("MICBIAS1", RT5682S_PWR_ANLG_2,
+		RT5682S_PWR_MB1_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY("MICBIAS2", RT5682S_PWR_ANLG_2,
+		RT5682S_PWR_MB2_BIT, 0, NULL, 0),
+
+	/* Input Lines */
+	SND_SOC_DAPM_INPUT("DMIC L1"),
+	SND_SOC_DAPM_INPUT("DMIC R1"),
+
+	SND_SOC_DAPM_INPUT("IN1P"),
+
+	SND_SOC_DAPM_SUPPLY("DMIC CLK", SND_SOC_NOPM, 0, 0,
+		set_dmic_clk, SND_SOC_DAPM_PRE_PMU),
+	SND_SOC_DAPM_SUPPLY("DMIC1 Power", RT5682S_DMIC_CTRL_1, RT5682S_DMIC_1_EN_SFT, 0,
+		set_dmic_power, SND_SOC_DAPM_POST_PMU | SND_SOC_DAPM_POST_PMD),
+
+	/* Boost */
+	SND_SOC_DAPM_PGA("BST1 CBJ", SND_SOC_NOPM, 0, 0, NULL, 0),
+
+	/* REC Mixer */
+	SND_SOC_DAPM_MIXER("RECMIX1L", SND_SOC_NOPM, 0, 0, rt5682s_rec1_l_mix,
+		ARRAY_SIZE(rt5682s_rec1_l_mix)),
+	SND_SOC_DAPM_MIXER("RECMIX1R", SND_SOC_NOPM, 0, 0, rt5682s_rec1_r_mix,
+		ARRAY_SIZE(rt5682s_rec1_r_mix)),
+	SND_SOC_DAPM_SUPPLY("RECMIX1L Power", RT5682S_CAL_REC,
+		RT5682S_PWR_RM1_L_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY("RECMIX1R Power", RT5682S_CAL_REC,
+		RT5682S_PWR_RM1_R_BIT, 0, NULL, 0),
+
+	/* ADCs */
+	SND_SOC_DAPM_ADC("ADC1 L", NULL, SND_SOC_NOPM, 0, 0),
+	SND_SOC_DAPM_ADC("ADC1 R", NULL, SND_SOC_NOPM, 0, 0),
+
+	SND_SOC_DAPM_SUPPLY("ADC1 L Power", RT5682S_PWR_DIG_1,
+		RT5682S_PWR_ADC_L1_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY("ADC1 R Power", RT5682S_PWR_DIG_1,
+		RT5682S_PWR_ADC_R1_BIT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY("ADC1 clock", RT5682S_CHOP_ADC,
+		RT5682S_CKGEN_ADC1_SFT, 0, NULL, 0),
+
+	/* ADC Mux */
+	SND_SOC_DAPM_MUX("Stereo1 ADC L1 Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_sto1_adc1l_mux),
+	SND_SOC_DAPM_MUX("Stereo1 ADC R1 Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_sto1_adc1r_mux),
+	SND_SOC_DAPM_MUX("Stereo1 ADC L2 Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_sto1_adc2l_mux),
+	SND_SOC_DAPM_MUX("Stereo1 ADC R2 Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_sto1_adc2r_mux),
+	SND_SOC_DAPM_MUX("Stereo1 ADC L Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_sto1_adcl_mux),
+	SND_SOC_DAPM_MUX("Stereo1 ADC R Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_sto1_adcr_mux),
+	SND_SOC_DAPM_MUX("IF1_ADC Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_if1_adc_slot_mux),
+
+	/* ADC Mixer */
+	SND_SOC_DAPM_SUPPLY("ADC Stereo1 Filter", RT5682S_PWR_DIG_2,
+		RT5682S_PWR_ADC_S1F_BIT, 0, set_filter_clk, SND_SOC_DAPM_PRE_PMU),
+	SND_SOC_DAPM_MIXER("Stereo1 ADC MIXL", RT5682S_STO1_ADC_DIG_VOL,
+		RT5682S_L_MUTE_SFT, 1, rt5682s_sto1_adc_l_mix,
+		ARRAY_SIZE(rt5682s_sto1_adc_l_mix)),
+	SND_SOC_DAPM_MIXER("Stereo1 ADC MIXR", RT5682S_STO1_ADC_DIG_VOL,
+		RT5682S_R_MUTE_SFT, 1, rt5682s_sto1_adc_r_mix,
+		ARRAY_SIZE(rt5682s_sto1_adc_r_mix)),
+
+	/* ADC PGA */
+	SND_SOC_DAPM_PGA("Stereo1 ADC MIX", SND_SOC_NOPM, 0, 0, NULL, 0),
+
+	/* Digital Interface */
+	SND_SOC_DAPM_SUPPLY("I2S1", RT5682S_PWR_DIG_1, RT5682S_PWR_I2S1_BIT,
+		0, set_i2s_clk, SND_SOC_DAPM_PRE_PMU),
+	SND_SOC_DAPM_SUPPLY("I2S2", RT5682S_PWR_DIG_1, RT5682S_PWR_I2S2_BIT,
+		0, set_i2s_clk, SND_SOC_DAPM_PRE_PMU),
+	SND_SOC_DAPM_PGA("IF1 DAC1 L", SND_SOC_NOPM, 0, 0, NULL, 0),
+	SND_SOC_DAPM_PGA("IF1 DAC1 R", SND_SOC_NOPM, 0, 0, NULL, 0),
+
+	/* Digital Interface Select */
+	SND_SOC_DAPM_MUX("IF1 01 ADC Swap Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_if1_01_adc_swap_mux),
+	SND_SOC_DAPM_MUX("IF1 23 ADC Swap Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_if1_23_adc_swap_mux),
+	SND_SOC_DAPM_MUX("IF1 45 ADC Swap Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_if1_45_adc_swap_mux),
+	SND_SOC_DAPM_MUX("IF1 67 ADC Swap Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_if1_67_adc_swap_mux),
+	SND_SOC_DAPM_MUX("IF2 ADC Swap Mux", SND_SOC_NOPM, 0, 0,
+		&rt5682s_if2_adc_swap_mux),
+
+	SND_SOC_DAPM_MUX("ADCDAT Mux", SND_SOC_NOPM, 0, 0, &rt5682s_adcdat_pin_ctrl),
+
+	/* Audio Interface */
+	SND_SOC_DAPM_AIF_OUT("AIF1TX", "AIF1 Capture", 0, RT5682S_I2S1_SDP,
+		RT5682S_SEL_ADCDAT_SFT, 1),
+	SND_SOC_DAPM_AIF_OUT("AIF2TX", "AIF2 Capture", 0, RT5682S_I2S2_SDP,
+		RT5682S_I2S2_PIN_CFG_SFT, 1),
+	SND_SOC_DAPM_AIF_IN("AIF1RX", "AIF1 Playback", 0, SND_SOC_NOPM, 0, 0),
+
+	/* Output Side */
+	/* DAC mixer before sound effect  */
+	SND_SOC_DAPM_MIXER("DAC1 MIXL", SND_SOC_NOPM, 0, 0,
+		rt5682s_dac_l_mix, ARRAY_SIZE(rt5682s_dac_l_mix)),
+	SND_SOC_DAPM_MIXER("DAC1 MIXR", SND_SOC_NOPM, 0, 0,
+		rt5682s_dac_r_mix, ARRAY_SIZE(rt5682s_dac_r_mix)),
+
+	/* DAC channel Mux */
+	SND_SOC_DAPM_MUX("DAC L1 Source", SND_SOC_NOPM, 0, 0, &rt5682s_alg_dac_l1_mux),
+	SND_SOC_DAPM_MUX("DAC R1 Source", SND_SOC_NOPM, 0, 0, &rt5682s_alg_dac_r1_mux),
+
+	/* DAC Mixer */
+	SND_SOC_DAPM_SUPPLY("DAC Stereo1 Filter", RT5682S_PWR_DIG_2,
+		RT5682S_PWR_DAC_S1F_BIT, 0, set_filter_clk, SND_SOC_DAPM_PRE_PMU),
+	SND_SOC_DAPM_MIXER("Stereo1 DAC MIXL", SND_SOC_NOPM, 0, 0,
+		rt5682s_sto1_dac_l_mix, ARRAY_SIZE(rt5682s_sto1_dac_l_mix)),
+	SND_SOC_DAPM_MIXER("Stereo1 DAC MIXR", SND_SOC_NOPM, 0, 0,
+		rt5682s_sto1_dac_r_mix, ARRAY_SIZE(rt5682s_sto1_dac_r_mix)),
+
+	/* DACs */
+	SND_SOC_DAPM_DAC("DAC L1", NULL, RT5682S_PWR_DIG_1, RT5682S_PWR_DAC_L1_BIT, 0),
+	SND_SOC_DAPM_DAC("DAC R1", NULL, RT5682S_PWR_DIG_1, RT5682S_PWR_DAC_R1_BIT, 0),
+
+	/* HPO */
+	SND_SOC_DAPM_PGA_S("HP Amp", 1, SND_SOC_NOPM, 0, 0, rt5682s_hp_amp_event,
+		SND_SOC_DAPM_POST_PMD | SND_SOC_DAPM_PRE_PMU | SND_SOC_DAPM_POST_PMU),
+
+	/* CLK DET */
+	SND_SOC_DAPM_SUPPLY("CLKDET SYS", RT5682S_CLK_DET,
+		RT5682S_SYS_CLK_DET_SFT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY("CLKDET PLL1", RT5682S_CLK_DET,
+		RT5682S_PLL1_CLK_DET_SFT, 0, NULL, 0),
+	SND_SOC_DAPM_SUPPLY("MCLK0 DET PWR", RT5682S_PWR_ANLG_2,
+		RT5682S_PWR_MCLK0_WD_BIT, 0, NULL, 0),
+
+	/* SAR */
+	SND_SOC_DAPM_SUPPLY("SAR", SND_SOC_NOPM, 0, 0, sar_power_event,
+		SND_SOC_DAPM_PRE_PMU | SND_SOC_DAPM_POST_PMD),
+
+	/* Output Lines */
+	SND_SOC_DAPM_OUTPUT("HPOL"),
+	SND_SOC_DAPM_OUTPUT("HPOR"),
+};
+
+static const struct snd_soc_dapm_route rt5682s_dapm_routes[] = {
+	/*PLL*/
+	{"ADC Stereo1 Filter", NULL, "PLLA", is_sys_clk_from_plla},
+	{"ADC Stereo1 Filter", NULL, "PLLB", is_sys_clk_from_pllb},
+	{"DAC Stereo1 Filter", NULL, "PLLA", is_sys_clk_from_plla},
+	{"DAC Stereo1 Filter", NULL, "PLLB", is_sys_clk_from_pllb},
+	{"PLLA", NULL, "PLLA_LDO"},
+	{"PLLA", NULL, "PLLA_BIAS"},
+	{"PLLA", NULL, "PLLA_RST"},
+	{"PLLB", NULL, "PLLB_LDO"},
+	{"PLLB", NULL, "PLLB_BIAS"},
+	{"PLLB", NULL, "PLLB_RST"},
+
+	/*ASRC*/
+	{"ADC Stereo1 Filter", NULL, "ADC STO1 ASRC", is_using_asrc},
+	{"DAC Stereo1 Filter", NULL, "DAC STO1 ASRC", is_using_asrc},
+	{"ADC STO1 ASRC", NULL, "AD ASRC"},
+	{"ADC STO1 ASRC", NULL, "DA ASRC"},
+	{"DAC STO1 ASRC", NULL, "AD ASRC"},
+	{"DAC STO1 ASRC", NULL, "DA ASRC"},
+
+	{"CLKDET SYS", NULL, "MCLK0 DET PWR"},
+
+	{"BST1 CBJ", NULL, "IN1P"},
+	{"BST1 CBJ", NULL, "SAR", is_headset_type},
+
+	{"RECMIX1L", "CBJ Switch", "BST1 CBJ"},
+	{"RECMIX1L", NULL, "RECMIX1L Power"},
+	{"RECMIX1R", "CBJ Switch", "BST1 CBJ"},
+	{"RECMIX1R", NULL, "RECMIX1R Power"},
+
+	{"ADC1 L", NULL, "RECMIX1L"},
+	{"ADC1 L", NULL, "ADC1 L Power"},
+	{"ADC1 L", NULL, "ADC1 clock"},
+	{"ADC1 R", NULL, "RECMIX1R"},
+	{"ADC1 R", NULL, "ADC1 R Power"},
+	{"ADC1 R", NULL, "ADC1 clock"},
+
+	{"DMIC L1", NULL, "DMIC CLK"},
+	{"DMIC L1", NULL, "DMIC1 Power"},
+	{"DMIC R1", NULL, "DMIC CLK"},
+	{"DMIC R1", NULL, "DMIC1 Power"},
+	{"DMIC CLK", NULL, "DMIC ASRC"},
+
+	{"Stereo1 ADC L Mux", "ADC1 L", "ADC1 L"},
+	{"Stereo1 ADC L Mux", "ADC1 R", "ADC1 R"},
+	{"Stereo1 ADC R Mux", "ADC1 L", "ADC1 L"},
+	{"Stereo1 ADC R Mux", "ADC1 R", "ADC1 R"},
+
+	{"Stereo1 ADC L1 Mux", "ADC", "Stereo1 ADC L Mux"},
+	{"Stereo1 ADC L1 Mux", "DAC MIX", "Stereo1 DAC MIXL"},
+	{"Stereo1 ADC L2 Mux", "DMIC", "DMIC L1"},
+	{"Stereo1 ADC L2 Mux", "DAC MIX", "Stereo1 DAC MIXL"},
+
+	{"Stereo1 ADC R1 Mux", "ADC", "Stereo1 ADC R Mux"},
+	{"Stereo1 ADC R1 Mux", "DAC MIX", "Stereo1 DAC MIXR"},
+	{"Stereo1 ADC R2 Mux", "DMIC", "DMIC R1"},
+	{"Stereo1 ADC R2 Mux", "DAC MIX", "Stereo1 DAC MIXR"},
+
+	{"Stereo1 ADC MIXL", "ADC1 Switch", "Stereo1 ADC L1 Mux"},
+	{"Stereo1 ADC MIXL", "ADC2 Switch", "Stereo1 ADC L2 Mux"},
+	{"Stereo1 ADC MIXL", NULL, "ADC Stereo1 Filter"},
+
+	{"Stereo1 ADC MIXR", "ADC1 Switch", "Stereo1 ADC R1 Mux"},
+	{"Stereo1 ADC MIXR", "ADC2 Switch", "Stereo1 ADC R2 Mux"},
+	{"Stereo1 ADC MIXR", NULL, "ADC Stereo1 Filter"},
+
+	{"Stereo1 ADC MIX", NULL, "Stereo1 ADC MIXL"},
+	{"Stereo1 ADC MIX", NULL, "Stereo1 ADC MIXR"},
+
+	{"IF1 01 ADC Swap Mux", "L/R", "Stereo1 ADC MIX"},
+	{"IF1 01 ADC Swap Mux", "L/L", "Stereo1 ADC MIX"},
+	{"IF1 01 ADC Swap Mux", "R/L", "Stereo1 ADC MIX"},
+	{"IF1 01 ADC Swap Mux", "R/R", "Stereo1 ADC MIX"},
+	{"IF1 23 ADC Swap Mux", "L/R", "Stereo1 ADC MIX"},
+	{"IF1 23 ADC Swap Mux", "R/L", "Stereo1 ADC MIX"},
+	{"IF1 23 ADC Swap Mux", "L/L", "Stereo1 ADC MIX"},
+	{"IF1 23 ADC Swap Mux", "R/R", "Stereo1 ADC MIX"},
+	{"IF1 45 ADC Swap Mux", "L/R", "Stereo1 ADC MIX"},
+	{"IF1 45 ADC Swap Mux", "R/L", "Stereo1 ADC MIX"},
+	{"IF1 45 ADC Swap Mux", "L/L", "Stereo1 ADC MIX"},
+	{"IF1 45 ADC Swap Mux", "R/R", "Stereo1 ADC MIX"},
+	{"IF1 67 ADC Swap Mux", "L/R", "Stereo1 ADC MIX"},
+	{"IF1 67 ADC Swap Mux", "R/L", "Stereo1 ADC MIX"},
+	{"IF1 67 ADC Swap Mux", "L/L", "Stereo1 ADC MIX"},
+	{"IF1 67 ADC Swap Mux", "R/R", "Stereo1 ADC MIX"},
+
+	{"IF1_ADC Mux", "Slot 0", "IF1 01 ADC Swap Mux"},
+	{"IF1_ADC Mux", "Slot 2", "IF1 23 ADC Swap Mux"},
+	{"IF1_ADC Mux", "Slot 4", "IF1 45 ADC Swap Mux"},
+	{"IF1_ADC Mux", "Slot 6", "IF1 67 ADC Swap Mux"},
+	{"ADCDAT Mux", "ADCDAT1", "IF1_ADC Mux"},
+	{"AIF1TX", NULL, "I2S1"},
+	{"AIF1TX", NULL, "ADCDAT Mux"},
+	{"IF2 ADC Swap Mux", "L/R", "Stereo1 ADC MIX"},
+	{"IF2 ADC Swap Mux", "R/L", "Stereo1 ADC MIX"},
+	{"IF2 ADC Swap Mux", "L/L", "Stereo1 ADC MIX"},
+	{"IF2 ADC Swap Mux", "R/R", "Stereo1 ADC MIX"},
+	{"ADCDAT Mux", "ADCDAT2", "IF2 ADC Swap Mux"},
+	{"AIF2TX", NULL, "ADCDAT Mux"},
+
+	{"IF1 DAC1 L", NULL, "AIF1RX"},
+	{"IF1 DAC1 L", NULL, "I2S1"},
+	{"IF1 DAC1 L", NULL, "DAC Stereo1 Filter"},
+	{"IF1 DAC1 R", NULL, "AIF1RX"},
+	{"IF1 DAC1 R", NULL, "I2S1"},
+	{"IF1 DAC1 R", NULL, "DAC Stereo1 Filter"},
+
+	{"DAC1 MIXL", "Stereo ADC Switch", "Stereo1 ADC MIXL"},
+	{"DAC1 MIXL", "DAC1 Switch", "IF1 DAC1 L"},
+	{"DAC1 MIXR", "Stereo ADC Switch", "Stereo1 ADC MIXR"},
+	{"DAC1 MIXR", "DAC1 Switch", "IF1 DAC1 R"},
+
+	{"Stereo1 DAC MIXL", "DAC L1 Switch", "DAC1 MIXL"},
+	{"Stereo1 DAC MIXL", "DAC R1 Switch", "DAC1 MIXR"},
+
+	{"Stereo1 DAC MIXR", "DAC R1 Switch", "DAC1 MIXR"},
+	{"Stereo1 DAC MIXR", "DAC L1 Switch", "DAC1 MIXL"},
+
+	{"DAC L1 Source", "DAC1", "DAC1 MIXL"},
+	{"DAC L1 Source", "Stereo1 DAC Mixer", "Stereo1 DAC MIXL"},
+	{"DAC R1 Source", "DAC1", "DAC1 MIXR"},
+	{"DAC R1 Source", "Stereo1 DAC Mixer", "Stereo1 DAC MIXR"},
+
+	{"DAC L1", NULL, "DAC L1 Source"},
+	{"DAC R1", NULL, "DAC R1 Source"},
+
+	{"HP Amp", NULL, "DAC L1"},
+	{"HP Amp", NULL, "DAC R1"},
+	{"HP Amp", NULL, "CLKDET SYS"},
+	{"HP Amp", NULL, "SAR", is_headset_type},
+
+	{"HPOL", NULL, "HP Amp"},
+	{"HPOR", NULL, "HP Amp"},
+};
+
+static int rt5682s_set_tdm_slot(struct snd_soc_dai *dai, unsigned int tx_mask,
+		unsigned int rx_mask, int slots, int slot_width)
+{
+	struct snd_soc_component *component = dai->component;
+	unsigned int cl, val = 0;
+
+	if (tx_mask || rx_mask)
+		snd_soc_component_update_bits(component,
+			RT5682S_TDM_ADDA_CTRL_2, RT5682S_TDM_EN, RT5682S_TDM_EN);
+	else
+		snd_soc_component_update_bits(component,
+			RT5682S_TDM_ADDA_CTRL_2, RT5682S_TDM_EN, 0);
+
+	switch (slots) {
+	case 4:
+		val |= RT5682S_TDM_TX_CH_4;
+		val |= RT5682S_TDM_RX_CH_4;
+		break;
+	case 6:
+		val |= RT5682S_TDM_TX_CH_6;
+		val |= RT5682S_TDM_RX_CH_6;
+		break;
+	case 8:
+		val |= RT5682S_TDM_TX_CH_8;
+		val |= RT5682S_TDM_RX_CH_8;
+		break;
+	case 2:
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	snd_soc_component_update_bits(component, RT5682S_TDM_CTRL,
+		RT5682S_TDM_TX_CH_MASK | RT5682S_TDM_RX_CH_MASK, val);
+
+	switch (slot_width) {
+	case 8:
+		if (tx_mask || rx_mask)
+			return -EINVAL;
+		cl = RT5682S_I2S1_TX_CHL_8 | RT5682S_I2S1_RX_CHL_8;
+		break;
+	case 16:
+		val = RT5682S_TDM_CL_16;
+		cl = RT5682S_I2S1_TX_CHL_16 | RT5682S_I2S1_RX_CHL_16;
+		break;
+	case 20:
+		val = RT5682S_TDM_CL_20;
+		cl = RT5682S_I2S1_TX_CHL_20 | RT5682S_I2S1_RX_CHL_20;
+		break;
+	case 24:
+		val = RT5682S_TDM_CL_24;
+		cl = RT5682S_I2S1_TX_CHL_24 | RT5682S_I2S1_RX_CHL_24;
+		break;
+	case 32:
+		val = RT5682S_TDM_CL_32;
+		cl = RT5682S_I2S1_TX_CHL_32 | RT5682S_I2S1_RX_CHL_32;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	snd_soc_component_update_bits(component, RT5682S_TDM_TCON_CTRL_1,
+		RT5682S_TDM_CL_MASK, val);
+	snd_soc_component_update_bits(component, RT5682S_I2S1_SDP,
+		RT5682S_I2S1_TX_CHL_MASK | RT5682S_I2S1_RX_CHL_MASK, cl);
+
+	return 0;
+}
+
+static int rt5682s_hw_params(struct snd_pcm_substream *substream,
+		struct snd_pcm_hw_params *params, struct snd_soc_dai *dai)
+{
+	struct snd_soc_component *component = dai->component;
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	unsigned int len_1 = 0, len_2 = 0;
+	int frame_size;
+
+	rt5682s->lrck[dai->id] = params_rate(params);
+
+	frame_size = snd_soc_params_to_frame_size(params);
+	if (frame_size < 0) {
+		dev_err(component->dev, "Unsupported frame size: %d\n", frame_size);
+		return -EINVAL;
+	}
+
+	switch (params_width(params)) {
+	case 16:
+		break;
+	case 20:
+		len_1 |= RT5682S_I2S1_DL_20;
+		len_2 |= RT5682S_I2S2_DL_20;
+		break;
+	case 24:
+		len_1 |= RT5682S_I2S1_DL_24;
+		len_2 |= RT5682S_I2S2_DL_24;
+		break;
+	case 32:
+		len_1 |= RT5682S_I2S1_DL_32;
+		len_2 |= RT5682S_I2S2_DL_24;
+		break;
+	case 8:
+		len_1 |= RT5682S_I2S2_DL_8;
+		len_2 |= RT5682S_I2S2_DL_8;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	switch (dai->id) {
+	case RT5682S_AIF1:
+		snd_soc_component_update_bits(component, RT5682S_I2S1_SDP,
+			RT5682S_I2S1_DL_MASK, len_1);
+		if (params_channels(params) == 1) /* mono mode */
+			snd_soc_component_update_bits(component, RT5682S_I2S1_SDP,
+				RT5682S_I2S1_MONO_MASK, RT5682S_I2S1_MONO_EN);
+		else
+			snd_soc_component_update_bits(component, RT5682S_I2S1_SDP,
+				RT5682S_I2S1_MONO_MASK, RT5682S_I2S1_MONO_DIS);
+		break;
+	case RT5682S_AIF2:
+		snd_soc_component_update_bits(component, RT5682S_I2S2_SDP,
+			RT5682S_I2S2_DL_MASK, len_2);
+		if (params_channels(params) == 1) /* mono mode */
+			snd_soc_component_update_bits(component, RT5682S_I2S2_SDP,
+				RT5682S_I2S2_MONO_MASK, RT5682S_I2S2_MONO_EN);
+		else
+			snd_soc_component_update_bits(component, RT5682S_I2S2_SDP,
+				RT5682S_I2S2_MONO_MASK, RT5682S_I2S2_MONO_DIS);
+		break;
+	default:
+		dev_err(component->dev, "Invalid dai->id: %d\n", dai->id);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int rt5682s_set_dai_fmt(struct snd_soc_dai *dai, unsigned int fmt)
+{
+	struct snd_soc_component *component = dai->component;
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	unsigned int reg_val = 0, tdm_ctrl = 0;
+
+	switch (fmt & SND_SOC_DAIFMT_MASTER_MASK) {
+	case SND_SOC_DAIFMT_CBM_CFM:
+		rt5682s->master[dai->id] = 1;
+		break;
+	case SND_SOC_DAIFMT_CBS_CFS:
+		rt5682s->master[dai->id] = 0;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	switch (fmt & SND_SOC_DAIFMT_INV_MASK) {
+	case SND_SOC_DAIFMT_NB_NF:
+		break;
+	case SND_SOC_DAIFMT_IB_NF:
+		reg_val |= RT5682S_I2S_BP_INV;
+		tdm_ctrl |= RT5682S_TDM_S_BP_INV;
+		break;
+	case SND_SOC_DAIFMT_NB_IF:
+		if (dai->id == RT5682S_AIF1)
+			tdm_ctrl |= RT5682S_TDM_S_LP_INV | RT5682S_TDM_M_BP_INV;
+		else
+			return -EINVAL;
+		break;
+	case SND_SOC_DAIFMT_IB_IF:
+		if (dai->id == RT5682S_AIF1)
+			tdm_ctrl |= RT5682S_TDM_S_BP_INV | RT5682S_TDM_S_LP_INV |
+				RT5682S_TDM_M_BP_INV | RT5682S_TDM_M_LP_INV;
+		else
+			return -EINVAL;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	switch (fmt & SND_SOC_DAIFMT_FORMAT_MASK) {
+	case SND_SOC_DAIFMT_I2S:
+		break;
+	case SND_SOC_DAIFMT_LEFT_J:
+		reg_val |= RT5682S_I2S_DF_LEFT;
+		tdm_ctrl |= RT5682S_TDM_DF_LEFT;
+		break;
+	case SND_SOC_DAIFMT_DSP_A:
+		reg_val |= RT5682S_I2S_DF_PCM_A;
+		tdm_ctrl |= RT5682S_TDM_DF_PCM_A;
+		break;
+	case SND_SOC_DAIFMT_DSP_B:
+		reg_val |= RT5682S_I2S_DF_PCM_B;
+		tdm_ctrl |= RT5682S_TDM_DF_PCM_B;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	switch (dai->id) {
+	case RT5682S_AIF1:
+		snd_soc_component_update_bits(component, RT5682S_I2S1_SDP,
+			RT5682S_I2S_DF_MASK, reg_val);
+		snd_soc_component_update_bits(component, RT5682S_TDM_TCON_CTRL_1,
+			RT5682S_TDM_MS_MASK | RT5682S_TDM_S_BP_MASK |
+			RT5682S_TDM_DF_MASK | RT5682S_TDM_M_BP_MASK |
+			RT5682S_TDM_M_LP_MASK | RT5682S_TDM_S_LP_MASK,
+			tdm_ctrl | rt5682s->master[dai->id]);
+		break;
+	case RT5682S_AIF2:
+		if (rt5682s->master[dai->id] == 0)
+			reg_val |= RT5682S_I2S2_MS_S;
+		snd_soc_component_update_bits(component, RT5682S_I2S2_SDP,
+			RT5682S_I2S2_MS_MASK | RT5682S_I2S_BP_MASK |
+			RT5682S_I2S_DF_MASK, reg_val);
+		break;
+	default:
+		dev_err(component->dev, "Invalid dai->id: %d\n", dai->id);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static int rt5682s_set_component_sysclk(struct snd_soc_component *component,
+		int clk_id, int source, unsigned int freq, int dir)
+{
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	unsigned int src = 0;
+
+	if (freq == rt5682s->sysclk && clk_id == rt5682s->sysclk_src)
+		return 0;
+
+	switch (clk_id) {
+	case RT5682S_SCLK_S_MCLK:
+		src = RT5682S_CLK_SRC_MCLK;
+		break;
+	case RT5682S_SCLK_S_PLL1:
+		src = RT5682S_CLK_SRC_PLL1;
+		break;
+	case RT5682S_SCLK_S_PLL2:
+		src = RT5682S_CLK_SRC_PLL2;
+		break;
+	case RT5682S_SCLK_S_RCCLK:
+		src = RT5682S_CLK_SRC_RCCLK;
+		break;
+	default:
+		dev_err(component->dev, "Invalid clock id (%d)\n", clk_id);
+		return -EINVAL;
+	}
+
+	snd_soc_component_update_bits(component, RT5682S_GLB_CLK,
+		RT5682S_SCLK_SRC_MASK, src << RT5682S_SCLK_SRC_SFT);
+	snd_soc_component_update_bits(component, RT5682S_ADDA_CLK_1,
+		RT5682S_I2S_M_CLK_SRC_MASK, src << RT5682S_I2S_M_CLK_SRC_SFT);
+	snd_soc_component_update_bits(component, RT5682S_I2S2_M_CLK_CTRL_1,
+		RT5682S_I2S2_M_CLK_SRC_MASK, src << RT5682S_I2S2_M_CLK_SRC_SFT);
+
+	rt5682s->sysclk = freq;
+	rt5682s->sysclk_src = clk_id;
+
+	dev_dbg(component->dev, "Sysclk is %dHz and clock id is %d\n",
+		freq, clk_id);
+
+	return 0;
+}
+
+static const struct pll_calc_map plla_table[] = {
+	{2048000, 24576000, 0, 46, 2, true, false, false, false},
+	{256000, 24576000, 0, 382, 2, true, false, false, false},
+	{512000, 24576000, 0, 190, 2, true, false, false, false},
+	{4096000, 24576000, 0, 22, 2, true, false, false, false},
+	{1024000, 24576000, 0, 94, 2, true, false, false, false},
+	{11289600, 22579200, 1, 22, 2, false, false, false, false},
+	{1411200, 22579200, 0, 62, 2, true, false, false, false},
+	{2822400, 22579200, 0, 30, 2, true, false, false, false},
+	{12288000, 24576000, 1, 22, 2, false, false, false, false},
+	{1536000, 24576000, 0, 62, 2, true, false, false, false},
+	{3072000, 24576000, 0, 30, 2, true, false, false, false},
+	{24576000, 49152000, 4, 22, 0, false, false, false, false},
+	{3072000, 49152000, 0, 30, 0, true, false, false, false},
+	{6144000, 49152000, 0, 30, 0, false, false, false, false},
+	{49152000, 98304000, 10, 22, 0, false, true, false, false},
+	{6144000, 98304000, 0, 30, 0, false, true, false, false},
+	{12288000, 98304000, 1, 22, 0, false, true, false, false},
+	{48000000, 3840000, 10, 22, 23, false, false, false, false},
+	{24000000, 3840000, 4, 22, 23, false, false, false, false},
+	{19200000, 3840000, 3, 23, 23, false, false, false, false},
+	{38400000, 3840000, 8, 23, 23, false, false, false, false},
+};
+
+static const struct pll_calc_map pllb_table[] = {
+	{48000000, 24576000, 8, 6, 3, false, false, false, false},
+	{48000000, 22579200, 23, 12, 3, false, false, false, true},
+	{24000000, 24576000, 3, 6, 3, false, false, false, false},
+	{24000000, 22579200, 23, 26, 3, false, false, false, true},
+	{19200000, 24576000, 2, 6, 3, false, false, false, false},
+	{19200000, 22579200, 3, 5, 3, false, false, false, true},
+	{38400000, 24576000, 6, 6, 3, false, false, false, false},
+	{38400000, 22579200, 8, 5, 3, false, false, false, true},
+	{3840000, 49152000, 0, 6, 0, true, false, false, false},
+};
+
+static int find_pll_inter_combination(unsigned int f_in, unsigned int f_out,
+		struct pll_calc_map *a, struct pll_calc_map *b)
+{
+	int i, j;
+
+	/* Look at PLLA table */
+	for (i = 0; i < ARRAY_SIZE(plla_table); i++) {
+		if (plla_table[i].freq_in == f_in && plla_table[i].freq_out == f_out) {
+			memcpy(a, plla_table + i, sizeof(*a));
+			return USE_PLLA;
+		}
+	}
+
+	/* Look at PLLB table */
+	for (i = 0; i < ARRAY_SIZE(pllb_table); i++) {
+		if (pllb_table[i].freq_in == f_in && pllb_table[i].freq_out == f_out) {
+			memcpy(b, pllb_table + i, sizeof(*b));
+			return USE_PLLB;
+		}
+	}
+
+	/* Find a combination of PLLA & PLLB */
+	for (i = ARRAY_SIZE(plla_table) - 1; i >= 0; i--) {
+		if (plla_table[i].freq_in == f_in && plla_table[i].freq_out == 3840000) {
+			for (j = ARRAY_SIZE(pllb_table) - 1; j >= 0; j--) {
+				if (pllb_table[j].freq_in == 3840000 &&
+					pllb_table[j].freq_out == f_out) {
+					memcpy(a, plla_table + i, sizeof(*a));
+					memcpy(b, pllb_table + j, sizeof(*b));
+					return USE_PLLAB;
+				}
+			}
+		}
+	}
+
+	return -EINVAL;
+}
+
+static int rt5682s_set_component_pll(struct snd_soc_component *component,
+		int pll_id, int source, unsigned int freq_in,
+		unsigned int freq_out)
+{
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	struct pll_calc_map a_map, b_map;
+
+	if (source == rt5682s->pll_src[pll_id] && freq_in == rt5682s->pll_in[pll_id] &&
+	    freq_out == rt5682s->pll_out[pll_id])
+		return 0;
+
+	if (!freq_in || !freq_out) {
+		dev_dbg(component->dev, "PLL disabled\n");
+		rt5682s->pll_in[pll_id] = 0;
+		rt5682s->pll_out[pll_id] = 0;
+		snd_soc_component_update_bits(component, RT5682S_GLB_CLK,
+			RT5682S_SCLK_SRC_MASK, RT5682S_CLK_SRC_MCLK << RT5682S_SCLK_SRC_SFT);
+		return 0;
+	}
+
+	switch (source) {
+	case RT5682S_PLL_S_MCLK:
+		snd_soc_component_update_bits(component, RT5682S_GLB_CLK,
+			RT5682S_PLL_SRC_MASK, RT5682S_PLL_SRC_MCLK);
+		break;
+	case RT5682S_PLL_S_BCLK1:
+		snd_soc_component_update_bits(component, RT5682S_GLB_CLK,
+			RT5682S_PLL_SRC_MASK, RT5682S_PLL_SRC_BCLK1);
+		break;
+	default:
+		dev_err(component->dev, "Unknown PLL Source %d\n", source);
+		return -EINVAL;
+	}
+
+	rt5682s->pll_comb = find_pll_inter_combination(freq_in, freq_out,
+							&a_map, &b_map);
+
+	if ((pll_id == RT5682S_PLL1 && rt5682s->pll_comb == USE_PLLA) ||
+	    (pll_id == RT5682S_PLL2 && (rt5682s->pll_comb == USE_PLLB ||
+					rt5682s->pll_comb == USE_PLLAB))) {
+		dev_dbg(component->dev,
+			"Supported freq conversion for PLL%d:(%d->%d): %d\n",
+			pll_id + 1, freq_in, freq_out, rt5682s->pll_comb);
+	} else {
+		dev_err(component->dev,
+			"Unsupported freq conversion for PLL%d:(%d->%d): %d\n",
+			pll_id + 1, freq_in, freq_out, rt5682s->pll_comb);
+		return -EINVAL;
+	}
+
+	if (rt5682s->pll_comb == USE_PLLA || rt5682s->pll_comb == USE_PLLAB) {
+		dev_dbg(component->dev,
+			"PLLA: fin=%d fout=%d m_bp=%d k_bp=%d m=%d n=%d k=%d\n",
+			a_map.freq_in, a_map.freq_out, a_map.m_bp, a_map.k_bp,
+			(a_map.m_bp ? 0 : a_map.m), a_map.n, (a_map.k_bp ? 0 : a_map.k));
+		snd_soc_component_update_bits(component, RT5682S_PLL_CTRL_1,
+			RT5682S_PLLA_N_MASK, a_map.n);
+		snd_soc_component_update_bits(component, RT5682S_PLL_CTRL_2,
+			RT5682S_PLLA_M_MASK | RT5682S_PLLA_K_MASK,
+			a_map.m << RT5682S_PLLA_M_SFT | a_map.k);
+		snd_soc_component_update_bits(component, RT5682S_PLL_CTRL_6,
+			RT5682S_PLLA_M_BP_MASK | RT5682S_PLLA_K_BP_MASK,
+			a_map.m_bp << RT5682S_PLLA_M_BP_SFT |
+			a_map.k_bp << RT5682S_PLLA_K_BP_SFT);
+	}
+
+	if (rt5682s->pll_comb == USE_PLLB || rt5682s->pll_comb == USE_PLLAB) {
+		dev_dbg(component->dev,
+			"PLLB: fin=%d fout=%d m_bp=%d k_bp=%d m=%d n=%d k=%d byp_ps=%d sel_ps=%d\n",
+			b_map.freq_in, b_map.freq_out, b_map.m_bp, b_map.k_bp,
+			(b_map.m_bp ? 0 : b_map.m), b_map.n, (b_map.k_bp ? 0 : b_map.k),
+			b_map.byp_ps, b_map.sel_ps);
+		snd_soc_component_update_bits(component, RT5682S_PLL_CTRL_3,
+			RT5682S_PLLB_N_MASK, b_map.n);
+		snd_soc_component_update_bits(component, RT5682S_PLL_CTRL_4,
+			RT5682S_PLLB_M_MASK | RT5682S_PLLB_K_MASK,
+			b_map.m << RT5682S_PLLB_M_SFT | b_map.k);
+		snd_soc_component_update_bits(component, RT5682S_PLL_CTRL_6,
+			RT5682S_PLLB_SEL_PS_MASK | RT5682S_PLLB_BYP_PS_MASK |
+			RT5682S_PLLB_M_BP_MASK | RT5682S_PLLB_K_BP_MASK,
+			b_map.sel_ps << RT5682S_PLLB_SEL_PS_SFT |
+			b_map.byp_ps << RT5682S_PLLB_BYP_PS_SFT |
+			b_map.m_bp << RT5682S_PLLB_M_BP_SFT |
+			b_map.k_bp << RT5682S_PLLB_K_BP_SFT);
+	}
+
+	if (rt5682s->pll_comb == USE_PLLB)
+		snd_soc_component_update_bits(component, RT5682S_PLL_CTRL_7,
+			RT5682S_PLLB_SRC_MASK, RT5682S_PLLB_SRC_DFIN);
+
+	rt5682s->pll_in[pll_id] = freq_in;
+	rt5682s->pll_out[pll_id] = freq_out;
+	rt5682s->pll_src[pll_id] = source;
+
+	return 0;
+}
+
+static int rt5682s_set_bclk1_ratio(struct snd_soc_dai *dai,
+		unsigned int ratio)
+{
+	struct snd_soc_component *component = dai->component;
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+
+	rt5682s->bclk[dai->id] = ratio;
+
+	switch (ratio) {
+	case 256:
+		snd_soc_component_update_bits(component, RT5682S_TDM_TCON_CTRL_1,
+			RT5682S_TDM_BCLK_MS1_MASK, RT5682S_TDM_BCLK_MS1_256);
+		break;
+	case 128:
+		snd_soc_component_update_bits(component, RT5682S_TDM_TCON_CTRL_1,
+			RT5682S_TDM_BCLK_MS1_MASK, RT5682S_TDM_BCLK_MS1_128);
+		break;
+	case 64:
+		snd_soc_component_update_bits(component, RT5682S_TDM_TCON_CTRL_1,
+			RT5682S_TDM_BCLK_MS1_MASK, RT5682S_TDM_BCLK_MS1_64);
+		break;
+	case 32:
+		snd_soc_component_update_bits(component, RT5682S_TDM_TCON_CTRL_1,
+			RT5682S_TDM_BCLK_MS1_MASK, RT5682S_TDM_BCLK_MS1_32);
+		break;
+	default:
+		dev_err(dai->dev, "Invalid bclk1 ratio %d\n", ratio);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int rt5682s_set_bclk2_ratio(struct snd_soc_dai *dai, unsigned int ratio)
+{
+	struct snd_soc_component *component = dai->component;
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+
+	rt5682s->bclk[dai->id] = ratio;
+
+	switch (ratio) {
+	case 64:
+		snd_soc_component_update_bits(component, RT5682S_ADDA_CLK_2,
+			RT5682S_I2S2_BCLK_MS2_MASK, RT5682S_I2S2_BCLK_MS2_64);
+		break;
+	case 32:
+		snd_soc_component_update_bits(component, RT5682S_ADDA_CLK_2,
+			RT5682S_I2S2_BCLK_MS2_MASK, RT5682S_I2S2_BCLK_MS2_32);
+		break;
+	default:
+		dev_err(dai->dev, "Invalid bclk2 ratio %d\n", ratio);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int rt5682s_set_bias_level(struct snd_soc_component *component,
+		enum snd_soc_bias_level level)
+{
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+
+	switch (level) {
+	case SND_SOC_BIAS_PREPARE:
+		regmap_update_bits(rt5682s->regmap, RT5682S_PWR_DIG_1,
+			RT5682S_PWR_LDO, RT5682S_PWR_LDO);
+		break;
+	case SND_SOC_BIAS_STANDBY:
+		regmap_update_bits(rt5682s->regmap, RT5682S_PWR_DIG_1,
+			RT5682S_DIG_GATE_CTRL, RT5682S_DIG_GATE_CTRL);
+		break;
+	case SND_SOC_BIAS_OFF:
+		regmap_update_bits(rt5682s->regmap, RT5682S_PWR_DIG_1,
+			RT5682S_DIG_GATE_CTRL | RT5682S_PWR_LDO, 0);
+		break;
+	case SND_SOC_BIAS_ON:
+		break;
+	}
+
+	return 0;
+}
+
+#ifdef CONFIG_COMMON_CLK
+#define CLK_PLL2_FIN 48000000
+#define CLK_48 48000
+#define CLK_44 44100
+
+static bool rt5682s_clk_check(struct rt5682s_priv *rt5682s)
+{
+	if (!rt5682s->master[RT5682S_AIF1]) {
+		dev_dbg(rt5682s->component->dev, "dai clk fmt not set correctly\n");
+		return false;
+	}
+	return true;
+}
+
+static int rt5682s_wclk_prepare(struct clk_hw *hw)
+{
+	struct rt5682s_priv *rt5682s =
+		container_of(hw, struct rt5682s_priv, dai_clks_hw[RT5682S_DAI_WCLK_IDX]);
+	struct snd_soc_component *component = rt5682s->component;
+	struct snd_soc_dapm_context *dapm = snd_soc_component_get_dapm(component);
+
+	if (!rt5682s_clk_check(rt5682s))
+		return -EINVAL;
+
+	snd_soc_dapm_mutex_lock(dapm);
+
+	snd_soc_dapm_force_enable_pin_unlocked(dapm, "MICBIAS");
+	snd_soc_component_update_bits(component, RT5682S_PWR_ANLG_1,
+		RT5682S_PWR_MB, RT5682S_PWR_MB);
+
+	snd_soc_dapm_force_enable_pin_unlocked(dapm, "Vref2");
+	snd_soc_component_update_bits(component, RT5682S_PWR_ANLG_1,
+		RT5682S_PWR_VREF2 | RT5682S_PWR_FV2, RT5682S_PWR_VREF2);
+	usleep_range(15000, 20000);
+	snd_soc_component_update_bits(component, RT5682S_PWR_ANLG_1,
+		RT5682S_PWR_FV2, RT5682S_PWR_FV2);
+
+	snd_soc_dapm_force_enable_pin_unlocked(dapm, "I2S1");
+	/* Only need to power PLLB due to the rate set restriction */
+	snd_soc_dapm_force_enable_pin_unlocked(dapm, "PLLB");
+	snd_soc_dapm_sync_unlocked(dapm);
+
+	snd_soc_dapm_mutex_unlock(dapm);
+
+	return 0;
+}
+
+static void rt5682s_wclk_unprepare(struct clk_hw *hw)
+{
+	struct rt5682s_priv *rt5682s =
+		container_of(hw, struct rt5682s_priv, dai_clks_hw[RT5682S_DAI_WCLK_IDX]);
+	struct snd_soc_component *component = rt5682s->component;
+	struct snd_soc_dapm_context *dapm = snd_soc_component_get_dapm(component);
+
+	if (!rt5682s_clk_check(rt5682s))
+		return;
+
+	snd_soc_dapm_mutex_lock(dapm);
+
+	snd_soc_dapm_disable_pin_unlocked(dapm, "MICBIAS");
+	snd_soc_dapm_disable_pin_unlocked(dapm, "Vref2");
+	if (!rt5682s->jack_type)
+		snd_soc_component_update_bits(component, RT5682S_PWR_ANLG_1,
+			RT5682S_PWR_VREF2 | RT5682S_PWR_FV2 | RT5682S_PWR_MB, 0);
+
+	snd_soc_dapm_disable_pin_unlocked(dapm, "I2S1");
+	snd_soc_dapm_disable_pin_unlocked(dapm, "PLLB");
+	snd_soc_dapm_sync_unlocked(dapm);
+
+	snd_soc_dapm_mutex_unlock(dapm);
+}
+
+static unsigned long rt5682s_wclk_recalc_rate(struct clk_hw *hw,
+					     unsigned long parent_rate)
+{
+	struct rt5682s_priv *rt5682s =
+		container_of(hw, struct rt5682s_priv, dai_clks_hw[RT5682S_DAI_WCLK_IDX]);
+	struct snd_soc_component *component = rt5682s->component;
+	const char * const clk_name = clk_hw_get_name(hw);
+
+	if (!rt5682s_clk_check(rt5682s))
+		return 0;
+	/*
+	 * Only accept to set wclk rate to 44.1k or 48kHz.
+	 */
+	if (rt5682s->lrck[RT5682S_AIF1] != CLK_48 &&
+	    rt5682s->lrck[RT5682S_AIF1] != CLK_44) {
+		dev_warn(component->dev, "%s: clk %s only support %d or %d Hz output\n",
+			__func__, clk_name, CLK_44, CLK_48);
+		return 0;
+	}
+
+	return rt5682s->lrck[RT5682S_AIF1];
+}
+
+static long rt5682s_wclk_round_rate(struct clk_hw *hw, unsigned long rate,
+				   unsigned long *parent_rate)
+{
+	struct rt5682s_priv *rt5682s =
+		container_of(hw, struct rt5682s_priv, dai_clks_hw[RT5682S_DAI_WCLK_IDX]);
+	struct snd_soc_component *component = rt5682s->component;
+	const char * const clk_name = clk_hw_get_name(hw);
+
+	if (!rt5682s_clk_check(rt5682s))
+		return -EINVAL;
+	/*
+	 * Only accept to set wclk rate to 44.1k or 48kHz.
+	 * It will force to 48kHz if not both.
+	 */
+	if (rate != CLK_48 && rate != CLK_44) {
+		dev_warn(component->dev, "%s: clk %s only support %d or %d Hz output\n",
+			__func__, clk_name, CLK_44, CLK_48);
+		rate = CLK_48;
+	}
+
+	return rate;
+}
+
+static int rt5682s_wclk_set_rate(struct clk_hw *hw, unsigned long rate,
+				unsigned long parent_rate)
+{
+	struct rt5682s_priv *rt5682s =
+		container_of(hw, struct rt5682s_priv, dai_clks_hw[RT5682S_DAI_WCLK_IDX]);
+	struct snd_soc_component *component = rt5682s->component;
+	struct clk *parent_clk;
+	const char * const clk_name = clk_hw_get_name(hw);
+	unsigned int clk_pll2_fout;
+
+	if (!rt5682s_clk_check(rt5682s))
+		return -EINVAL;
+
+	/*
+	 * Whether the wclk's parent clk (mclk) exists or not, please ensure
+	 * it is fixed or set to 48MHz before setting wclk rate. It's a
+	 * temporary limitation. Only accept 48MHz clk as the clk provider.
+	 *
+	 * It will set the codec anyway by assuming mclk is 48MHz.
+	 */
+	parent_clk = clk_get_parent(hw->clk);
+	if (!parent_clk)
+		dev_warn(component->dev,
+			"Parent mclk of wclk not acquired in driver. Please ensure mclk was provided as %d Hz.\n",
+			CLK_PLL2_FIN);
+
+	if (parent_rate != CLK_PLL2_FIN)
+		dev_warn(component->dev, "clk %s only support %d Hz input\n",
+			clk_name, CLK_PLL2_FIN);
+
+	/*
+	 * To achieve the rate conversion from 48MHz to 44.1k or 48kHz,
+	 * PLL2 is needed.
+	 */
+	clk_pll2_fout = rate * 512;
+	rt5682s_set_component_pll(component, RT5682S_PLL2, RT5682S_PLL_S_MCLK,
+		CLK_PLL2_FIN, clk_pll2_fout);
+
+	rt5682s_set_component_sysclk(component, RT5682S_SCLK_S_PLL2, 0,
+		clk_pll2_fout, SND_SOC_CLOCK_IN);
+
+	rt5682s->lrck[RT5682S_AIF1] = rate;
+
+	return 0;
+}
+
+static unsigned long rt5682s_bclk_recalc_rate(struct clk_hw *hw,
+					     unsigned long parent_rate)
+{
+	struct rt5682s_priv *rt5682s =
+		container_of(hw, struct rt5682s_priv, dai_clks_hw[RT5682S_DAI_BCLK_IDX]);
+	struct snd_soc_component *component = rt5682s->component;
+	unsigned int bclks_per_wclk;
+
+	bclks_per_wclk = snd_soc_component_read(component, RT5682S_TDM_TCON_CTRL_1);
+
+	switch (bclks_per_wclk & RT5682S_TDM_BCLK_MS1_MASK) {
+	case RT5682S_TDM_BCLK_MS1_256:
+		return parent_rate * 256;
+	case RT5682S_TDM_BCLK_MS1_128:
+		return parent_rate * 128;
+	case RT5682S_TDM_BCLK_MS1_64:
+		return parent_rate * 64;
+	case RT5682S_TDM_BCLK_MS1_32:
+		return parent_rate * 32;
+	default:
+		return 0;
+	}
+}
+
+static unsigned long rt5682s_bclk_get_factor(unsigned long rate,
+					    unsigned long parent_rate)
+{
+	unsigned long factor;
+
+	factor = rate / parent_rate;
+	if (factor < 64)
+		return 32;
+	else if (factor < 128)
+		return 64;
+	else if (factor < 256)
+		return 128;
+	else
+		return 256;
+}
+
+static long rt5682s_bclk_round_rate(struct clk_hw *hw, unsigned long rate,
+				   unsigned long *parent_rate)
+{
+	struct rt5682s_priv *rt5682s =
+		container_of(hw, struct rt5682s_priv, dai_clks_hw[RT5682S_DAI_BCLK_IDX]);
+	unsigned long factor;
+
+	if (!*parent_rate || !rt5682s_clk_check(rt5682s))
+		return -EINVAL;
+
+	/*
+	 * BCLK rates are set as a multiplier of WCLK in HW.
+	 * We don't allow changing the parent WCLK. We just do
+	 * some rounding down based on the parent WCLK rate
+	 * and find the appropriate multiplier of BCLK to
+	 * get the rounded down BCLK value.
+	 */
+	factor = rt5682s_bclk_get_factor(rate, *parent_rate);
+
+	return *parent_rate * factor;
+}
+
+static int rt5682s_bclk_set_rate(struct clk_hw *hw, unsigned long rate,
+				unsigned long parent_rate)
+{
+	struct rt5682s_priv *rt5682s =
+		container_of(hw, struct rt5682s_priv, dai_clks_hw[RT5682S_DAI_BCLK_IDX]);
+	struct snd_soc_component *component = rt5682s->component;
+	struct snd_soc_dai *dai;
+	unsigned long factor;
+
+	if (!rt5682s_clk_check(rt5682s))
+		return -EINVAL;
+
+	factor = rt5682s_bclk_get_factor(rate, parent_rate);
+
+	for_each_component_dais(component, dai)
+		if (dai->id == RT5682S_AIF1)
+			break;
+	if (!dai) {
+		dev_err(component->dev, "dai %d not found in component\n",
+			RT5682S_AIF1);
+		return -ENODEV;
+	}
+
+	return rt5682s_set_bclk1_ratio(dai, factor);
+}
+
+static const struct clk_ops rt5682s_dai_clk_ops[RT5682S_DAI_NUM_CLKS] = {
+	[RT5682S_DAI_WCLK_IDX] = {
+		.prepare = rt5682s_wclk_prepare,
+		.unprepare = rt5682s_wclk_unprepare,
+		.recalc_rate = rt5682s_wclk_recalc_rate,
+		.round_rate = rt5682s_wclk_round_rate,
+		.set_rate = rt5682s_wclk_set_rate,
+	},
+	[RT5682S_DAI_BCLK_IDX] = {
+		.recalc_rate = rt5682s_bclk_recalc_rate,
+		.round_rate = rt5682s_bclk_round_rate,
+		.set_rate = rt5682s_bclk_set_rate,
+	},
+};
+
+static int rt5682s_register_dai_clks(struct snd_soc_component *component)
+{
+	struct device *dev = component->dev;
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	struct rt5682s_platform_data *pdata = &rt5682s->pdata;
+	struct clk_hw *dai_clk_hw;
+	int i, ret;
+
+	for (i = 0; i < RT5682S_DAI_NUM_CLKS; ++i) {
+		struct clk_init_data init = { };
+
+		dai_clk_hw = &rt5682s->dai_clks_hw[i];
+
+		switch (i) {
+		case RT5682S_DAI_WCLK_IDX:
+			/* Make MCLK the parent of WCLK */
+			if (rt5682s->mclk) {
+				init.parent_data = &(struct clk_parent_data){
+					.fw_name = "mclk",
+				};
+				init.num_parents = 1;
+			}
+			break;
+		case RT5682S_DAI_BCLK_IDX:
+			/* Make WCLK the parent of BCLK */
+			init.parent_hws = &(const struct clk_hw *){
+				&rt5682s->dai_clks_hw[RT5682S_DAI_WCLK_IDX]
+			};
+			init.num_parents = 1;
+			break;
+		default:
+			dev_err(dev, "Invalid clock index\n");
+			return -EINVAL;
+		}
+
+		init.name = pdata->dai_clk_names[i];
+		init.ops = &rt5682s_dai_clk_ops[i];
+		init.flags = CLK_GET_RATE_NOCACHE | CLK_SET_RATE_GATE;
+		dai_clk_hw->init = &init;
+
+		ret = devm_clk_hw_register(dev, dai_clk_hw);
+		if (ret) {
+			dev_warn(dev, "Failed to register %s: %d\n", init.name, ret);
+			return ret;
+		}
+
+		if (dev->of_node) {
+			devm_of_clk_add_hw_provider(dev, of_clk_hw_simple_get, dai_clk_hw);
+		} else {
+			ret = devm_clk_hw_register_clkdev(dev, dai_clk_hw,
+							  init.name, dev_name(dev));
+			if (ret)
+				return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int rt5682s_dai_probe_clks(struct snd_soc_component *component)
+{
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	int ret;
+
+	/* Check if MCLK provided */
+	rt5682s->mclk = devm_clk_get(component->dev, "mclk");
+	if (IS_ERR(rt5682s->mclk)) {
+		if (PTR_ERR(rt5682s->mclk) != -ENOENT) {
+			ret = PTR_ERR(rt5682s->mclk);
+			return ret;
+		}
+		rt5682s->mclk = NULL;
+	}
+
+	/* Register CCF DAI clock control */
+	ret = rt5682s_register_dai_clks(component);
+	if (ret)
+		return ret;
+
+	/* Initial setup for CCF */
+	rt5682s->lrck[RT5682S_AIF1] = CLK_48;
+
+	return 0;
+}
+#else
+static inline int rt5682s_dai_probe_clks(struct snd_soc_component *component)
+{
+	return 0;
+}
+#endif /* CONFIG_COMMON_CLK */
+
+static int rt5682s_probe(struct snd_soc_component *component)
+{
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+	struct snd_soc_dapm_context *dapm = &component->dapm;
+	int ret;
+
+	rt5682s->component = component;
+
+	ret = rt5682s_dai_probe_clks(component);
+	if (ret)
+		return ret;
+
+	snd_soc_dapm_disable_pin(dapm, "MICBIAS");
+	snd_soc_dapm_disable_pin(dapm, "Vref2");
+	snd_soc_dapm_sync(dapm);
+	return 0;
+}
+
+static void rt5682s_remove(struct snd_soc_component *component)
+{
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+
+	rt5682s_reset(rt5682s);
+}
+
+#ifdef CONFIG_PM
+static int rt5682s_suspend(struct snd_soc_component *component)
+{
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+
+	cancel_delayed_work_sync(&rt5682s->jack_detect_work);
+	cancel_delayed_work_sync(&rt5682s->jd_check_work);
+
+	if (rt5682s->hs_jack && rt5682s->jack_type == SND_JACK_HEADSET)
+		snd_soc_component_update_bits(component, RT5682S_4BTN_IL_CMD_2,
+			RT5682S_4BTN_IL_MASK, RT5682S_4BTN_IL_DIS);
+
+	regcache_cache_only(rt5682s->regmap, true);
+	regcache_mark_dirty(rt5682s->regmap);
+
+	return 0;
+}
+
+static int rt5682s_resume(struct snd_soc_component *component)
+{
+	struct rt5682s_priv *rt5682s = snd_soc_component_get_drvdata(component);
+
+	regcache_cache_only(rt5682s->regmap, false);
+	regcache_sync(rt5682s->regmap);
+
+	if (rt5682s->hs_jack) {
+		rt5682s->jack_type = 0;
+		mod_delayed_work(system_power_efficient_wq,
+			&rt5682s->jack_detect_work, msecs_to_jiffies(250));
+	}
+
+	return 0;
+}
+#else
+#define rt5682s_suspend NULL
+#define rt5682s_resume NULL
+#endif
+
+const struct snd_soc_dai_ops rt5682s_aif1_dai_ops = {
+	.hw_params = rt5682s_hw_params,
+	.set_fmt = rt5682s_set_dai_fmt,
+	.set_tdm_slot = rt5682s_set_tdm_slot,
+	.set_bclk_ratio = rt5682s_set_bclk1_ratio,
+};
+
+const struct snd_soc_dai_ops rt5682s_aif2_dai_ops = {
+	.hw_params = rt5682s_hw_params,
+	.set_fmt = rt5682s_set_dai_fmt,
+	.set_bclk_ratio = rt5682s_set_bclk2_ratio,
+};
+
+const struct snd_soc_component_driver rt5682s_soc_component_dev = {
+	.probe = rt5682s_probe,
+	.remove = rt5682s_remove,
+	.suspend = rt5682s_suspend,
+	.resume = rt5682s_resume,
+	.set_bias_level = rt5682s_set_bias_level,
+	.controls = rt5682s_snd_controls,
+	.num_controls = ARRAY_SIZE(rt5682s_snd_controls),
+	.dapm_widgets = rt5682s_dapm_widgets,
+	.num_dapm_widgets = ARRAY_SIZE(rt5682s_dapm_widgets),
+	.dapm_routes = rt5682s_dapm_routes,
+	.num_dapm_routes = ARRAY_SIZE(rt5682s_dapm_routes),
+	.set_sysclk = rt5682s_set_component_sysclk,
+	.set_pll = rt5682s_set_component_pll,
+	.set_jack = rt5682s_set_jack_detect,
+	.use_pmdown_time	= 1,
+	.endianness		= 1,
+	.non_legacy_dai_naming	= 1,
+};
+
+static int rt5682s_parse_dt(struct rt5682s_priv *rt5682s, struct device *dev)
+{
+	device_property_read_u32(dev, "realtek,dmic1-data-pin",
+		&rt5682s->pdata.dmic1_data_pin);
+	device_property_read_u32(dev, "realtek,dmic1-clk-pin",
+		&rt5682s->pdata.dmic1_clk_pin);
+	device_property_read_u32(dev, "realtek,jd-src",
+		&rt5682s->pdata.jd_src);
+	device_property_read_u32(dev, "realtek,dmic-clk-rate-hz",
+		&rt5682s->pdata.dmic_clk_rate);
+	device_property_read_u32(dev, "realtek,dmic-delay-ms",
+		&rt5682s->pdata.dmic_delay);
+
+	rt5682s->pdata.ldo1_en = of_get_named_gpio(dev->of_node,
+		"realtek,ldo1-en-gpios", 0);
+
+	if (device_property_read_string_array(dev, "clock-output-names",
+					      rt5682s->pdata.dai_clk_names,
+					      RT5682S_DAI_NUM_CLKS) < 0)
+		dev_warn(dev, "Using default DAI clk names: %s, %s\n",
+			 rt5682s->pdata.dai_clk_names[RT5682S_DAI_WCLK_IDX],
+			 rt5682s->pdata.dai_clk_names[RT5682S_DAI_BCLK_IDX]);
+
+	rt5682s->pdata.dmic_clk_driving_high = device_property_read_bool(dev,
+		"realtek,dmic-clk-driving-high");
+
+	return 0;
+}
+
+static void rt5682s_calibrate(struct rt5682s_priv *rt5682s)
+{
+	unsigned int count, value;
+
+	mutex_lock(&rt5682s->calibrate_mutex);
+
+	regmap_write(rt5682s->regmap, RT5682S_PWR_ANLG_1, 0xaa80);
+	usleep_range(15000, 20000);
+	regmap_write(rt5682s->regmap, RT5682S_PWR_ANLG_1, 0xfa80);
+	regmap_write(rt5682s->regmap, RT5682S_PWR_DIG_1, 0x01c0);
+	regmap_write(rt5682s->regmap, RT5682S_MICBIAS_2, 0x0380);
+	regmap_write(rt5682s->regmap, RT5682S_GLB_CLK, 0x8000);
+	regmap_write(rt5682s->regmap, RT5682S_ADDA_CLK_1, 0x1001);
+	regmap_write(rt5682s->regmap, RT5682S_CHOP_DAC_2, 0x3030);
+	regmap_write(rt5682s->regmap, RT5682S_CHOP_ADC, 0xb000);
+	regmap_write(rt5682s->regmap, RT5682S_STO1_ADC_MIXER, 0x686c);
+	regmap_write(rt5682s->regmap, RT5682S_CAL_REC, 0x5151);
+	regmap_write(rt5682s->regmap, RT5682S_HP_CALIB_CTRL_2, 0x0321);
+	regmap_write(rt5682s->regmap, RT5682S_HP_LOGIC_CTRL_2, 0x0004);
+	regmap_write(rt5682s->regmap, RT5682S_HP_CALIB_CTRL_1, 0x7c00);
+	regmap_write(rt5682s->regmap, RT5682S_HP_CALIB_CTRL_1, 0xfc00);
+
+	for (count = 0; count < 60; count++) {
+		regmap_read(rt5682s->regmap, RT5682S_HP_CALIB_ST_1, &value);
+		if (!(value & 0x8000))
+			break;
+
+		usleep_range(10000, 10005);
+	}
+
+	if (count >= 60)
+		dev_err(rt5682s->component->dev, "HP Calibration Failure\n");
+
+	/* restore settings */
+	regmap_write(rt5682s->regmap, RT5682S_MICBIAS_2, 0x0180);
+	regmap_write(rt5682s->regmap, RT5682S_CAL_REC, 0x5858);
+	regmap_write(rt5682s->regmap, RT5682S_STO1_ADC_MIXER, 0xc0c4);
+	regmap_write(rt5682s->regmap, RT5682S_HP_CALIB_CTRL_2, 0x0320);
+	regmap_write(rt5682s->regmap, RT5682S_PWR_DIG_1, 0x00c0);
+	regmap_write(rt5682s->regmap, RT5682S_PWR_ANLG_1, 0x0800);
+	regmap_write(rt5682s->regmap, RT5682S_GLB_CLK, 0x0000);
+
+	mutex_unlock(&rt5682s->calibrate_mutex);
+}
+
+static const struct regmap_config rt5682s_regmap = {
+	.reg_bits = 16,
+	.val_bits = 16,
+	.max_register = RT5682S_MAX_REG,
+	.volatile_reg = rt5682s_volatile_register,
+	.readable_reg = rt5682s_readable_register,
+	.cache_type = REGCACHE_RBTREE,
+	.reg_defaults = rt5682s_reg,
+	.num_reg_defaults = ARRAY_SIZE(rt5682s_reg),
+	.use_single_read = true,
+	.use_single_write = true,
+};
+
+static struct snd_soc_dai_driver rt5682s_dai[] = {
+	{
+		.name = "rt5682s-aif1",
+		.id = RT5682S_AIF1,
+		.playback = {
+			.stream_name = "AIF1 Playback",
+			.channels_min = 1,
+			.channels_max = 2,
+			.rates = RT5682S_STEREO_RATES,
+			.formats = RT5682S_FORMATS,
+		},
+		.capture = {
+			.stream_name = "AIF1 Capture",
+			.channels_min = 1,
+			.channels_max = 2,
+			.rates = RT5682S_STEREO_RATES,
+			.formats = RT5682S_FORMATS,
+		},
+		.ops = &rt5682s_aif1_dai_ops,
+	},
+	{
+		.name = "rt5682s-aif2",
+		.id = RT5682S_AIF2,
+		.capture = {
+			.stream_name = "AIF2 Capture",
+			.channels_min = 1,
+			.channels_max = 2,
+			.rates = RT5682S_STEREO_RATES,
+			.formats = RT5682S_FORMATS,
+		},
+		.ops = &rt5682s_aif2_dai_ops,
+	},
+};
+
+static void rt5682s_i2c_disable_regulators(void *data)
+{
+	struct rt5682s_priv *rt5682s = data;
+
+	regulator_bulk_disable(ARRAY_SIZE(rt5682s->supplies), rt5682s->supplies);
+}
+
+static int rt5682s_i2c_probe(struct i2c_client *i2c,
+		const struct i2c_device_id *id)
+{
+	struct rt5682s_platform_data *pdata = dev_get_platdata(&i2c->dev);
+	struct rt5682s_priv *rt5682s;
+	int i, ret;
+	unsigned int val;
+
+	rt5682s = devm_kzalloc(&i2c->dev, sizeof(struct rt5682s_priv), GFP_KERNEL);
+	if (!rt5682s)
+		return -ENOMEM;
+
+	i2c_set_clientdata(i2c, rt5682s);
+
+	rt5682s->pdata = i2s_default_platform_data;
+
+	if (pdata)
+		rt5682s->pdata = *pdata;
+	else
+		rt5682s_parse_dt(rt5682s, &i2c->dev);
+
+	rt5682s->regmap = devm_regmap_init_i2c(i2c, &rt5682s_regmap);
+	if (IS_ERR(rt5682s->regmap)) {
+		ret = PTR_ERR(rt5682s->regmap);
+		dev_err(&i2c->dev, "Failed to allocate register map: %d\n", ret);
+		return ret;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(rt5682s->supplies); i++)
+		rt5682s->supplies[i].supply = rt5682s_supply_names[i];
+
+	ret = devm_regulator_bulk_get(&i2c->dev,
+			ARRAY_SIZE(rt5682s->supplies), rt5682s->supplies);
+	if (ret) {
+		dev_err(&i2c->dev, "Failed to request supplies: %d\n", ret);
+		return ret;
+	}
+
+	ret = devm_add_action_or_reset(&i2c->dev, rt5682s_i2c_disable_regulators, rt5682s);
+	if (ret)
+		return ret;
+
+	ret = regulator_bulk_enable(ARRAY_SIZE(rt5682s->supplies), rt5682s->supplies);
+	if (ret) {
+		dev_err(&i2c->dev, "Failed to enable supplies: %d\n", ret);
+		return ret;
+	}
+
+	if (gpio_is_valid(rt5682s->pdata.ldo1_en)) {
+		if (devm_gpio_request_one(&i2c->dev, rt5682s->pdata.ldo1_en,
+					  GPIOF_OUT_INIT_HIGH, "rt5682s"))
+			dev_err(&i2c->dev, "Fail gpio_request gpio_ldo\n");
+	}
+
+	/* Sleep for 50 ms minimum */
+	usleep_range(50000, 55000);
+
+	regmap_read(rt5682s->regmap, RT5682S_DEVICE_ID, &val);
+	if (val != DEVICE_ID) {
+		dev_err(&i2c->dev, "Device with ID register %x is not rt5682s\n", val);
+		return -ENODEV;
+	}
+
+	rt5682s_reset(rt5682s);
+	rt5682s_apply_patch_list(rt5682s, &i2c->dev);
+
+	regmap_update_bits(rt5682s->regmap, RT5682S_PWR_DIG_2,
+		RT5682S_DLDO_I_LIMIT_MASK, RT5682S_DLDO_I_LIMIT_DIS);
+	usleep_range(20000, 25000);
+
+	mutex_init(&rt5682s->calibrate_mutex);
+	mutex_init(&rt5682s->sar_mutex);
+	rt5682s_calibrate(rt5682s);
+
+	regmap_update_bits(rt5682s->regmap, RT5682S_MICBIAS_2,
+		RT5682S_PWR_CLK25M_MASK | RT5682S_PWR_CLK1M_MASK,
+		RT5682S_PWR_CLK25M_PD | RT5682S_PWR_CLK1M_PU);
+	regmap_update_bits(rt5682s->regmap, RT5682S_PWR_ANLG_1,
+		RT5682S_PWR_BG, RT5682S_PWR_BG);
+	regmap_update_bits(rt5682s->regmap, RT5682S_HP_LOGIC_CTRL_2,
+		RT5682S_HP_SIG_SRC_MASK, RT5682S_HP_SIG_SRC_1BIT_CTL);
+	regmap_update_bits(rt5682s->regmap, RT5682S_HP_CHARGE_PUMP_2,
+		RT5682S_PM_HP_MASK, RT5682S_PM_HP_HV);
+
+	/* DMIC data pin */
+	switch (rt5682s->pdata.dmic1_data_pin) {
+	case RT5682S_DMIC1_DATA_NULL:
+		break;
+	case RT5682S_DMIC1_DATA_GPIO2: /* share with LRCK2 */
+		regmap_update_bits(rt5682s->regmap, RT5682S_DMIC_CTRL_1,
+			RT5682S_DMIC_1_DP_MASK, RT5682S_DMIC_1_DP_GPIO2);
+		regmap_update_bits(rt5682s->regmap, RT5682S_GPIO_CTRL_1,
+			RT5682S_GP2_PIN_MASK, RT5682S_GP2_PIN_DMIC_SDA);
+		break;
+	case RT5682S_DMIC1_DATA_GPIO5: /* share with DACDAT1 */
+		regmap_update_bits(rt5682s->regmap, RT5682S_DMIC_CTRL_1,
+			RT5682S_DMIC_1_DP_MASK, RT5682S_DMIC_1_DP_GPIO5);
+		regmap_update_bits(rt5682s->regmap, RT5682S_GPIO_CTRL_1,
+			RT5682S_GP5_PIN_MASK, RT5682S_GP5_PIN_DMIC_SDA);
+		break;
+	default:
+		dev_warn(&i2c->dev, "invalid DMIC_DAT pin\n");
+		break;
+	}
+
+	/* DMIC clk pin */
+	switch (rt5682s->pdata.dmic1_clk_pin) {
+	case RT5682S_DMIC1_CLK_NULL:
+		break;
+	case RT5682S_DMIC1_CLK_GPIO1: /* share with IRQ */
+		regmap_update_bits(rt5682s->regmap, RT5682S_GPIO_CTRL_1,
+			RT5682S_GP1_PIN_MASK, RT5682S_GP1_PIN_DMIC_CLK);
+		break;
+	case RT5682S_DMIC1_CLK_GPIO3: /* share with BCLK2 */
+		regmap_update_bits(rt5682s->regmap, RT5682S_GPIO_CTRL_1,
+			RT5682S_GP3_PIN_MASK, RT5682S_GP3_PIN_DMIC_CLK);
+		if (rt5682s->pdata.dmic_clk_driving_high)
+			regmap_update_bits(rt5682s->regmap, RT5682S_PAD_DRIVING_CTRL,
+				RT5682S_PAD_DRV_GP3_MASK, RT5682S_PAD_DRV_GP3_HIGH);
+		break;
+	default:
+		dev_warn(&i2c->dev, "invalid DMIC_CLK pin\n");
+		break;
+	}
+
+	INIT_DELAYED_WORK(&rt5682s->jack_detect_work, rt5682s_jack_detect_handler);
+	INIT_DELAYED_WORK(&rt5682s->jd_check_work, rt5682s_jd_check_handler);
+
+	if (i2c->irq) {
+		ret = devm_request_threaded_irq(&i2c->dev, i2c->irq, NULL, rt5682s_irq,
+			IRQF_TRIGGER_RISING | IRQF_TRIGGER_FALLING | IRQF_ONESHOT,
+			"rt5682s", rt5682s);
+		if (ret)
+			dev_err(&i2c->dev, "Failed to reguest IRQ: %d\n", ret);
+	}
+
+	return devm_snd_soc_register_component(&i2c->dev, &rt5682s_soc_component_dev,
+			rt5682s_dai, ARRAY_SIZE(rt5682s_dai));
+}
+
+static void rt5682s_i2c_shutdown(struct i2c_client *client)
+{
+	struct rt5682s_priv *rt5682s = i2c_get_clientdata(client);
+
+	disable_irq(client->irq);
+	cancel_delayed_work_sync(&rt5682s->jack_detect_work);
+	cancel_delayed_work_sync(&rt5682s->jd_check_work);
+
+	rt5682s_reset(rt5682s);
+}
+
+static int rt5682s_i2c_remove(struct i2c_client *client)
+{
+	rt5682s_i2c_shutdown(client);
+
+	return 0;
+}
+
+static const struct of_device_id rt5682s_of_match[] = {
+	{.compatible = "realtek,rt5682s"},
+	{},
+};
+MODULE_DEVICE_TABLE(of, rt5682s_of_match);
+
+static const struct acpi_device_id rt5682s_acpi_match[] = {
+	{"RTL5682", 0,},
+	{},
+};
+MODULE_DEVICE_TABLE(acpi, rt5682s_acpi_match);
+
+static const struct i2c_device_id rt5682s_i2c_id[] = {
+	{"rt5682s", 0},
+	{}
+};
+MODULE_DEVICE_TABLE(i2c, rt5682s_i2c_id);
+
+static struct i2c_driver rt5682s_i2c_driver = {
+	.driver = {
+		.name = "rt5682s",
+		.of_match_table = rt5682s_of_match,
+		.acpi_match_table = rt5682s_acpi_match,
+		.probe_type = PROBE_PREFER_ASYNCHRONOUS,
+	},
+	.probe = rt5682s_i2c_probe,
+	.remove = rt5682s_i2c_remove,
+	.shutdown = rt5682s_i2c_shutdown,
+	.id_table = rt5682s_i2c_id,
+};
+module_i2c_driver(rt5682s_i2c_driver);
+
+MODULE_DESCRIPTION("ASoC RT5682I-VS driver");
+MODULE_AUTHOR("Derek Fang <derek.fang@realtek.com>");
+MODULE_LICENSE("GPL v2");
diff -ruN a/sound/soc/codecs/rt5682s.h b/sound/soc/codecs/rt5682s.h
--- a/sound/soc/codecs/rt5682s.h	1970-01-01 01:00:00.000000000 +0100
+++ b/sound/soc/codecs/rt5682s.h	2021-12-23 08:36:02.000000000 +0100
@@ -0,0 +1,1455 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * rt5682s.h  --  RT5682I-VS ALSA SoC audio driver
+ *
+ * Copyright 2021 Realtek Microelectronics
+ * Author: Derek Fang <derek.fang@realtek.com>
+ */
+
+#ifndef __RT5682S_H__
+#define __RT5682S_H__
+
+#include <sound/rt5682s.h>
+#include <linux/regulator/consumer.h>
+#include <linux/clk.h>
+#include <linux/clkdev.h>
+#include <linux/clk-provider.h>
+
+
+/* Info */
+#define RT5682S_RESET				0x0000
+#define RT5682S_VERSION_ID			0x00fd
+#define RT5682S_VENDOR_ID			0x00fe
+#define RT5682S_DEVICE_ID			0x00ff
+/*  I/O - Output */
+#define RT5682S_HP_CTRL_1			0x0002
+#define RT5682S_HP_CTRL_2			0x0003
+#define RT5682S_HPL_GAIN			0x0005
+#define RT5682S_HPR_GAIN			0x0006
+
+#define RT5682S_I2C_CTRL			0x0008
+
+/* I/O - Input */
+#define RT5682S_CBJ_BST_CTRL			0x000b
+#define RT5682S_CBJ_DET_CTRL			0x000f
+#define RT5682S_CBJ_CTRL_1			0x0010
+#define RT5682S_CBJ_CTRL_2			0x0011
+#define RT5682S_CBJ_CTRL_3			0x0012
+#define RT5682S_CBJ_CTRL_4			0x0013
+#define RT5682S_CBJ_CTRL_5			0x0014
+#define RT5682S_CBJ_CTRL_6			0x0015
+#define RT5682S_CBJ_CTRL_7			0x0016
+#define RT5682S_CBJ_CTRL_8			0x0017
+/* I/O - ADC/DAC/DMIC */
+#define RT5682S_DAC1_DIG_VOL			0x0019
+#define RT5682S_STO1_ADC_DIG_VOL		0x001c
+#define RT5682S_STO1_ADC_BOOST			0x001f
+#define RT5682S_HP_IMP_GAIN_1			0x0022
+#define RT5682S_HP_IMP_GAIN_2			0x0023
+/* Mixer - D-D */
+#define RT5682S_SIDETONE_CTRL			0x0024
+#define RT5682S_STO1_ADC_MIXER			0x0026
+#define RT5682S_AD_DA_MIXER			0x0029
+#define RT5682S_STO1_DAC_MIXER			0x002a
+#define RT5682S_A_DAC1_MUX			0x002b
+#define RT5682S_DIG_INF2_DATA			0x0030
+/* Mixer - ADC */
+#define RT5682S_REC_MIXER			0x003c
+#define RT5682S_CAL_REC				0x0044
+/* HP Analog Offset Control */
+#define RT5682S_HP_ANA_OST_CTRL_1		0x004b
+#define RT5682S_HP_ANA_OST_CTRL_2		0x004c
+#define RT5682S_HP_ANA_OST_CTRL_3		0x004d
+/* Power */
+#define RT5682S_PWR_DIG_1			0x0061
+#define RT5682S_PWR_DIG_2			0x0062
+#define RT5682S_PWR_ANLG_1			0x0063
+#define RT5682S_PWR_ANLG_2			0x0064
+#define RT5682S_PWR_ANLG_3			0x0065
+#define RT5682S_PWR_MIXER			0x0066
+
+#define RT5682S_MB_CTRL				0x0067
+#define RT5682S_CLK_GATE_TCON_1			0x0068
+#define RT5682S_CLK_GATE_TCON_2			0x0069
+#define RT5682S_CLK_GATE_TCON_3			0x006a
+/* Clock Detect */
+#define RT5682S_CLK_DET				0x006b
+/* Filter Auto Reset */
+#define RT5682S_RESET_LPF_CTRL			0x006c
+#define RT5682S_RESET_HPF_CTRL			0x006d
+/* DMIC */
+#define RT5682S_DMIC_CTRL_1			0x006e
+#define RT5682S_LPF_AD_DMIC			0x006f
+/* Format - ADC/DAC */
+#define RT5682S_I2S1_SDP			0x0070
+#define RT5682S_I2S2_SDP			0x0071
+#define RT5682S_ADDA_CLK_1			0x0073
+#define RT5682S_ADDA_CLK_2			0x0074
+#define RT5682S_I2S1_F_DIV_CTRL_1		0x0075
+#define RT5682S_I2S1_F_DIV_CTRL_2		0x0076
+/* Format - TDM Control */
+#define RT5682S_TDM_CTRL			0x0079
+#define RT5682S_TDM_ADDA_CTRL_1			0x007a
+#define RT5682S_TDM_ADDA_CTRL_2			0x007b
+#define RT5682S_DATA_SEL_CTRL_1			0x007c
+#define RT5682S_TDM_TCON_CTRL_1			0x007e
+#define RT5682S_TDM_TCON_CTRL_2			0x007f
+/* Function - Analog */
+#define RT5682S_GLB_CLK				0x0080
+#define RT5682S_PLL_TRACK_1			0x0083
+#define RT5682S_PLL_TRACK_2			0x0084
+#define RT5682S_PLL_TRACK_3			0x0085
+#define RT5682S_PLL_TRACK_4			0x0086
+#define RT5682S_PLL_TRACK_5			0x0087
+#define RT5682S_PLL_TRACK_6			0x0088
+#define RT5682S_PLL_TRACK_11			0x008c
+#define RT5682S_DEPOP_1				0x008e
+#define RT5682S_HP_CHARGE_PUMP_1		0x008f
+#define RT5682S_HP_CHARGE_PUMP_2		0x0091
+#define RT5682S_HP_CHARGE_PUMP_3		0x0092
+#define RT5682S_MICBIAS_1			0x0093
+#define RT5682S_MICBIAS_2			0x0094
+#define RT5682S_MICBIAS_3			0x0095
+
+#define RT5682S_PLL_TRACK_12			0x0096
+#define RT5682S_PLL_TRACK_14			0x0097
+#define RT5682S_PLL_CTRL_1			0x0098
+#define RT5682S_PLL_CTRL_2			0x0099
+#define RT5682S_PLL_CTRL_3			0x009a
+#define RT5682S_PLL_CTRL_4			0x009b
+#define RT5682S_PLL_CTRL_5			0x009c
+#define RT5682S_PLL_CTRL_6			0x009d
+#define RT5682S_PLL_CTRL_7			0x009e
+
+#define RT5682S_RC_CLK_CTRL			0x009f
+#define RT5682S_I2S2_M_CLK_CTRL_1		0x00a0
+#define RT5682S_I2S2_F_DIV_CTRL_1		0x00a3
+#define RT5682S_I2S2_F_DIV_CTRL_2		0x00a4
+
+#define RT5682S_IRQ_CTRL_1			0x00b6
+#define RT5682S_IRQ_CTRL_2			0x00b7
+#define RT5682S_IRQ_CTRL_3			0x00b8
+#define RT5682S_IRQ_CTRL_4			0x00b9
+#define RT5682S_INT_ST_1			0x00be
+#define RT5682S_GPIO_CTRL_1			0x00c0
+#define RT5682S_GPIO_CTRL_2			0x00c1
+#define RT5682S_GPIO_ST				0x00c2
+#define RT5682S_HP_AMP_DET_CTRL_1		0x00d0
+#define RT5682S_MID_HP_AMP_DET			0x00d2
+#define RT5682S_LOW_HP_AMP_DET			0x00d3
+#define RT5682S_DELAY_BUF_CTRL			0x00d4
+#define RT5682S_SV_ZCD_1			0x00d9
+#define RT5682S_SV_ZCD_2			0x00da
+#define RT5682S_IL_CMD_1			0x00db
+#define RT5682S_IL_CMD_2			0x00dc
+#define RT5682S_IL_CMD_3			0x00dd
+#define RT5682S_IL_CMD_4			0x00de
+#define RT5682S_IL_CMD_5			0x00df
+#define RT5682S_IL_CMD_6			0x00e0
+#define RT5682S_4BTN_IL_CMD_1			0x00e2
+#define RT5682S_4BTN_IL_CMD_2			0x00e3
+#define RT5682S_4BTN_IL_CMD_3			0x00e4
+#define RT5682S_4BTN_IL_CMD_4			0x00e5
+#define RT5682S_4BTN_IL_CMD_5			0x00e6
+#define RT5682S_4BTN_IL_CMD_6			0x00e7
+#define RT5682S_4BTN_IL_CMD_7			0x00e8
+
+#define RT5682S_ADC_STO1_HP_CTRL_1		0x00ea
+#define RT5682S_ADC_STO1_HP_CTRL_2		0x00eb
+#define RT5682S_AJD1_CTRL			0x00f0
+#define RT5682S_JD_CTRL_1			0x00f6
+/* General Control */
+#define RT5682S_DUMMY_1				0x00fa
+#define RT5682S_DUMMY_2				0x00fb
+#define RT5682S_DUMMY_3				0x00fc
+
+#define RT5682S_DAC_ADC_DIG_VOL1		0x0100
+#define RT5682S_BIAS_CUR_CTRL_2			0x010b
+#define RT5682S_BIAS_CUR_CTRL_3			0x010c
+#define RT5682S_BIAS_CUR_CTRL_4			0x010d
+#define RT5682S_BIAS_CUR_CTRL_5			0x010e
+#define RT5682S_BIAS_CUR_CTRL_6			0x010f
+#define RT5682S_BIAS_CUR_CTRL_7			0x0110
+#define RT5682S_BIAS_CUR_CTRL_8			0x0111
+#define RT5682S_BIAS_CUR_CTRL_9			0x0112
+#define RT5682S_BIAS_CUR_CTRL_10		0x0113
+#define RT5682S_VREF_REC_OP_FB_CAP_CTRL_1	0x0117
+#define RT5682S_VREF_REC_OP_FB_CAP_CTRL_2	0x0118
+#define RT5682S_CHARGE_PUMP_1			0x0125
+#define RT5682S_DIG_IN_CTRL_1			0x0132
+#define RT5682S_PAD_DRIVING_CTRL		0x0136
+#define RT5682S_CHOP_DAC_1			0x0139
+#define RT5682S_CHOP_DAC_2			0x013a
+#define RT5682S_CHOP_ADC			0x013b
+#define RT5682S_CALIB_ADC_CTRL			0x013c
+#define RT5682S_VOL_TEST			0x013f
+#define RT5682S_SPKVDD_DET_ST			0x0142
+#define RT5682S_TEST_MODE_CTRL_1		0x0145
+#define RT5682S_TEST_MODE_CTRL_2		0x0146
+#define RT5682S_TEST_MODE_CTRL_3		0x0147
+#define RT5682S_TEST_MODE_CTRL_4		0x0148
+#define RT5682S_PLL_INTERNAL_1			0x0156
+#define RT5682S_PLL_INTERNAL_2			0x0157
+#define RT5682S_PLL_INTERNAL_3			0x0158
+#define RT5682S_PLL_INTERNAL_4			0x0159
+#define RT5682S_STO_NG2_CTRL_1			0x0160
+#define RT5682S_STO_NG2_CTRL_2			0x0161
+#define RT5682S_STO_NG2_CTRL_3			0x0162
+#define RT5682S_STO_NG2_CTRL_4			0x0163
+#define RT5682S_STO_NG2_CTRL_5			0x0164
+#define RT5682S_STO_NG2_CTRL_6			0x0165
+#define RT5682S_STO_NG2_CTRL_7			0x0166
+#define RT5682S_STO_NG2_CTRL_8			0x0167
+#define RT5682S_STO_NG2_CTRL_9			0x0168
+#define RT5682S_STO_NG2_CTRL_10			0x0169
+#define RT5682S_STO1_DAC_SIL_DET		0x0190
+#define RT5682S_SIL_PSV_CTRL1			0x0194
+#define RT5682S_SIL_PSV_CTRL2			0x0195
+#define RT5682S_SIL_PSV_CTRL3			0x0197
+#define RT5682S_SIL_PSV_CTRL4			0x0198
+#define RT5682S_SIL_PSV_CTRL5			0x0199
+#define RT5682S_HP_IMP_SENS_CTRL_1		0x01ac
+#define RT5682S_HP_IMP_SENS_CTRL_2		0x01ad
+#define RT5682S_HP_IMP_SENS_CTRL_3		0x01ae
+#define RT5682S_HP_IMP_SENS_CTRL_4		0x01af
+#define RT5682S_HP_IMP_SENS_CTRL_5		0x01b0
+#define RT5682S_HP_IMP_SENS_CTRL_6		0x01b1
+#define RT5682S_HP_IMP_SENS_CTRL_7		0x01b2
+#define RT5682S_HP_IMP_SENS_CTRL_8		0x01b3
+#define RT5682S_HP_IMP_SENS_CTRL_9		0x01b4
+#define RT5682S_HP_IMP_SENS_CTRL_10		0x01b5
+#define RT5682S_HP_IMP_SENS_CTRL_11		0x01b6
+#define RT5682S_HP_IMP_SENS_CTRL_12		0x01b7
+#define RT5682S_HP_IMP_SENS_CTRL_13		0x01b8
+#define RT5682S_HP_IMP_SENS_CTRL_14		0x01b9
+#define RT5682S_HP_IMP_SENS_CTRL_15		0x01ba
+#define RT5682S_HP_IMP_SENS_CTRL_16		0x01bb
+#define RT5682S_HP_IMP_SENS_CTRL_17		0x01bc
+#define RT5682S_HP_IMP_SENS_CTRL_18		0x01bd
+#define RT5682S_HP_IMP_SENS_CTRL_19		0x01be
+#define RT5682S_HP_IMP_SENS_CTRL_20		0x01bf
+#define RT5682S_HP_IMP_SENS_CTRL_21		0x01c0
+#define RT5682S_HP_IMP_SENS_CTRL_22		0x01c1
+#define RT5682S_HP_IMP_SENS_CTRL_23		0x01c2
+#define RT5682S_HP_IMP_SENS_CTRL_24		0x01c3
+#define RT5682S_HP_IMP_SENS_CTRL_25		0x01c4
+#define RT5682S_HP_IMP_SENS_CTRL_26		0x01c5
+#define RT5682S_HP_IMP_SENS_CTRL_27		0x01c6
+#define RT5682S_HP_IMP_SENS_CTRL_28		0x01c7
+#define RT5682S_HP_IMP_SENS_CTRL_29		0x01c8
+#define RT5682S_HP_IMP_SENS_CTRL_30		0x01c9
+#define RT5682S_HP_IMP_SENS_CTRL_31		0x01ca
+#define RT5682S_HP_IMP_SENS_CTRL_32		0x01cb
+#define RT5682S_HP_IMP_SENS_CTRL_33		0x01cc
+#define RT5682S_HP_IMP_SENS_CTRL_34		0x01cd
+#define RT5682S_HP_IMP_SENS_CTRL_35		0x01ce
+#define RT5682S_HP_IMP_SENS_CTRL_36		0x01cf
+#define RT5682S_HP_IMP_SENS_CTRL_37		0x01d0
+#define RT5682S_HP_IMP_SENS_CTRL_38		0x01d1
+#define RT5682S_HP_IMP_SENS_CTRL_39		0x01d2
+#define RT5682S_HP_IMP_SENS_CTRL_40		0x01d3
+#define RT5682S_HP_IMP_SENS_CTRL_41		0x01d4
+#define RT5682S_HP_IMP_SENS_CTRL_42		0x01d5
+#define RT5682S_HP_IMP_SENS_CTRL_43		0x01d6
+#define RT5682S_HP_IMP_SENS_CTRL_44		0x01d7
+#define RT5682S_HP_IMP_SENS_CTRL_45		0x01d8
+#define RT5682S_HP_IMP_SENS_CTRL_46		0x01d9
+#define RT5682S_HP_LOGIC_CTRL_1			0x01da
+#define RT5682S_HP_LOGIC_CTRL_2			0x01db
+#define RT5682S_HP_LOGIC_CTRL_3			0x01dc
+#define RT5682S_HP_CALIB_CTRL_1			0x01de
+#define RT5682S_HP_CALIB_CTRL_2			0x01df
+#define RT5682S_HP_CALIB_CTRL_3			0x01e0
+#define RT5682S_HP_CALIB_CTRL_4			0x01e1
+#define RT5682S_HP_CALIB_CTRL_5			0x01e2
+#define RT5682S_HP_CALIB_CTRL_6			0x01e3
+#define RT5682S_HP_CALIB_CTRL_7			0x01e4
+#define RT5682S_HP_CALIB_CTRL_8			0x01e5
+#define RT5682S_HP_CALIB_CTRL_9			0x01e6
+#define RT5682S_HP_CALIB_CTRL_10		0x01e7
+#define RT5682S_HP_CALIB_CTRL_11		0x01e8
+#define RT5682S_HP_CALIB_ST_1			0x01ea
+#define RT5682S_HP_CALIB_ST_2			0x01eb
+#define RT5682S_HP_CALIB_ST_3			0x01ec
+#define RT5682S_HP_CALIB_ST_4			0x01ed
+#define RT5682S_HP_CALIB_ST_5			0x01ee
+#define RT5682S_HP_CALIB_ST_6			0x01ef
+#define RT5682S_HP_CALIB_ST_7			0x01f0
+#define RT5682S_HP_CALIB_ST_8			0x01f1
+#define RT5682S_HP_CALIB_ST_9			0x01f2
+#define RT5682S_HP_CALIB_ST_10			0x01f3
+#define RT5682S_HP_CALIB_ST_11			0x01f4
+#define RT5682S_SAR_IL_CMD_1			0x0210
+#define RT5682S_SAR_IL_CMD_2			0x0211
+#define RT5682S_SAR_IL_CMD_3			0x0212
+#define RT5682S_SAR_IL_CMD_4			0x0213
+#define RT5682S_SAR_IL_CMD_5			0x0214
+#define RT5682S_SAR_IL_CMD_6			0x0215
+#define RT5682S_SAR_IL_CMD_7			0x0216
+#define RT5682S_SAR_IL_CMD_8			0x0217
+#define RT5682S_SAR_IL_CMD_9			0x0218
+#define RT5682S_SAR_IL_CMD_10			0x0219
+#define RT5682S_SAR_IL_CMD_11			0x021a
+#define RT5682S_SAR_IL_CMD_12			0x021b
+#define RT5682S_SAR_IL_CMD_13			0x021c
+#define RT5682S_SAR_IL_CMD_14			0x021d
+#define RT5682S_DUMMY_4				0x02fa
+#define RT5682S_DUMMY_5				0x02fb
+#define RT5682S_DUMMY_6				0x02fc
+#define RT5682S_VERSION_ID_HIDE			0x03fe
+#define RT5682S_VERSION_ID_CUS			0x03ff
+#define RT5682S_SCAN_CTL			0x0500
+#define RT5682S_HP_AMP_DET			0x0600
+#define RT5682S_BIAS_CUR_CTRL_11		0x0610
+#define RT5682S_BIAS_CUR_CTRL_12		0x0611
+#define RT5682S_BIAS_CUR_CTRL_13		0x0620
+#define RT5682S_BIAS_CUR_CTRL_14		0x0621
+#define RT5682S_BIAS_CUR_CTRL_15		0x0630
+#define RT5682S_BIAS_CUR_CTRL_16		0x0631
+#define RT5682S_BIAS_CUR_CTRL_17		0x0640
+#define RT5682S_BIAS_CUR_CTRL_18		0x0641
+#define RT5682S_I2C_TRANS_CTRL			0x07fa
+#define RT5682S_DUMMY_7				0x08fa
+#define RT5682S_DUMMY_8				0x08fb
+#define RT5682S_DMIC_FLOAT_DET			0x0d00
+#define RT5682S_HA_CMP_OP_1			0x1100
+#define RT5682S_HA_CMP_OP_2			0x1101
+#define RT5682S_HA_CMP_OP_3			0x1102
+#define RT5682S_HA_CMP_OP_4			0x1103
+#define RT5682S_HA_CMP_OP_5			0x1104
+#define RT5682S_HA_CMP_OP_6			0x1105
+#define RT5682S_HA_CMP_OP_7			0x1106
+#define RT5682S_HA_CMP_OP_8			0x1107
+#define RT5682S_HA_CMP_OP_9			0x1108
+#define RT5682S_HA_CMP_OP_10			0x1109
+#define RT5682S_HA_CMP_OP_11			0x110a
+#define RT5682S_HA_CMP_OP_12			0x110b
+#define RT5682S_HA_CMP_OP_13			0x110c
+#define RT5682S_HA_CMP_OP_14			0x1111
+#define RT5682S_HA_CMP_OP_15			0x1112
+#define RT5682S_HA_CMP_OP_16			0x1113
+#define RT5682S_HA_CMP_OP_17			0x1114
+#define RT5682S_HA_CMP_OP_18			0x1115
+#define RT5682S_HA_CMP_OP_19			0x1116
+#define RT5682S_HA_CMP_OP_20			0x1117
+#define RT5682S_HA_CMP_OP_21			0x1118
+#define RT5682S_HA_CMP_OP_22			0x1119
+#define RT5682S_HA_CMP_OP_23			0x111a
+#define RT5682S_HA_CMP_OP_24			0x111b
+#define RT5682S_HA_CMP_OP_25			0x111c
+#define RT5682S_NEW_CBJ_DET_CTL_1		0x1401
+#define RT5682S_NEW_CBJ_DET_CTL_2		0x1402
+#define RT5682S_NEW_CBJ_DET_CTL_3		0x1403
+#define RT5682S_NEW_CBJ_DET_CTL_4		0x1404
+#define RT5682S_NEW_CBJ_DET_CTL_5		0x1406
+#define RT5682S_NEW_CBJ_DET_CTL_6		0x1407
+#define RT5682S_NEW_CBJ_DET_CTL_7		0x1408
+#define RT5682S_NEW_CBJ_DET_CTL_8		0x1409
+#define RT5682S_NEW_CBJ_DET_CTL_9		0x140a
+#define RT5682S_NEW_CBJ_DET_CTL_10		0x140b
+#define RT5682S_NEW_CBJ_DET_CTL_11		0x140c
+#define RT5682S_NEW_CBJ_DET_CTL_12		0x140d
+#define RT5682S_NEW_CBJ_DET_CTL_13		0x140e
+#define RT5682S_NEW_CBJ_DET_CTL_14		0x140f
+#define RT5682S_NEW_CBJ_DET_CTL_15		0x1410
+#define RT5682S_NEW_CBJ_DET_CTL_16		0x1411
+#define RT5682S_DA_FILTER_1			0x1801
+#define RT5682S_DA_FILTER_2			0x1802
+#define RT5682S_DA_FILTER_3			0x1803
+#define RT5682S_DA_FILTER_4			0x1804
+#define RT5682S_DA_FILTER_5			0x1805
+#define RT5682S_CLK_SW_TEST_1			0x2c00
+#define RT5682S_CLK_SW_TEST_2			0x3400
+#define RT5682S_CLK_SW_TEST_3			0x3404
+#define RT5682S_CLK_SW_TEST_4			0x3405
+#define RT5682S_CLK_SW_TEST_5			0x3406
+#define RT5682S_CLK_SW_TEST_6			0x3407
+#define RT5682S_CLK_SW_TEST_7			0x3408
+#define RT5682S_CLK_SW_TEST_8			0x3409
+#define RT5682S_CLK_SW_TEST_9			0x340a
+#define RT5682S_CLK_SW_TEST_10			0x340b
+#define RT5682S_CLK_SW_TEST_11			0x340c
+#define RT5682S_CLK_SW_TEST_12			0x340d
+#define RT5682S_CLK_SW_TEST_13			0x340e
+#define RT5682S_CLK_SW_TEST_14			0x340f
+#define RT5682S_EFUSE_MANU_WRITE_1		0x3410
+#define RT5682S_EFUSE_MANU_WRITE_2		0x3411
+#define RT5682S_EFUSE_MANU_WRITE_3		0x3412
+#define RT5682S_EFUSE_MANU_WRITE_4		0x3413
+#define RT5682S_EFUSE_MANU_WRITE_5		0x3414
+#define RT5682S_EFUSE_MANU_WRITE_6		0x3415
+#define RT5682S_EFUSE_READ_1			0x3424
+#define RT5682S_EFUSE_READ_2			0x3425
+#define RT5682S_EFUSE_READ_3			0x3426
+#define RT5682S_EFUSE_READ_4			0x3427
+#define RT5682S_EFUSE_READ_5			0x3428
+#define RT5682S_EFUSE_READ_6			0x3429
+#define RT5682S_EFUSE_READ_7			0x342a
+#define RT5682S_EFUSE_READ_8			0x342b
+#define RT5682S_EFUSE_READ_9			0x342c
+#define RT5682S_EFUSE_READ_10			0x342d
+#define RT5682S_EFUSE_READ_11			0x342e
+#define RT5682S_EFUSE_READ_12			0x342f
+#define RT5682S_EFUSE_READ_13			0x3430
+#define RT5682S_EFUSE_READ_14			0x3431
+#define RT5682S_EFUSE_READ_15			0x3432
+#define RT5682S_EFUSE_READ_16			0x3433
+#define RT5682S_EFUSE_READ_17			0x3434
+#define RT5682S_EFUSE_READ_18			0x3435
+#define RT5682S_EFUSE_TIMING_CTL_1		0x3440
+#define RT5682S_EFUSE_TIMING_CTL_2		0x3441
+#define RT5682S_PILOT_DIG_CTL_1			0x3500
+#define RT5682S_PILOT_DIG_CTL_2			0x3501
+#define RT5682S_HP_AMP_DET_CTL_1		0x3b00
+#define RT5682S_HP_AMP_DET_CTL_2		0x3b01
+#define RT5682S_HP_AMP_DET_CTL_3		0x3b02
+#define RT5682S_HP_AMP_DET_CTL_4		0x3b03
+
+#define RT5682S_MAX_REG				(RT5682S_HP_AMP_DET_CTL_4)
+
+/* global definition */
+#define RT5682S_L_MUTE				(0x1 << 15)
+#define RT5682S_L_MUTE_SFT			15
+#define RT5682S_R_MUTE				(0x1 << 7)
+#define RT5682S_R_MUTE_SFT			7
+#define RT5682S_L_VOL_SFT			8
+#define RT5682S_R_VOL_SFT			0
+#define RT5682S_CLK_SRC_MCLK			(0x0)
+#define RT5682S_CLK_SRC_PLL1			(0x1)
+#define RT5682S_CLK_SRC_PLL2			(0x2)
+#define RT5682S_CLK_SRC_RCCLK			(0x4) /* 25M */
+
+
+/* Headphone Amp Control 2 (0x0003) */
+#define RT5682S_HPO_L_PATH_MASK			(0x1 << 14)
+#define RT5682S_HPO_L_PATH_EN			(0x1 << 14)
+#define RT5682S_HPO_L_PATH_DIS			(0x0 << 14)
+#define RT5682S_HPO_R_PATH_MASK			(0x1 << 13)
+#define RT5682S_HPO_R_PATH_EN			(0x1 << 13)
+#define RT5682S_HPO_R_PATH_DIS			(0x0 << 13)
+#define RT5682S_HPO_SEL_IP_EN_SW		(0x1)
+#define RT5682S_HPO_IP_EN_GATING		(0x1)
+#define RT5682S_HPO_IP_NO_GATING		(0x0)
+
+/*Headphone Amp L/R Analog Gain and Digital NG2 Gain Control (0x0005 0x0006)*/
+#define RT5682S_G_HP				(0xf << 8)
+#define RT5682S_G_HP_SFT			8
+#define RT5682S_G_STO_DA_DMIX			(0xf)
+#define RT5682S_G_STO_DA_SFT			0
+
+/* Embeeded Jack and Type Detection Control 2 (0x0010) */
+#define RT5682S_EMB_JD_MASK			(0x1 << 15)
+#define RT5682S_EMB_JD_EN			(0x1 << 15)
+#define RT5682S_EMB_JD_EN_SFT			15
+#define RT5682S_EMB_JD_RST			(0x1 << 14)
+#define RT5682S_JD_MODE				(0x1 << 13)
+#define RT5682S_JD_MODE_SFT			13
+#define RT5682S_DET_TYPE			(0x1 << 12)
+#define RT5682S_DET_TYPE_SFT			12
+#define RT5682S_POLA_EXT_JD_MASK		(0x1 << 11)
+#define RT5682S_POLA_EXT_JD_LOW			(0x1 << 11)
+#define RT5682S_POLA_EXT_JD_HIGH		(0x0 << 11)
+#define RT5682S_SEL_FAST_OFF_MASK		(0x3 << 9)
+#define RT5682S_SEL_FAST_OFF_SFT		9
+#define RT5682S_POL_FAST_OFF_MASK		(0x1 << 8)
+#define RT5682S_POL_FAST_OFF_HIGH		(0x1 << 8)
+#define RT5682S_POL_FAST_OFF_LOW		(0x0 << 8)
+#define RT5682S_FAST_OFF_MASK			(0x1 << 7)
+#define RT5682S_FAST_OFF_EN			(0x1 << 7)
+#define RT5682S_FAST_OFF_DIS			(0x0 << 7)
+#define RT5682S_VREF_POW_MASK			(0x1 << 6)
+#define RT5682S_VREF_POW_FSM			(0x0 << 6)
+#define RT5682S_VREF_POW_REG			(0x1 << 6)
+#define RT5682S_MB1_PATH_BIT			5
+#define RT5682S_MB1_PATH_MASK			(0x1 << 5)
+#define RT5682S_CTRL_MB1_REG			(0x1 << 5)
+#define RT5682S_CTRL_MB1_FSM			(0x0 << 5)
+#define RT5682S_MB2_PATH_BIT			4
+#define RT5682S_MB2_PATH_MASK			(0x1 << 4)
+#define RT5682S_CTRL_MB2_REG			(0x1 << 4)
+#define RT5682S_CTRL_MB2_FSM			(0x0 << 4)
+#define RT5682S_TRIG_JD_MASK			(0x1 << 3)
+#define RT5682S_TRIG_JD_HIGH			(0x1 << 3)
+#define RT5682S_TRIG_JD_LOW			(0x0 << 3)
+#define RT5682S_MIC_CAP_MASK			(0x1 << 1)
+#define RT5682S_MIC_CAP_HS			(0x1 << 1)
+#define RT5682S_MIC_CAP_HP			(0x0 << 1)
+#define RT5682S_MIC_CAP_SRC_MASK		(0x1)
+#define RT5682S_MIC_CAP_SRC_REG			(0x1)
+#define RT5682S_MIC_CAP_SRC_ANA			(0x0)
+
+/* Embeeded Jack and Type Detection Control 3 (0x0011) */
+#define RT5682S_SEL_CBJ_TYPE_SLOW		(0x1 << 15)
+#define RT5682S_SEL_CBJ_TYPE_NORM		(0x0 << 15)
+#define RT5682S_SEL_CBJ_TYPE_MASK		(0x1 << 15)
+#define RT5682S_POW_BG_MB1_MASK			(0x1 << 13)
+#define RT5682S_POW_BG_MB1_REG			(0x1 << 13)
+#define RT5682S_POW_BG_MB1_FSM			(0x0 << 13)
+#define RT5682S_POW_BG_MB2_MASK			(0x1 << 12)
+#define RT5682S_POW_BG_MB2_REG			(0x1 << 12)
+#define RT5682S_POW_BG_MB2_FSM			(0x0 << 12)
+#define RT5682S_EXT_JD_SRC			(0x7 << 4)
+#define RT5682S_EXT_JD_SRC_SFT			4
+#define RT5682S_EXT_JD_SRC_GPIO_JD1		(0x0 << 4)
+#define RT5682S_EXT_JD_SRC_GPIO_JD2		(0x1 << 4)
+#define RT5682S_EXT_JD_SRC_JDH			(0x2 << 4)
+#define RT5682S_EXT_JD_SRC_JDL			(0x3 << 4)
+#define RT5682S_EXT_JD_SRC_MANUAL		(0x4 << 4)
+#define RT5682S_JACK_TYPE_MASK			(0x3)
+
+/* Combo Jack and Type Detection Control 4 (0x0012) */
+#define RT5682S_CBJ_IN_BUF_MASK			(0x1 << 7)
+#define RT5682S_CBJ_IN_BUF_EN			(0x1 << 7)
+#define RT5682S_CBJ_IN_BUF_DIS			(0x0 << 7)
+#define RT5682S_CBJ_IN_BUF_BIT			7
+
+/* Combo Jack and Type Detection Control 5 (0x0013) */
+#define RT5682S_SEL_SHT_MID_TON_MASK		(0x3 << 12)
+#define RT5682S_SEL_SHT_MID_TON_2		(0x0 << 12)
+#define RT5682S_SEL_SHT_MID_TON_3		(0x1 << 12)
+#define RT5682S_CBJ_JD_TEST_MASK		(0x1 << 6)
+#define RT5682S_CBJ_JD_TEST_NORM		(0x0 << 6)
+#define RT5682S_CBJ_JD_TEST_MODE		(0x1 << 6)
+
+/* Combo Jack and Type Detection Control 6 (0x0014) */
+#define RT5682S_JD_FAST_OFF_SRC_MASK		(0x7 << 8)
+#define RT5682S_JD_FAST_OFF_SRC_JDH		(0x6 << 8)
+#define RT5682S_JD_FAST_OFF_SRC_GPIO6		(0x5 << 8)
+#define RT5682S_JD_FAST_OFF_SRC_GPIO5		(0x4 << 8)
+#define RT5682S_JD_FAST_OFF_SRC_GPIO4		(0x3 << 8)
+#define RT5682S_JD_FAST_OFF_SRC_GPIO3		(0x2 << 8)
+#define RT5682S_JD_FAST_OFF_SRC_GPIO2		(0x1 << 8)
+#define RT5682S_JD_FAST_OFF_SRC_GPIO1		(0x0 << 8)
+
+/* DAC1 Digital Volume (0x0019) */
+#define RT5682S_DAC_L1_VOL_MASK			(0xff << 8)
+#define RT5682S_DAC_L1_VOL_SFT			8
+#define RT5682S_DAC_R1_VOL_MASK			(0xff)
+#define RT5682S_DAC_R1_VOL_SFT			0
+
+/* ADC Digital Volume Control (0x001c) */
+#define RT5682S_ADC_L_VOL_MASK			(0x7f << 8)
+#define RT5682S_ADC_L_VOL_SFT			8
+#define RT5682S_ADC_R_VOL_MASK			(0x7f)
+#define RT5682S_ADC_R_VOL_SFT			0
+
+/* Stereo1 ADC Boost Gain Control (0x001f) */
+#define RT5682S_STO1_ADC_L_BST_MASK		(0x3 << 14)
+#define RT5682S_STO1_ADC_L_BST_SFT		14
+#define RT5682S_STO1_ADC_R_BST_MASK		(0x3 << 12)
+#define RT5682S_STO1_ADC_R_BST_SFT		12
+
+/* Sidetone Control (0x0024) */
+#define RT5682S_ST_SRC_SEL			(0x1 << 8)
+#define RT5682S_ST_SRC_SFT			8
+#define RT5682S_ST_EN_MASK			(0x1 << 6)
+#define RT5682S_ST_DIS				(0x0 << 6)
+#define RT5682S_ST_EN				(0x1 << 6)
+#define RT5682S_ST_EN_SFT			6
+
+/* Stereo1 ADC Mixer Control (0x0026) */
+#define RT5682S_M_STO1_ADC_L1			(0x1 << 15)
+#define RT5682S_M_STO1_ADC_L1_SFT		15
+#define RT5682S_M_STO1_ADC_L2			(0x1 << 14)
+#define RT5682S_M_STO1_ADC_L2_SFT		14
+#define RT5682S_STO1_ADC1L_SRC_MASK		(0x1 << 13)
+#define RT5682S_STO1_ADC1L_SRC_SFT		13
+#define RT5682S_STO1_ADC1_SRC_ADC		(0x1 << 13)
+#define RT5682S_STO1_ADC1_SRC_DACMIX		(0x0 << 13)
+#define RT5682S_STO1_ADC2L_SRC_MASK		(0x1 << 12)
+#define RT5682S_STO1_ADC2L_SRC_SFT		12
+#define RT5682S_STO1_ADCL_SRC_MASK		(0x3 << 10)
+#define RT5682S_STO1_ADCL_SRC_SFT		10
+#define RT5682S_M_STO1_ADC_R1			(0x1 << 7)
+#define RT5682S_M_STO1_ADC_R1_SFT		7
+#define RT5682S_M_STO1_ADC_R2			(0x1 << 6)
+#define RT5682S_M_STO1_ADC_R2_SFT		6
+#define RT5682S_STO1_ADC1R_SRC_MASK		(0x1 << 5)
+#define RT5682S_STO1_ADC1R_SRC_SFT		5
+#define RT5682S_STO1_ADC2R_SRC_MASK		(0x1 << 4)
+#define RT5682S_STO1_ADC2R_SRC_SFT		4
+#define RT5682S_STO1_ADCR_SRC_MASK		(0x3 << 2)
+#define RT5682S_STO1_ADCR_SRC_SFT		2
+
+/* ADC Mixer to DAC Mixer Control (0x0029) */
+#define RT5682S_M_ADCMIX_L			(0x1 << 15)
+#define RT5682S_M_ADCMIX_L_SFT			15
+#define RT5682S_M_DAC1_L			(0x1 << 14)
+#define RT5682S_M_DAC1_L_SFT			14
+#define RT5682S_M_ADCMIX_R			(0x1 << 7)
+#define RT5682S_M_ADCMIX_R_SFT			7
+#define RT5682S_M_DAC1_R			(0x1 << 6)
+#define RT5682S_M_DAC1_R_SFT			6
+
+/* Stereo1 DAC Mixer Control (0x002a) */
+#define RT5682S_M_DAC_L1_STO_L			(0x1 << 15)
+#define RT5682S_M_DAC_L1_STO_L_SFT		15
+#define RT5682S_G_DAC_L1_STO_L_MASK		(0x1 << 14)
+#define RT5682S_G_DAC_L1_STO_L_SFT		14
+#define RT5682S_M_DAC_R1_STO_L			(0x1 << 13)
+#define RT5682S_M_DAC_R1_STO_L_SFT		13
+#define RT5682S_G_DAC_R1_STO_L_MASK		(0x1 << 12)
+#define RT5682S_G_DAC_R1_STO_L_SFT		12
+#define RT5682S_M_DAC_L1_STO_R			(0x1 << 7)
+#define RT5682S_M_DAC_L1_STO_R_SFT		7
+#define RT5682S_G_DAC_L1_STO_R_MASK		(0x1 << 6)
+#define RT5682S_G_DAC_L1_STO_R_SFT		6
+#define RT5682S_M_DAC_R1_STO_R			(0x1 << 5)
+#define RT5682S_M_DAC_R1_STO_R_SFT		5
+#define RT5682S_G_DAC_R1_STO_R_MASK		(0x1 << 4)
+#define RT5682S_G_DAC_R1_STO_R_SFT		4
+
+/* Analog DAC1 Input Source Control (0x002b) */
+#define RT5682S_M_ST_STO_L			(0x1 << 9)
+#define RT5682S_M_ST_STO_L_SFT			9
+#define RT5682S_M_ST_STO_R			(0x1 << 8)
+#define RT5682S_M_ST_STO_R_SFT			8
+#define RT5682S_DAC_L1_SRC_MASK			(0x1 << 4)
+#define RT5682S_A_DACL1_SFT			4
+#define RT5682S_DAC_R1_SRC_MASK			(0x1)
+#define RT5682S_A_DACR1_SFT			0
+
+/* Digital Interface Data Control (0x0030) */
+#define RT5682S_IF2_DAC_SEL_MASK		(0x3 << 2)
+#define RT5682S_IF2_DAC_SEL_SFT			2
+#define RT5682S_IF2_ADC_SEL_MASK		(0x3 << 0)
+#define RT5682S_IF2_ADC_SEL_SFT			0
+
+/* REC Left/Right Mixer Control 2 (0x003c) */
+#define RT5682S_BST_CBJ_MASK			(0x3f << 8)
+#define RT5682S_BST_CBJ_SFT			8
+#define RT5682S_M_CBJ_RM1_L			(0x1 << 7)
+#define RT5682S_M_CBJ_RM1_L_SFT			7
+#define RT5682S_M_CBJ_RM1_R			(0x1 << 6)
+#define RT5682S_M_CBJ_RM1_R_SFT			6
+
+/* REC Left/Right Mixer Calibration Control(0x0044) */
+#define RT5682S_PWR_RM1_R_BIT			8
+#define RT5682S_PWR_RM1_L_BIT			0
+
+/* Power Management for Digital 1 (0x0061) */
+#define RT5682S_PWR_I2S1			(0x1 << 15)
+#define RT5682S_PWR_I2S1_BIT			15
+#define RT5682S_PWR_I2S2			(0x1 << 14)
+#define RT5682S_PWR_I2S2_BIT			14
+#define RT5682S_PRE_CHR_DAC_L1			(0x1 << 13)
+#define RT5682S_PRE_CHR_DAC_L1_BIT		13
+#define RT5682S_PRE_CHR_DAC_R1			(0x1 << 12)
+#define RT5682S_PRE_CHR_DAC_R1_BIT		12
+#define RT5682S_PWR_DAC_L1			(0x1 << 11)
+#define RT5682S_PWR_DAC_L1_BIT			11
+#define RT5682S_PWR_DAC_R1			(0x1 << 10)
+#define RT5682S_PWR_DAC_R1_BIT			10
+#define RT5682S_PWR_LDO				(0x1 << 8)
+#define RT5682S_PWR_LDO_BIT			8
+#define RT5682S_PWR_D2S_L			(0x1 << 7)
+#define RT5682S_PWR_D2S_L_BIT			7
+#define RT5682S_PWR_D2S_R			(0x1 << 6)
+#define RT5682S_PWR_D2S_R_BIT			6
+#define RT5682S_PWR_ADC_L1			(0x1 << 4)
+#define RT5682S_PWR_ADC_L1_BIT			4
+#define RT5682S_PWR_ADC_R1			(0x1 << 3)
+#define RT5682S_PWR_ADC_R1_BIT			3
+#define RT5682S_EFUSE_SW_EN			(0x1 << 2)
+#define RT5682S_EFUSE_SW_DIS			(0x0 << 2)
+#define RT5682S_PWR_EFUSE			(0x1 << 1)
+#define RT5682S_PWR_EFUSE_BIT			1
+#define RT5682S_DIG_GATE_CTRL			(0x1 << 0)
+#define RT5682S_DIG_GATE_CTRL_SFT		0
+
+/* Power Management for Digital 2 (0x0062) */
+#define RT5682S_PWR_ADC_S1F			(0x1 << 15)
+#define RT5682S_PWR_ADC_S1F_BIT			15
+#define RT5682S_PWR_DAC_S1F			(0x1 << 10)
+#define RT5682S_PWR_DAC_S1F_BIT			10
+#define RT5682S_DLDO_I_LIMIT_MASK		(0x1 << 7)
+#define RT5682S_DLDO_I_LIMIT_EN			(0x1 << 7)
+#define RT5682S_DLDO_I_LIMIT_DIS		(0x0 << 7)
+#define RT5682S_DLDO_I_BIAS_SEL_4		(0x1 << 6)
+#define RT5682S_DLDO_I_BIAS_SEL_0		(0x0 << 6)
+#define RT5682S_DLDO_REG_TEST_1			(0x1 << 5)
+#define RT5682S_DLDO_REG_TEST_0			(0x0 << 5)
+#define RT5682S_DLDO_SRC_REG			(0x1 << 4)
+#define RT5682S_DLDO_SRC_EFUSE			(0x0 << 4)
+
+/* Power Management for Analog 1 (0x0063) */
+#define RT5682S_PWR_VREF1			(0x1 << 15)
+#define RT5682S_PWR_VREF1_BIT			15
+#define RT5682S_PWR_FV1				(0x1 << 14)
+#define RT5682S_PWR_FV1_BIT			14
+#define RT5682S_PWR_VREF2			(0x1 << 13)
+#define RT5682S_PWR_VREF2_BIT			13
+#define RT5682S_PWR_FV2				(0x1 << 12)
+#define RT5682S_PWR_FV2_BIT			12
+#define RT5682S_LDO1_DBG_MASK			(0x3 << 10)
+#define RT5682S_PWR_MB				(0x1 << 9)
+#define RT5682S_PWR_MB_BIT			9
+#define RT5682S_PWR_BG				(0x1 << 7)
+#define RT5682S_PWR_BG_BIT			7
+#define RT5682S_LDO1_BYPASS_MASK		(0x1 << 6)
+#define RT5682S_LDO1_BYPASS			(0x1 << 6)
+#define RT5682S_LDO1_NOT_BYPASS			(0x0 << 6)
+
+/* Power Management for Analog 2 (0x0064) */
+#define RT5682S_PWR_MCLK0_WD			(0x1 << 15)
+#define RT5682S_PWR_MCLK0_WD_BIT		15
+#define RT5682S_PWR_MCLK1_WD			(0x1 << 14)
+#define RT5682S_PWR_MCLK1_WD_BIT		14
+#define RT5682S_RST_MCLK0			(0x1 << 13)
+#define RT5682S_RST_MCLK0_BIT			13
+#define RT5682S_RST_MCLK1			(0x1 << 12)
+#define RT5682S_RST_MCLK1_BIT			12
+#define RT5682S_PWR_MB1				(0x1 << 11)
+#define RT5682S_PWR_MB1_PWR_DOWN		(0x0 << 11)
+#define RT5682S_PWR_MB1_BIT			11
+#define RT5682S_PWR_MB2				(0x1 << 10)
+#define RT5682S_PWR_MB2_PWR_DOWN		(0x0 << 10)
+#define RT5682S_PWR_MB2_BIT			10
+#define RT5682S_PWR_JD_MASK			(0x1 << 0)
+#define RT5682S_PWR_JD_ENABLE			(0x1 << 0)
+#define RT5682S_PWR_JD_DISABLE			(0x0 << 0)
+
+/* Power Management for Analog 3 (0x0065) */
+#define RT5682S_PWR_LDO_PLLA			(0x1 << 15)
+#define RT5682S_PWR_LDO_PLLA_BIT		15
+#define RT5682S_PWR_LDO_PLLB			(0x1 << 14)
+#define RT5682S_PWR_LDO_PLLB_BIT		14
+#define RT5682S_PWR_BIAS_PLLA			(0x1 << 13)
+#define RT5682S_PWR_BIAS_PLLA_BIT		13
+#define RT5682S_PWR_BIAS_PLLB			(0x1 << 12)
+#define RT5682S_PWR_BIAS_PLLB_BIT		12
+#define RT5682S_PWR_CBJ				(0x1 << 9)
+#define RT5682S_PWR_CBJ_BIT			9
+#define RT5682S_RSTB_PLLB			(0x1 << 7)
+#define RT5682S_RSTB_PLLB_BIT			7
+#define RT5682S_RSTB_PLLA			(0x1 << 6)
+#define RT5682S_RSTB_PLLA_BIT			6
+#define RT5682S_PWR_PLLB			(0x1 << 5)
+#define RT5682S_PWR_PLLB_BIT			5
+#define RT5682S_PWR_PLLA			(0x1 << 4)
+#define RT5682S_PWR_PLLA_BIT			4
+#define RT5682S_PWR_LDO_MB2			(0x1 << 2)
+#define RT5682S_PWR_LDO_MB2_BIT			2
+#define RT5682S_PWR_LDO_MB1			(0x1 << 1)
+#define RT5682S_PWR_LDO_MB1_BIT			1
+#define RT5682S_PWR_BGLDO			(0x1 << 0)
+#define RT5682S_PWR_BGLDO_BIT			0
+
+/* Power Management for Mixer (0x0066) */
+#define RT5682S_PWR_CLK_COMP_8FS		(0x1 << 15)
+#define RT5682S_PWR_CLK_COMP_8FS_BIT		15
+#define RT5682S_DBG_BGLDO_MASK			(0x3 << 12)
+#define RT5682S_DBG_BGLDO_SFT			12
+#define RT5682S_DBG_BGLDO_MB1_MASK		(0x3 << 10)
+#define RT5682S_DBG_BGLDO_MB1_SFT		10
+#define RT5682S_DBG_BGLDO_MB2_MASK		(0x3 << 8)
+#define RT5682S_DBG_BGLDO_MB2_SFT		8
+#define RT5682S_DLDO_BGLDO_MASK			(0x3 << 6)
+#define RT5682S_DLDO_BGLDO_MB2_SFT		6
+#define RT5682S_PWR_STO1_DAC_L			(0x1 << 5)
+#define RT5682S_PWR_STO1_DAC_L_BIT		5
+#define RT5682S_PWR_STO1_DAC_R			(0x1 << 4)
+#define RT5682S_PWR_STO1_DAC_R_BIT		4
+#define RT5682S_DVO_BGLDO_MB1_MASK		(0x3 << 2)
+#define RT5682S_DVO_BGLDO_MB1_SFT		2
+#define RT5682S_DVO_BGLDO_MB2_MASK		(0x3 << 0)
+
+/* MCLK and System Clock Detection Control (0x006b) */
+#define RT5682S_SYS_CLK_DET			(0x1 << 15)
+#define RT5682S_SYS_CLK_DET_SFT			15
+#define RT5682S_PLL1_CLK_DET			(0x1 << 14)
+#define RT5682S_PLL1_CLK_DET_SFT		14
+
+/* Digital Microphone Control 1 (0x006e) */
+#define RT5682S_DMIC_1_EN_MASK			(0x1 << 15)
+#define RT5682S_DMIC_1_EN_SFT			15
+#define RT5682S_DMIC_1_DIS			(0x0 << 15)
+#define RT5682S_DMIC_1_EN			(0x1 << 15)
+#define RT5682S_FIFO_CLK_DIV_MASK		(0x7 << 12)
+#define RT5682S_FIFO_CLK_DIV_2			(0x1 << 12)
+#define RT5682S_DMIC_1_DP_MASK			(0x3 << 4)
+#define RT5682S_DMIC_1_DP_SFT			4
+#define RT5682S_DMIC_1_DP_GPIO2			(0x0 << 4)
+#define RT5682S_DMIC_1_DP_GPIO5			(0x1 << 4)
+#define RT5682S_DMIC_CLK_MASK			(0xf << 0)
+#define RT5682S_DMIC_CLK_SFT			0
+
+/* I2S1 Audio Serial Data Port Control (0x0070) */
+#define RT5682S_SEL_ADCDAT_MASK			(0x1 << 15)
+#define RT5682S_SEL_ADCDAT_OUT			(0x0 << 15)
+#define RT5682S_SEL_ADCDAT_IN			(0x1 << 15)
+#define RT5682S_SEL_ADCDAT_SFT			15
+#define RT5682S_I2S1_TX_CHL_MASK		(0x7 << 12)
+#define RT5682S_I2S1_TX_CHL_SFT			12
+#define RT5682S_I2S1_TX_CHL_16			(0x0 << 12)
+#define RT5682S_I2S1_TX_CHL_20			(0x1 << 12)
+#define RT5682S_I2S1_TX_CHL_24			(0x2 << 12)
+#define RT5682S_I2S1_TX_CHL_32			(0x3 << 12)
+#define RT5682S_I2S1_TX_CHL_8			(0x4 << 12)
+#define RT5682S_I2S1_RX_CHL_MASK		(0x7 << 8)
+#define RT5682S_I2S1_RX_CHL_SFT			8
+#define RT5682S_I2S1_RX_CHL_16			(0x0 << 8)
+#define RT5682S_I2S1_RX_CHL_20			(0x1 << 8)
+#define RT5682S_I2S1_RX_CHL_24			(0x2 << 8)
+#define RT5682S_I2S1_RX_CHL_32			(0x3 << 8)
+#define RT5682S_I2S1_RX_CHL_8			(0x4 << 8)
+#define RT5682S_I2S1_MONO_MASK			(0x1 << 7)
+#define RT5682S_I2S1_MONO_EN			(0x1 << 7)
+#define RT5682S_I2S1_MONO_DIS			(0x0 << 7)
+#define RT5682S_I2S1_DL_MASK			(0x7 << 4)
+#define RT5682S_I2S1_DL_SFT			4
+#define RT5682S_I2S1_DL_16			(0x0 << 4)
+#define RT5682S_I2S1_DL_20			(0x1 << 4)
+#define RT5682S_I2S1_DL_24			(0x2 << 4)
+#define RT5682S_I2S1_DL_32			(0x3 << 4)
+#define RT5682S_I2S1_DL_8			(0x4 << 4)
+
+/* I2S1/2 Audio Serial Data Port Control (0x0071) */
+#define RT5682S_I2S2_MS_MASK			(0x1 << 15)
+#define RT5682S_I2S2_MS_SFT			15
+#define RT5682S_I2S2_MS_M			(0x0 << 15)
+#define RT5682S_I2S2_MS_S			(0x1 << 15)
+#define RT5682S_I2S2_PIN_CFG_MASK		(0x1 << 14)
+#define RT5682S_I2S2_PIN_CFG_SFT		14
+#define RT5682S_I2S2_OUT_MASK			(0x1 << 9)
+#define RT5682S_I2S2_OUT_SFT			9
+#define RT5682S_I2S2_OUT_UM			(0x0 << 9)
+#define RT5682S_I2S2_OUT_M			(0x1 << 9)
+#define RT5682S_I2S_BP_MASK			(0x1 << 8)
+#define RT5682S_I2S_BP_SFT			8
+#define RT5682S_I2S_BP_NOR			(0x0 << 8)
+#define RT5682S_I2S_BP_INV			(0x1 << 8)
+#define RT5682S_I2S2_MONO_MASK			(0x1 << 7)
+#define RT5682S_I2S2_MONO_EN			(0x1 << 7)
+#define RT5682S_I2S2_MONO_DIS			(0x0 << 7)
+#define RT5682S_I2S2_DL_MASK			(0x7 << 4)
+#define RT5682S_I2S2_DL_SFT			4
+#define RT5682S_I2S2_DL_8			(0x0 << 4)
+#define RT5682S_I2S2_DL_16			(0x1 << 4)
+#define RT5682S_I2S2_DL_20			(0x2 << 4)
+#define RT5682S_I2S2_DL_24			(0x3 << 4)
+#define RT5682S_I2S2_DL_32			(0x4 << 4)
+#define RT5682S_I2S_DF_MASK			(0x7)
+#define RT5682S_I2S_DF_SFT			0
+#define RT5682S_I2S_DF_I2S			(0x0)
+#define RT5682S_I2S_DF_LEFT			(0x1)
+#define RT5682S_I2S_DF_PCM_A			(0x2)
+#define RT5682S_I2S_DF_PCM_B			(0x3)
+#define RT5682S_I2S_DF_PCM_A_N			(0x6)
+#define RT5682S_I2S_DF_PCM_B_N			(0x7)
+
+/* ADC/DAC Clock Control 1 (0x0073) */
+#define RT5682S_ADC_OSR_MASK			(0xf << 12)
+#define RT5682S_ADC_OSR_SFT			12
+#define RT5682S_ADC_OSR_D_1			(0x0 << 12)
+#define RT5682S_ADC_OSR_D_2			(0x1 << 12)
+#define RT5682S_ADC_OSR_D_4			(0x2 << 12)
+#define RT5682S_ADC_OSR_D_6			(0x3 << 12)
+#define RT5682S_ADC_OSR_D_8			(0x4 << 12)
+#define RT5682S_ADC_OSR_D_12			(0x5 << 12)
+#define RT5682S_ADC_OSR_D_16			(0x6 << 12)
+#define RT5682S_ADC_OSR_D_24			(0x7 << 12)
+#define RT5682S_ADC_OSR_D_32			(0x8 << 12)
+#define RT5682S_ADC_OSR_D_48			(0x9 << 12)
+#define RT5682S_I2S_M_D_MASK			(0xf << 8)
+#define RT5682S_I2S_M_D_SFT			8
+#define RT5682S_I2S_M_D_1			(0x0 << 8)
+#define RT5682S_I2S_M_D_2			(0x1 << 8)
+#define RT5682S_I2S_M_D_3			(0x2 << 8)
+#define RT5682S_I2S_M_D_4			(0x3 << 8)
+#define RT5682S_I2S_M_D_6			(0x4 << 8)
+#define RT5682S_I2S_M_D_8			(0x5 << 8)
+#define RT5682S_I2S_M_D_12			(0x6 << 8)
+#define RT5682S_I2S_M_D_16			(0x7 << 8)
+#define RT5682S_I2S_M_D_24			(0x8 << 8)
+#define RT5682S_I2S_M_D_32			(0x9 << 8)
+#define RT5682S_I2S_M_D_48			(0x10 << 8)
+#define RT5682S_I2S_M_CLK_SRC_MASK		(0x7 << 4)
+#define RT5682S_I2S_M_CLK_SRC_SFT		4
+#define RT5682S_DAC_OSR_MASK			(0xf << 0)
+#define RT5682S_DAC_OSR_SFT			0
+#define RT5682S_DAC_OSR_D_1			(0x0 << 0)
+#define RT5682S_DAC_OSR_D_2			(0x1 << 0)
+#define RT5682S_DAC_OSR_D_4			(0x2 << 0)
+#define RT5682S_DAC_OSR_D_6			(0x3 << 0)
+#define RT5682S_DAC_OSR_D_8			(0x4 << 0)
+#define RT5682S_DAC_OSR_D_12			(0x5 << 0)
+#define RT5682S_DAC_OSR_D_16			(0x6 << 0)
+#define RT5682S_DAC_OSR_D_24			(0x7 << 0)
+#define RT5682S_DAC_OSR_D_32			(0x8 << 0)
+#define RT5682S_DAC_OSR_D_48			(0x9 << 0)
+
+/* ADC/DAC Clock Control 2 (0x0074) */
+#define RT5682S_I2S2_BCLK_MS2_MASK		(0x1 << 11)
+#define RT5682S_I2S2_BCLK_MS2_SFT		11
+#define RT5682S_I2S2_BCLK_MS2_32		(0x0 << 11)
+#define RT5682S_I2S2_BCLK_MS2_64		(0x1 << 11)
+
+
+/* TDM control 1 (0x0079) */
+#define RT5682S_TDM_TX_CH_MASK			(0x3 << 12)
+#define RT5682S_TDM_TX_CH_2			(0x0 << 12)
+#define RT5682S_TDM_TX_CH_4			(0x1 << 12)
+#define RT5682S_TDM_TX_CH_6			(0x2 << 12)
+#define RT5682S_TDM_TX_CH_8			(0x3 << 12)
+#define RT5682S_TDM_RX_CH_MASK			(0x3 << 8)
+#define RT5682S_TDM_RX_CH_2			(0x0 << 8)
+#define RT5682S_TDM_RX_CH_4			(0x1 << 8)
+#define RT5682S_TDM_RX_CH_6			(0x2 << 8)
+#define RT5682S_TDM_RX_CH_8			(0x3 << 8)
+#define RT5682S_TDM_ADC_LCA_MASK		(0x7 << 4)
+#define RT5682S_TDM_ADC_LCA_SFT			4
+#define RT5682S_TDM_ADC_DL_SFT			0
+
+/* TDM control 2 (0x007a) */
+#define RT5682S_IF1_ADC1_SEL_SFT		14
+#define RT5682S_IF1_ADC2_SEL_SFT		12
+#define RT5682S_IF1_ADC3_SEL_SFT		10
+#define RT5682S_IF1_ADC4_SEL_SFT		8
+#define RT5682S_TDM_ADC_SEL_SFT			3
+
+/* TDM control 3 (0x007b) */
+#define RT5682S_TDM_EN				(0x1 << 7)
+
+/* TDM/I2S control (0x007e) */
+#define RT5682S_TDM_S_BP_MASK			(0x1 << 15)
+#define RT5682S_TDM_S_BP_SFT			15
+#define RT5682S_TDM_S_BP_NOR			(0x0 << 15)
+#define RT5682S_TDM_S_BP_INV			(0x1 << 15)
+#define RT5682S_TDM_S_LP_MASK			(0x1 << 14)
+#define RT5682S_TDM_S_LP_SFT			14
+#define RT5682S_TDM_S_LP_NOR			(0x0 << 14)
+#define RT5682S_TDM_S_LP_INV			(0x1 << 14)
+#define RT5682S_TDM_DF_MASK			(0x7 << 11)
+#define RT5682S_TDM_DF_SFT			11
+#define RT5682S_TDM_DF_I2S			(0x0 << 11)
+#define RT5682S_TDM_DF_LEFT			(0x1 << 11)
+#define RT5682S_TDM_DF_PCM_A			(0x2 << 11)
+#define RT5682S_TDM_DF_PCM_B			(0x3 << 11)
+#define RT5682S_TDM_DF_PCM_A_N			(0x6 << 11)
+#define RT5682S_TDM_DF_PCM_B_N			(0x7 << 11)
+#define RT5682S_TDM_BCLK_MS1_MASK		(0x3 << 8)
+#define RT5682S_TDM_BCLK_MS1_SFT		8
+#define RT5682S_TDM_BCLK_MS1_32			(0x0 << 8)
+#define RT5682S_TDM_BCLK_MS1_64			(0x1 << 8)
+#define RT5682S_TDM_BCLK_MS1_128		(0x2 << 8)
+#define RT5682S_TDM_BCLK_MS1_256		(0x3 << 8)
+#define RT5682S_TDM_BCLK_MS1_16			(0x4 << 8)
+#define RT5682S_TDM_CL_MASK			(0x3 << 4)
+#define RT5682S_TDM_CL_16			(0x0 << 4)
+#define RT5682S_TDM_CL_20			(0x1 << 4)
+#define RT5682S_TDM_CL_24			(0x2 << 4)
+#define RT5682S_TDM_CL_32			(0x3 << 4)
+#define RT5682S_TDM_M_BP_MASK			(0x1 << 2)
+#define RT5682S_TDM_M_BP_SFT			2
+#define RT5682S_TDM_M_BP_NOR			(0x0 << 2)
+#define RT5682S_TDM_M_BP_INV			(0x1 << 2)
+#define RT5682S_TDM_M_LP_MASK			(0x1 << 1)
+#define RT5682S_TDM_M_LP_SFT			1
+#define RT5682S_TDM_M_LP_NOR			(0x0 << 1)
+#define RT5682S_TDM_M_LP_INV			(0x1 << 1)
+#define RT5682S_TDM_MS_MASK			(0x1 << 0)
+#define RT5682S_TDM_MS_SFT			0
+#define RT5682S_TDM_MS_S			(0x0 << 0)
+#define RT5682S_TDM_MS_M			(0x1 << 0)
+
+/* Global Clock Control (0x0080) */
+#define RT5682S_SCLK_SRC_MASK			(0x7 << 13)
+#define RT5682S_SCLK_SRC_SFT			13
+#define RT5682S_PLL_SRC_MASK			(0x3 << 8)
+#define RT5682S_PLL_SRC_SFT			8
+#define RT5682S_PLL_SRC_MCLK			(0x0 << 8)
+#define RT5682S_PLL_SRC_BCLK1			(0x1 << 8)
+#define RT5682S_PLL_SRC_RC			(0x3 << 8)
+
+/* PLL tracking mode 1 (0x0083) */
+#define RT5682S_DA_ASRC_MASK			(0x1 << 13)
+#define RT5682S_DA_ASRC_SFT			13
+#define RT5682S_DAC_STO1_ASRC_MASK		(0x1 << 12)
+#define RT5682S_DAC_STO1_ASRC_SFT		12
+#define RT5682S_AD_ASRC_MASK			(0x1 << 8)
+#define RT5682S_AD_ASRC_SFT			8
+#define RT5682S_AD_ASRC_SEL_MASK		(0x1 << 4)
+#define RT5682S_AD_ASRC_SEL_SFT			4
+#define RT5682S_DMIC_ASRC_MASK			(0x1 << 3)
+#define RT5682S_DMIC_ASRC_SFT			3
+#define RT5682S_ADC_STO1_ASRC_MASK		(0x1 << 2)
+#define RT5682S_ADC_STO1_ASRC_SFT		2
+#define RT5682S_DA_ASRC_SEL_MASK		(0x1 << 0)
+#define RT5682S_DA_ASRC_SEL_SFT			0
+
+/* PLL tracking mode 2 3 (0x0084)(0x0085)*/
+#define RT5682S_FILTER_CLK_SEL_MASK		(0x7 << 12)
+#define RT5682S_FILTER_CLK_SEL_SFT		12
+#define RT5682S_FILTER_CLK_DIV_MASK		(0xf << 8)
+#define RT5682S_FILTER_CLK_DIV_SFT		8
+
+/* ASRC Control 4 (0x0086) */
+#define RT5682S_ASRCIN_FTK_N1_MASK		(0x3 << 14)
+#define RT5682S_ASRCIN_FTK_N1_SFT		14
+#define RT5682S_ASRCIN_FTK_N2_MASK		(0x3 << 12)
+#define RT5682S_ASRCIN_FTK_N2_SFT		12
+#define RT5682S_ASRCIN_FTK_M1_MASK		(0x7 << 8)
+#define RT5682S_ASRCIN_FTK_M1_SFT		8
+#define RT5682S_ASRCIN_FTK_M2_MASK		(0x7 << 4)
+#define RT5682S_ASRCIN_FTK_M2_SFT		4
+
+/* Depop Mode Control 1 (0x008e) */
+#define RT5682S_OUT_HP_L_EN			(0x1 << 6)
+#define RT5682S_OUT_HP_R_EN			(0x1 << 5)
+#define RT5682S_LDO_PUMP_EN			(0x1 << 4)
+#define RT5682S_LDO_PUMP_EN_SFT			4
+#define RT5682S_PUMP_EN				(0x1 << 3)
+#define RT5682S_PUMP_EN_SFT			3
+#define RT5682S_CAPLESS_L_EN			(0x1 << 1)
+#define RT5682S_CAPLESS_L_EN_SFT		1
+#define RT5682S_CAPLESS_R_EN			(0x1 << 0)
+#define RT5682S_CAPLESS_R_EN_SFT		0
+
+/* Depop Mode Control 2 (0x8f) */
+#define RT5682S_RAMP_MASK			(0x1 << 12)
+#define RT5682S_RAMP_SFT			12
+#define RT5682S_RAMP_DIS			(0x0 << 12)
+#define RT5682S_RAMP_EN				(0x1 << 12)
+#define RT5682S_BPS_MASK			(0x1 << 11)
+#define RT5682S_BPS_SFT				11
+#define RT5682S_BPS_DIS				(0x0 << 11)
+#define RT5682S_BPS_EN				(0x1 << 11)
+#define RT5682S_FAST_UPDN_MASK			(0x1 << 10)
+#define RT5682S_FAST_UPDN_SFT			10
+#define RT5682S_FAST_UPDN_DIS			(0x0 << 10)
+#define RT5682S_FAST_UPDN_EN			(0x1 << 10)
+#define RT5682S_VLO_MASK			(0x1 << 7)
+#define RT5682S_VLO_SFT				7
+#define RT5682S_VLO_3V				(0x0 << 7)
+#define RT5682S_VLO_33V				(0x1 << 7)
+
+/* HPOUT charge pump 1 (0x0091) */
+#define RT5682S_OSW_L_MASK			(0x1 << 11)
+#define RT5682S_OSW_L_SFT			11
+#define RT5682S_OSW_L_DIS			(0x0 << 11)
+#define RT5682S_OSW_L_EN			(0x1 << 11)
+#define RT5682S_OSW_R_MASK			(0x1 << 10)
+#define RT5682S_OSW_R_SFT			10
+#define RT5682S_OSW_R_DIS			(0x0 << 10)
+#define RT5682S_OSW_R_EN			(0x1 << 10)
+#define RT5682S_PM_HP_MASK			(0x3 << 8)
+#define RT5682S_PM_HP_SFT			8
+#define RT5682S_PM_HP_LV			(0x0 << 8)
+#define RT5682S_PM_HP_MV			(0x1 << 8)
+#define RT5682S_PM_HP_HV			(0x2 << 8)
+
+/* Micbias Control1 (0x93) */
+#define RT5682S_MIC1_OV_MASK			(0x3 << 14)
+#define RT5682S_MIC1_OV_SFT			14
+#define RT5682S_MIC1_OV_2V7			(0x0 << 14)
+#define RT5682S_MIC1_OV_2V4			(0x1 << 14)
+#define RT5682S_MIC1_OV_2V25			(0x3 << 14)
+#define RT5682S_MIC1_OV_1V8			(0x4 << 14)
+#define RT5682S_MIC2_OV_MASK			(0x3 << 8)
+#define RT5682S_MIC2_OV_SFT			8
+#define RT5682S_MIC2_OV_2V7			(0x0 << 8)
+#define RT5682S_MIC2_OV_2V4			(0x1 << 8)
+#define RT5682S_MIC2_OV_2V25			(0x3 << 8)
+#define RT5682S_MIC2_OV_1V8			(0x4 << 8)
+
+/* Micbias Control2 (0x0094) */
+#define RT5682S_PWR_CLK25M_MASK			(0x1 << 9)
+#define RT5682S_PWR_CLK25M_SFT			9
+#define RT5682S_PWR_CLK25M_PD			(0x0 << 9)
+#define RT5682S_PWR_CLK25M_PU			(0x1 << 9)
+#define RT5682S_PWR_CLK1M_MASK			(0x1 << 8)
+#define RT5682S_PWR_CLK1M_SFT			8
+#define RT5682S_PWR_CLK1M_PD			(0x0 << 8)
+#define RT5682S_PWR_CLK1M_PU			(0x1 << 8)
+
+/* PLL M/N/K Code Control 1 (0x0098) */
+#define RT5682S_PLLA_N_MASK			(0x1ff << 0)
+
+/* PLL M/N/K Code Control 2 (0x0099) */
+#define RT5682S_PLLA_M_MASK			(0x1f << 8)
+#define RT5682S_PLLA_M_SFT			8
+#define RT5682S_PLLA_K_MASK			(0x1f << 0)
+
+/* PLL M/N/K Code Control 3 (0x009a) */
+#define RT5682S_PLLB_N_MASK			(0x3ff << 0)
+
+/* PLL M/N/K Code Control 4 (0x009b) */
+#define RT5682S_PLLB_M_MASK			(0x1f << 8)
+#define RT5682S_PLLB_M_SFT			8
+#define RT5682S_PLLB_K_MASK			(0x1f << 0)
+
+/* PLL M/N/K Code Control 6 (0x009d) */
+#define RT5682S_PLLB_SEL_PS_MASK		(0x1 << 13)
+#define RT5682S_PLLB_SEL_PS_SFT			13
+#define RT5682S_PLLB_BYP_PS_MASK		(0x1 << 12)
+#define RT5682S_PLLB_BYP_PS_SFT			12
+#define RT5682S_PLLB_M_BP_MASK			(0x1 << 11)
+#define RT5682S_PLLB_M_BP_SFT			11
+#define RT5682S_PLLB_K_BP_MASK			(0x1 << 10)
+#define RT5682S_PLLB_K_BP_SFT			10
+#define RT5682S_PLLA_M_BP_MASK			(0x1 << 7)
+#define RT5682S_PLLA_M_BP_SFT			7
+#define RT5682S_PLLA_K_BP_MASK			(0x1 << 6)
+#define RT5682S_PLLA_K_BP_SFT			6
+
+/* PLL M/N/K Code Control 7 (0x009e) */
+#define RT5682S_PLLB_SRC_MASK			(0x3 << 0)
+#define RT5682S_PLLB_SRC_DFIN			(0x1)
+#define RT5682S_PLLB_SRC_PLLA			(0x0)
+
+/* RC Clock Control (0x009f) */
+#define RT5682S_POW_IRQ				(0x1 << 15)
+#define RT5682S_POW_JDH				(0x1 << 14)
+
+/* I2S2 Master Mode Clock Control 1 (0x00a0) */
+#define RT5682S_I2S2_M_CLK_SRC_MASK		(0x7 << 4)
+#define RT5682S_I2S2_M_CLK_SRC_SFT		4
+#define RT5682S_I2S2_M_D_MASK			(0xf << 0)
+#define RT5682S_I2S2_M_D_1			(0x0)
+#define RT5682S_I2S2_M_D_2			(0x1)
+#define RT5682S_I2S2_M_D_3			(0x2)
+#define RT5682S_I2S2_M_D_4			(0x3)
+#define RT5682S_I2S2_M_D_6			(0x4)
+#define RT5682S_I2S2_M_D_8			(0x5)
+#define RT5682S_I2S2_M_D_12			(0x6)
+#define RT5682S_I2S2_M_D_16			(0x7)
+#define RT5682S_I2S2_M_D_24			(0x8)
+#define RT5682S_I2S2_M_D_32			(0x9)
+#define RT5682S_I2S2_M_D_48			(0xa)
+#define RT5682S_I2S2_M_D_SFT			0
+
+/* IRQ Control 1 (0x00b6) */
+#define RT5682S_JD1_PULSE_EN_MASK		(0x1 << 10)
+#define RT5682S_JD1_PULSE_EN_SFT		10
+#define RT5682S_JD1_PULSE_DIS			(0x0 << 10)
+#define RT5682S_JD1_PULSE_EN			(0x1 << 10)
+
+/* IRQ Control 2 (0x00b7) */
+#define RT5682S_JD1_EN_MASK			(0x1 << 15)
+#define RT5682S_JD1_EN_SFT			15
+#define RT5682S_JD1_DIS				(0x0 << 15)
+#define RT5682S_JD1_EN				(0x1 << 15)
+#define RT5682S_JD1_POL_MASK			(0x1 << 13)
+#define RT5682S_JD1_POL_NOR			(0x0 << 13)
+#define RT5682S_JD1_POL_INV			(0x1 << 13)
+#define RT5682S_JD1_IRQ_MASK			(0x1 << 10)
+#define RT5682S_JD1_IRQ_LEV			(0x0 << 10)
+#define RT5682S_JD1_IRQ_PUL			(0x1 << 10)
+
+/* IRQ Control 3 (0x00b8) */
+#define RT5682S_IL_IRQ_MASK			(0x1 << 7)
+#define RT5682S_IL_IRQ_DIS			(0x0 << 7)
+#define RT5682S_IL_IRQ_EN			(0x1 << 7)
+#define RT5682S_IL_IRQ_TYPE_MASK		(0x1 << 4)
+#define RT5682S_IL_IRQ_LEV			(0x0 << 4)
+#define RT5682S_IL_IRQ_PUL			(0x1 << 4)
+
+/* GPIO Control 1 (0x00c0) */
+#define RT5682S_GP1_PIN_MASK			(0x3 << 14)
+#define RT5682S_GP1_PIN_SFT			14
+#define RT5682S_GP1_PIN_GPIO1			(0x0 << 14)
+#define RT5682S_GP1_PIN_IRQ			(0x1 << 14)
+#define RT5682S_GP1_PIN_DMIC_CLK		(0x2 << 14)
+#define RT5682S_GP2_PIN_MASK			(0x3 << 12)
+#define RT5682S_GP2_PIN_SFT			12
+#define RT5682S_GP2_PIN_GPIO2			(0x0 << 12)
+#define RT5682S_GP2_PIN_LRCK2			(0x1 << 12)
+#define RT5682S_GP2_PIN_DMIC_SDA		(0x2 << 12)
+#define RT5682S_GP3_PIN_MASK			(0x3 << 10)
+#define RT5682S_GP3_PIN_SFT			10
+#define RT5682S_GP3_PIN_GPIO3			(0x0 << 10)
+#define RT5682S_GP3_PIN_BCLK2			(0x1 << 10)
+#define RT5682S_GP3_PIN_DMIC_CLK		(0x2 << 10)
+#define RT5682S_GP4_PIN_MASK			(0x3 << 8)
+#define RT5682S_GP4_PIN_SFT			8
+#define RT5682S_GP4_PIN_GPIO4			(0x0 << 8)
+#define RT5682S_GP4_PIN_ADCDAT1			(0x1 << 8)
+#define RT5682S_GP4_PIN_DMIC_CLK		(0x2 << 8)
+#define RT5682S_GP4_PIN_ADCDAT2			(0x3 << 8)
+#define RT5682S_GP5_PIN_MASK			(0x3 << 6)
+#define RT5682S_GP5_PIN_SFT			6
+#define RT5682S_GP5_PIN_GPIO5			(0x0 << 6)
+#define RT5682S_GP5_PIN_DACDAT1			(0x1 << 6)
+#define RT5682S_GP5_PIN_DMIC_SDA		(0x2 << 6)
+#define RT5682S_GP6_PIN_MASK			(0x1 << 5)
+#define RT5682S_GP6_PIN_SFT			5
+#define RT5682S_GP6_PIN_GPIO6			(0x0 << 5)
+#define RT5682S_GP6_PIN_LRCK1			(0x1 << 5)
+
+/* GPIO Control 2 (0x00c1)*/
+#define RT5682S_GP1_PF_MASK			(0x1 << 15)
+#define RT5682S_GP1_PF_IN			(0x0 << 15)
+#define RT5682S_GP1_PF_OUT			(0x1 << 15)
+#define RT5682S_GP1_OUT_MASK			(0x1 << 14)
+#define RT5682S_GP1_OUT_L			(0x0 << 14)
+#define RT5682S_GP1_OUT_H			(0x1 << 14)
+#define RT5682S_GP2_PF_MASK			(0x1 << 13)
+#define RT5682S_GP2_PF_IN			(0x0 << 13)
+#define RT5682S_GP2_PF_OUT			(0x1 << 13)
+#define RT5682S_GP2_OUT_MASK			(0x1 << 12)
+#define RT5682S_GP2_OUT_L			(0x0 << 12)
+#define RT5682S_GP2_OUT_H			(0x1 << 12)
+#define RT5682S_GP3_PF_MASK			(0x1 << 11)
+#define RT5682S_GP3_PF_IN			(0x0 << 11)
+#define RT5682S_GP3_PF_OUT			(0x1 << 11)
+#define RT5682S_GP3_OUT_MASK			(0x1 << 10)
+#define RT5682S_GP3_OUT_L			(0x0 << 10)
+#define RT5682S_GP3_OUT_H			(0x1 << 10)
+#define RT5682S_GP4_PF_MASK			(0x1 << 9)
+#define RT5682S_GP4_PF_IN			(0x0 << 9)
+#define RT5682S_GP4_PF_OUT			(0x1 << 9)
+#define RT5682S_GP4_OUT_MASK			(0x1 << 8)
+#define RT5682S_GP4_OUT_L			(0x0 << 8)
+#define RT5682S_GP4_OUT_H			(0x1 << 8)
+#define RT5682S_GP5_PF_MASK			(0x1 << 7)
+#define RT5682S_GP5_PF_IN			(0x0 << 7)
+#define RT5682S_GP5_PF_OUT			(0x1 << 7)
+#define RT5682S_GP5_OUT_MASK			(0x1 << 6)
+#define RT5682S_GP5_OUT_L			(0x0 << 6)
+#define RT5682S_GP5_OUT_H			(0x1 << 6)
+#define RT5682S_GP6_PF_MASK			(0x1 << 5)
+#define RT5682S_GP6_PF_IN			(0x0 << 5)
+#define RT5682S_GP6_PF_OUT			(0x1 << 5)
+#define RT5682S_GP6_OUT_MASK			(0x1 << 4)
+#define RT5682S_GP6_OUT_L			(0x0 << 4)
+#define RT5682S_GP6_OUT_H			(0x1 << 4)
+
+/* GPIO Status (0x00c2) */
+#define RT5682S_GP6_ST				(0x1 << 6)
+#define RT5682S_GP5_ST				(0x1 << 5)
+#define RT5682S_GP4_ST				(0x1 << 4)
+#define RT5682S_GP3_ST				(0x1 << 3)
+#define RT5682S_GP2_ST				(0x1 << 2)
+#define RT5682S_GP1_ST				(0x1 << 1)
+
+/* Soft volume and zero cross control 1 (0x00d9) */
+#define RT5682S_ZCD_MASK			(0x1 << 10)
+#define RT5682S_ZCD_SFT				10
+#define RT5682S_ZCD_PD				(0x0 << 10)
+#define RT5682S_ZCD_PU				(0x1 << 10)
+
+/* 4 Button Inline Command Control 2 (0x00e3) */
+#define RT5682S_4BTN_IL_MASK			(0x1 << 15)
+#define RT5682S_4BTN_IL_EN			(0x1 << 15)
+#define RT5682S_4BTN_IL_DIS			(0x0 << 15)
+#define RT5682S_4BTN_IL_RST_MASK		(0x1 << 14)
+#define RT5682S_4BTN_IL_NOR			(0x1 << 14)
+#define RT5682S_4BTN_IL_RST			(0x0 << 14)
+
+/* 4 Button Inline Command Control 3~6 (0x00e5~0x00e8) */
+#define RT5682S_4BTN_IL_HOLD_WIN_MASK		(0x7f << 8)
+#define RT5682S_4BTN_IL_HOLD_WIN_SFT		8
+#define RT5682S_4BTN_IL_CLICK_WIN_MASK		(0x7f)
+#define RT5682S_4BTN_IL_CLICK_WIN_SFT		0
+
+/* Analog JD Control (0x00f0) */
+#define RT5682S_JDH_RS_MASK			(0x1 << 4)
+#define RT5682S_JDH_NO_PLUG			(0x1 << 4)
+#define RT5682S_JDH_PLUG			(0x0 << 4)
+
+/* Charge Pump Internal Register1 (0x0125) */
+#define RT5682S_CP_CLK_HP_MASK			(0x3 << 4)
+#define RT5682S_CP_CLK_HP_100KHZ		(0x0 << 4)
+#define RT5682S_CP_CLK_HP_200KHZ		(0x1 << 4)
+#define RT5682S_CP_CLK_HP_300KHZ		(0x2 << 4)
+#define RT5682S_CP_CLK_HP_600KHZ		(0x3 << 4)
+
+/* Pad Driving Control (0x0136) */
+#define RT5682S_PAD_DRV_GP1_MASK		(0x1 << 14)
+#define RT5682S_PAD_DRV_GP1_HIGH		(0x1 << 14)
+#define RT5682S_PAD_DRV_GP1_LOW			(0x0 << 14)
+#define RT5682S_PAD_DRV_GP2_MASK		(0x1 << 12)
+#define RT5682S_PAD_DRV_GP2_HIGH		(0x1 << 12)
+#define RT5682S_PAD_DRV_GP2_LOW			(0x0 << 12)
+#define RT5682S_PAD_DRV_GP3_MASK		(0x1 << 10)
+#define RT5682S_PAD_DRV_GP3_HIGH		(0x1 << 10)
+#define RT5682S_PAD_DRV_GP3_LOW			(0x0 << 10)
+#define RT5682S_PAD_DRV_GP4_MASK		(0x1 << 8)
+#define RT5682S_PAD_DRV_GP4_HIGH		(0x1 << 8)
+#define RT5682S_PAD_DRV_GP4_LOW			(0x0 << 8)
+#define RT5682S_PAD_DRV_GP5_MASK		(0x1 << 6)
+#define RT5682S_PAD_DRV_GP5_HIGH		(0x1 << 6)
+#define RT5682S_PAD_DRV_GP5_LOW			(0x0 << 6)
+#define RT5682S_PAD_DRV_GP6_MASK		(0x1 << 4)
+#define RT5682S_PAD_DRV_GP6_HIGH		(0x1 << 4)
+#define RT5682S_PAD_DRV_GP6_LOW			(0x0 << 4)
+
+/* Chopper and Clock control for DAC (0x013a)*/
+#define RT5682S_CKXEN_DAC1_MASK			(0x1 << 13)
+#define RT5682S_CKXEN_DAC1_SFT			13
+#define RT5682S_CKGEN_DAC1_MASK			(0x1 << 12)
+#define RT5682S_CKGEN_DAC1_SFT			12
+
+/* Chopper and Clock control for ADC (0x013b)*/
+#define RT5682S_CKXEN_ADC1_MASK			(0x1 << 13)
+#define RT5682S_CKXEN_ADC1_SFT			13
+#define RT5682S_CKGEN_ADC1_MASK			(0x1 << 12)
+#define RT5682S_CKGEN_ADC1_SFT			12
+
+/* Volume test (0x013f)*/
+#define RT5682S_SEL_CLK_VOL_MASK		(0x1 << 15)
+#define RT5682S_SEL_CLK_VOL_EN			(0x1 << 15)
+#define RT5682S_SEL_CLK_VOL_DIS			(0x0 << 15)
+
+/* Test Mode Control 1 (0x0145) */
+#define RT5682S_AD2DA_LB_MASK			(0x1 << 10)
+#define RT5682S_AD2DA_LB_SFT			10
+
+/* Stereo Noise Gate Control 1 (0x0160) */
+#define RT5682S_NG2_EN_MASK			(0x1 << 15)
+#define RT5682S_NG2_EN				(0x1 << 15)
+#define RT5682S_NG2_DIS				(0x0 << 15)
+
+/* Stereo1 DAC Silence Detection Control (0x0190) */
+#define RT5682S_DEB_STO_DAC_MASK		(0x7 << 4)
+#define RT5682S_DEB_80_MS			(0x0 << 4)
+
+/* HP Behavior Logic Control 2 (0x01db) */
+#define RT5682S_HP_SIG_SRC_MASK			(0x3)
+#define RT5682S_HP_SIG_SRC_1BIT_CTL		(0x3)
+#define RT5682S_HP_SIG_SRC_REG			(0x2)
+#define RT5682S_HP_SIG_SRC_IMPE_REG		(0x1)
+#define RT5682S_HP_SIG_SRC_DC_CALI		(0x0)
+
+/* SAR ADC Inline Command Control 1 (0x0210) */
+#define RT5682S_SAR_BUTDET_MASK			(0x1 << 15)
+#define RT5682S_SAR_BUTDET_EN			(0x1 << 15)
+#define RT5682S_SAR_BUTDET_DIS			(0x0 << 15)
+#define RT5682S_SAR_BUTDET_POW_MASK		(0x1 << 14)
+#define RT5682S_SAR_BUTDET_POW_SAV		(0x1 << 14)
+#define RT5682S_SAR_BUTDET_POW_NORM		(0x0 << 14)
+#define RT5682S_SAR_BUTDET_RST_MASK		(0x1 << 13)
+#define RT5682S_SAR_BUTDET_RST_NORM		(0x1 << 13)
+#define RT5682S_SAR_BUTDET_RST			(0x0 << 13)
+#define RT5682S_SAR_POW_MASK			(0x1 << 12)
+#define RT5682S_SAR_POW_EN			(0x1 << 12)
+#define RT5682S_SAR_POW_DIS			(0x0 << 12)
+#define RT5682S_SAR_RST_MASK			(0x1 << 11)
+#define RT5682S_SAR_RST_NORMAL			(0x1 << 11)
+#define RT5682S_SAR_RST				(0x0 << 11)
+#define RT5682S_SAR_BYPASS_MASK			(0x1 << 10)
+#define RT5682S_SAR_BYPASS_EN			(0x1 << 10)
+#define RT5682S_SAR_BYPASS_DIS			(0x0 << 10)
+#define RT5682S_SAR_SEL_MB1_2_MASK		(0x3 << 8)
+#define RT5682S_SAR_SEL_MB1_2_SFT		8
+#define RT5682S_SAR_SEL_MODE_MASK		(0x1 << 7)
+#define RT5682S_SAR_SEL_MODE_CMP		(0x1 << 7)
+#define RT5682S_SAR_SEL_MODE_ADC		(0x0 << 7)
+#define RT5682S_SAR_SEL_MB1_2_CTL_MASK		(0x1 << 5)
+#define RT5682S_SAR_SEL_MB1_2_AUTO		(0x1 << 5)
+#define RT5682S_SAR_SEL_MB1_2_MANU		(0x0 << 5)
+#define RT5682S_SAR_SEL_SIGNAL_MASK		(0x1 << 4)
+#define RT5682S_SAR_SEL_SIGNAL_AUTO		(0x1 << 4)
+#define RT5682S_SAR_SEL_SIGNAL_MANU		(0x0 << 4)
+
+/* SAR ADC Inline Command Control 2 (0x0211) */
+#define RT5682S_SAR_ADC_PSV_MASK		(0x1 << 4)
+#define RT5682S_SAR_ADC_PSV_ENTRY		(0x1 << 4)
+
+
+/* SAR ADC Inline Command Control 13 (0x021c) */
+#define RT5682S_SAR_SOUR_MASK			(0x3f)
+#define RT5682S_SAR_SOUR_BTN			(0x3f)
+#define RT5682S_SAR_SOUR_TYPE			(0x0)
+
+
+#define RT5682S_STEREO_RATES SNDRV_PCM_RATE_8000_192000
+#define RT5682S_FORMATS (SNDRV_PCM_FMTBIT_S16_LE | SNDRV_PCM_FMTBIT_S20_3LE | \
+		SNDRV_PCM_FMTBIT_S24_LE | SNDRV_PCM_FMTBIT_S8)
+
+/* System Clock Source */
+enum {
+	RT5682S_SCLK_S_MCLK,
+	RT5682S_SCLK_S_PLL1,
+	RT5682S_SCLK_S_PLL2,
+	RT5682S_SCLK_S_RCCLK,
+};
+
+/* PLL Source */
+enum {
+	RT5682S_PLL_S_MCLK,
+	RT5682S_PLL_S_BCLK1,
+	RT5682S_PLL_S_BCLK2,
+	RT5682S_PLL_S_RCCLK,
+};
+
+enum {
+	RT5682S_PLL1,
+	RT5682S_PLL2,
+	RT5682S_PLLS,
+};
+
+enum {
+	RT5682S_AIF1,
+	RT5682S_AIF2,
+	RT5682S_AIFS
+};
+
+/* filter mask */
+enum {
+	RT5682S_DA_STEREO1_FILTER = 0x1,
+	RT5682S_AD_STEREO1_FILTER = (0x1 << 1),
+};
+
+enum {
+	RT5682S_CLK_SEL_SYS,
+	RT5682S_CLK_SEL_I2S1_ASRC,
+	RT5682S_CLK_SEL_I2S2_ASRC,
+};
+
+enum {
+	USE_PLLA,
+	USE_PLLB,
+	USE_PLLAB,
+};
+
+struct pll_calc_map {
+	unsigned int freq_in;
+	unsigned int freq_out;
+	int m;
+	int n;
+	int k;
+	bool m_bp;
+	bool k_bp;
+	bool byp_ps;
+	bool sel_ps;
+};
+
+#define RT5682S_NUM_SUPPLIES 2
+
+struct rt5682s_priv {
+	struct snd_soc_component *component;
+	struct rt5682s_platform_data pdata;
+	struct regmap *regmap;
+	struct snd_soc_jack *hs_jack;
+	struct regulator_bulk_data supplies[RT5682S_NUM_SUPPLIES];
+	struct delayed_work jack_detect_work;
+	struct delayed_work jd_check_work;
+	struct mutex calibrate_mutex;
+	struct mutex sar_mutex;
+
+#ifdef CONFIG_COMMON_CLK
+	struct clk_hw dai_clks_hw[RT5682S_DAI_NUM_CLKS];
+	struct clk *mclk;
+#endif
+
+	int sysclk;
+	int sysclk_src;
+	int lrck[RT5682S_AIFS];
+	int bclk[RT5682S_AIFS];
+	int master[RT5682S_AIFS];
+
+	int pll_src[RT5682S_PLLS];
+	int pll_in[RT5682S_PLLS];
+	int pll_out[RT5682S_PLLS];
+	int pll_comb;
+
+	int jack_type;
+	int irq_work_delay_time;
+};
+
+int rt5682s_sel_asrc_clk_src(struct snd_soc_component *component,
+		unsigned int filter_mask, unsigned int clk_src);
+
+#endif /* __RT5682S_H__ */
diff -ruN a/sound/soc/intel/boards/bdw-rt5650.c b/sound/soc/intel/boards/bdw-rt5650.c
--- a/sound/soc/intel/boards/bdw-rt5650.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bdw-rt5650.c	2021-12-23 08:36:03.000000000 +0100
@@ -251,7 +251,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_DSP_B | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = broadwell_ssp0_fixup,
 		.ops = &bdw_rt5650_ops,
diff -ruN a/sound/soc/intel/boards/bdw-rt5677.c b/sound/soc/intel/boards/bdw-rt5677.c
--- a/sound/soc/intel/boards/bdw-rt5677.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bdw-rt5677.c	2021-12-23 08:36:03.000000000 +0100
@@ -351,7 +351,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = broadwell_ssp0_fixup,
 		.ops = &bdw_rt5677_ops,
diff -ruN a/sound/soc/intel/boards/broadwell.c b/sound/soc/intel/boards/broadwell.c
--- a/sound/soc/intel/boards/broadwell.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/broadwell.c	2021-12-23 08:36:03.000000000 +0100
@@ -217,7 +217,7 @@
 		.no_pcm = 1,
 		.init = broadwell_rt286_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = broadwell_ssp0_fixup,
 		.ops = &broadwell_rt286_ops,
diff -ruN a/sound/soc/intel/boards/bxt_da7219_max98357a.c b/sound/soc/intel/boards/bxt_da7219_max98357a.c
--- a/sound/soc/intel/boards/bxt_da7219_max98357a.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bxt_da7219_max98357a.c	2021-12-23 08:36:03.000000000 +0100
@@ -572,7 +572,7 @@
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S |
 			SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = broxton_ssp_fixup,
 		.dpcm_playback = 1,
@@ -585,7 +585,7 @@
 		.no_pcm = 1,
 		.init = broxton_da7219_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = broxton_ssp_fixup,
 		.dpcm_playback = 1,
diff -ruN a/sound/soc/intel/boards/bxt_rt298.c b/sound/soc/intel/boards/bxt_rt298.c
--- a/sound/soc/intel/boards/bxt_rt298.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bxt_rt298.c	2021-12-23 08:36:03.000000000 +0100
@@ -468,7 +468,7 @@
 		.no_pcm = 1,
 		.init = broxton_rt298_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_DSP_A | SND_SOC_DAIFMT_NB_NF |
-						SND_SOC_DAIFMT_CBS_CFS,
+						SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = broxton_ssp5_fixup,
 		.ops = &broxton_rt298_ops,
diff -ruN a/sound/soc/intel/boards/bytcht_cx2072x.c b/sound/soc/intel/boards/bytcht_cx2072x.c
--- a/sound/soc/intel/boards/bytcht_cx2072x.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bytcht_cx2072x.c	2021-12-23 08:36:03.000000000 +0100
@@ -126,7 +126,7 @@
 	ret = snd_soc_dai_set_fmt(asoc_rtd_to_cpu(rtd, 0),
 				SND_SOC_DAIFMT_I2S     |
 				SND_SOC_DAIFMT_NB_NF   |
-				SND_SOC_DAIFMT_CBS_CFS);
+				SND_SOC_DAIFMT_CBC_CFC);
 	if (ret < 0) {
 		dev_err(rtd->dev, "can't set format to I2S, err %d\n", ret);
 		return ret;
@@ -195,7 +195,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF
-					      | SND_SOC_DAIFMT_CBS_CFS,
+					      | SND_SOC_DAIFMT_CBC_CFC,
 		.init = byt_cht_cx2072x_init,
 		.be_hw_params_fixup = byt_cht_cx2072x_fixup,
 		.dpcm_playback = 1,
diff -ruN a/sound/soc/intel/boards/bytcht_da7213.c b/sound/soc/intel/boards/bytcht_da7213.c
--- a/sound/soc/intel/boards/bytcht_da7213.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bytcht_da7213.c	2021-12-23 08:36:03.000000000 +0100
@@ -81,7 +81,7 @@
 	ret = snd_soc_dai_set_fmt(asoc_rtd_to_cpu(rtd, 0),
 				  SND_SOC_DAIFMT_I2S     |
 				  SND_SOC_DAIFMT_NB_NF   |
-				  SND_SOC_DAIFMT_CBS_CFS);
+				  SND_SOC_DAIFMT_CBC_CFC);
 	if (ret < 0) {
 		dev_err(rtd->dev, "can't set format to I2S, err %d\n", ret);
 		return ret;
@@ -195,7 +195,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF
-						| SND_SOC_DAIFMT_CBS_CFS,
+						| SND_SOC_DAIFMT_CBC_CFC,
 		.be_hw_params_fixup = codec_fixup,
 		.dpcm_playback = 1,
 		.dpcm_capture = 1,
diff -ruN a/sound/soc/intel/boards/bytcht_es8316.c b/sound/soc/intel/boards/bytcht_es8316.c
--- a/sound/soc/intel/boards/bytcht_es8316.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bytcht_es8316.c	2021-12-23 08:36:03.000000000 +0100
@@ -265,7 +265,7 @@
 	ret = snd_soc_dai_set_fmt(asoc_rtd_to_cpu(rtd, 0),
 				SND_SOC_DAIFMT_I2S     |
 				SND_SOC_DAIFMT_NB_NF   |
-				SND_SOC_DAIFMT_CBS_CFS
+				SND_SOC_DAIFMT_CBC_CFC
 		);
 	if (ret < 0) {
 		dev_err(rtd->dev, "can't set format to I2S, err %d\n", ret);
@@ -336,7 +336,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF
-						| SND_SOC_DAIFMT_CBS_CFS,
+						| SND_SOC_DAIFMT_CBC_CFC,
 		.be_hw_params_fixup = byt_cht_es8316_codec_fixup,
 		.dpcm_playback = 1,
 		.dpcm_capture = 1,
diff -ruN a/sound/soc/intel/boards/bytcht_nocodec.c b/sound/soc/intel/boards/bytcht_nocodec.c
--- a/sound/soc/intel/boards/bytcht_nocodec.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bytcht_nocodec.c	2021-12-23 08:36:03.000000000 +0100
@@ -61,7 +61,7 @@
 	ret = snd_soc_dai_set_fmt(asoc_rtd_to_cpu(rtd, 0),
 				  SND_SOC_DAIFMT_I2S     |
 				  SND_SOC_DAIFMT_NB_NF   |
-				  SND_SOC_DAIFMT_CBS_CFS);
+				  SND_SOC_DAIFMT_CBC_CFC);
 
 	if (ret < 0) {
 		dev_err(rtd->dev, "can't set format to I2S, err %d\n", ret);
@@ -141,7 +141,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF
-						| SND_SOC_DAIFMT_CBS_CFS,
+						| SND_SOC_DAIFMT_CBC_CFC,
 		.be_hw_params_fixup = codec_fixup,
 		.ignore_suspend = 1,
 		.dpcm_playback = 1,
diff -ruN a/sound/soc/intel/boards/bytcr_rt5640.c b/sound/soc/intel/boards/bytcr_rt5640.c
--- a/sound/soc/intel/boards/bytcr_rt5640.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bytcr_rt5640.c	2021-12-23 08:36:03.000000000 +0100
@@ -1336,7 +1336,7 @@
 	ret = snd_soc_dai_set_fmt(asoc_rtd_to_cpu(rtd, 0),
 				  SND_SOC_DAIFMT_I2S     |
 				  SND_SOC_DAIFMT_NB_NF   |
-				  SND_SOC_DAIFMT_CBS_CFS);
+				  SND_SOC_DAIFMT_CBC_CFC);
 	if (ret < 0) {
 		dev_err(rtd->dev, "can't set format to I2S, err %d\n", ret);
 		return ret;
@@ -1411,7 +1411,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF
-						| SND_SOC_DAIFMT_CBS_CFS,
+						| SND_SOC_DAIFMT_CBC_CFC,
 		.be_hw_params_fixup = byt_rt5640_codec_fixup,
 		.dpcm_playback = 1,
 		.dpcm_capture = 1,
diff -ruN a/sound/soc/intel/boards/bytcr_rt5651.c b/sound/soc/intel/boards/bytcr_rt5651.c
--- a/sound/soc/intel/boards/bytcr_rt5651.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bytcr_rt5651.c	2021-12-23 08:36:03.000000000 +0100
@@ -713,7 +713,7 @@
 	ret = snd_soc_dai_set_fmt(asoc_rtd_to_cpu(rtd, 0),
 				  SND_SOC_DAIFMT_I2S     |
 				  SND_SOC_DAIFMT_NB_NF   |
-				  SND_SOC_DAIFMT_CBS_CFS
+				  SND_SOC_DAIFMT_CBC_CFC
 				  );
 
 	if (ret < 0) {
@@ -798,7 +798,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF
-						| SND_SOC_DAIFMT_CBS_CFS,
+						| SND_SOC_DAIFMT_CBC_CFC,
 		.be_hw_params_fixup = byt_rt5651_codec_fixup,
 		.dpcm_playback = 1,
 		.dpcm_capture = 1,
diff -ruN a/sound/soc/intel/boards/bytcr_wm5102.c b/sound/soc/intel/boards/bytcr_wm5102.c
--- a/sound/soc/intel/boards/bytcr_wm5102.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/bytcr_wm5102.c	2021-12-23 08:36:03.000000000 +0100
@@ -265,7 +265,7 @@
 	ret = snd_soc_dai_set_fmt(asoc_rtd_to_cpu(rtd, 0),
 				  SND_SOC_DAIFMT_I2S     |
 				  SND_SOC_DAIFMT_NB_NF   |
-				  SND_SOC_DAIFMT_CBS_CFS);
+				  SND_SOC_DAIFMT_CBC_CFC);
 	if (ret) {
 		dev_err(rtd->dev, "Error setting format to I2S: %d\n", ret);
 		return ret;
@@ -349,7 +349,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF
-						| SND_SOC_DAIFMT_CBS_CFS,
+						| SND_SOC_DAIFMT_CBC_CFC,
 		.be_hw_params_fixup = byt_wm5102_codec_fixup,
 		.dpcm_playback = 1,
 		.dpcm_capture = 1,
diff -ruN a/sound/soc/intel/boards/cht_bsw_max98090_ti.c b/sound/soc/intel/boards/cht_bsw_max98090_ti.c
--- a/sound/soc/intel/boards/cht_bsw_max98090_ti.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/cht_bsw_max98090_ti.c	2021-12-23 08:36:03.000000000 +0100
@@ -264,7 +264,7 @@
 	}
 
 	fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF
-				| SND_SOC_DAIFMT_CBS_CFS;
+				| SND_SOC_DAIFMT_CBC_CFC;
 
 	ret = snd_soc_dai_set_fmt(asoc_rtd_to_cpu(rtd, 0), fmt);
 	if (ret < 0) {
@@ -372,7 +372,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF
-					| SND_SOC_DAIFMT_CBS_CFS,
+					| SND_SOC_DAIFMT_CBC_CFC,
 		.init = cht_codec_init,
 		.be_hw_params_fixup = cht_codec_fixup,
 		.dpcm_playback = 1,
diff -ruN a/sound/soc/intel/boards/cht_bsw_nau8824.c b/sound/soc/intel/boards/cht_bsw_nau8824.c
--- a/sound/soc/intel/boards/cht_bsw_nau8824.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/cht_bsw_nau8824.c	2021-12-23 08:36:03.000000000 +0100
@@ -214,7 +214,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_DSP_B | SND_SOC_DAIFMT_IB_NF
-			| SND_SOC_DAIFMT_CBS_CFS,
+			| SND_SOC_DAIFMT_CBC_CFC,
 		.init = cht_codec_init,
 		.be_hw_params_fixup = cht_codec_fixup,
 		.dpcm_playback = 1,
diff -ruN a/sound/soc/intel/boards/cht_bsw_rt5645.c b/sound/soc/intel/boards/cht_bsw_rt5645.c
--- a/sound/soc/intel/boards/cht_bsw_rt5645.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/cht_bsw_rt5645.c	2021-12-23 08:36:03.000000000 +0100
@@ -362,7 +362,7 @@
 		ret = snd_soc_dai_set_fmt(asoc_rtd_to_cpu(rtd, 0),
 					SND_SOC_DAIFMT_I2S     |
 					SND_SOC_DAIFMT_NB_NF   |
-					SND_SOC_DAIFMT_CBS_CFS
+					SND_SOC_DAIFMT_CBC_CFC
 			);
 		if (ret < 0) {
 			dev_err(rtd->dev, "can't set format to I2S, err %d\n", ret);
@@ -372,7 +372,7 @@
 		ret = snd_soc_dai_set_fmt(asoc_rtd_to_codec(rtd, 0),
 					SND_SOC_DAIFMT_I2S     |
 					SND_SOC_DAIFMT_NB_NF   |
-					SND_SOC_DAIFMT_CBS_CFS
+					SND_SOC_DAIFMT_CBC_CFC
 			);
 		if (ret < 0) {
 			dev_err(rtd->dev, "can't set format to I2S, err %d\n", ret);
@@ -396,7 +396,7 @@
 		ret = snd_soc_dai_set_fmt(asoc_rtd_to_codec(rtd, 0),
 					SND_SOC_DAIFMT_DSP_B |
 					SND_SOC_DAIFMT_IB_NF |
-					SND_SOC_DAIFMT_CBS_CFS);
+					SND_SOC_DAIFMT_CBC_CFC);
 		if (ret < 0) {
 			dev_err(rtd->dev, "can't set format to TDM %d\n", ret);
 			return ret;
diff -ruN a/sound/soc/intel/boards/cht_bsw_rt5672.c b/sound/soc/intel/boards/cht_bsw_rt5672.c
--- a/sound/soc/intel/boards/cht_bsw_rt5672.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/cht_bsw_rt5672.c	2021-12-23 08:36:03.000000000 +0100
@@ -300,7 +300,7 @@
 	ret = snd_soc_dai_set_fmt(asoc_rtd_to_cpu(rtd, 0),
 				  SND_SOC_DAIFMT_I2S     |
 				  SND_SOC_DAIFMT_NB_NF   |
-				  SND_SOC_DAIFMT_CBS_CFS);
+				  SND_SOC_DAIFMT_CBC_CFC);
 	if (ret < 0) {
 		dev_err(rtd->dev, "can't set format to I2S, err %d\n", ret);
 		return ret;
diff -ruN a/sound/soc/intel/boards/glk_rt5682_max98357a.c b/sound/soc/intel/boards/glk_rt5682_max98357a.c
--- a/sound/soc/intel/boards/glk_rt5682_max98357a.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/glk_rt5682_max98357a.c	2021-12-23 08:36:03.000000000 +0100
@@ -18,14 +18,18 @@
 #include <sound/soc.h>
 #include <sound/soc-acpi.h>
 #include "../../codecs/rt5682.h"
+#include "../../codecs/rt5682s.h"
 #include "../../codecs/hdac_hdmi.h"
 #include "hda_dsp_common.h"
 
 /* The platform clock outputs 19.2Mhz clock to codec as I2S MCLK */
 #define GLK_PLAT_CLK_FREQ 19200000
 #define RT5682_PLL_FREQ (48000 * 512)
-#define GLK_REALTEK_CODEC_DAI "rt5682-aif1"
+#define RT5682_DAI_NAME "rt5682-aif1"
+#define RT5682S_DAI_NAME "rt5682s-aif1"
 #define GLK_MAXIM_CODEC_DAI "HiFi"
+#define RT5682_DEV0_NAME "i2c-10EC5682:00"
+#define RT5682S_DEV0_NAME "i2c-RTL5682:00"
 #define MAXIM_DEV0_NAME "MX98357A:00"
 #define DUAL_CHANNEL 2
 #define QUAD_CHANNEL 4
@@ -43,6 +47,7 @@
 	struct snd_soc_jack geminilake_headset;
 	struct list_head hdmi_pcm_list;
 	bool common_hdmi_codec_drv;
+	int is_rt5682s;
 };
 
 enum {
@@ -139,9 +144,19 @@
 	struct snd_soc_component *component = asoc_rtd_to_codec(rtd, 0)->component;
 	struct snd_soc_dai *codec_dai = asoc_rtd_to_codec(rtd, 0);
 	struct snd_soc_jack *jack;
-	int ret;
+	int pll_id, pll_source, clk_id, ret;
+
+	if (ctx->is_rt5682s) {
+		pll_id = RT5682S_PLL2;
+		pll_source = RT5682S_PLL_S_MCLK;
+		clk_id = RT5682S_SCLK_S_PLL2;
+	} else {
+		pll_id = RT5682_PLL1;
+		pll_source = RT5682_PLL1_S_MCLK;
+		clk_id = RT5682_SCLK_S_PLL1;
+	}
 
-	ret = snd_soc_dai_set_pll(codec_dai, 0, RT5682_PLL1_S_MCLK,
+	ret = snd_soc_dai_set_pll(codec_dai, pll_id, pll_source,
 					GLK_PLAT_CLK_FREQ, RT5682_PLL_FREQ);
 	if (ret < 0) {
 		dev_err(rtd->dev, "can't set codec pll: %d\n", ret);
@@ -149,7 +164,7 @@
 	}
 
 	/* Configure sysclk for codec */
-	ret = snd_soc_dai_set_sysclk(codec_dai, RT5682_SCLK_S_PLL1,
+	ret = snd_soc_dai_set_sysclk(codec_dai, clk_id,
 					RT5682_PLL_FREQ, SND_SOC_CLOCK_IN);
 	if (ret < 0)
 		dev_err(rtd->dev, "snd_soc_dai_set_sysclk err = %d\n", ret);
@@ -344,9 +359,12 @@
 
 SND_SOC_DAILINK_DEF(ssp2_pin,
 	DAILINK_COMP_ARRAY(COMP_CPU("SSP2 Pin")));
-SND_SOC_DAILINK_DEF(ssp2_codec,
-	DAILINK_COMP_ARRAY(COMP_CODEC("i2c-10EC5682:00",
-				      GLK_REALTEK_CODEC_DAI)));
+SND_SOC_DAILINK_DEF(ssp2_codec_5682,
+	DAILINK_COMP_ARRAY(COMP_CODEC(RT5682_DEV0_NAME,
+				      RT5682_DAI_NAME)));
+SND_SOC_DAILINK_DEF(ssp2_codec_5682s,
+	DAILINK_COMP_ARRAY(COMP_CODEC(RT5682S_DEV0_NAME,
+				      RT5682S_DAI_NAME)));
 
 SND_SOC_DAILINK_DEF(dmic_pin,
 	DAILINK_COMP_ARRAY(COMP_CPU("DMIC01 Pin")));
@@ -473,7 +491,7 @@
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S |
 			SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = geminilake_ssp_fixup,
 		.dpcm_playback = 1,
@@ -486,13 +504,13 @@
 		.no_pcm = 1,
 		.init = geminilake_rt5682_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = geminilake_ssp_fixup,
 		.ops = &geminilake_rt5682_ops,
 		.dpcm_playback = 1,
 		.dpcm_capture = 1,
-		SND_SOC_DAILINK_REG(ssp2_pin, ssp2_codec, platform),
+		SND_SOC_DAILINK_REG(ssp2_pin, ssp2_codec_5682, platform),
 	},
 	{
 		.name = "dmic01",
@@ -592,12 +610,28 @@
 	struct snd_soc_acpi_mach *mach;
 	const char *platform_name;
 	struct snd_soc_card *card;
-	int ret;
+	int ret, i;
 
 	ctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL);
 	if (!ctx)
 		return -ENOMEM;
 
+	/* Detect the headset codec variant */
+	if (acpi_dev_present("RTL5682", NULL, -1)) {
+		/* ALC5682I-VS is detected */
+		ctx->is_rt5682s = 1;
+
+		for (i = 0; i < glk_audio_card_rt5682_m98357a.num_links; i++) {
+			if (strcmp(geminilake_dais[i].name, "SSP2-Codec"))
+				continue;
+
+			/* update the dai link to use rt5682s codec */
+			geminilake_dais[i].codecs = ssp2_codec_5682s;
+			geminilake_dais[i].num_codecs = ARRAY_SIZE(ssp2_codec_5682s);
+			break;
+		}
+	}
+
 	INIT_LIST_HEAD(&ctx->hdmi_pcm_list);
 
 	card = &glk_audio_card_rt5682_m98357a;
diff -ruN a/sound/soc/intel/boards/haswell.c b/sound/soc/intel/boards/haswell.c
--- a/sound/soc/intel/boards/haswell.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/haswell.c	2021-12-23 08:36:03.000000000 +0100
@@ -145,7 +145,7 @@
 		.id = 0,
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = haswell_ssp0_fixup,
 		.ops = &haswell_rt5640_ops,
diff -ruN a/sound/soc/intel/boards/kbl_da7219_max98357a.c b/sound/soc/intel/boards/kbl_da7219_max98357a.c
--- a/sound/soc/intel/boards/kbl_da7219_max98357a.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/kbl_da7219_max98357a.c	2021-12-23 08:36:03.000000000 +0100
@@ -518,7 +518,7 @@
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S |
 			SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = kabylake_ssp_fixup,
 		.dpcm_playback = 1,
@@ -531,7 +531,7 @@
 		.no_pcm = 1,
 		.init = kabylake_da7219_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = kabylake_ssp_fixup,
 		.dpcm_playback = 1,
diff -ruN a/sound/soc/intel/boards/kbl_da7219_max98927.c b/sound/soc/intel/boards/kbl_da7219_max98927.c
--- a/sound/soc/intel/boards/kbl_da7219_max98927.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/kbl_da7219_max98927.c	2021-12-23 08:36:03.000000000 +0100
@@ -764,7 +764,7 @@
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_DSP_B |
 			SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.dpcm_playback = 1,
 		.dpcm_capture = 1,
 		.ignore_pmdown_time = 1,
@@ -779,7 +779,7 @@
 		.no_pcm = 1,
 		.init = kabylake_da7219_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = kabylake_ssp_fixup,
 		.dpcm_playback = 1,
@@ -907,7 +907,7 @@
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_DSP_B |
 			SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.dpcm_playback = 1,
 		.dpcm_capture = 1,
 		.ignore_pmdown_time = 1,
diff -ruN a/sound/soc/intel/boards/kbl_rt5660.c b/sound/soc/intel/boards/kbl_rt5660.c
--- a/sound/soc/intel/boards/kbl_rt5660.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/kbl_rt5660.c	2021-12-23 08:36:03.000000000 +0100
@@ -436,7 +436,7 @@
 		.exit = kabylake_rt5660_codec_exit,
 		.dai_fmt = SND_SOC_DAIFMT_I2S |
 		SND_SOC_DAIFMT_NB_NF |
-		SND_SOC_DAIFMT_CBS_CFS,
+		SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = kabylake_ssp0_fixup,
 		.ops = &kabylake_rt5660_ops,
diff -ruN a/sound/soc/intel/boards/kbl_rt5663_max98927.c b/sound/soc/intel/boards/kbl_rt5663_max98927.c
--- a/sound/soc/intel/boards/kbl_rt5663_max98927.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/kbl_rt5663_max98927.c	2021-12-23 08:36:03.000000000 +0100
@@ -767,7 +767,7 @@
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_DSP_B |
 			SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = kabylake_ssp_fixup,
 		.dpcm_playback = 1,
@@ -781,7 +781,7 @@
 		.no_pcm = 1,
 		.init = kabylake_rt5663_max98927_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = kabylake_ssp_fixup,
 		.ops = &kabylake_rt5663_ops,
@@ -879,7 +879,7 @@
 		.no_pcm = 1,
 		.init = kabylake_rt5663_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = kabylake_ssp_fixup,
 		.ops = &kabylake_rt5663_ops,
diff -ruN a/sound/soc/intel/boards/kbl_rt5663_rt5514_max98927.c b/sound/soc/intel/boards/kbl_rt5663_rt5514_max98927.c
--- a/sound/soc/intel/boards/kbl_rt5663_rt5514_max98927.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/kbl_rt5663_rt5514_max98927.c	2021-12-23 08:36:03.000000000 +0100
@@ -53,8 +53,10 @@
 	struct snd_soc_jack kabylake_headset;
 	struct list_head hdmi_pcm_list;
 	struct snd_soc_jack kabylake_hdmi[2];
-	struct clk *mclk;
-	struct clk *sclk;
+	struct clk *ssp0_mclk;
+	struct clk *ssp0_sclk;
+	struct clk *ssp1_mclk;
+	struct clk *ssp1_sclk;
 };
 
 enum {
@@ -77,13 +79,31 @@
 };
 
 static int platform_clock_control(struct snd_soc_dapm_widget *w,
-			struct snd_kcontrol *k, int  event)
+			struct snd_kcontrol *k, int event, int ssp_num)
 {
 	struct snd_soc_dapm_context *dapm = w->dapm;
 	struct snd_soc_card *card = dapm->card;
 	struct kbl_codec_private *priv = snd_soc_card_get_drvdata(card);
+	struct clk *mclk, *sclk;
+	unsigned long sclk_rate;
 	int ret = 0;
 
+	switch (ssp_num) {
+	case 0:
+		mclk = priv->ssp0_mclk;
+		sclk = priv->ssp0_sclk;
+		sclk_rate = 6144000;
+		break;
+	case 1:
+		mclk = priv->ssp1_mclk;
+		sclk = priv->ssp1_sclk;
+		sclk_rate = 3072000;
+		break;
+	default:
+		dev_err(card->dev, "Invalid ssp_num %d\n", ssp_num);
+		return -EINVAL;
+	}
+
 	/*
 	 * MCLK/SCLK need to be ON early for a successful synchronization of
 	 * codec internal clock. And the clocks are turned off during
@@ -91,38 +111,46 @@
 	 */
 	switch (event) {
 	case SND_SOC_DAPM_PRE_PMU:
+		if (__clk_is_enabled(mclk))
+			return 0;
+
 		/* Enable MCLK */
-		ret = clk_set_rate(priv->mclk, 24000000);
+		ret = clk_set_rate(mclk, 24000000);
 		if (ret < 0) {
-			dev_err(card->dev, "Can't set rate for mclk, err: %d\n",
-				ret);
+			dev_err(card->dev, "Can't set rate for ssp%d_mclk, err: %d\n",
+				ssp_num, ret);
 			return ret;
 		}
 
-		ret = clk_prepare_enable(priv->mclk);
+		ret = clk_prepare_enable(mclk);
 		if (ret < 0) {
-			dev_err(card->dev, "Can't enable mclk, err: %d\n", ret);
+			dev_err(card->dev, "Can't enable ssp%d_mclk, err: %d\n",
+				ssp_num, ret);
 			return ret;
 		}
 
 		/* Enable SCLK */
-		ret = clk_set_rate(priv->sclk, 3072000);
+		ret = clk_set_rate(sclk, sclk_rate);
 		if (ret < 0) {
-			dev_err(card->dev, "Can't set rate for sclk, err: %d\n",
-				ret);
-			clk_disable_unprepare(priv->mclk);
+			dev_err(card->dev, "Can't set rate for ssp%d_sclk, err: %d\n",
+				ssp_num, ret);
+			clk_disable_unprepare(mclk);
 			return ret;
 		}
 
-		ret = clk_prepare_enable(priv->sclk);
+		ret = clk_prepare_enable(sclk);
 		if (ret < 0) {
-			dev_err(card->dev, "Can't enable sclk, err: %d\n", ret);
-			clk_disable_unprepare(priv->mclk);
+			dev_err(card->dev, "Can't enable ssp%d_sclk, err: %d\n",
+				ssp_num, ret);
+			clk_disable_unprepare(mclk);
 		}
 		break;
 	case SND_SOC_DAPM_POST_PMD:
-		clk_disable_unprepare(priv->mclk);
-		clk_disable_unprepare(priv->sclk);
+		if (!__clk_is_enabled(mclk))
+			return 0;
+
+		clk_disable_unprepare(mclk);
+		clk_disable_unprepare(sclk);
 		break;
 	default:
 		return 0;
@@ -131,6 +159,18 @@
 	return 0;
 }
 
+static int platform_clock_control_ssp0(struct snd_soc_dapm_widget *w,
+			struct snd_kcontrol *k, int event)
+{
+	return platform_clock_control(w, k, event, 0);
+}
+
+static int platform_clock_control_ssp1(struct snd_soc_dapm_widget *w,
+			struct snd_kcontrol *k, int event)
+{
+	return platform_clock_control(w, k, event, 1);
+}
+
 static const struct snd_soc_dapm_widget kabylake_widgets[] = {
 	SND_SOC_DAPM_HP("Headphone Jack", NULL),
 	SND_SOC_DAPM_MIC("Headset Mic", NULL),
@@ -139,15 +179,17 @@
 	SND_SOC_DAPM_MIC("DMIC", NULL),
 	SND_SOC_DAPM_SPK("HDMI1", NULL),
 	SND_SOC_DAPM_SPK("HDMI2", NULL),
-	SND_SOC_DAPM_SUPPLY("Platform Clock", SND_SOC_NOPM, 0, 0,
-			platform_clock_control, SND_SOC_DAPM_PRE_PMU |
+	SND_SOC_DAPM_SUPPLY("Platform Clock SSP0", SND_SOC_NOPM, 0, 0,
+			platform_clock_control_ssp0, SND_SOC_DAPM_PRE_PMU |
+			SND_SOC_DAPM_POST_PMD),
+	SND_SOC_DAPM_SUPPLY("Platform Clock SSP1", SND_SOC_NOPM, 0, 0,
+			platform_clock_control_ssp1, SND_SOC_DAPM_PRE_PMU |
 			SND_SOC_DAPM_POST_PMD),
-
 };
 
 static const struct snd_soc_dapm_route kabylake_map[] = {
 	/* Headphones */
-	{ "Headphone Jack", NULL, "Platform Clock" },
+	{ "Headphone Jack", NULL, "Platform Clock SSP1" },
 	{ "Headphone Jack", NULL, "HPOL" },
 	{ "Headphone Jack", NULL, "HPOR" },
 
@@ -156,7 +198,7 @@
 	{ "Right Spk", NULL, "Right BE_OUT" },
 
 	/* other jacks */
-	{ "Headset Mic", NULL, "Platform Clock" },
+	{ "Headset Mic", NULL, "Platform Clock SSP1" },
 	{ "IN1P", NULL, "Headset Mic" },
 	{ "IN1N", NULL, "Headset Mic" },
 
@@ -180,6 +222,7 @@
 	{ "ssp0 Rx", NULL, "Right HiFi Capture" },
 
 	/* DMIC */
+	{ "DMIC", NULL, "Platform Clock SSP0" },
 	{ "DMIC1L", NULL, "DMIC" },
 	{ "DMIC1R", NULL, "DMIC" },
 	{ "DMIC2L", NULL, "DMIC" },
@@ -639,7 +682,7 @@
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_DSP_B |
 			SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = kabylake_ssp_fixup,
 		.dpcm_playback = 1,
@@ -653,7 +696,7 @@
 		.no_pcm = 1,
 		.init = kabylake_rt5663_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = kabylake_ssp_fixup,
 		.ops = &kabylake_rt5663_ops,
@@ -689,7 +732,7 @@
 	if (!component || strcmp(component->name, RT5514_DEV_NAME))
 		return 0;
 
-	if (IS_ERR(priv->mclk))
+	if (IS_ERR(priv->ssp0_mclk))
 		return 0;
 
 	/*
@@ -700,20 +743,20 @@
 	switch (level) {
 	case SND_SOC_BIAS_PREPARE:
 		if (dapm->bias_level == SND_SOC_BIAS_ON) {
-			if (!__clk_is_enabled(priv->mclk))
+			if (!__clk_is_enabled(priv->ssp0_mclk))
 				return 0;
 			dev_dbg(card->dev, "Disable mclk");
-			clk_disable_unprepare(priv->mclk);
+			clk_disable_unprepare(priv->ssp0_mclk);
 		} else {
 			dev_dbg(card->dev, "Enable mclk");
-			ret = clk_set_rate(priv->mclk, 24000000);
+			ret = clk_set_rate(priv->ssp0_mclk, 24000000);
 			if (ret) {
 				dev_err(card->dev, "Can't set rate for mclk, err: %d\n",
 					ret);
 				return ret;
 			}
 
-			ret = clk_prepare_enable(priv->mclk);
+			ret = clk_prepare_enable(priv->ssp0_mclk);
 			if (ret) {
 				dev_err(card->dev, "Can't enable mclk, err: %d\n",
 					ret);
@@ -782,6 +825,29 @@
 	.late_probe = kabylake_card_late_probe,
 };
 
+static int kabylake_audio_clk_get(struct device *dev, const char *id,
+	struct clk **clk)
+{
+	int ret = 0;
+
+	if (!clk)
+		return -EINVAL;
+
+	*clk = devm_clk_get(dev, id);
+	if (IS_ERR(*clk)) {
+		ret = PTR_ERR(*clk);
+		if (ret == -ENOENT) {
+			dev_info(dev, "Failed to get %s, defer probe\n", id);
+			return -EPROBE_DEFER;
+		}
+
+		dev_err(dev, "Failed to get %s with err:%d\n", id, ret);
+		return ret;
+	}
+
+	return ret;
+}
+
 static int kabylake_audio_probe(struct platform_device *pdev)
 {
 	struct kbl_codec_private *ctx;
@@ -802,33 +868,21 @@
 		dmic_constraints = mach->mach_params.dmic_num == 2 ?
 			&constraints_dmic_2ch : &constraints_dmic_channels;
 
-	ctx->mclk = devm_clk_get(&pdev->dev, "ssp1_mclk");
-	if (IS_ERR(ctx->mclk)) {
-		ret = PTR_ERR(ctx->mclk);
-		if (ret == -ENOENT) {
-			dev_info(&pdev->dev,
-				"Failed to get ssp1_mclk, defer probe\n");
-			return -EPROBE_DEFER;
-		}
+	ret = kabylake_audio_clk_get(&pdev->dev, "ssp0_mclk", &ctx->ssp0_mclk);
+	if (ret != 0)
+		return ret;
 
-		dev_err(&pdev->dev, "Failed to get ssp1_mclk with err:%d\n",
-								ret);
+	ret = kabylake_audio_clk_get(&pdev->dev, "ssp0_sclk", &ctx->ssp0_sclk);
+	if (ret != 0)
 		return ret;
-	}
 
-	ctx->sclk = devm_clk_get(&pdev->dev, "ssp1_sclk");
-	if (IS_ERR(ctx->sclk)) {
-		ret = PTR_ERR(ctx->sclk);
-		if (ret == -ENOENT) {
-			dev_info(&pdev->dev,
-				"Failed to get ssp1_sclk, defer probe\n");
-			return -EPROBE_DEFER;
-		}
+	ret = kabylake_audio_clk_get(&pdev->dev, "ssp1_mclk", &ctx->ssp1_mclk);
+	if (ret != 0)
+		return ret;
 
-		dev_err(&pdev->dev, "Failed to get ssp1_sclk with err:%d\n",
-								ret);
+	ret = kabylake_audio_clk_get(&pdev->dev, "ssp1_sclk", &ctx->ssp1_sclk);
+	if (ret != 0)
 		return ret;
-	}
 
 	return devm_snd_soc_register_card(&pdev->dev, &kabylake_audio_card);
 }
diff -ruN a/sound/soc/intel/boards/Kconfig b/sound/soc/intel/boards/Kconfig
--- a/sound/soc/intel/boards/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/Kconfig	2021-12-23 08:36:03.000000000 +0100
@@ -371,7 +371,7 @@
 
 config SND_SOC_INTEL_KBL_DA7219_MAX98357A_MACH
 	tristate "KBL with DA7219 and MAX98357A in I2S Mode"
-	depends on I2C && ACPI
+	depends on I2C && ACPI && GPIOLIB
 	depends on MFD_INTEL_LPSS || COMPILE_TEST
 	select SND_SOC_INTEL_DA7219_MAX98357A_GENERIC
 	help
@@ -427,6 +427,7 @@
 	depends on MFD_INTEL_LPSS || COMPILE_TEST
 	depends on SND_HDA_CODEC_HDMI && SND_SOC_SOF_HDA_AUDIO_CODEC
 	select SND_SOC_RT5682_I2C
+	select SND_SOC_RT5682S
 	select SND_SOC_MAX98357A
 	select SND_SOC_DMIC
 	select SND_SOC_HDAC_HDMI
@@ -466,10 +467,12 @@
 		    (MFD_INTEL_LPSS || COMPILE_TEST)) ||\
 		   (SND_SOC_SOF_BAYTRAIL && (X86_INTEL_LPSS || COMPILE_TEST))
 	select SND_SOC_MAX98373_I2C
+	select SND_SOC_MAX98390
 	select SND_SOC_RT1011
 	select SND_SOC_RT1015
 	select SND_SOC_RT1015P
 	select SND_SOC_RT5682_I2C
+	select SND_SOC_RT5682S
 	select SND_SOC_DMIC
 	select SND_SOC_HDAC_HDMI
 	select SND_SOC_INTEL_HDA_DSP_COMMON
@@ -511,6 +514,39 @@
 	  Say Y or m if you have such a device.
 	  If unsure select "N".
 
+config SND_SOC_INTEL_SOF_ES8336_MACH
+	tristate "SOF with ES8336 codec in I2S mode"
+	depends on I2C && ACPI && GPIOLIB
+	depends on MFD_INTEL_LPSS || COMPILE_TEST
+	depends on SND_HDA_CODEC_HDMI && SND_SOC_SOF_HDA_AUDIO_CODEC
+	select SND_SOC_ES8316
+	select SND_SOC_DMIC
+	select SND_SOC_INTEL_HDA_DSP_COMMON
+	help
+	   This adds support for ASoC machine driver for SOF platforms
+	   with es8336 codec.
+	   Say Y if you have such a device.
+	   If unsure select "N".
+
+config SND_SOC_INTEL_SOF_NAU8825_MACH
+	tristate "SOF with nau8825 codec in I2S Mode"
+	depends on I2C && ACPI && GPIOLIB
+	depends on ((SND_HDA_CODEC_HDMI && SND_SOC_SOF_HDA_AUDIO_CODEC) &&\
+		    (MFD_INTEL_LPSS || COMPILE_TEST))
+	select SND_SOC_NAU8825
+	select SND_SOC_RT1015P
+	select SND_SOC_MAX98373_I2C
+	select SND_SOC_MAX98357A
+	select SND_SOC_DMIC
+	select SND_SOC_HDAC_HDMI
+	select SND_SOC_INTEL_HDA_DSP_COMMON
+	select SND_SOC_INTEL_SOF_MAXIM_COMMON
+	help
+	   This adds support for ASoC machine driver for SOF platforms
+	   with nau8825 codec.
+	   Say Y if you have such a device.
+	   If unsure select "N".
+
 endif ## SND_SOC_SOF_HDA_LINK || SND_SOC_SOF_BAYTRAIL
 
 if (SND_SOC_SOF_COMETLAKE && SND_SOC_SOF_HDA_LINK)
diff -ruN a/sound/soc/intel/boards/Makefile b/sound/soc/intel/boards/Makefile
--- a/sound/soc/intel/boards/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/Makefile	2021-12-23 08:36:03.000000000 +0100
@@ -21,6 +21,8 @@
 snd-soc-sst-byt-cht-nocodec-objs := bytcht_nocodec.o
 snd-soc-sof_rt5682-objs := sof_rt5682.o sof_realtek_common.o
 snd-soc-sof_cs42l42-objs := sof_cs42l42.o
+snd-soc-sof_es8336-objs := sof_es8336.o
+snd-soc-sof_nau8825-objs := sof_nau8825.o sof_realtek_common.o
 snd-soc-cml_rt1011_rt5682-objs := cml_rt1011_rt5682.o
 snd-soc-kbl_da7219_max98357a-objs := kbl_da7219_max98357a.o
 snd-soc-kbl_da7219_max98927-objs := kbl_da7219_max98927.o
@@ -42,6 +44,8 @@
 			sof_sdw_dmic.o sof_sdw_hdmi.o
 obj-$(CONFIG_SND_SOC_INTEL_SOF_RT5682_MACH) += snd-soc-sof_rt5682.o
 obj-$(CONFIG_SND_SOC_INTEL_SOF_CS42L42_MACH) += snd-soc-sof_cs42l42.o
+obj-$(CONFIG_SND_SOC_INTEL_SOF_ES8336_MACH) += snd-soc-sof_es8336.o
+obj-$(CONFIG_SND_SOC_INTEL_SOF_NAU8825_MACH) += snd-soc-sof_nau8825.o
 obj-$(CONFIG_SND_SOC_INTEL_HASWELL_MACH) += snd-soc-sst-haswell.o
 obj-$(CONFIG_SND_SOC_INTEL_BXT_DA7219_MAX98357A_COMMON) += snd-soc-sst-bxt-da7219_max98357a.o
 obj-$(CONFIG_SND_SOC_INTEL_BXT_RT298_MACH) += snd-soc-sst-bxt-rt298.o
diff -ruN a/sound/soc/intel/boards/skl_nau88l25_max98357a.c b/sound/soc/intel/boards/skl_nau88l25_max98357a.c
--- a/sound/soc/intel/boards/skl_nau88l25_max98357a.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/skl_nau88l25_max98357a.c	2021-12-23 08:36:03.000000000 +0100
@@ -539,7 +539,7 @@
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_I2S |
 			SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = skylake_ssp_fixup,
 		.dpcm_playback = 1,
@@ -552,7 +552,7 @@
 		.no_pcm = 1,
 		.init = skylake_nau8825_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = skylake_ssp_fixup,
 		.ops = &skylake_nau8825_ops,
diff -ruN a/sound/soc/intel/boards/skl_nau88l25_ssm4567.c b/sound/soc/intel/boards/skl_nau88l25_ssm4567.c
--- a/sound/soc/intel/boards/skl_nau88l25_ssm4567.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/skl_nau88l25_ssm4567.c	2021-12-23 08:36:03.000000000 +0100
@@ -578,7 +578,7 @@
 		.no_pcm = 1,
 		.dai_fmt = SND_SOC_DAIFMT_DSP_A |
 			SND_SOC_DAIFMT_IB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.init = skylake_ssm4567_codec_init,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = skylake_ssp_fixup,
@@ -593,7 +593,7 @@
 		.no_pcm = 1,
 		.init = skylake_nau8825_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S | SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = skylake_ssp_fixup,
 		.ops = &skylake_nau8825_ops,
diff -ruN a/sound/soc/intel/boards/skl_rt286.c b/sound/soc/intel/boards/skl_rt286.c
--- a/sound/soc/intel/boards/skl_rt286.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/skl_rt286.c	2021-12-23 08:36:03.000000000 +0100
@@ -434,7 +434,7 @@
 		.init = skylake_rt286_codec_init,
 		.dai_fmt = SND_SOC_DAIFMT_I2S |
 			SND_SOC_DAIFMT_NB_NF |
-			SND_SOC_DAIFMT_CBS_CFS,
+			SND_SOC_DAIFMT_CBC_CFC,
 		.ignore_pmdown_time = 1,
 		.be_hw_params_fixup = skylake_ssp0_fixup,
 		.ops = &skylake_rt286_ops,
diff -ruN a/sound/soc/intel/boards/sof_es8336.c b/sound/soc/intel/boards/sof_es8336.c
--- a/sound/soc/intel/boards/sof_es8336.c	1970-01-01 01:00:00.000000000 +0100
+++ b/sound/soc/intel/boards/sof_es8336.c	2021-12-23 08:36:03.000000000 +0100
@@ -0,0 +1,569 @@
+// SPDX-License-Identifier: GPL-2.0-only
+// Copyright(c) 2021 Intel Corporation.
+
+/*
+ * Intel SOF Machine Driver with es8336 Codec
+ */
+
+#include <linux/device.h>
+#include <linux/dmi.h>
+#include <linux/gpio/consumer.h>
+#include <linux/gpio/machine.h>
+#include <linux/i2c.h>
+#include <linux/input.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <sound/jack.h>
+#include <sound/pcm.h>
+#include <sound/pcm_params.h>
+#include <sound/soc.h>
+#include <sound/soc-acpi.h>
+#include "hda_dsp_common.h"
+
+#define SOF_ES8336_SSP_CODEC(quirk)		((quirk) & GENMASK(3, 0))
+#define SOF_ES8336_SSP_CODEC_MASK		(GENMASK(3, 0))
+
+#define SOF_ES8336_TGL_GPIO_QUIRK		BIT(4)
+#define SOF_ES8336_ENABLE_DMIC			BIT(5)
+
+static unsigned long quirk;
+
+static int quirk_override = -1;
+module_param_named(quirk, quirk_override, int, 0444);
+MODULE_PARM_DESC(quirk, "Board-specific quirk override");
+
+struct sof_es8336_private {
+	struct device *codec_dev;
+	struct gpio_desc *gpio_pa;
+	struct snd_soc_jack jack;
+	struct list_head hdmi_pcm_list;
+	bool speaker_en;
+};
+
+struct sof_hdmi_pcm {
+	struct list_head head;
+	struct snd_soc_dai *codec_dai;
+	int device;
+};
+
+static const struct acpi_gpio_params pa_enable_gpio = { 0, 0, true };
+static const struct acpi_gpio_mapping acpi_es8336_gpios[] = {
+	{ "pa-enable-gpios", &pa_enable_gpio, 1 },
+	{ }
+};
+
+static const struct acpi_gpio_params quirk_pa_enable_gpio = { 1, 0, true };
+static const struct acpi_gpio_mapping quirk_acpi_es8336_gpios[] = {
+	{ "pa-enable-gpios", &quirk_pa_enable_gpio, 1 },
+	{ }
+};
+
+static const struct acpi_gpio_mapping *gpio_mapping = acpi_es8336_gpios;
+
+static void log_quirks(struct device *dev)
+{
+	dev_info(dev, "quirk SSP%ld",  SOF_ES8336_SSP_CODEC(quirk));
+}
+
+static int sof_es8316_speaker_power_event(struct snd_soc_dapm_widget *w,
+					  struct snd_kcontrol *kcontrol, int event)
+{
+	struct snd_soc_card *card = w->dapm->card;
+	struct sof_es8336_private *priv = snd_soc_card_get_drvdata(card);
+
+	if (SND_SOC_DAPM_EVENT_ON(event))
+		priv->speaker_en = false;
+	else
+		priv->speaker_en = true;
+
+	gpiod_set_value_cansleep(priv->gpio_pa, priv->speaker_en);
+
+	return 0;
+}
+
+static const struct snd_soc_dapm_widget sof_es8316_widgets[] = {
+	SND_SOC_DAPM_SPK("Speaker", NULL),
+	SND_SOC_DAPM_HP("Headphone", NULL),
+	SND_SOC_DAPM_MIC("Headset Mic", NULL),
+	SND_SOC_DAPM_MIC("Internal Mic", NULL),
+
+	SND_SOC_DAPM_SUPPLY("Speaker Power", SND_SOC_NOPM, 0, 0,
+			    sof_es8316_speaker_power_event,
+			    SND_SOC_DAPM_PRE_PMD | SND_SOC_DAPM_POST_PMU),
+};
+
+static const struct snd_soc_dapm_widget dmic_widgets[] = {
+	SND_SOC_DAPM_MIC("SoC DMIC", NULL),
+};
+
+static const struct snd_soc_dapm_route sof_es8316_audio_map[] = {
+	{"Headphone", NULL, "HPOL"},
+	{"Headphone", NULL, "HPOR"},
+
+	/*
+	 * There is no separate speaker output instead the speakers are muxed to
+	 * the HP outputs. The mux is controlled by the "Speaker Power" supply.
+	 */
+	{"Speaker", NULL, "HPOL"},
+	{"Speaker", NULL, "HPOR"},
+	{"Speaker", NULL, "Speaker Power"},
+};
+
+static const struct snd_soc_dapm_route sof_es8316_intmic_in1_map[] = {
+	{"MIC1", NULL, "Internal Mic"},
+	{"MIC2", NULL, "Headset Mic"},
+};
+
+static const struct snd_soc_dapm_route dmic_map[] = {
+	/* digital mics */
+	{"DMic", NULL, "SoC DMIC"},
+};
+
+static const struct snd_kcontrol_new sof_es8316_controls[] = {
+	SOC_DAPM_PIN_SWITCH("Speaker"),
+	SOC_DAPM_PIN_SWITCH("Headphone"),
+	SOC_DAPM_PIN_SWITCH("Headset Mic"),
+	SOC_DAPM_PIN_SWITCH("Internal Mic"),
+};
+
+static struct snd_soc_jack_pin sof_es8316_jack_pins[] = {
+	{
+		.pin	= "Headphone",
+		.mask	= SND_JACK_HEADPHONE,
+	},
+	{
+		.pin	= "Headset Mic",
+		.mask	= SND_JACK_MICROPHONE,
+	},
+};
+
+static int dmic_init(struct snd_soc_pcm_runtime *runtime)
+{
+	struct snd_soc_card *card = runtime->card;
+	int ret;
+
+	ret = snd_soc_dapm_new_controls(&card->dapm, dmic_widgets,
+					ARRAY_SIZE(dmic_widgets));
+	if (ret) {
+		dev_err(card->dev, "DMic widget addition failed: %d\n", ret);
+		return ret;
+	}
+
+	ret = snd_soc_dapm_add_routes(&card->dapm, dmic_map,
+				      ARRAY_SIZE(dmic_map));
+	if (ret)
+		dev_err(card->dev, "DMic map addition failed: %d\n", ret);
+
+	return ret;
+}
+
+static int sof_hdmi_init(struct snd_soc_pcm_runtime *runtime)
+{
+	struct sof_es8336_private *priv = snd_soc_card_get_drvdata(runtime->card);
+	struct snd_soc_dai *dai = asoc_rtd_to_codec(runtime, 0);
+	struct sof_hdmi_pcm *pcm;
+
+	pcm = devm_kzalloc(runtime->card->dev, sizeof(*pcm), GFP_KERNEL);
+	if (!pcm)
+		return -ENOMEM;
+
+	/* dai_link id is 1:1 mapped to the PCM device */
+	pcm->device = runtime->dai_link->id;
+	pcm->codec_dai = dai;
+
+	list_add_tail(&pcm->head, &priv->hdmi_pcm_list);
+
+	return 0;
+}
+
+static int sof_es8316_init(struct snd_soc_pcm_runtime *runtime)
+{
+	struct snd_soc_component *codec = asoc_rtd_to_codec(runtime, 0)->component;
+	struct snd_soc_card *card = runtime->card;
+	struct sof_es8336_private *priv = snd_soc_card_get_drvdata(card);
+	const struct snd_soc_dapm_route *custom_map;
+	int num_routes;
+	int ret;
+
+	card->dapm.idle_bias_off = true;
+
+	custom_map = sof_es8316_intmic_in1_map;
+	num_routes = ARRAY_SIZE(sof_es8316_intmic_in1_map);
+
+	ret = snd_soc_dapm_add_routes(&card->dapm, custom_map, num_routes);
+	if (ret)
+		return ret;
+
+	ret = snd_soc_card_jack_new(card, "Headset",
+				    SND_JACK_HEADSET | SND_JACK_BTN_0,
+				    &priv->jack, sof_es8316_jack_pins,
+				    ARRAY_SIZE(sof_es8316_jack_pins));
+	if (ret) {
+		dev_err(card->dev, "jack creation failed %d\n", ret);
+		return ret;
+	}
+
+	snd_jack_set_key(priv->jack.jack, SND_JACK_BTN_0, KEY_PLAYPAUSE);
+
+	snd_soc_component_set_jack(codec, &priv->jack, NULL);
+
+	return 0;
+}
+
+static void sof_es8316_exit(struct snd_soc_pcm_runtime *rtd)
+{
+	struct snd_soc_component *component = asoc_rtd_to_codec(rtd, 0)->component;
+
+	snd_soc_component_set_jack(component, NULL, NULL);
+}
+
+static int sof_es8336_quirk_cb(const struct dmi_system_id *id)
+{
+	quirk = (unsigned long)id->driver_data;
+
+	if (quirk & SOF_ES8336_TGL_GPIO_QUIRK)
+		gpio_mapping = quirk_acpi_es8336_gpios;
+
+	return 1;
+}
+
+static const struct dmi_system_id sof_es8336_quirk_table[] = {
+	{
+		.callback = sof_es8336_quirk_cb,
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "CHUWI Innovation And Technology"),
+			DMI_MATCH(DMI_BOARD_NAME, "Hi10 X"),
+		},
+		.driver_data = (void *)SOF_ES8336_SSP_CODEC(2)
+	},
+	{
+		.callback = sof_es8336_quirk_cb,
+		.matches = {
+			DMI_MATCH(DMI_SYS_VENDOR, "IP3 tech"),
+			DMI_MATCH(DMI_BOARD_NAME, "WN1"),
+		},
+		.driver_data = (void *)(SOF_ES8336_SSP_CODEC(0) |
+					SOF_ES8336_TGL_GPIO_QUIRK |
+					SOF_ES8336_ENABLE_DMIC)
+	},
+	{}
+};
+
+static int sof_es8336_hw_params(struct snd_pcm_substream *substream,
+				struct snd_pcm_hw_params *params)
+{
+	struct snd_soc_pcm_runtime *rtd = asoc_substream_to_rtd(substream);
+	struct snd_soc_dai *codec_dai = asoc_rtd_to_codec(rtd, 0);
+	const int sysclk = 19200000;
+	int ret;
+
+	ret = snd_soc_dai_set_sysclk(codec_dai, 1, sysclk, SND_SOC_CLOCK_OUT);
+	if (ret < 0) {
+		dev_err(rtd->dev, "%s, Failed to set ES8336 SYSCLK: %d\n",
+			__func__, ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+/* machine stream operations */
+static struct snd_soc_ops sof_es8336_ops = {
+	.hw_params = sof_es8336_hw_params,
+};
+
+static struct snd_soc_dai_link_component platform_component[] = {
+	{
+		/* name might be overridden during probe */
+		.name = "0000:00:1f.3"
+	}
+};
+
+SND_SOC_DAILINK_DEF(ssp1_codec,
+	DAILINK_COMP_ARRAY(COMP_CODEC("i2c-ESSX8336:00", "ES8316 HiFi")));
+
+static struct snd_soc_dai_link_component dmic_component[] = {
+	{
+		.name = "dmic-codec",
+		.dai_name = "dmic-hifi",
+	}
+};
+
+static int sof_es8336_late_probe(struct snd_soc_card *card)
+{
+	struct sof_es8336_private *priv = snd_soc_card_get_drvdata(card);
+	struct sof_hdmi_pcm *pcm;
+
+	if (list_empty(&priv->hdmi_pcm_list))
+		return -ENOENT;
+
+	pcm = list_first_entry(&priv->hdmi_pcm_list, struct sof_hdmi_pcm, head);
+
+	return hda_dsp_hdmi_build_controls(card, pcm->codec_dai->component);
+}
+
+/* SoC card */
+static struct snd_soc_card sof_es8336_card = {
+	.name = "essx8336", /* sof- prefix added automatically */
+	.owner = THIS_MODULE,
+	.dapm_widgets = sof_es8316_widgets,
+	.num_dapm_widgets = ARRAY_SIZE(sof_es8316_widgets),
+	.dapm_routes = sof_es8316_audio_map,
+	.num_dapm_routes = ARRAY_SIZE(sof_es8316_audio_map),
+	.controls = sof_es8316_controls,
+	.num_controls = ARRAY_SIZE(sof_es8316_controls),
+	.fully_routed = true,
+	.late_probe = sof_es8336_late_probe,
+	.num_links = 1,
+};
+
+static struct snd_soc_dai_link *sof_card_dai_links_create(struct device *dev,
+							  int ssp_codec,
+							  int dmic_be_num,
+							  int hdmi_num)
+{
+	struct snd_soc_dai_link_component *cpus;
+	struct snd_soc_dai_link *links;
+	struct snd_soc_dai_link_component *idisp_components;
+	int hdmi_id_offset = 0;
+	int id = 0;
+	int i;
+
+	links = devm_kcalloc(dev, sof_es8336_card.num_links,
+			     sizeof(struct snd_soc_dai_link), GFP_KERNEL);
+	cpus = devm_kcalloc(dev, sof_es8336_card.num_links,
+			    sizeof(struct snd_soc_dai_link_component), GFP_KERNEL);
+	if (!links || !cpus)
+		goto devm_err;
+
+	/* codec SSP */
+	links[id].name = devm_kasprintf(dev, GFP_KERNEL,
+					"SSP%d-Codec", ssp_codec);
+	if (!links[id].name)
+		goto devm_err;
+
+	links[id].id = id;
+	links[id].codecs = ssp1_codec;
+	links[id].num_codecs = ARRAY_SIZE(ssp1_codec);
+	links[id].platforms = platform_component;
+	links[id].num_platforms = ARRAY_SIZE(platform_component);
+	links[id].init = sof_es8316_init;
+	links[id].exit = sof_es8316_exit;
+	links[id].ops = &sof_es8336_ops;
+	links[id].nonatomic = true;
+	links[id].dpcm_playback = 1;
+	links[id].dpcm_capture = 1;
+	links[id].no_pcm = 1;
+	links[id].cpus = &cpus[id];
+	links[id].num_cpus = 1;
+
+	links[id].cpus->dai_name = devm_kasprintf(dev, GFP_KERNEL,
+						  "SSP%d Pin",
+						  ssp_codec);
+	if (!links[id].cpus->dai_name)
+		goto devm_err;
+
+	id++;
+
+	/* dmic */
+	if (dmic_be_num > 0) {
+		/* at least we have dmic01 */
+		links[id].name = "dmic01";
+		links[id].cpus = &cpus[id];
+		links[id].cpus->dai_name = "DMIC01 Pin";
+		links[id].init = dmic_init;
+		if (dmic_be_num > 1) {
+			/* set up 2 BE links at most */
+			links[id + 1].name = "dmic16k";
+			links[id + 1].cpus = &cpus[id + 1];
+			links[id + 1].cpus->dai_name = "DMIC16k Pin";
+			dmic_be_num = 2;
+		}
+	} else {
+		/* HDMI dai link starts at 3 according to current topology settings */
+		hdmi_id_offset = 2;
+	}
+
+	for (i = 0; i < dmic_be_num; i++) {
+		links[id].id = id;
+		links[id].num_cpus = 1;
+		links[id].codecs = dmic_component;
+		links[id].num_codecs = ARRAY_SIZE(dmic_component);
+		links[id].platforms = platform_component;
+		links[id].num_platforms = ARRAY_SIZE(platform_component);
+		links[id].ignore_suspend = 1;
+		links[id].dpcm_capture = 1;
+		links[id].no_pcm = 1;
+
+		id++;
+	}
+
+	/* HDMI */
+	if (hdmi_num > 0) {
+		idisp_components = devm_kzalloc(dev,
+						sizeof(struct snd_soc_dai_link_component) *
+						hdmi_num, GFP_KERNEL);
+		if (!idisp_components)
+			goto devm_err;
+	}
+
+	for (i = 1; i <= hdmi_num; i++) {
+		links[id].name = devm_kasprintf(dev, GFP_KERNEL,
+						"iDisp%d", i);
+		if (!links[id].name)
+			goto devm_err;
+
+		links[id].id = id + hdmi_id_offset;
+		links[id].cpus = &cpus[id];
+		links[id].num_cpus = 1;
+		links[id].cpus->dai_name = devm_kasprintf(dev, GFP_KERNEL,
+							  "iDisp%d Pin", i);
+		if (!links[id].cpus->dai_name)
+			goto devm_err;
+
+		idisp_components[i - 1].name = "ehdaudio0D2";
+		idisp_components[i - 1].dai_name = devm_kasprintf(dev,
+								  GFP_KERNEL,
+								  "intel-hdmi-hifi%d",
+								  i);
+		if (!idisp_components[i - 1].dai_name)
+			goto devm_err;
+
+		links[id].codecs = &idisp_components[i - 1];
+		links[id].num_codecs = 1;
+		links[id].platforms = platform_component;
+		links[id].num_platforms = ARRAY_SIZE(platform_component);
+		links[id].init = sof_hdmi_init;
+		links[id].dpcm_playback = 1;
+		links[id].no_pcm = 1;
+
+		id++;
+	}
+
+	return links;
+
+devm_err:
+	return NULL;
+}
+
+ /* i2c-<HID>:00 with HID being 8 chars */
+static char codec_name[SND_ACPI_I2C_ID_LEN];
+
+static int sof_es8336_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct snd_soc_card *card;
+	struct snd_soc_acpi_mach *mach = pdev->dev.platform_data;
+	struct sof_es8336_private *priv;
+	struct acpi_device *adev;
+	struct snd_soc_dai_link *dai_links;
+	struct device *codec_dev;
+	int dmic_be_num = 0;
+	int hdmi_num = 3;
+	int ret;
+
+	priv = devm_kzalloc(dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	card = &sof_es8336_card;
+	card->dev = dev;
+
+	if (!dmi_check_system(sof_es8336_quirk_table))
+		quirk = SOF_ES8336_SSP_CODEC(2);
+
+	if (quirk & SOF_ES8336_ENABLE_DMIC)
+		dmic_be_num = 2;
+
+	if (quirk_override != -1) {
+		dev_info(dev, "Overriding quirk 0x%lx => 0x%x\n",
+			 quirk, quirk_override);
+		quirk = quirk_override;
+	}
+	log_quirks(dev);
+
+	sof_es8336_card.num_links += dmic_be_num + hdmi_num;
+	dai_links = sof_card_dai_links_create(dev,
+					      SOF_ES8336_SSP_CODEC(quirk),
+					      dmic_be_num, hdmi_num);
+	if (!dai_links)
+		return -ENOMEM;
+
+	sof_es8336_card.dai_link = dai_links;
+
+	/* fixup codec name based on HID */
+	adev = acpi_dev_get_first_match_dev(mach->id, NULL, -1);
+	if (adev) {
+		snprintf(codec_name, sizeof(codec_name),
+			 "i2c-%s", acpi_dev_name(adev));
+		put_device(&adev->dev);
+		dai_links[0].codecs->name = codec_name;
+	}
+
+	ret = snd_soc_fixup_dai_links_platform_name(&sof_es8336_card,
+						    mach->mach_params.platform);
+	if (ret)
+		return ret;
+
+	/* get speaker enable GPIO */
+	codec_dev = bus_find_device_by_name(&i2c_bus_type, NULL, codec_name);
+	if (!codec_dev)
+		return -EPROBE_DEFER;
+
+	ret = devm_acpi_dev_add_driver_gpios(codec_dev, gpio_mapping);
+	if (ret)
+		dev_warn(codec_dev, "unable to add GPIO mapping table\n");
+
+	priv->gpio_pa = gpiod_get(codec_dev, "pa-enable", GPIOD_OUT_LOW);
+	if (IS_ERR(priv->gpio_pa)) {
+		ret = PTR_ERR(priv->gpio_pa);
+		dev_err(codec_dev, "%s, could not get pa-enable: %d\n",
+			__func__, ret);
+		goto err;
+	}
+
+	priv->codec_dev = codec_dev;
+	INIT_LIST_HEAD(&priv->hdmi_pcm_list);
+
+	snd_soc_card_set_drvdata(card, priv);
+
+	ret = devm_snd_soc_register_card(dev, card);
+	if (ret) {
+		gpiod_put(priv->gpio_pa);
+		dev_err(dev, "snd_soc_register_card failed: %d\n", ret);
+		goto err;
+	}
+	platform_set_drvdata(pdev, &sof_es8336_card);
+	return 0;
+
+err:
+	put_device(codec_dev);
+	return ret;
+}
+
+static int sof_es8336_remove(struct platform_device *pdev)
+{
+	struct snd_soc_card *card = platform_get_drvdata(pdev);
+	struct sof_es8336_private *priv = snd_soc_card_get_drvdata(card);
+
+	gpiod_put(priv->gpio_pa);
+	put_device(priv->codec_dev);
+
+	return 0;
+}
+
+static struct platform_driver sof_es8336_driver = {
+	.driver = {
+		.name = "sof-essx8336",
+		.pm = &snd_soc_pm_ops,
+	},
+	.probe = sof_es8336_probe,
+	.remove = sof_es8336_remove,
+};
+module_platform_driver(sof_es8336_driver);
+
+MODULE_DESCRIPTION("ASoC Intel(R) SOF + ES8336 Machine driver");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:sof-essx8336");
+MODULE_IMPORT_NS(SND_SOC_INTEL_HDA_DSP_COMMON);
diff -ruN a/sound/soc/intel/boards/sof_maxim_common.c b/sound/soc/intel/boards/sof_maxim_common.c
--- a/sound/soc/intel/boards/sof_maxim_common.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/sof_maxim_common.c	2021-12-23 08:36:03.000000000 +0100
@@ -5,6 +5,7 @@
 #include <linux/string.h>
 #include <sound/pcm.h>
 #include <sound/soc.h>
+#include <sound/soc-acpi.h>
 #include <sound/soc-dai.h>
 #include <sound/soc-dapm.h>
 #include <uapi/sound/asound.h>
@@ -134,6 +135,186 @@
 EXPORT_SYMBOL_NS(max_98373_set_codec_conf, SND_SOC_INTEL_SOF_MAXIM_COMMON);
 
 /*
+ * Maxim MAX98390
+ */
+const struct snd_soc_dapm_route max_98390_dapm_routes[] = {
+	/* speaker */
+	{ "Left Spk", NULL, "Left BE_OUT" },
+	{ "Right Spk", NULL, "Right BE_OUT" },
+};
+
+static const struct snd_kcontrol_new max_98390_tt_kcontrols[] = {
+	SOC_DAPM_PIN_SWITCH("TL Spk"),
+	SOC_DAPM_PIN_SWITCH("TR Spk"),
+};
+
+static const struct snd_soc_dapm_widget max_98390_tt_dapm_widgets[] = {
+	SND_SOC_DAPM_SPK("TL Spk", NULL),
+	SND_SOC_DAPM_SPK("TR Spk", NULL),
+};
+
+const struct snd_soc_dapm_route max_98390_tt_dapm_routes[] = {
+	/* Tweeter speaker */
+	{ "TL Spk", NULL, "Tweeter Left BE_OUT" },
+	{ "TR Spk", NULL, "Tweeter Right BE_OUT" },
+};
+
+static struct snd_soc_codec_conf max_98390_codec_conf[] = {
+	{
+		.dlc = COMP_CODEC_CONF(MAX_98390_DEV0_NAME),
+		.name_prefix = "Right",
+	},
+	{
+		.dlc = COMP_CODEC_CONF(MAX_98390_DEV1_NAME),
+		.name_prefix = "Left",
+	},
+};
+
+static struct snd_soc_codec_conf max_98390_4spk_codec_conf[] = {
+	{
+		.dlc = COMP_CODEC_CONF(MAX_98390_DEV0_NAME),
+		.name_prefix = "Right",
+	},
+	{
+		.dlc = COMP_CODEC_CONF(MAX_98390_DEV1_NAME),
+		.name_prefix = "Left",
+	},
+	{
+		.dlc = COMP_CODEC_CONF(MAX_98390_DEV2_NAME),
+		.name_prefix = "Tweeter Right",
+	},
+	{
+		.dlc = COMP_CODEC_CONF(MAX_98390_DEV3_NAME),
+		.name_prefix = "Tweeter Left",
+	},
+};
+
+struct snd_soc_dai_link_component max_98390_components[] = {
+	{
+		.name = MAX_98390_DEV0_NAME,
+		.dai_name = MAX_98390_CODEC_DAI,
+	},
+	{
+		.name = MAX_98390_DEV1_NAME,
+		.dai_name = MAX_98390_CODEC_DAI,
+	},
+};
+EXPORT_SYMBOL_NS(max_98390_components, SND_SOC_INTEL_SOF_MAXIM_COMMON);
+
+struct snd_soc_dai_link_component max_98390_4spk_components[] = {
+	{
+		.name = MAX_98390_DEV0_NAME,
+		.dai_name = MAX_98390_CODEC_DAI,
+	},
+	{
+		.name = MAX_98390_DEV1_NAME,
+		.dai_name = MAX_98390_CODEC_DAI,
+	},
+	{
+		.name = MAX_98390_DEV2_NAME,
+		.dai_name = MAX_98390_CODEC_DAI,
+	},
+	{
+		.name = MAX_98390_DEV3_NAME,
+		.dai_name = MAX_98390_CODEC_DAI,
+	},
+};
+EXPORT_SYMBOL_NS(max_98390_4spk_components, SND_SOC_INTEL_SOF_MAXIM_COMMON);
+
+static int max_98390_hw_params(struct snd_pcm_substream *substream,
+				struct snd_pcm_hw_params *params)
+{
+	struct snd_soc_pcm_runtime *rtd = asoc_substream_to_rtd(substream);
+	struct snd_soc_dai *codec_dai;
+	int i;
+
+	for_each_rtd_codec_dais(rtd, i, codec_dai) {
+
+		if (i >= ARRAY_SIZE(max_98390_4spk_components)) {
+			dev_err(codec_dai->dev, "invalid codec index %d\n", i);
+			return -ENODEV;
+		}
+
+		if (!strcmp(codec_dai->component->name, MAX_98390_DEV0_NAME)) {
+			/* DEV0 tdm slot configuration Right */
+			snd_soc_dai_set_tdm_slot(codec_dai, 0x01, 3, 4, 32);
+		}
+		if (!strcmp(codec_dai->component->name, MAX_98390_DEV1_NAME)) {
+			/* DEV1 tdm slot configuration Left */
+			snd_soc_dai_set_tdm_slot(codec_dai, 0x02, 3, 4, 32);
+		}
+
+		if (!strcmp(codec_dai->component->name, MAX_98390_DEV2_NAME)) {
+			/* DEVi2 tdm slot configuration Tweeter Right */
+			snd_soc_dai_set_tdm_slot(codec_dai, 0x04, 3, 4, 32);
+		}
+		if (!strcmp(codec_dai->component->name, MAX_98390_DEV3_NAME)) {
+			/* DEV3 tdm slot configuration Tweeter Left */
+			snd_soc_dai_set_tdm_slot(codec_dai, 0x08, 3, 4, 32);
+		}
+	}
+	return 0;
+}
+
+int max_98390_spk_codec_init(struct snd_soc_pcm_runtime *rtd)
+{
+	struct snd_soc_card *card = rtd->card;
+	int ret;
+
+	/* add regular speakers dapm route */
+	ret = snd_soc_dapm_add_routes(&card->dapm, max_98390_dapm_routes,
+					ARRAY_SIZE(max_98390_dapm_routes));
+	if (ret) {
+		dev_err(rtd->dev, "unable to add Left/Right Speaker dapm, ret %d\n", ret);
+		return ret;
+	}
+
+	/* add widgets/controls/dapm for tweeter speakers */
+	if (acpi_dev_present("MX98390", "3", -1)) {
+		ret = snd_soc_dapm_new_controls(&card->dapm, max_98390_tt_dapm_widgets,
+							ARRAY_SIZE(max_98390_tt_dapm_widgets));
+
+		if (ret) {
+			dev_err(rtd->dev, "unable to add tweeter dapm controls, ret %d\n", ret);
+			/* Don't need to add routes if widget addition failed */
+			return ret;
+		}
+
+		ret = snd_soc_add_card_controls(card, max_98390_tt_kcontrols,
+						ARRAY_SIZE(max_98390_tt_kcontrols));
+		if (ret) {
+			dev_err(rtd->dev, "unable to add tweeter card controls, ret %d\n", ret);
+			return ret;
+		}
+
+		ret = snd_soc_dapm_add_routes(&card->dapm, max_98390_tt_dapm_routes,
+						ARRAY_SIZE(max_98390_tt_dapm_routes));
+		if (ret)
+			dev_err(rtd->dev,
+				"unable to add Tweeter Left/Right Speaker dapm, ret %d\n", ret);
+	}
+	return ret;
+}
+EXPORT_SYMBOL_NS(max_98390_spk_codec_init, SND_SOC_INTEL_SOF_MAXIM_COMMON);
+
+struct snd_soc_ops max_98390_ops = {
+	.hw_params = max_98390_hw_params,
+};
+EXPORT_SYMBOL_NS(max_98390_ops, SND_SOC_INTEL_SOF_MAXIM_COMMON);
+
+void max_98390_set_codec_conf(struct snd_soc_card *card, int ch)
+{
+	if (ch == ARRAY_SIZE(max_98390_4spk_codec_conf)) {
+		card->codec_conf = max_98390_4spk_codec_conf;
+		card->num_configs = ARRAY_SIZE(max_98390_4spk_codec_conf);
+	} else {
+		card->codec_conf = max_98390_codec_conf;
+		card->num_configs = ARRAY_SIZE(max_98390_codec_conf);
+	}
+}
+EXPORT_SYMBOL_NS(max_98390_set_codec_conf, SND_SOC_INTEL_SOF_MAXIM_COMMON);
+
+/*
  * Maxim MAX98357A/MAX98360A
  */
 static const struct snd_kcontrol_new max_98357a_kcontrols[] = {
diff -ruN a/sound/soc/intel/boards/sof_maxim_common.h b/sound/soc/intel/boards/sof_maxim_common.h
--- a/sound/soc/intel/boards/sof_maxim_common.h	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/sof_maxim_common.h	2021-12-23 08:36:03.000000000 +0100
@@ -25,6 +25,22 @@
 int max_98373_trigger(struct snd_pcm_substream *substream, int cmd);
 
 /*
+ * Maxim MAX98390
+ */
+#define MAX_98390_CODEC_DAI     "max98390-aif1"
+#define MAX_98390_DEV0_NAME     "i2c-MX98390:00"
+#define MAX_98390_DEV1_NAME     "i2c-MX98390:01"
+#define MAX_98390_DEV2_NAME     "i2c-MX98390:02"
+#define MAX_98390_DEV3_NAME     "i2c-MX98390:03"
+
+extern struct snd_soc_dai_link_component max_98390_components[2];
+extern struct snd_soc_dai_link_component max_98390_4spk_components[4];
+extern struct snd_soc_ops max_98390_ops;
+
+void max_98390_set_codec_conf(struct snd_soc_card *card, int ch);
+int max_98390_spk_codec_init(struct snd_soc_pcm_runtime *rtd);
+
+/*
  * Maxim MAX98357A/MAX98360A
  */
 #define MAX_98357A_CODEC_DAI	"HiFi"
diff -ruN a/sound/soc/intel/boards/sof_nau8825.c b/sound/soc/intel/boards/sof_nau8825.c
--- a/sound/soc/intel/boards/sof_nau8825.c	1970-01-01 01:00:00.000000000 +0100
+++ b/sound/soc/intel/boards/sof_nau8825.c	2021-12-23 08:36:03.000000000 +0100
@@ -0,0 +1,651 @@
+// SPDX-License-Identifier: GPL-2.0-only
+// Copyright(c) 2021 Intel Corporation.
+// Copyright(c) 2021 Nuvoton Corporation.
+
+/*
+ * Intel SOF Machine Driver with Nuvoton headphone codec NAU8825
+ * and speaker codec RT1019P MAX98360a or MAX98373
+ */
+#include <linux/i2c.h>
+#include <linux/input.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/dmi.h>
+#include <sound/core.h>
+#include <sound/jack.h>
+#include <sound/pcm.h>
+#include <sound/pcm_params.h>
+#include <sound/soc.h>
+#include <sound/sof.h>
+#include <sound/soc-acpi.h>
+#include "../../codecs/nau8825.h"
+#include "../common/soc-intel-quirks.h"
+#include "hda_dsp_common.h"
+#include "sof_realtek_common.h"
+#include "sof_maxim_common.h"
+
+#define NAME_SIZE 32
+
+#define SOF_NAU8825_SSP_CODEC(quirk)		((quirk) & GENMASK(2, 0))
+#define SOF_NAU8825_SSP_CODEC_MASK		(GENMASK(2, 0))
+#define SOF_SPEAKER_AMP_PRESENT		BIT(3)
+#define SOF_NAU8825_SSP_AMP_SHIFT		4
+#define SOF_NAU8825_SSP_AMP_MASK		(GENMASK(6, 4))
+#define SOF_NAU8825_SSP_AMP(quirk)	\
+	(((quirk) << SOF_NAU8825_SSP_AMP_SHIFT) & SOF_NAU8825_SSP_AMP_MASK)
+#define SOF_NAU8825_NUM_HDMIDEV_SHIFT		7
+#define SOF_NAU8825_NUM_HDMIDEV_MASK		(GENMASK(9, 7))
+#define SOF_NAU8825_NUM_HDMIDEV(quirk)	\
+	(((quirk) << SOF_NAU8825_NUM_HDMIDEV_SHIFT) & SOF_NAU8825_NUM_HDMIDEV_MASK)
+
+/* BT audio offload: reserve 3 bits for future */
+#define SOF_BT_OFFLOAD_SSP_SHIFT		10
+#define SOF_BT_OFFLOAD_SSP_MASK		(GENMASK(12, 10))
+#define SOF_BT_OFFLOAD_SSP(quirk)	\
+	(((quirk) << SOF_BT_OFFLOAD_SSP_SHIFT) & SOF_BT_OFFLOAD_SSP_MASK)
+#define SOF_SSP_BT_OFFLOAD_PRESENT		BIT(13)
+#define SOF_RT1019P_SPEAKER_AMP_PRESENT	BIT(14)
+#define SOF_MAX98373_SPEAKER_AMP_PRESENT	BIT(15)
+#define SOF_MAX98360A_SPEAKER_AMP_PRESENT	BIT(16)
+
+static unsigned long sof_nau8825_quirk = SOF_NAU8825_SSP_CODEC(0);
+
+struct sof_hdmi_pcm {
+	struct list_head head;
+	struct snd_soc_dai *codec_dai;
+	int device;
+};
+
+struct sof_card_private {
+	struct clk *mclk;
+	struct snd_soc_jack sof_headset;
+	struct list_head hdmi_pcm_list;
+};
+
+static int sof_hdmi_init(struct snd_soc_pcm_runtime *rtd)
+{
+	struct sof_card_private *ctx = snd_soc_card_get_drvdata(rtd->card);
+	struct snd_soc_dai *dai = asoc_rtd_to_codec(rtd, 0);
+	struct sof_hdmi_pcm *pcm;
+
+	pcm = devm_kzalloc(rtd->card->dev, sizeof(*pcm), GFP_KERNEL);
+	if (!pcm)
+		return -ENOMEM;
+
+	/* dai_link id is 1:1 mapped to the PCM device */
+	pcm->device = rtd->dai_link->id;
+	pcm->codec_dai = dai;
+
+	list_add_tail(&pcm->head, &ctx->hdmi_pcm_list);
+
+	return 0;
+}
+
+static int sof_nau8825_codec_init(struct snd_soc_pcm_runtime *rtd)
+{
+	struct sof_card_private *ctx = snd_soc_card_get_drvdata(rtd->card);
+	struct snd_soc_component *component = asoc_rtd_to_codec(rtd, 0)->component;
+
+	struct snd_soc_jack *jack;
+	int ret;
+
+	/*
+	 * Headset buttons map to the google Reference headset.
+	 * These can be configured by userspace.
+	 */
+	ret = snd_soc_card_jack_new(rtd->card, "Headset Jack",
+				    SND_JACK_HEADSET | SND_JACK_BTN_0 |
+				    SND_JACK_BTN_1 | SND_JACK_BTN_2 |
+				    SND_JACK_BTN_3,
+				    &ctx->sof_headset, NULL, 0);
+	if (ret) {
+		dev_err(rtd->dev, "Headset Jack creation failed: %d\n", ret);
+		return ret;
+	}
+
+	jack = &ctx->sof_headset;
+
+	snd_jack_set_key(jack->jack, SND_JACK_BTN_0, KEY_PLAYPAUSE);
+	snd_jack_set_key(jack->jack, SND_JACK_BTN_1, KEY_VOICECOMMAND);
+	snd_jack_set_key(jack->jack, SND_JACK_BTN_2, KEY_VOLUMEUP);
+	snd_jack_set_key(jack->jack, SND_JACK_BTN_3, KEY_VOLUMEDOWN);
+	ret = snd_soc_component_set_jack(component, jack, NULL);
+
+	if (ret) {
+		dev_err(rtd->dev, "Headset Jack call-back failed: %d\n", ret);
+		return ret;
+	}
+
+	return ret;
+};
+
+static void sof_nau8825_codec_exit(struct snd_soc_pcm_runtime *rtd)
+{
+	struct snd_soc_component *component = asoc_rtd_to_codec(rtd, 0)->component;
+
+	snd_soc_component_set_jack(component, NULL, NULL);
+}
+
+static int sof_nau8825_hw_params(struct snd_pcm_substream *substream,
+				 struct snd_pcm_hw_params *params)
+{
+	struct snd_soc_pcm_runtime *rtd = asoc_substream_to_rtd(substream);
+	struct snd_soc_dai *codec_dai = asoc_rtd_to_codec(rtd, 0);
+	int clk_freq, ret;
+
+	clk_freq = sof_dai_get_bclk(rtd); /* BCLK freq */
+
+	if (clk_freq <= 0) {
+		dev_err(rtd->dev, "get bclk freq failed: %d\n", clk_freq);
+		return -EINVAL;
+	}
+
+	/* Configure clock for codec */
+	ret = snd_soc_dai_set_sysclk(codec_dai, NAU8825_CLK_FLL_BLK, 0,
+				     SND_SOC_CLOCK_IN);
+	if (ret < 0) {
+		dev_err(codec_dai->dev, "can't set BCLK clock %d\n", ret);
+		return ret;
+	}
+
+	/* Configure pll for codec */
+	ret = snd_soc_dai_set_pll(codec_dai, 0, 0, clk_freq,
+				  params_rate(params) * 256);
+	if (ret < 0) {
+		dev_err(codec_dai->dev, "can't set BCLK: %d\n", ret);
+		return ret;
+	}
+
+	return ret;
+}
+
+static struct snd_soc_ops sof_nau8825_ops = {
+	.hw_params = sof_nau8825_hw_params,
+};
+
+static struct snd_soc_dai_link_component platform_component[] = {
+	{
+		/* name might be overridden during probe */
+		.name = "0000:00:1f.3"
+	}
+};
+
+static int sof_card_late_probe(struct snd_soc_card *card)
+{
+	struct sof_card_private *ctx = snd_soc_card_get_drvdata(card);
+	struct snd_soc_dapm_context *dapm = &card->dapm;
+	struct sof_hdmi_pcm *pcm;
+	int err;
+
+	if (list_empty(&ctx->hdmi_pcm_list))
+		return -EINVAL;
+
+	pcm = list_first_entry(&ctx->hdmi_pcm_list, struct sof_hdmi_pcm, head);
+
+	if (sof_nau8825_quirk & SOF_MAX98373_SPEAKER_AMP_PRESENT) {
+		/* Disable Left and Right Spk pin after boot */
+		snd_soc_dapm_disable_pin(dapm, "Left Spk");
+		snd_soc_dapm_disable_pin(dapm, "Right Spk");
+		err = snd_soc_dapm_sync(dapm);
+		if (err < 0)
+			return err;
+	}
+
+	return hda_dsp_hdmi_build_controls(card, pcm->codec_dai->component);
+}
+
+static const struct snd_kcontrol_new sof_controls[] = {
+	SOC_DAPM_PIN_SWITCH("Headphone Jack"),
+	SOC_DAPM_PIN_SWITCH("Headset Mic"),
+	SOC_DAPM_PIN_SWITCH("Left Spk"),
+	SOC_DAPM_PIN_SWITCH("Right Spk"),
+};
+
+static const struct snd_kcontrol_new speaker_controls[] = {
+	SOC_DAPM_PIN_SWITCH("Spk"),
+};
+
+static const struct snd_soc_dapm_widget sof_widgets[] = {
+	SND_SOC_DAPM_HP("Headphone Jack", NULL),
+	SND_SOC_DAPM_MIC("Headset Mic", NULL),
+	SND_SOC_DAPM_SPK("Left Spk", NULL),
+	SND_SOC_DAPM_SPK("Right Spk", NULL),
+};
+
+static const struct snd_soc_dapm_widget speaker_widgets[] = {
+	SND_SOC_DAPM_SPK("Spk", NULL),
+};
+
+static const struct snd_soc_dapm_widget dmic_widgets[] = {
+	SND_SOC_DAPM_MIC("SoC DMIC", NULL),
+};
+
+static const struct snd_soc_dapm_route sof_map[] = {
+	/* HP jack connectors - unknown if we have jack detection */
+	{ "Headphone Jack", NULL, "HPOL" },
+	{ "Headphone Jack", NULL, "HPOR" },
+
+	/* other jacks */
+	{ "MIC", NULL, "Headset Mic" },
+};
+
+static const struct snd_soc_dapm_route speaker_map[] = {
+	/* speaker */
+	{ "Spk", NULL, "Speaker" },
+};
+
+static const struct snd_soc_dapm_route dmic_map[] = {
+	/* digital mics */
+	{"DMic", NULL, "SoC DMIC"},
+};
+
+static int speaker_codec_init(struct snd_soc_pcm_runtime *rtd)
+{
+	struct snd_soc_card *card = rtd->card;
+	int ret;
+
+	ret = snd_soc_dapm_new_controls(&card->dapm, speaker_widgets,
+					ARRAY_SIZE(speaker_widgets));
+	if (ret) {
+		dev_err(rtd->dev, "unable to add dapm controls, ret %d\n", ret);
+		/* Don't need to add routes if widget addition failed */
+		return ret;
+	}
+
+	ret = snd_soc_add_card_controls(card, speaker_controls,
+					ARRAY_SIZE(speaker_controls));
+	if (ret) {
+		dev_err(rtd->dev, "unable to add card controls, ret %d\n", ret);
+		return ret;
+	}
+
+	ret = snd_soc_dapm_add_routes(&card->dapm, speaker_map,
+				      ARRAY_SIZE(speaker_map));
+
+	if (ret)
+		dev_err(rtd->dev, "Speaker map addition failed: %d\n", ret);
+	return ret;
+}
+
+static int dmic_init(struct snd_soc_pcm_runtime *rtd)
+{
+	struct snd_soc_card *card = rtd->card;
+	int ret;
+
+	ret = snd_soc_dapm_new_controls(&card->dapm, dmic_widgets,
+					ARRAY_SIZE(dmic_widgets));
+	if (ret) {
+		dev_err(card->dev, "DMic widget addition failed: %d\n", ret);
+		/* Don't need to add routes if widget addition failed */
+		return ret;
+	}
+
+	ret = snd_soc_dapm_add_routes(&card->dapm, dmic_map,
+				      ARRAY_SIZE(dmic_map));
+
+	if (ret)
+		dev_err(card->dev, "DMic map addition failed: %d\n", ret);
+
+	return ret;
+}
+
+/* sof audio machine driver for nau8825 codec */
+static struct snd_soc_card sof_audio_card_nau8825 = {
+	.name = "nau8825", /* the sof- prefix is added by the core */
+	.owner = THIS_MODULE,
+	.controls = sof_controls,
+	.num_controls = ARRAY_SIZE(sof_controls),
+	.dapm_widgets = sof_widgets,
+	.num_dapm_widgets = ARRAY_SIZE(sof_widgets),
+	.dapm_routes = sof_map,
+	.num_dapm_routes = ARRAY_SIZE(sof_map),
+	.fully_routed = true,
+	.late_probe = sof_card_late_probe,
+};
+
+static struct snd_soc_dai_link_component nau8825_component[] = {
+	{
+		.name = "i2c-10508825:00",
+		.dai_name = "nau8825-hifi",
+	}
+};
+
+static struct snd_soc_dai_link_component dmic_component[] = {
+	{
+		.name = "dmic-codec",
+		.dai_name = "dmic-hifi",
+	}
+};
+
+static struct snd_soc_dai_link_component rt1019p_component[] = {
+	{
+		.name = "RTL1019:00",
+		.dai_name = "HiFi",
+	}
+};
+
+static struct snd_soc_dai_link_component dummy_component[] = {
+	{
+		.name = "snd-soc-dummy",
+		.dai_name = "snd-soc-dummy-dai",
+	}
+};
+
+static struct snd_soc_dai_link *sof_card_dai_links_create(struct device *dev,
+							  int ssp_codec,
+							  int ssp_amp,
+							  int dmic_be_num,
+							  int hdmi_num)
+{
+	struct snd_soc_dai_link_component *idisp_components;
+	struct snd_soc_dai_link_component *cpus;
+	struct snd_soc_dai_link *links;
+	int i, id = 0;
+
+	links = devm_kzalloc(dev, sizeof(struct snd_soc_dai_link) *
+			     sof_audio_card_nau8825.num_links, GFP_KERNEL);
+	cpus = devm_kzalloc(dev, sizeof(struct snd_soc_dai_link_component) *
+			     sof_audio_card_nau8825.num_links, GFP_KERNEL);
+	if (!links || !cpus)
+		goto devm_err;
+
+	/* codec SSP */
+	links[id].name = devm_kasprintf(dev, GFP_KERNEL,
+					"SSP%d-Codec", ssp_codec);
+	if (!links[id].name)
+		goto devm_err;
+
+	links[id].id = id;
+	links[id].codecs = nau8825_component;
+	links[id].num_codecs = ARRAY_SIZE(nau8825_component);
+	links[id].platforms = platform_component;
+	links[id].num_platforms = ARRAY_SIZE(platform_component);
+	links[id].init = sof_nau8825_codec_init;
+	links[id].exit = sof_nau8825_codec_exit;
+	links[id].ops = &sof_nau8825_ops;
+	links[id].dpcm_playback = 1;
+	links[id].dpcm_capture = 1;
+	links[id].no_pcm = 1;
+	links[id].cpus = &cpus[id];
+	links[id].num_cpus = 1;
+
+	links[id].cpus->dai_name = devm_kasprintf(dev, GFP_KERNEL,
+						  "SSP%d Pin",
+						  ssp_codec);
+	if (!links[id].cpus->dai_name)
+		goto devm_err;
+
+	id++;
+
+	/* dmic */
+	if (dmic_be_num > 0) {
+		/* at least we have dmic01 */
+		links[id].name = "dmic01";
+		links[id].cpus = &cpus[id];
+		links[id].cpus->dai_name = "DMIC01 Pin";
+		links[id].init = dmic_init;
+		if (dmic_be_num > 1) {
+			/* set up 2 BE links at most */
+			links[id + 1].name = "dmic16k";
+			links[id + 1].cpus = &cpus[id + 1];
+			links[id + 1].cpus->dai_name = "DMIC16k Pin";
+			dmic_be_num = 2;
+		}
+	}
+
+	for (i = 0; i < dmic_be_num; i++) {
+		links[id].id = id;
+		links[id].num_cpus = 1;
+		links[id].codecs = dmic_component;
+		links[id].num_codecs = ARRAY_SIZE(dmic_component);
+		links[id].platforms = platform_component;
+		links[id].num_platforms = ARRAY_SIZE(platform_component);
+		links[id].ignore_suspend = 1;
+		links[id].dpcm_capture = 1;
+		links[id].no_pcm = 1;
+		id++;
+	}
+
+	/* HDMI */
+	if (hdmi_num > 0) {
+		idisp_components = devm_kzalloc(dev,
+						sizeof(struct snd_soc_dai_link_component) *
+						hdmi_num, GFP_KERNEL);
+		if (!idisp_components)
+			goto devm_err;
+	}
+	for (i = 1; i <= hdmi_num; i++) {
+		links[id].name = devm_kasprintf(dev, GFP_KERNEL,
+						"iDisp%d", i);
+		if (!links[id].name)
+			goto devm_err;
+
+		links[id].id = id;
+		links[id].cpus = &cpus[id];
+		links[id].num_cpus = 1;
+		links[id].cpus->dai_name = devm_kasprintf(dev, GFP_KERNEL,
+							  "iDisp%d Pin", i);
+		if (!links[id].cpus->dai_name)
+			goto devm_err;
+
+		idisp_components[i - 1].name = "ehdaudio0D2";
+		idisp_components[i - 1].dai_name = devm_kasprintf(dev,
+								  GFP_KERNEL,
+								  "intel-hdmi-hifi%d",
+								  i);
+		if (!idisp_components[i - 1].dai_name)
+			goto devm_err;
+
+		links[id].codecs = &idisp_components[i - 1];
+		links[id].num_codecs = 1;
+		links[id].platforms = platform_component;
+		links[id].num_platforms = ARRAY_SIZE(platform_component);
+		links[id].init = sof_hdmi_init;
+		links[id].dpcm_playback = 1;
+		links[id].no_pcm = 1;
+		id++;
+	}
+
+	/* speaker amp */
+	if (sof_nau8825_quirk & SOF_SPEAKER_AMP_PRESENT) {
+		links[id].name = devm_kasprintf(dev, GFP_KERNEL,
+						"SSP%d-Codec", ssp_amp);
+		if (!links[id].name)
+			goto devm_err;
+
+		links[id].id = id;
+		if (sof_nau8825_quirk & SOF_RT1019P_SPEAKER_AMP_PRESENT) {
+			links[id].codecs = rt1019p_component;
+			links[id].num_codecs = ARRAY_SIZE(rt1019p_component);
+			links[id].init = speaker_codec_init;
+		} else if (sof_nau8825_quirk &
+				SOF_MAX98373_SPEAKER_AMP_PRESENT) {
+			links[id].codecs = max_98373_components;
+			links[id].num_codecs = ARRAY_SIZE(max_98373_components);
+			links[id].init = max_98373_spk_codec_init;
+			links[id].ops = &max_98373_ops;
+			/* feedback stream */
+			links[id].dpcm_capture = 1;
+		} else if (sof_nau8825_quirk &
+				SOF_MAX98360A_SPEAKER_AMP_PRESENT) {
+			max_98360a_dai_link(&links[id]);
+		} else {
+			goto devm_err;
+		}
+
+		links[id].platforms = platform_component;
+		links[id].num_platforms = ARRAY_SIZE(platform_component);
+		links[id].dpcm_playback = 1;
+		links[id].no_pcm = 1;
+		links[id].cpus = &cpus[id];
+		links[id].num_cpus = 1;
+		links[id].cpus->dai_name = devm_kasprintf(dev, GFP_KERNEL,
+							  "SSP%d Pin",
+							  ssp_amp);
+		if (!links[id].cpus->dai_name)
+			goto devm_err;
+		id++;
+	}
+
+	/* BT audio offload */
+	if (sof_nau8825_quirk & SOF_SSP_BT_OFFLOAD_PRESENT) {
+		int port = (sof_nau8825_quirk & SOF_BT_OFFLOAD_SSP_MASK) >>
+				SOF_BT_OFFLOAD_SSP_SHIFT;
+
+		links[id].id = id;
+		links[id].cpus = &cpus[id];
+		links[id].cpus->dai_name = devm_kasprintf(dev, GFP_KERNEL,
+							  "SSP%d Pin", port);
+		if (!links[id].cpus->dai_name)
+			goto devm_err;
+		links[id].name = devm_kasprintf(dev, GFP_KERNEL, "SSP%d-BT", port);
+		if (!links[id].name)
+			goto devm_err;
+		links[id].codecs = dummy_component;
+		links[id].num_codecs = ARRAY_SIZE(dummy_component);
+		links[id].platforms = platform_component;
+		links[id].num_platforms = ARRAY_SIZE(platform_component);
+		links[id].dpcm_playback = 1;
+		links[id].dpcm_capture = 1;
+		links[id].no_pcm = 1;
+		links[id].num_cpus = 1;
+	}
+
+	return links;
+devm_err:
+	return NULL;
+}
+
+static int sof_audio_probe(struct platform_device *pdev)
+{
+	struct snd_soc_dai_link *dai_links;
+	struct snd_soc_acpi_mach *mach;
+	struct sof_card_private *ctx;
+	int dmic_be_num, hdmi_num;
+	int ret, ssp_amp, ssp_codec;
+
+	ctx = devm_kzalloc(&pdev->dev, sizeof(*ctx), GFP_KERNEL);
+	if (!ctx)
+		return -ENOMEM;
+
+	if (pdev->id_entry && pdev->id_entry->driver_data)
+		sof_nau8825_quirk = (unsigned long)pdev->id_entry->driver_data;
+
+	mach = pdev->dev.platform_data;
+
+	/* A speaker amp might not be present when the quirk claims one is.
+	 * Detect this via whether the machine driver match includes quirk_data.
+	 */
+	if ((sof_nau8825_quirk & SOF_SPEAKER_AMP_PRESENT) && !mach->quirk_data)
+		sof_nau8825_quirk &= ~SOF_SPEAKER_AMP_PRESENT;
+
+	dev_dbg(&pdev->dev, "sof_nau8825_quirk = %lx\n", sof_nau8825_quirk);
+
+	/* default number of DMIC DAI's */
+	dmic_be_num = 2;
+	hdmi_num = (sof_nau8825_quirk & SOF_NAU8825_NUM_HDMIDEV_MASK) >>
+			SOF_NAU8825_NUM_HDMIDEV_SHIFT;
+	/* default number of HDMI DAI's */
+	if (!hdmi_num)
+		hdmi_num = 3;
+
+	ssp_amp = (sof_nau8825_quirk & SOF_NAU8825_SSP_AMP_MASK) >>
+			SOF_NAU8825_SSP_AMP_SHIFT;
+
+	ssp_codec = sof_nau8825_quirk & SOF_NAU8825_SSP_CODEC_MASK;
+
+	/* compute number of dai links */
+	sof_audio_card_nau8825.num_links = 1 + dmic_be_num + hdmi_num;
+
+	if (sof_nau8825_quirk & SOF_SPEAKER_AMP_PRESENT)
+		sof_audio_card_nau8825.num_links++;
+
+	if (sof_nau8825_quirk & SOF_MAX98373_SPEAKER_AMP_PRESENT)
+		max_98373_set_codec_conf(&sof_audio_card_nau8825);
+
+	if (sof_nau8825_quirk & SOF_SSP_BT_OFFLOAD_PRESENT)
+		sof_audio_card_nau8825.num_links++;
+
+	dai_links = sof_card_dai_links_create(&pdev->dev, ssp_codec, ssp_amp,
+					      dmic_be_num, hdmi_num);
+	if (!dai_links)
+		return -ENOMEM;
+
+	sof_audio_card_nau8825.dai_link = dai_links;
+
+	INIT_LIST_HEAD(&ctx->hdmi_pcm_list);
+
+	sof_audio_card_nau8825.dev = &pdev->dev;
+
+	/* set platform name for each dailink */
+	ret = snd_soc_fixup_dai_links_platform_name(&sof_audio_card_nau8825,
+						    mach->mach_params.platform);
+	if (ret)
+		return ret;
+
+	snd_soc_card_set_drvdata(&sof_audio_card_nau8825, ctx);
+
+	return devm_snd_soc_register_card(&pdev->dev,
+					  &sof_audio_card_nau8825);
+}
+
+static const struct platform_device_id board_ids[] = {
+	{
+		.name = "sof_nau8825",
+		.driver_data = (kernel_ulong_t)(SOF_NAU8825_SSP_CODEC(0) |
+					SOF_NAU8825_NUM_HDMIDEV(4) |
+					SOF_BT_OFFLOAD_SSP(2) |
+					SOF_SSP_BT_OFFLOAD_PRESENT),
+
+	},
+	{
+		.name = "adl_rt1019p_nau8825",
+		.driver_data = (kernel_ulong_t)(SOF_NAU8825_SSP_CODEC(0) |
+					SOF_SPEAKER_AMP_PRESENT |
+					SOF_RT1019P_SPEAKER_AMP_PRESENT |
+					SOF_NAU8825_SSP_AMP(2) |
+					SOF_NAU8825_NUM_HDMIDEV(4)),
+	},
+	{
+		.name = "adl_max98373_nau8825",
+		.driver_data = (kernel_ulong_t)(SOF_NAU8825_SSP_CODEC(0) |
+					SOF_SPEAKER_AMP_PRESENT |
+					SOF_MAX98373_SPEAKER_AMP_PRESENT |
+					SOF_NAU8825_SSP_AMP(1) |
+					SOF_NAU8825_NUM_HDMIDEV(4) |
+					SOF_BT_OFFLOAD_SSP(2) |
+					SOF_SSP_BT_OFFLOAD_PRESENT),
+	},
+	{
+		/* The limitation of length of char array, shorten the name */
+		.name = "adl_mx98360a_nau8825",
+		.driver_data = (kernel_ulong_t)(SOF_NAU8825_SSP_CODEC(0) |
+					SOF_SPEAKER_AMP_PRESENT |
+					SOF_MAX98360A_SPEAKER_AMP_PRESENT |
+					SOF_NAU8825_SSP_AMP(1) |
+					SOF_NAU8825_NUM_HDMIDEV(4) |
+					SOF_BT_OFFLOAD_SSP(2) |
+					SOF_SSP_BT_OFFLOAD_PRESENT),
+
+	},
+	{ }
+};
+MODULE_DEVICE_TABLE(platform, board_ids);
+
+static struct platform_driver sof_audio = {
+	.probe = sof_audio_probe,
+	.driver = {
+		.name = "sof_nau8825",
+		.pm = &snd_soc_pm_ops,
+	},
+	.id_table = board_ids,
+};
+module_platform_driver(sof_audio)
+
+/* Module information */
+MODULE_DESCRIPTION("SOF Audio Machine driver for NAU8825");
+MODULE_AUTHOR("David Lin <ctlin0@nuvoton.com>");
+MODULE_AUTHOR("Mac Chiang <mac.chiang@intel.com>");
+MODULE_LICENSE("GPL");
+MODULE_IMPORT_NS(SND_SOC_INTEL_HDA_DSP_COMMON);
+MODULE_IMPORT_NS(SND_SOC_INTEL_SOF_MAXIM_COMMON);
diff -ruN a/sound/soc/intel/boards/sof_rt5682.c b/sound/soc/intel/boards/sof_rt5682.c
--- a/sound/soc/intel/boards/sof_rt5682.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/boards/sof_rt5682.c	2021-12-23 08:36:03.000000000 +0100
@@ -18,9 +18,11 @@
 #include <sound/soc.h>
 #include <sound/sof.h>
 #include <sound/rt5682.h>
+#include <sound/rt5682s.h>
 #include <sound/soc-acpi.h>
 #include "../../codecs/rt1015.h"
 #include "../../codecs/rt5682.h"
+#include "../../codecs/rt5682s.h"
 #include "../../codecs/hdac_hdmi.h"
 #include "../common/soc-intel-quirks.h"
 #include "hda_dsp_common.h"
@@ -56,6 +58,10 @@
 #define SOF_BT_OFFLOAD_SSP(quirk)	\
 	(((quirk) << SOF_BT_OFFLOAD_SSP_SHIFT) & SOF_BT_OFFLOAD_SSP_MASK)
 #define SOF_SSP_BT_OFFLOAD_PRESENT		BIT(22)
+#define SOF_RT5682S_HEADPHONE_CODEC_PRESENT	BIT(23)
+#define SOF_MAX98390_SPEAKER_AMP_PRESENT	BIT(24)
+#define SOF_MAX98390_TWEETER_SPEAKER_PRESENT	BIT(25)
+
 
 /* Default: MCLK on, MCLK 19.2M, SSP0  */
 static unsigned long sof_rt5682_quirk = SOF_RT5682_MCLK_EN |
@@ -176,6 +182,36 @@
 					SOF_RT5682_SSP_AMP(2) |
 					SOF_RT5682_NUM_HDMIDEV(4)),
 	},
+	{
+		.callback = sof_rt5682_quirk_cb,
+		.matches = {
+			DMI_MATCH(DMI_PRODUCT_FAMILY, "Google_Brya"),
+			DMI_MATCH(DMI_OEM_STRING, "AUDIO-MAX98390_ALC5682I_I2S"),
+		},
+		.driver_data = (void *)(SOF_RT5682_MCLK_EN |
+					SOF_RT5682_SSP_CODEC(0) |
+					SOF_SPEAKER_AMP_PRESENT |
+					SOF_MAX98390_SPEAKER_AMP_PRESENT |
+					SOF_RT5682_SSP_AMP(2) |
+					SOF_RT5682_NUM_HDMIDEV(4)),
+	},
+	{
+		.callback = sof_rt5682_quirk_cb,
+		.matches = {
+			DMI_MATCH(DMI_PRODUCT_FAMILY, "Google_Brya"),
+			DMI_MATCH(DMI_OEM_STRING, "AUDIO-MAX98390_ALC5682I_I2S_4SPK"),
+		},
+		.driver_data = (void *)(SOF_RT5682_MCLK_EN |
+					SOF_RT5682_SSP_CODEC(0) |
+					SOF_SPEAKER_AMP_PRESENT |
+					SOF_MAX98390_SPEAKER_AMP_PRESENT |
+					SOF_MAX98390_TWEETER_SPEAKER_PRESENT |
+					SOF_RT5682_SSP_AMP(1) |
+					SOF_RT5682_NUM_HDMIDEV(4) |
+					SOF_BT_OFFLOAD_SSP(2) |
+					SOF_SSP_BT_OFFLOAD_PRESENT),
+
+	},
 	{}
 };
 
@@ -208,9 +244,16 @@
 	/* need to enable ASRC function for 24MHz mclk rate */
 	if ((sof_rt5682_quirk & SOF_RT5682_MCLK_EN) &&
 	    (sof_rt5682_quirk & SOF_RT5682_MCLK_24MHZ)) {
-		rt5682_sel_asrc_clk_src(component, RT5682_DA_STEREO1_FILTER |
-					RT5682_AD_STEREO1_FILTER,
-					RT5682_CLK_SEL_I2S1_ASRC);
+		if (sof_rt5682_quirk & SOF_RT5682S_HEADPHONE_CODEC_PRESENT)
+			rt5682s_sel_asrc_clk_src(component,
+						 RT5682S_DA_STEREO1_FILTER |
+						 RT5682S_AD_STEREO1_FILTER,
+						 RT5682S_CLK_SEL_I2S1_ASRC);
+		else
+			rt5682_sel_asrc_clk_src(component,
+						RT5682_DA_STEREO1_FILTER |
+						RT5682_AD_STEREO1_FILTER,
+						RT5682_CLK_SEL_I2S1_ASRC);
 	}
 
 	if (sof_rt5682_quirk & SOF_RT5682_MCLK_BYTCHT_EN) {
@@ -277,7 +320,7 @@
 	struct snd_soc_pcm_runtime *rtd = asoc_substream_to_rtd(substream);
 	struct sof_card_private *ctx = snd_soc_card_get_drvdata(rtd->card);
 	struct snd_soc_dai *codec_dai = asoc_rtd_to_codec(rtd, 0);
-	int clk_id, clk_freq, pll_out, ret;
+	int pll_id, pll_source, pll_in, pll_out, clk_id, ret;
 
 	if (sof_rt5682_quirk & SOF_RT5682_MCLK_EN) {
 		if (sof_rt5682_quirk & SOF_RT5682_MCLK_BYTCHT_EN) {
@@ -289,35 +332,52 @@
 			}
 		}
 
-		clk_id = RT5682_PLL1_S_MCLK;
+		if (sof_rt5682_quirk & SOF_RT5682S_HEADPHONE_CODEC_PRESENT)
+			pll_source = RT5682S_PLL_S_MCLK;
+		else
+			pll_source = RT5682_PLL1_S_MCLK;
 
 		/* get the tplg configured mclk. */
-		clk_freq = sof_dai_get_mclk(rtd);
+		pll_in = sof_dai_get_mclk(rtd);
 
 		/* mclk from the quirk is the first choice */
 		if (sof_rt5682_quirk & SOF_RT5682_MCLK_24MHZ) {
-			if (clk_freq != 24000000)
+			if (pll_in != 24000000)
 				dev_warn(rtd->dev, "configure wrong mclk in tplg, please use 24MHz.\n");
-			clk_freq = 24000000;
-		} else if (clk_freq == 0) {
+			pll_in = 24000000;
+		} else if (pll_in == 0) {
 			/* use default mclk if not specified correct in topology */
-			clk_freq = 19200000;
-		} else if (clk_freq < 0) {
-			return clk_freq;
+			pll_in = 19200000;
+		} else if (pll_in < 0) {
+			return pll_in;
 		}
 	} else {
-		clk_id = RT5682_PLL1_S_BCLK1;
-		clk_freq = params_rate(params) * 50;
+		if (sof_rt5682_quirk & SOF_RT5682S_HEADPHONE_CODEC_PRESENT)
+			pll_source = RT5682S_PLL_S_BCLK1;
+		else
+			pll_source = RT5682_PLL1_S_BCLK1;
+
+		pll_in = params_rate(params) * 50;
+	}
+
+	if (sof_rt5682_quirk & SOF_RT5682S_HEADPHONE_CODEC_PRESENT) {
+		pll_id = RT5682S_PLL2;
+		clk_id = RT5682S_SCLK_S_PLL2;
+	} else {
+		pll_id = RT5682_PLL1;
+		clk_id = RT5682_SCLK_S_PLL1;
 	}
 
 	pll_out = params_rate(params) * 512;
 
-	ret = snd_soc_dai_set_pll(codec_dai, 0, clk_id, clk_freq, pll_out);
+	/* Configure pll for codec */
+	ret = snd_soc_dai_set_pll(codec_dai, pll_id, pll_source, pll_in,
+				  pll_out);
 	if (ret < 0)
 		dev_err(rtd->dev, "snd_soc_dai_set_pll err = %d\n", ret);
 
 	/* Configure sysclk for codec */
-	ret = snd_soc_dai_set_sysclk(codec_dai, RT5682_SCLK_S_PLL1,
+	ret = snd_soc_dai_set_sysclk(codec_dai, clk_id,
 				     pll_out, SND_SOC_CLOCK_IN);
 	if (ret < 0)
 		dev_err(rtd->dev, "snd_soc_dai_set_sysclk err = %d\n", ret);
@@ -459,6 +519,7 @@
 		if (err < 0)
 			return err;
 	}
+
 	return hdac_hdmi_jack_port_init(component, &card->dapm);
 }
 
@@ -560,6 +621,13 @@
 	}
 };
 
+static struct snd_soc_dai_link_component rt5682s_component[] = {
+	{
+		.name = "i2c-RTL5682:00",
+		.dai_name = "rt5682s-aif1",
+	}
+};
+
 static struct snd_soc_dai_link_component dmic_component[] = {
 	{
 		.name = "dmic-codec",
@@ -610,8 +678,13 @@
 		goto devm_err;
 
 	links[id].id = id;
-	links[id].codecs = rt5682_component;
-	links[id].num_codecs = ARRAY_SIZE(rt5682_component);
+	if (sof_rt5682_quirk & SOF_RT5682S_HEADPHONE_CODEC_PRESENT) {
+		links[id].codecs = rt5682s_component;
+		links[id].num_codecs = ARRAY_SIZE(rt5682s_component);
+	} else {
+		links[id].codecs = rt5682_component;
+		links[id].num_codecs = ARRAY_SIZE(rt5682_component);
+	}
 	links[id].platforms = platform_component;
 	links[id].num_platforms = ARRAY_SIZE(platform_component);
 	links[id].init = sof_rt5682_codec_init;
@@ -745,6 +818,20 @@
 		} else if (sof_rt5682_quirk &
 				SOF_RT1011_SPEAKER_AMP_PRESENT) {
 			sof_rt1011_dai_link(&links[id]);
+		} else if (sof_rt5682_quirk &
+				SOF_MAX98390_SPEAKER_AMP_PRESENT) {
+			if (sof_rt5682_quirk &
+				SOF_MAX98390_TWEETER_SPEAKER_PRESENT) {
+				links[id].codecs = max_98390_4spk_components;
+				links[id].num_codecs = ARRAY_SIZE(max_98390_4spk_components);
+			} else {
+				links[id].codecs = max_98390_components;
+				links[id].num_codecs = ARRAY_SIZE(max_98390_components);
+			}
+			links[id].init = max_98390_spk_codec_init;
+			links[id].ops = &max_98390_ops;
+			links[id].dpcm_capture = 1;
+
 		} else {
 			max_98357a_dai_link(&links[id]);
 		}
@@ -825,6 +912,10 @@
 	if ((sof_rt5682_quirk & SOF_SPEAKER_AMP_PRESENT) && !mach->quirk_data)
 		sof_rt5682_quirk &= ~SOF_SPEAKER_AMP_PRESENT;
 
+	/* Detect the headset codec variant */
+	if (acpi_dev_present("RTL5682", NULL, -1))
+		sof_rt5682_quirk |= SOF_RT5682S_HEADPHONE_CODEC_PRESENT;
+
 	if (soc_intel_is_byt() || soc_intel_is_cht()) {
 		is_legacy_cpu = 1;
 		dmic_be_num = 0;
@@ -881,6 +972,14 @@
 		sof_rt1011_codec_conf(&sof_audio_card_rt5682);
 	else if (sof_rt5682_quirk & SOF_RT1015P_SPEAKER_AMP_PRESENT)
 		sof_rt1015p_codec_conf(&sof_audio_card_rt5682);
+	else if (sof_rt5682_quirk & SOF_MAX98390_SPEAKER_AMP_PRESENT) {
+		if (sof_rt5682_quirk & SOF_MAX98390_TWEETER_SPEAKER_PRESENT)
+			max_98390_set_codec_conf(&sof_audio_card_rt5682,
+						ARRAY_SIZE(max_98390_4spk_components));
+		else
+			max_98390_set_codec_conf(&sof_audio_card_rt5682,
+						ARRAY_SIZE(max_98390_components));
+	}
 
 	if (sof_rt5682_quirk & SOF_SSP_BT_OFFLOAD_PRESENT)
 		sof_audio_card_rt5682.num_links++;
@@ -920,7 +1019,7 @@
 		.name = "sof_rt5682",
 	},
 	{
-		.name = "tgl_mx98357a_rt5682",
+		.name = "tgl_mx98357_rt5682",
 		.driver_data = (kernel_ulong_t)(SOF_RT5682_MCLK_EN |
 					SOF_RT5682_SSP_CODEC(0) |
 					SOF_SPEAKER_AMP_PRESENT |
@@ -950,7 +1049,7 @@
 					SOF_SSP_BT_OFFLOAD_PRESENT),
 	},
 	{
-		.name = "jsl_rt5682_mx98360a",
+		.name = "jsl_rt5682_mx98360",
 		.driver_data = (kernel_ulong_t)(SOF_RT5682_MCLK_EN |
 					SOF_RT5682_MCLK_24MHZ |
 					SOF_RT5682_SSP_CODEC(0) |
@@ -1000,13 +1099,47 @@
 					SOF_SSP_BT_OFFLOAD_PRESENT),
 	},
 	{
-		.name = "adl_mx98357a_rt5682",
+		.name = "adl_mx98357_rt5682",
 		.driver_data = (kernel_ulong_t)(SOF_RT5682_MCLK_EN |
 					SOF_RT5682_SSP_CODEC(0) |
 					SOF_SPEAKER_AMP_PRESENT |
 					SOF_RT5682_SSP_AMP(2) |
 					SOF_RT5682_NUM_HDMIDEV(4)),
 	},
+	{
+		.name = "adl_mx98360_rt5682",
+		.driver_data = (kernel_ulong_t)(SOF_RT5682_MCLK_EN |
+					SOF_RT5682_SSP_CODEC(0) |
+					SOF_SPEAKER_AMP_PRESENT |
+					SOF_MAX98360A_SPEAKER_AMP_PRESENT |
+					SOF_RT5682_SSP_AMP(1) |
+					SOF_RT5682_NUM_HDMIDEV(4) |
+					SOF_BT_OFFLOAD_SSP(2) |
+					SOF_SSP_BT_OFFLOAD_PRESENT),
+	},
+	{
+		.name = "adl_mx98390_rt5682",
+		.driver_data = (kernel_ulong_t)(SOF_RT5682_MCLK_EN |
+					SOF_RT5682_SSP_CODEC(0) |
+					SOF_SPEAKER_AMP_PRESENT |
+					SOF_MAX98390_SPEAKER_AMP_PRESENT |
+					SOF_RT5682_SSP_AMP(1) |
+					SOF_RT5682_NUM_HDMIDEV(4) |
+					SOF_BT_OFFLOAD_SSP(2) |
+					SOF_SSP_BT_OFFLOAD_PRESENT),
+	},
+	{
+		.name = "adl_mx98390_rt5682s",
+		.driver_data = (kernel_ulong_t)(SOF_RT5682_MCLK_EN |
+					SOF_RT5682_SSP_CODEC(0) |
+					SOF_RT5682S_HEADPHONE_CODEC_PRESENT |
+					SOF_SPEAKER_AMP_PRESENT |
+					SOF_MAX98390_SPEAKER_AMP_PRESENT |
+					SOF_RT5682_SSP_AMP(1) |
+					SOF_RT5682_NUM_HDMIDEV(4) |
+					SOF_BT_OFFLOAD_SSP(2) |
+					SOF_SSP_BT_OFFLOAD_PRESENT),
+	},
 	{ }
 };
 MODULE_DEVICE_TABLE(platform, board_ids);
@@ -1026,6 +1159,7 @@
 MODULE_AUTHOR("Bard Liao <bard.liao@intel.com>");
 MODULE_AUTHOR("Sathya Prakash M R <sathya.prakash.m.r@intel.com>");
 MODULE_AUTHOR("Brent Lu <brent.lu@intel.com>");
+MODULE_AUTHOR("Mac Chiang <mac.chiang@intel.com>");
 MODULE_LICENSE("GPL v2");
 MODULE_IMPORT_NS(SND_SOC_INTEL_HDA_DSP_COMMON);
 MODULE_IMPORT_NS(SND_SOC_INTEL_SOF_MAXIM_COMMON);
diff -ruN a/sound/soc/intel/common/soc-acpi-intel-adl-match.c b/sound/soc/intel/common/soc-acpi-intel-adl-match.c
--- a/sound/soc/intel/common/soc-acpi-intel-adl-match.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/common/soc-acpi-intel-adl-match.c	2021-12-23 08:36:03.000000000 +0100
@@ -280,9 +280,29 @@
 	.codecs = {"MX98357A"}
 };
 
+static const struct snd_soc_acpi_codecs adl_max98360a_amp = {
+	.num_codecs = 1,
+	.codecs = {"MX98360A"}
+};
+
+static const struct snd_soc_acpi_codecs adl_rt5682_rt5682s_hp = {
+	.num_codecs = 2,
+	.codecs = {"10EC5682", "RTL5682"},
+};
+
+static const struct snd_soc_acpi_codecs adl_max98390_amp = {
+	.num_codecs = 1,
+	.codecs = {"MX98390"}
+};
+
+static const struct snd_soc_acpi_codecs adl_rt1019p_amp = {
+	.num_codecs = 1,
+	.codecs = {"RTL1019"}
+};
+
 struct snd_soc_acpi_mach snd_soc_acpi_intel_adl_machines[] = {
 	{
-		.id = "10EC5682",
+		.comp_ids = &adl_rt5682_rt5682s_hp,
 		.drv_name = "adl_mx98373_rt5682",
 		.machine_quirk = snd_soc_acpi_codec_list,
 		.quirk_data = &adl_max98373_amp,
@@ -290,13 +310,67 @@
 		.sof_tplg_filename = "sof-adl-max98373-rt5682.tplg",
 	},
 	{
-		.id = "10EC5682",
-		.drv_name = "adl_mx98357a_rt5682",
+		.comp_ids = &adl_rt5682_rt5682s_hp,
+		.drv_name = "adl_mx98357_rt5682",
 		.machine_quirk = snd_soc_acpi_codec_list,
 		.quirk_data = &adl_max98357a_amp,
 		.sof_fw_filename = "sof-adl.ri",
 		.sof_tplg_filename = "sof-adl-max98357a-rt5682.tplg",
 	},
+	{
+		.comp_ids = &adl_rt5682_rt5682s_hp,
+		.drv_name = "adl_mx98360_rt5682",
+		.machine_quirk = snd_soc_acpi_codec_list,
+		.quirk_data = &adl_max98360a_amp,
+		.sof_fw_filename = "sof-adl.ri",
+		.sof_tplg_filename = "sof-adl-max98360a-rt5682.tplg",
+	},
+	{
+		.id = "10EC5682",
+		.drv_name = "adl_mx98390_rt5682",
+		.machine_quirk = snd_soc_acpi_codec_list,
+		.quirk_data = &adl_max98390_amp,
+		.sof_fw_filename = "sof-adl.ri",
+		.sof_tplg_filename = "sof-adl-max98390-rt5682.tplg",
+	},
+	{
+		.id = "RTL5682",
+		.drv_name = "adl_mx98390_rt5682s",
+		.machine_quirk = snd_soc_acpi_codec_list,
+		.quirk_data = &adl_max98390_amp,
+		.sof_fw_filename = "sof-adl.ri",
+		.sof_tplg_filename = "sof-adl-max98390-rt5682.tplg",
+	},
+	{
+		.id = "10508825",
+		.drv_name = "adl_rt1019p_nau8825",
+		.machine_quirk = snd_soc_acpi_codec_list,
+		.quirk_data = &adl_rt1019p_amp,
+		.sof_fw_filename = "sof-adl.ri",
+		.sof_tplg_filename = "sof-adl-rt1019-nau8825.tplg",
+	},
+	{
+		.id = "10508825",
+		.drv_name = "adl_max98373_nau8825",
+		.machine_quirk = snd_soc_acpi_codec_list,
+		.quirk_data = &adl_max98373_amp,
+		.sof_fw_filename = "sof-adl.ri",
+		.sof_tplg_filename = "sof-adl-max98373-nau8825.tplg",
+	},
+	{
+		.id = "10508825",
+		.drv_name = "adl_mx98360a_nau8825",
+		.machine_quirk = snd_soc_acpi_codec_list,
+		.quirk_data = &adl_max98360a_amp,
+		.sof_fw_filename = "sof-adl.ri",
+		.sof_tplg_filename = "sof-adl-mx98360a-nau8825.tplg",
+	},
+	{
+		.id = "10508825",
+		.drv_name = "sof_nau8825",
+		.sof_fw_filename = "sof-adl.ri",
+		.sof_tplg_filename = "sof-adl-nau8825.tplg",
+	},
 	{},
 };
 EXPORT_SYMBOL_GPL(snd_soc_acpi_intel_adl_machines);
diff -ruN a/sound/soc/intel/common/soc-acpi-intel-bxt-match.c b/sound/soc/intel/common/soc-acpi-intel-bxt-match.c
--- a/sound/soc/intel/common/soc-acpi-intel-bxt-match.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/common/soc-acpi-intel-bxt-match.c	2021-12-23 08:36:03.000000000 +0100
@@ -82,6 +82,12 @@
 		.sof_fw_filename = "sof-apl.ri",
 		.sof_tplg_filename = "sof-apl-tdf8532.tplg",
 	},
+	{
+		.id = "ESSX8336",
+		.drv_name = "sof-essx8336",
+		.sof_fw_filename = "sof-apl.ri",
+		.sof_tplg_filename = "sof-apl-es8336.tplg",
+	},
 	{},
 };
 EXPORT_SYMBOL_GPL(snd_soc_acpi_intel_bxt_machines);
diff -ruN a/sound/soc/intel/common/soc-acpi-intel-glk-match.c b/sound/soc/intel/common/soc-acpi-intel-glk-match.c
--- a/sound/soc/intel/common/soc-acpi-intel-glk-match.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/common/soc-acpi-intel-glk-match.c	2021-12-23 08:36:03.000000000 +0100
@@ -41,6 +41,14 @@
 		.sof_tplg_filename = "sof-glk-rt5682.tplg",
 	},
 	{
+		.id = "RTL5682",
+		.drv_name = "glk_rt5682_max98357a",
+		.machine_quirk = snd_soc_acpi_codec_list,
+		.quirk_data = &glk_codecs,
+		.sof_fw_filename = "sof-glk.ri",
+		.sof_tplg_filename = "sof-glk-rt5682.tplg",
+	},
+	{
 		.id = "10134242",
 		.drv_name = "glk_cs4242_mx98357a",
 		.fw_filename = "intel/dsp_fw_glk.bin",
@@ -49,7 +57,12 @@
 		.sof_fw_filename = "sof-glk.ri",
 		.sof_tplg_filename = "sof-glk-cs42l42.tplg",
 	},
-
+	{
+		.id = "ESSX8336",
+		.drv_name = "sof-essx8336",
+		.sof_fw_filename = "sof-glk.ri",
+		.sof_tplg_filename = "sof-glk-es8336.tplg",
+	},
 	{},
 };
 EXPORT_SYMBOL_GPL(snd_soc_acpi_intel_glk_machines);
diff -ruN a/sound/soc/intel/common/soc-acpi-intel-jsl-match.c b/sound/soc/intel/common/soc-acpi-intel-jsl-match.c
--- a/sound/soc/intel/common/soc-acpi-intel-jsl-match.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/common/soc-acpi-intel-jsl-match.c	2021-12-23 08:36:03.000000000 +0100
@@ -29,6 +29,11 @@
 	.codecs = {"MX98360A"}
 };
 
+static const struct snd_soc_acpi_codecs rt5682_rt5682s_hp = {
+	.num_codecs = 2,
+	.codecs = {"10EC5682", "RTL5682"},
+};
+
 /*
  * When adding new entry to the snd_soc_acpi_intel_jsl_machines array,
  * use .quirk_data member to distinguish different machine driver,
@@ -50,7 +55,7 @@
 		.sof_tplg_filename = "sof-jsl-da7219-mx98360a.tplg",
 	},
 	{
-		.id = "10EC5682",
+		.comp_ids = &rt5682_rt5682s_hp,
 		.drv_name = "jsl_rt5682_rt1015",
 		.sof_fw_filename = "sof-jsl.ri",
 		.machine_quirk = snd_soc_acpi_codec_list,
@@ -58,7 +63,7 @@
 		.sof_tplg_filename = "sof-jsl-rt5682-rt1015.tplg",
 	},
 	{
-		.id = "10EC5682",
+		.comp_ids = &rt5682_rt5682s_hp,
 		.drv_name = "jsl_rt5682_rt1015p",
 		.sof_fw_filename = "sof-jsl.ri",
 		.machine_quirk = snd_soc_acpi_codec_list,
@@ -66,8 +71,8 @@
 		.sof_tplg_filename = "sof-jsl-rt5682-rt1015.tplg",
 	},
 	{
-		.id = "10EC5682",
-		.drv_name = "jsl_rt5682_mx98360a",
+		.comp_ids = &rt5682_rt5682s_hp,
+		.drv_name = "jsl_rt5682_mx98360",
 		.sof_fw_filename = "sof-jsl.ri",
 		.machine_quirk = snd_soc_acpi_codec_list,
 		.quirk_data = &mx98360a_spk,
@@ -81,6 +86,12 @@
 		.quirk_data = &mx98360a_spk,
 		.sof_tplg_filename = "sof-jsl-cs42l42-mx98360a.tplg",
 	},
+	{
+		.id = "ESSX8336",
+		.drv_name = "sof-essx8336",
+		.sof_fw_filename = "sof-jsl.ri",
+		.sof_tplg_filename = "sof-jsl-es8336.tplg",
+	},
 	{},
 };
 EXPORT_SYMBOL_GPL(snd_soc_acpi_intel_jsl_machines);
diff -ruN a/sound/soc/intel/common/soc-acpi-intel-tgl-match.c b/sound/soc/intel/common/soc-acpi-intel-tgl-match.c
--- a/sound/soc/intel/common/soc-acpi-intel-tgl-match.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/common/soc-acpi-intel-tgl-match.c	2021-12-23 08:36:03.000000000 +0100
@@ -358,17 +358,22 @@
 	.codecs = {"10EC1011"}
 };
 
+static const struct snd_soc_acpi_codecs tgl_rt5682_rt5682s_hp = {
+	.num_codecs = 2,
+	.codecs = {"10EC5682", "RTL5682"},
+};
+
 struct snd_soc_acpi_mach snd_soc_acpi_intel_tgl_machines[] = {
 	{
-		.id = "10EC5682",
-		.drv_name = "tgl_mx98357a_rt5682",
+		.comp_ids = &tgl_rt5682_rt5682s_hp,
+		.drv_name = "tgl_mx98357_rt5682",
 		.machine_quirk = snd_soc_acpi_codec_list,
 		.quirk_data = &tgl_codecs,
 		.sof_fw_filename = "sof-tgl.ri",
 		.sof_tplg_filename = "sof-tgl-max98357a-rt5682.tplg",
 	},
 	{
-		.id = "10EC5682",
+		.comp_ids = &tgl_rt5682_rt5682s_hp,
 		.drv_name = "tgl_mx98373_rt5682",
 		.machine_quirk = snd_soc_acpi_codec_list,
 		.quirk_data = &tgl_max98373_amp,
@@ -376,13 +381,19 @@
 		.sof_tplg_filename = "sof-tgl-max98373-rt5682.tplg",
 	},
 	{
-		.id = "10EC5682",
+		.comp_ids = &tgl_rt5682_rt5682s_hp,
 		.drv_name = "tgl_rt1011_rt5682",
 		.machine_quirk = snd_soc_acpi_codec_list,
 		.quirk_data = &tgl_rt1011_amp,
 		.sof_fw_filename = "sof-tgl.ri",
 		.sof_tplg_filename = "sof-tgl-rt1011-rt5682.tplg",
 	},
+	{
+		.id = "ESSX8336",
+		.drv_name = "sof-essx8336",
+		.sof_fw_filename = "sof-tgl.ri",
+		.sof_tplg_filename = "sof-tgl-es8336.tplg",
+	},
 	{},
 };
 EXPORT_SYMBOL_GPL(snd_soc_acpi_intel_tgl_machines);
diff -ruN a/sound/soc/intel/common/soc-intel-quirks.h b/sound/soc/intel/common/soc-intel-quirks.h
--- a/sound/soc/intel/common/soc-intel-quirks.h	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/common/soc-intel-quirks.h	2021-12-23 08:36:03.000000000 +0100
@@ -9,34 +9,13 @@
 #ifndef _SND_SOC_INTEL_QUIRKS_H
 #define _SND_SOC_INTEL_QUIRKS_H
 
+#include <linux/platform_data/x86/soc.h>
+
 #if IS_ENABLED(CONFIG_X86)
 
 #include <linux/dmi.h>
-#include <asm/cpu_device_id.h>
-#include <asm/intel-family.h>
 #include <asm/iosf_mbi.h>
 
-#define SOC_INTEL_IS_CPU(soc, type)				\
-static inline bool soc_intel_is_##soc(void)			\
-{								\
-	static const struct x86_cpu_id soc##_cpu_ids[] = {	\
-		X86_MATCH_INTEL_FAM6_MODEL(type, NULL),		\
-		{}						\
-	};							\
-	const struct x86_cpu_id *id;				\
-								\
-	id = x86_match_cpu(soc##_cpu_ids);			\
-	if (id)							\
-		return true;					\
-	return false;						\
-}
-
-SOC_INTEL_IS_CPU(byt, ATOM_SILVERMONT);
-SOC_INTEL_IS_CPU(cht, ATOM_AIRMONT);
-SOC_INTEL_IS_CPU(apl, ATOM_GOLDMONT);
-SOC_INTEL_IS_CPU(glk, ATOM_GOLDMONT_PLUS);
-SOC_INTEL_IS_CPU(cml, KABYLAKE_L);
-
 static inline bool soc_intel_is_byt_cr(struct platform_device *pdev)
 {
 	/*
@@ -114,30 +93,6 @@
 	return false;
 }
 
-static inline bool soc_intel_is_byt(void)
-{
-	return false;
-}
-
-static inline bool soc_intel_is_cht(void)
-{
-	return false;
-}
-
-static inline bool soc_intel_is_apl(void)
-{
-	return false;
-}
-
-static inline bool soc_intel_is_glk(void)
-{
-	return false;
-}
-
-static inline bool soc_intel_is_cml(void)
-{
-	return false;
-}
 #endif
 
- #endif /* _SND_SOC_INTEL_QUIRKS_H */
+#endif /* _SND_SOC_INTEL_QUIRKS_H */
diff -ruN a/sound/soc/intel/skylake/skl-topology.c b/sound/soc/intel/skylake/skl-topology.c
--- a/sound/soc/intel/skylake/skl-topology.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/intel/skylake/skl-topology.c	2021-12-23 08:36:03.000000000 +0100
@@ -3637,7 +3637,7 @@
 	return 0;
 }
 
-static void skl_tplg_complete(struct snd_soc_component *component)
+static int skl_tplg_complete(struct snd_soc_component *component)
 {
 	struct snd_soc_dobj *dobj;
 	struct snd_soc_acpi_mach *mach;
@@ -3646,7 +3646,7 @@
 
 	val = kmalloc(sizeof(*val), GFP_KERNEL);
 	if (!val)
-		return;
+		return -ENOMEM;
 
 	mach = dev_get_platdata(component->card->dev);
 	list_for_each_entry(dobj, &component->dobj_list, list) {
@@ -3671,7 +3671,9 @@
 			}
 		}
 	}
+
 	kfree(val);
+	return 0;
 }
 
 static struct snd_soc_tplg_ops skl_tplg_ops  = {
diff -ruN a/sound/soc/mediatek/Kconfig b/sound/soc/mediatek/Kconfig
--- a/sound/soc/mediatek/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/mediatek/Kconfig	2021-12-23 08:36:03.000000000 +0100
@@ -120,7 +120,7 @@
 
 config SND_SOC_MT8183_MT6358_TS3A227E_MAX98357A
 	tristate "ASoC Audio driver for MT8183 with MT6358 TS3A227E MAX98357A RT1015 codec"
-	depends on I2C
+	depends on I2C && GPIOLIB
 	depends on SND_SOC_MT8183
 	select SND_SOC_MT6358
 	select SND_SOC_MAX98357A
@@ -138,7 +138,7 @@
 
 config SND_SOC_MT8183_DA7219_MAX98357A
 	tristate "ASoC Audio driver for MT8183 with DA7219 MAX98357A RT1015 codec"
-	depends on SND_SOC_MT8183 && I2C
+	depends on SND_SOC_MT8183 && I2C && GPIOLIB
 	select SND_SOC_MT6358
 	select SND_SOC_MAX98357A
 	select SND_SOC_RT1015
@@ -173,7 +173,7 @@
 
 config SND_SOC_MT8192_MT6359_RT1015_RT5682
 	tristate "ASoC Audio driver for MT8192 with MT6359 RT1015 RT5682 codec"
-	depends on I2C
+	depends on I2C && GPIOLIB
 	depends on SND_SOC_MT8192 && MTK_PMIC_WRAP
 	select SND_SOC_MT6359
 	select SND_SOC_RT1015
@@ -200,7 +200,7 @@
 
 config SND_SOC_MT8195_MT6359_RT1019_RT5682
 	tristate "ASoC Audio driver for MT8195 with MT6359 RT1019 RT5682 codec"
-	depends on I2C
+	depends on I2C && GPIOLIB
 	depends on SND_SOC_MT8195 && MTK_PMIC_WRAP
 	select SND_SOC_MT6359
 	select SND_SOC_RT1015P
@@ -212,3 +212,18 @@
 	  with the MT6359 RT1019 RT5682 audio codec.
 	  Select Y if you have such device.
 	  If unsure select "N".
+
+config SND_SOC_MT8195_MT6359_RT1011_RT5682
+	tristate "ASoC Audio driver for MT8195 with MT6359 RT1011 RT5682 codec"
+	depends on I2C
+	depends on SND_SOC_MT8195 && MTK_PMIC_WRAP
+	select SND_SOC_MT6359
+	select SND_SOC_RT1011
+	select SND_SOC_RT5682_I2C
+	select SND_SOC_DMIC
+	select SND_SOC_HDMI_CODEC
+	help
+	  This adds ASoC driver for Mediatek MT8195 boards
+	  with the MT6359 RT1011 RT5682 audio codec.
+	  Select Y if you have such device.
+	  If unsure select "N".
diff -ruN a/sound/soc/mediatek/mt8195/Makefile b/sound/soc/mediatek/mt8195/Makefile
--- a/sound/soc/mediatek/mt8195/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/mediatek/mt8195/Makefile	2021-12-23 08:36:03.000000000 +0100
@@ -13,3 +13,4 @@
 
 # machine driver
 obj-$(CONFIG_SND_SOC_MT8195_MT6359_RT1019_RT5682) += mt8195-mt6359-rt1019-rt5682.o
+obj-$(CONFIG_SND_SOC_MT8195_MT6359_RT1011_RT5682) += mt8195-mt6359-rt1011-rt5682.o
diff -ruN a/sound/soc/mediatek/mt8195/mt8195-afe-pcm.c b/sound/soc/mediatek/mt8195/mt8195-afe-pcm.c
--- a/sound/soc/mediatek/mt8195/mt8195-afe-pcm.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/mediatek/mt8195/mt8195-afe-pcm.c	2021-12-23 08:36:03.000000000 +0100
@@ -3266,9 +3266,7 @@
 	.driver = {
 		   .name = "mt8195-audio",
 		   .of_match_table = mt8195_afe_pcm_dt_match,
-#ifdef CONFIG_PM
 		   .pm = &mt8195_afe_pm_ops,
-#endif
 	},
 	.probe = mt8195_afe_pcm_dev_probe,
 	.remove = mt8195_afe_pcm_dev_remove,
diff -ruN a/sound/soc/mediatek/mt8195/mt8195-audsys-clk.c b/sound/soc/mediatek/mt8195/mt8195-audsys-clk.c
--- a/sound/soc/mediatek/mt8195/mt8195-audsys-clk.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/mediatek/mt8195/mt8195-audsys-clk.c	2021-12-23 08:36:03.000000000 +0100
@@ -59,93 +59,93 @@
 
 static const struct afe_gate aud_clks[CLK_AUD_NR_CLK] = {
 	/* AUD0 */
-	GATE_AUD0(CLK_AUD_AFE, "aud_afe", "a1sys_hp_sel", 2),
-	GATE_AUD0(CLK_AUD_LRCK_CNT, "aud_lrck_cnt", "a1sys_hp_sel", 4),
-	GATE_AUD0(CLK_AUD_SPDIFIN_TUNER_APLL, "aud_spdifin_tuner_apll", "apll4_sel", 10),
-	GATE_AUD0(CLK_AUD_SPDIFIN_TUNER_DBG, "aud_spdifin_tuner_dbg", "apll4_sel", 11),
-	GATE_AUD0(CLK_AUD_UL_TML, "aud_ul_tml", "a1sys_hp_sel", 18),
-	GATE_AUD0(CLK_AUD_APLL1_TUNER, "aud_apll1_tuner", "apll1_sel", 19),
-	GATE_AUD0(CLK_AUD_APLL2_TUNER, "aud_apll2_tuner", "apll2_sel", 20),
-	GATE_AUD0(CLK_AUD_TOP0_SPDF, "aud_top0_spdf", "aud_iec_sel", 21),
-	GATE_AUD0(CLK_AUD_APLL, "aud_apll", "apll1_sel", 23),
-	GATE_AUD0(CLK_AUD_APLL2, "aud_apll2", "apll2_sel", 24),
-	GATE_AUD0(CLK_AUD_DAC, "aud_dac", "a1sys_hp_sel", 25),
-	GATE_AUD0(CLK_AUD_DAC_PREDIS, "aud_dac_predis", "a1sys_hp_sel", 26),
-	GATE_AUD0(CLK_AUD_TML, "aud_tml", "a1sys_hp_sel", 27),
-	GATE_AUD0(CLK_AUD_ADC, "aud_adc", "a1sys_hp_sel", 28),
-	GATE_AUD0(CLK_AUD_DAC_HIRES, "aud_dac_hires", "audio_h_sel", 31),
+	GATE_AUD0(CLK_AUD_AFE, "aud_afe", "top_a1sys_hp", 2),
+	GATE_AUD0(CLK_AUD_LRCK_CNT, "aud_lrck_cnt", "top_a1sys_hp", 4),
+	GATE_AUD0(CLK_AUD_SPDIFIN_TUNER_APLL, "aud_spdifin_tuner_apll", "top_apll4", 10),
+	GATE_AUD0(CLK_AUD_SPDIFIN_TUNER_DBG, "aud_spdifin_tuner_dbg", "top_apll4", 11),
+	GATE_AUD0(CLK_AUD_UL_TML, "aud_ul_tml", "top_a1sys_hp", 18),
+	GATE_AUD0(CLK_AUD_APLL1_TUNER, "aud_apll1_tuner", "top_apll1", 19),
+	GATE_AUD0(CLK_AUD_APLL2_TUNER, "aud_apll2_tuner", "top_apll2", 20),
+	GATE_AUD0(CLK_AUD_TOP0_SPDF, "aud_top0_spdf", "top_aud_iec_clk", 21),
+	GATE_AUD0(CLK_AUD_APLL, "aud_apll", "top_apll1", 23),
+	GATE_AUD0(CLK_AUD_APLL2, "aud_apll2", "top_apll2", 24),
+	GATE_AUD0(CLK_AUD_DAC, "aud_dac", "top_a1sys_hp", 25),
+	GATE_AUD0(CLK_AUD_DAC_PREDIS, "aud_dac_predis", "top_a1sys_hp", 26),
+	GATE_AUD0(CLK_AUD_TML, "aud_tml", "top_a1sys_hp", 27),
+	GATE_AUD0(CLK_AUD_ADC, "aud_adc", "top_a1sys_hp", 28),
+	GATE_AUD0(CLK_AUD_DAC_HIRES, "aud_dac_hires", "top_audio_h", 31),
 
 	/* AUD1 */
-	GATE_AUD1(CLK_AUD_A1SYS_HP, "aud_a1sys_hp", "a1sys_hp_sel", 2),
-	GATE_AUD1(CLK_AUD_AFE_DMIC1, "aud_afe_dmic1", "a1sys_hp_sel", 10),
-	GATE_AUD1(CLK_AUD_AFE_DMIC2, "aud_afe_dmic2", "a1sys_hp_sel", 11),
-	GATE_AUD1(CLK_AUD_AFE_DMIC3, "aud_afe_dmic3", "a1sys_hp_sel", 12),
-	GATE_AUD1(CLK_AUD_AFE_DMIC4, "aud_afe_dmic4", "a1sys_hp_sel", 13),
-	GATE_AUD1(CLK_AUD_AFE_26M_DMIC_TM, "aud_afe_26m_dmic_tm", "a1sys_hp_sel", 14),
-	GATE_AUD1(CLK_AUD_UL_TML_HIRES, "aud_ul_tml_hires", "audio_h_sel", 16),
-	GATE_AUD1(CLK_AUD_ADC_HIRES, "aud_adc_hires", "audio_h_sel", 17),
-	GATE_AUD1(CLK_AUD_ADDA6_ADC, "aud_adda6_adc", "a1sys_hp_sel", 18),
-	GATE_AUD1(CLK_AUD_ADDA6_ADC_HIRES, "aud_adda6_adc_hires", "audio_h_sel", 19),
+	GATE_AUD1(CLK_AUD_A1SYS_HP, "aud_a1sys_hp", "top_a1sys_hp", 2),
+	GATE_AUD1(CLK_AUD_AFE_DMIC1, "aud_afe_dmic1", "top_a1sys_hp", 10),
+	GATE_AUD1(CLK_AUD_AFE_DMIC2, "aud_afe_dmic2", "top_a1sys_hp", 11),
+	GATE_AUD1(CLK_AUD_AFE_DMIC3, "aud_afe_dmic3", "top_a1sys_hp", 12),
+	GATE_AUD1(CLK_AUD_AFE_DMIC4, "aud_afe_dmic4", "top_a1sys_hp", 13),
+	GATE_AUD1(CLK_AUD_AFE_26M_DMIC_TM, "aud_afe_26m_dmic_tm", "top_a1sys_hp", 14),
+	GATE_AUD1(CLK_AUD_UL_TML_HIRES, "aud_ul_tml_hires", "top_audio_h", 16),
+	GATE_AUD1(CLK_AUD_ADC_HIRES, "aud_adc_hires", "top_audio_h", 17),
+	GATE_AUD1(CLK_AUD_ADDA6_ADC, "aud_adda6_adc", "top_a1sys_hp", 18),
+	GATE_AUD1(CLK_AUD_ADDA6_ADC_HIRES, "aud_adda6_adc_hires", "top_audio_h", 19),
 
 	/* AUD3 */
-	GATE_AUD3(CLK_AUD_LINEIN_TUNER, "aud_linein_tuner", "apll5_sel", 5),
-	GATE_AUD3(CLK_AUD_EARC_TUNER, "aud_earc_tuner", "apll3_sel", 7),
+	GATE_AUD3(CLK_AUD_LINEIN_TUNER, "aud_linein_tuner", "top_apll5", 5),
+	GATE_AUD3(CLK_AUD_EARC_TUNER, "aud_earc_tuner", "top_apll3", 7),
 
 	/* AUD4 */
-	GATE_AUD4(CLK_AUD_I2SIN, "aud_i2sin", "a1sys_hp_sel", 0),
-	GATE_AUD4(CLK_AUD_TDM_IN, "aud_tdm_in", "a1sys_hp_sel", 1),
-	GATE_AUD4(CLK_AUD_I2S_OUT, "aud_i2s_out", "a1sys_hp_sel", 6),
-	GATE_AUD4(CLK_AUD_TDM_OUT, "aud_tdm_out", "a1sys_hp_sel", 7),
-	GATE_AUD4(CLK_AUD_HDMI_OUT, "aud_hdmi_out", "a1sys_hp_sel", 8),
-	GATE_AUD4(CLK_AUD_ASRC11, "aud_asrc11", "a1sys_hp_sel", 16),
-	GATE_AUD4(CLK_AUD_ASRC12, "aud_asrc12", "a1sys_hp_sel", 17),
+	GATE_AUD4(CLK_AUD_I2SIN, "aud_i2sin", "top_a1sys_hp", 0),
+	GATE_AUD4(CLK_AUD_TDM_IN, "aud_tdm_in", "top_a1sys_hp", 1),
+	GATE_AUD4(CLK_AUD_I2S_OUT, "aud_i2s_out", "top_a1sys_hp", 6),
+	GATE_AUD4(CLK_AUD_TDM_OUT, "aud_tdm_out", "top_a1sys_hp", 7),
+	GATE_AUD4(CLK_AUD_HDMI_OUT, "aud_hdmi_out", "top_a1sys_hp", 8),
+	GATE_AUD4(CLK_AUD_ASRC11, "aud_asrc11", "top_a1sys_hp", 16),
+	GATE_AUD4(CLK_AUD_ASRC12, "aud_asrc12", "top_a1sys_hp", 17),
 	GATE_AUD4(CLK_AUD_MULTI_IN, "aud_multi_in", "mphone_slave_b", 19),
-	GATE_AUD4(CLK_AUD_INTDIR, "aud_intdir", "intdir_sel", 20),
-	GATE_AUD4(CLK_AUD_A1SYS, "aud_a1sys", "a1sys_hp_sel", 21),
-	GATE_AUD4(CLK_AUD_A2SYS, "aud_a2sys", "a2sys_sel", 22),
-	GATE_AUD4(CLK_AUD_PCMIF, "aud_pcmif", "a1sys_hp_sel", 24),
-	GATE_AUD4(CLK_AUD_A3SYS, "aud_a3sys", "a3sys_sel", 30),
-	GATE_AUD4(CLK_AUD_A4SYS, "aud_a4sys", "a4sys_sel", 31),
+	GATE_AUD4(CLK_AUD_INTDIR, "aud_intdir", "top_intdir", 20),
+	GATE_AUD4(CLK_AUD_A1SYS, "aud_a1sys", "top_a1sys_hp", 21),
+	GATE_AUD4(CLK_AUD_A2SYS, "aud_a2sys", "top_a2sys_hf", 22),
+	GATE_AUD4(CLK_AUD_PCMIF, "aud_pcmif", "top_a1sys_hp", 24),
+	GATE_AUD4(CLK_AUD_A3SYS, "aud_a3sys", "top_a3sys_hf", 30),
+	GATE_AUD4(CLK_AUD_A4SYS, "aud_a4sys", "top_a4sys_hf", 31),
 
 	/* AUD5 */
-	GATE_AUD5(CLK_AUD_MEMIF_UL1, "aud_memif_ul1", "a1sys_hp_sel", 0),
-	GATE_AUD5(CLK_AUD_MEMIF_UL2, "aud_memif_ul2", "a1sys_hp_sel", 1),
-	GATE_AUD5(CLK_AUD_MEMIF_UL3, "aud_memif_ul3", "a1sys_hp_sel", 2),
-	GATE_AUD5(CLK_AUD_MEMIF_UL4, "aud_memif_ul4", "a1sys_hp_sel", 3),
-	GATE_AUD5(CLK_AUD_MEMIF_UL5, "aud_memif_ul5", "a1sys_hp_sel", 4),
-	GATE_AUD5(CLK_AUD_MEMIF_UL6, "aud_memif_ul6", "a1sys_hp_sel", 5),
-	GATE_AUD5(CLK_AUD_MEMIF_UL8, "aud_memif_ul8", "a1sys_hp_sel", 7),
-	GATE_AUD5(CLK_AUD_MEMIF_UL9, "aud_memif_ul9", "a1sys_hp_sel", 8),
-	GATE_AUD5(CLK_AUD_MEMIF_UL10, "aud_memif_ul10", "a1sys_hp_sel", 9),
-	GATE_AUD5(CLK_AUD_MEMIF_DL2, "aud_memif_dl2", "a1sys_hp_sel", 18),
-	GATE_AUD5(CLK_AUD_MEMIF_DL3, "aud_memif_dl3", "a1sys_hp_sel", 19),
-	GATE_AUD5(CLK_AUD_MEMIF_DL6, "aud_memif_dl6", "a1sys_hp_sel", 22),
-	GATE_AUD5(CLK_AUD_MEMIF_DL7, "aud_memif_dl7", "a1sys_hp_sel", 23),
-	GATE_AUD5(CLK_AUD_MEMIF_DL8, "aud_memif_dl8", "a1sys_hp_sel", 24),
-	GATE_AUD5(CLK_AUD_MEMIF_DL10, "aud_memif_dl10", "a1sys_hp_sel", 26),
-	GATE_AUD5(CLK_AUD_MEMIF_DL11, "aud_memif_dl11", "a1sys_hp_sel", 27),
+	GATE_AUD5(CLK_AUD_MEMIF_UL1, "aud_memif_ul1", "top_a1sys_hp", 0),
+	GATE_AUD5(CLK_AUD_MEMIF_UL2, "aud_memif_ul2", "top_a1sys_hp", 1),
+	GATE_AUD5(CLK_AUD_MEMIF_UL3, "aud_memif_ul3", "top_a1sys_hp", 2),
+	GATE_AUD5(CLK_AUD_MEMIF_UL4, "aud_memif_ul4", "top_a1sys_hp", 3),
+	GATE_AUD5(CLK_AUD_MEMIF_UL5, "aud_memif_ul5", "top_a1sys_hp", 4),
+	GATE_AUD5(CLK_AUD_MEMIF_UL6, "aud_memif_ul6", "top_a1sys_hp", 5),
+	GATE_AUD5(CLK_AUD_MEMIF_UL8, "aud_memif_ul8", "top_a1sys_hp", 7),
+	GATE_AUD5(CLK_AUD_MEMIF_UL9, "aud_memif_ul9", "top_a1sys_hp", 8),
+	GATE_AUD5(CLK_AUD_MEMIF_UL10, "aud_memif_ul10", "top_a1sys_hp", 9),
+	GATE_AUD5(CLK_AUD_MEMIF_DL2, "aud_memif_dl2", "top_a1sys_hp", 18),
+	GATE_AUD5(CLK_AUD_MEMIF_DL3, "aud_memif_dl3", "top_a1sys_hp", 19),
+	GATE_AUD5(CLK_AUD_MEMIF_DL6, "aud_memif_dl6", "top_a1sys_hp", 22),
+	GATE_AUD5(CLK_AUD_MEMIF_DL7, "aud_memif_dl7", "top_a1sys_hp", 23),
+	GATE_AUD5(CLK_AUD_MEMIF_DL8, "aud_memif_dl8", "top_a1sys_hp", 24),
+	GATE_AUD5(CLK_AUD_MEMIF_DL10, "aud_memif_dl10", "top_a1sys_hp", 26),
+	GATE_AUD5(CLK_AUD_MEMIF_DL11, "aud_memif_dl11", "top_a1sys_hp", 27),
 
 	/* AUD6 */
-	GATE_AUD6(CLK_AUD_GASRC0, "aud_gasrc0", "asm_h_sel", 0),
-	GATE_AUD6(CLK_AUD_GASRC1, "aud_gasrc1", "asm_h_sel", 1),
-	GATE_AUD6(CLK_AUD_GASRC2, "aud_gasrc2", "asm_h_sel", 2),
-	GATE_AUD6(CLK_AUD_GASRC3, "aud_gasrc3", "asm_h_sel", 3),
-	GATE_AUD6(CLK_AUD_GASRC4, "aud_gasrc4", "asm_h_sel", 4),
-	GATE_AUD6(CLK_AUD_GASRC5, "aud_gasrc5", "asm_h_sel", 5),
-	GATE_AUD6(CLK_AUD_GASRC6, "aud_gasrc6", "asm_h_sel", 6),
-	GATE_AUD6(CLK_AUD_GASRC7, "aud_gasrc7", "asm_h_sel", 7),
-	GATE_AUD6(CLK_AUD_GASRC8, "aud_gasrc8", "asm_h_sel", 8),
-	GATE_AUD6(CLK_AUD_GASRC9, "aud_gasrc9", "asm_h_sel", 9),
-	GATE_AUD6(CLK_AUD_GASRC10, "aud_gasrc10", "asm_h_sel", 10),
-	GATE_AUD6(CLK_AUD_GASRC11, "aud_gasrc11", "asm_h_sel", 11),
-	GATE_AUD6(CLK_AUD_GASRC12, "aud_gasrc12", "asm_h_sel", 12),
-	GATE_AUD6(CLK_AUD_GASRC13, "aud_gasrc13", "asm_h_sel", 13),
-	GATE_AUD6(CLK_AUD_GASRC14, "aud_gasrc14", "asm_h_sel", 14),
-	GATE_AUD6(CLK_AUD_GASRC15, "aud_gasrc15", "asm_h_sel", 15),
-	GATE_AUD6(CLK_AUD_GASRC16, "aud_gasrc16", "asm_h_sel", 16),
-	GATE_AUD6(CLK_AUD_GASRC17, "aud_gasrc17", "asm_h_sel", 17),
-	GATE_AUD6(CLK_AUD_GASRC18, "aud_gasrc18", "asm_h_sel", 18),
-	GATE_AUD6(CLK_AUD_GASRC19, "aud_gasrc19", "asm_h_sel", 19),
+	GATE_AUD6(CLK_AUD_GASRC0, "aud_gasrc0", "top_asm_h", 0),
+	GATE_AUD6(CLK_AUD_GASRC1, "aud_gasrc1", "top_asm_h", 1),
+	GATE_AUD6(CLK_AUD_GASRC2, "aud_gasrc2", "top_asm_h", 2),
+	GATE_AUD6(CLK_AUD_GASRC3, "aud_gasrc3", "top_asm_h", 3),
+	GATE_AUD6(CLK_AUD_GASRC4, "aud_gasrc4", "top_asm_h", 4),
+	GATE_AUD6(CLK_AUD_GASRC5, "aud_gasrc5", "top_asm_h", 5),
+	GATE_AUD6(CLK_AUD_GASRC6, "aud_gasrc6", "top_asm_h", 6),
+	GATE_AUD6(CLK_AUD_GASRC7, "aud_gasrc7", "top_asm_h", 7),
+	GATE_AUD6(CLK_AUD_GASRC8, "aud_gasrc8", "top_asm_h", 8),
+	GATE_AUD6(CLK_AUD_GASRC9, "aud_gasrc9", "top_asm_h", 9),
+	GATE_AUD6(CLK_AUD_GASRC10, "aud_gasrc10", "top_asm_h", 10),
+	GATE_AUD6(CLK_AUD_GASRC11, "aud_gasrc11", "top_asm_h", 11),
+	GATE_AUD6(CLK_AUD_GASRC12, "aud_gasrc12", "top_asm_h", 12),
+	GATE_AUD6(CLK_AUD_GASRC13, "aud_gasrc13", "top_asm_h", 13),
+	GATE_AUD6(CLK_AUD_GASRC14, "aud_gasrc14", "top_asm_h", 14),
+	GATE_AUD6(CLK_AUD_GASRC15, "aud_gasrc15", "top_asm_h", 15),
+	GATE_AUD6(CLK_AUD_GASRC16, "aud_gasrc16", "top_asm_h", 16),
+	GATE_AUD6(CLK_AUD_GASRC17, "aud_gasrc17", "top_asm_h", 17),
+	GATE_AUD6(CLK_AUD_GASRC18, "aud_gasrc18", "top_asm_h", 18),
+	GATE_AUD6(CLK_AUD_GASRC19, "aud_gasrc19", "top_asm_h", 19),
 };
 
 int mt8195_audsys_clk_register(struct mtk_base_afe *afe)
diff -ruN a/sound/soc/mediatek/mt8195/mt8195-mt6359-rt1011-rt5682.c b/sound/soc/mediatek/mt8195/mt8195-mt6359-rt1011-rt5682.c
--- a/sound/soc/mediatek/mt8195/mt8195-mt6359-rt1011-rt5682.c	1970-01-01 01:00:00.000000000 +0100
+++ b/sound/soc/mediatek/mt8195/mt8195-mt6359-rt1011-rt5682.c	2021-12-23 08:36:03.000000000 +0100
@@ -0,0 +1,1155 @@
+// SPDX-License-Identifier: GPL-2.0
+//
+// mt8195-mt6359-rt1011-rt5682.c  --
+//	MT8195-MT6359-RT1011-RT5682 ALSA SoC machine driver
+//
+// Copyright (c) 2021 MediaTek Inc.
+// Author: Trevor Wu <trevor.wu@mediatek.com>
+//
+
+#include <linux/input.h>
+#include <linux/module.h>
+#include <linux/pm_runtime.h>
+#include <sound/jack.h>
+#include <sound/pcm_params.h>
+#include <sound/rt5682.h>
+#include <sound/soc.h>
+#include "../../codecs/mt6359.h"
+#include "../../codecs/rt1011.h"
+#include "../../codecs/rt5682.h"
+#include "../common/mtk-afe-platform-driver.h"
+#include "mt8195-afe-common.h"
+
+#define RT1011_CODEC_DAI	"rt1011-aif"
+#define RT1011_DEV0_NAME	"rt1011.2-0038"
+#define RT1011_DEV1_NAME	"rt1011.2-0039"
+
+#define RT5682_CODEC_DAI	"rt5682-aif1"
+#define RT5682_DEV0_NAME	"rt5682.2-001a"
+
+struct mt8195_mt6359_rt1011_rt5682_priv {
+	struct device_node *platform_node;
+	struct device_node *hdmi_node;
+	struct device_node *dp_node;
+	struct snd_soc_jack headset_jack;
+	struct snd_soc_jack dp_jack;
+	struct snd_soc_jack hdmi_jack;
+};
+
+static const struct snd_soc_dapm_widget
+mt8195_mt6359_rt1011_rt5682_widgets[] = {
+	SND_SOC_DAPM_SPK("Left Speaker", NULL),
+	SND_SOC_DAPM_SPK("Right Speaker", NULL),
+	SND_SOC_DAPM_HP("Headphone Jack", NULL),
+	SND_SOC_DAPM_MIC("Headset Mic", NULL),
+};
+
+static const struct snd_soc_dapm_route mt8195_mt6359_rt1011_rt5682_routes[] = {
+	/* speaker */
+	{ "Left Speaker", NULL, "Left SPO" },
+	{ "Right Speaker", NULL, "Right SPO" },
+	/* headset */
+	{ "Headphone Jack", NULL, "HPOL" },
+	{ "Headphone Jack", NULL, "HPOR" },
+	{ "IN1P", NULL, "Headset Mic" },
+};
+
+static const struct snd_kcontrol_new mt8195_mt6359_rt1011_rt5682_controls[] = {
+	SOC_DAPM_PIN_SWITCH("Left Speaker"),
+	SOC_DAPM_PIN_SWITCH("Right Speaker"),
+	SOC_DAPM_PIN_SWITCH("Headphone Jack"),
+	SOC_DAPM_PIN_SWITCH("Headset Mic"),
+};
+
+static int mt8195_rt5682_etdm_hw_params(struct snd_pcm_substream *substream,
+					struct snd_pcm_hw_params *params)
+{
+	struct snd_soc_pcm_runtime *rtd = substream->private_data;
+	struct snd_soc_card *card = rtd->card;
+	struct snd_soc_dai *cpu_dai = asoc_rtd_to_cpu(rtd, 0);
+	struct snd_soc_dai *codec_dai = asoc_rtd_to_codec(rtd, 0);
+	unsigned int rate = params_rate(params);
+	int bitwidth;
+	int ret;
+
+	bitwidth = snd_pcm_format_width(params_format(params));
+	if (bitwidth < 0) {
+		dev_err(card->dev, "invalid bit width: %d\n", bitwidth);
+		return bitwidth;
+	}
+
+	ret = snd_soc_dai_set_tdm_slot(codec_dai, 0x00, 0x0, 0x2, bitwidth);
+	if (ret) {
+		dev_err(card->dev, "failed to set tdm slot\n");
+		return ret;
+	}
+
+	ret = snd_soc_dai_set_pll(codec_dai, RT5682_PLL1, RT5682_PLL1_S_BCLK1,
+				  rate * 64, rate * 512);
+	if (ret) {
+		dev_err(card->dev, "failed to set pll\n");
+		return ret;
+	}
+
+	ret = snd_soc_dai_set_sysclk(codec_dai, RT5682_SCLK_S_PLL1,
+				     rate * 512, SND_SOC_CLOCK_IN);
+	if (ret) {
+		dev_err(card->dev, "failed to set sysclk\n");
+		return ret;
+	}
+
+	return snd_soc_dai_set_sysclk(cpu_dai, 0, rate * 128,
+				      SND_SOC_CLOCK_OUT);
+}
+
+static const struct snd_soc_ops mt8195_rt5682_etdm_ops = {
+	.hw_params = mt8195_rt5682_etdm_hw_params,
+};
+
+static int mt8195_rt1011_etdm_hw_params(struct snd_pcm_substream *substream,
+					struct snd_pcm_hw_params *params)
+{
+	struct snd_soc_pcm_runtime *rtd = asoc_substream_to_rtd(substream);
+	struct snd_soc_dai *codec_dai;
+	struct snd_soc_card *card = rtd->card;
+	int srate, i, ret = 0;
+
+	srate = params_rate(params);
+
+	for_each_rtd_codec_dais(rtd, i, codec_dai) {
+		ret = snd_soc_dai_set_pll(codec_dai, 0, RT1011_PLL1_S_BCLK,
+					  64 * srate, 256 * srate);
+		if (ret < 0) {
+			dev_err(card->dev, "codec_dai clock not set\n");
+			return ret;
+		}
+
+		ret = snd_soc_dai_set_sysclk(codec_dai,
+					     RT1011_FS_SYS_PRE_S_PLL1,
+					     256 * srate, SND_SOC_CLOCK_IN);
+		if (ret < 0) {
+			dev_err(card->dev, "codec_dai clock not set\n");
+			return ret;
+		}
+	}
+	return ret;
+}
+
+static const struct snd_soc_ops mt8195_rt1011_etdm_ops = {
+	.hw_params = mt8195_rt1011_etdm_hw_params,
+};
+
+#define CKSYS_AUD_TOP_CFG 0x032c
+#define CKSYS_AUD_TOP_MON 0x0330
+
+static int mt8195_mt6359_mtkaif_calibration(struct snd_soc_pcm_runtime *rtd)
+{
+	struct snd_soc_component *cmpnt_afe =
+		snd_soc_rtdcom_lookup(rtd, AFE_PCM_NAME);
+	struct snd_soc_component *cmpnt_codec =
+		asoc_rtd_to_codec(rtd, 0)->component;
+	struct mtk_base_afe *afe = snd_soc_component_get_drvdata(cmpnt_afe);
+	struct mt8195_afe_private *afe_priv = afe->platform_priv;
+	struct mtkaif_param *param = &afe_priv->mtkaif_params;
+	int chosen_phase_1, chosen_phase_2, chosen_phase_3;
+	int prev_cycle_1, prev_cycle_2, prev_cycle_3;
+	int test_done_1, test_done_2, test_done_3;
+	int cycle_1, cycle_2, cycle_3;
+	int mtkaif_chosen_phase[MT8195_MTKAIF_MISO_NUM];
+	int mtkaif_phase_cycle[MT8195_MTKAIF_MISO_NUM];
+	int mtkaif_calibration_num_phase;
+	bool mtkaif_calibration_ok;
+	unsigned int monitor;
+	int counter;
+	int phase;
+	int i;
+
+	dev_dbg(afe->dev, "%s(), start\n", __func__);
+
+	param->mtkaif_calibration_ok = false;
+	for (i = 0; i < MT8195_MTKAIF_MISO_NUM; i++) {
+		param->mtkaif_chosen_phase[i] = -1;
+		param->mtkaif_phase_cycle[i] = 0;
+		mtkaif_chosen_phase[i] = -1;
+		mtkaif_phase_cycle[i] = 0;
+	}
+
+	if (IS_ERR(afe_priv->topckgen)) {
+		dev_info(afe->dev, "%s() Cannot find topckgen controller\n",
+			 __func__);
+		return 0;
+	}
+
+	pm_runtime_get_sync(afe->dev);
+	mt6359_mtkaif_calibration_enable(cmpnt_codec);
+
+	/* set test type to synchronizer pulse */
+	regmap_update_bits(afe_priv->topckgen,
+			   CKSYS_AUD_TOP_CFG, 0xffff, 0x4);
+	mtkaif_calibration_num_phase = 42;	/* mt6359: 0 ~ 42 */
+	mtkaif_calibration_ok = true;
+
+	for (phase = 0;
+	     phase <= mtkaif_calibration_num_phase && mtkaif_calibration_ok;
+	     phase++) {
+		mt6359_set_mtkaif_calibration_phase(cmpnt_codec,
+						    phase, phase, phase);
+
+		regmap_update_bits(afe_priv->topckgen,
+				   CKSYS_AUD_TOP_CFG, 0x1, 0x1);
+
+		test_done_1 = 0;
+		test_done_2 = 0;
+		test_done_3 = 0;
+		cycle_1 = -1;
+		cycle_2 = -1;
+		cycle_3 = -1;
+		counter = 0;
+		while (!(test_done_1 & test_done_2 & test_done_3)) {
+			regmap_read(afe_priv->topckgen,
+				    CKSYS_AUD_TOP_MON, &monitor);
+			test_done_1 = (monitor >> 28) & 0x1;
+			test_done_2 = (monitor >> 29) & 0x1;
+			test_done_3 = (monitor >> 30) & 0x1;
+			if (test_done_1 == 1)
+				cycle_1 = monitor & 0xf;
+
+			if (test_done_2 == 1)
+				cycle_2 = (monitor >> 4) & 0xf;
+
+			if (test_done_3 == 1)
+				cycle_3 = (monitor >> 8) & 0xf;
+
+			/* handle if never test done */
+			if (++counter > 10000) {
+				dev_info(afe->dev, "%s(), test fail, cycle_1 %d, cycle_2 %d, cycle_3 %d, monitor 0x%x\n",
+					 __func__,
+					 cycle_1, cycle_2, cycle_3, monitor);
+				mtkaif_calibration_ok = false;
+				break;
+			}
+		}
+
+		if (phase == 0) {
+			prev_cycle_1 = cycle_1;
+			prev_cycle_2 = cycle_2;
+			prev_cycle_3 = cycle_3;
+		}
+
+		if (cycle_1 != prev_cycle_1 &&
+		    mtkaif_chosen_phase[MT8195_MTKAIF_MISO_0] < 0) {
+			mtkaif_chosen_phase[MT8195_MTKAIF_MISO_0] = phase - 1;
+			mtkaif_phase_cycle[MT8195_MTKAIF_MISO_0] = prev_cycle_1;
+		}
+
+		if (cycle_2 != prev_cycle_2 &&
+		    mtkaif_chosen_phase[MT8195_MTKAIF_MISO_1] < 0) {
+			mtkaif_chosen_phase[MT8195_MTKAIF_MISO_1] = phase - 1;
+			mtkaif_phase_cycle[MT8195_MTKAIF_MISO_1] = prev_cycle_2;
+		}
+
+		if (cycle_3 != prev_cycle_3 &&
+		    mtkaif_chosen_phase[MT8195_MTKAIF_MISO_2] < 0) {
+			mtkaif_chosen_phase[MT8195_MTKAIF_MISO_2] = phase - 1;
+			mtkaif_phase_cycle[MT8195_MTKAIF_MISO_2] = prev_cycle_3;
+		}
+
+		regmap_update_bits(afe_priv->topckgen,
+				   CKSYS_AUD_TOP_CFG, 0x1, 0x0);
+
+		if (mtkaif_chosen_phase[MT8195_MTKAIF_MISO_0] >= 0 &&
+		    mtkaif_chosen_phase[MT8195_MTKAIF_MISO_1] >= 0 &&
+		    mtkaif_chosen_phase[MT8195_MTKAIF_MISO_2] >= 0)
+			break;
+	}
+
+	if (mtkaif_chosen_phase[MT8195_MTKAIF_MISO_0] < 0) {
+		mtkaif_calibration_ok = false;
+		chosen_phase_1 = 0;
+	} else {
+		chosen_phase_1 = mtkaif_chosen_phase[MT8195_MTKAIF_MISO_0];
+	}
+
+	if (mtkaif_chosen_phase[MT8195_MTKAIF_MISO_1] < 0) {
+		mtkaif_calibration_ok = false;
+		chosen_phase_2 = 0;
+	} else {
+		chosen_phase_2 = mtkaif_chosen_phase[MT8195_MTKAIF_MISO_1];
+	}
+
+	if (mtkaif_chosen_phase[MT8195_MTKAIF_MISO_2] < 0) {
+		mtkaif_calibration_ok = false;
+		chosen_phase_3 = 0;
+	} else {
+		chosen_phase_3 = mtkaif_chosen_phase[MT8195_MTKAIF_MISO_2];
+	}
+
+	mt6359_set_mtkaif_calibration_phase(cmpnt_codec,
+					    chosen_phase_1,
+					    chosen_phase_2,
+					    chosen_phase_3);
+
+	mt6359_mtkaif_calibration_disable(cmpnt_codec);
+	pm_runtime_put(afe->dev);
+
+	param->mtkaif_calibration_ok = mtkaif_calibration_ok;
+	param->mtkaif_chosen_phase[MT8195_MTKAIF_MISO_0] = chosen_phase_1;
+	param->mtkaif_chosen_phase[MT8195_MTKAIF_MISO_1] = chosen_phase_2;
+	param->mtkaif_chosen_phase[MT8195_MTKAIF_MISO_2] = chosen_phase_3;
+	for (i = 0; i < MT8195_MTKAIF_MISO_NUM; i++)
+		param->mtkaif_phase_cycle[i] = mtkaif_phase_cycle[i];
+
+	dev_info(afe->dev, "%s(), end, calibration ok %d\n",
+		 __func__, param->mtkaif_calibration_ok);
+
+	return 0;
+}
+
+static int mt8195_mt6359_init(struct snd_soc_pcm_runtime *rtd)
+{
+	struct snd_soc_component *cmpnt_codec =
+		asoc_rtd_to_codec(rtd, 0)->component;
+
+	/* set mtkaif protocol */
+	mt6359_set_mtkaif_protocol(cmpnt_codec,
+				   MT6359_MTKAIF_PROTOCOL_2_CLK_P2);
+
+	/* mtkaif calibration */
+	mt8195_mt6359_mtkaif_calibration(rtd);
+
+	return 0;
+}
+
+static int mt8195_rt5682_init(struct snd_soc_pcm_runtime *rtd)
+{
+	struct snd_soc_component *cmpnt_codec =
+		asoc_rtd_to_codec(rtd, 0)->component;
+	struct mt8195_mt6359_rt1011_rt5682_priv *priv =
+		snd_soc_card_get_drvdata(rtd->card);
+	struct snd_soc_jack *jack = &priv->headset_jack;
+	int ret;
+
+	ret = snd_soc_card_jack_new(rtd->card, "Headset Jack",
+				    SND_JACK_HEADSET | SND_JACK_BTN_0 |
+				    SND_JACK_BTN_1 | SND_JACK_BTN_2 |
+				    SND_JACK_BTN_3,
+				    jack, NULL, 0);
+	if (ret) {
+		dev_err(rtd->dev, "Headset Jack creation failed: %d\n", ret);
+		return ret;
+	}
+
+	snd_jack_set_key(jack->jack, SND_JACK_BTN_0, KEY_PLAYPAUSE);
+	snd_jack_set_key(jack->jack, SND_JACK_BTN_1, KEY_VOICECOMMAND);
+	snd_jack_set_key(jack->jack, SND_JACK_BTN_2, KEY_VOLUMEUP);
+	snd_jack_set_key(jack->jack, SND_JACK_BTN_3, KEY_VOLUMEDOWN);
+
+	ret = snd_soc_component_set_jack(cmpnt_codec, jack, NULL);
+	if (ret) {
+		dev_err(rtd->dev, "Headset Jack set failed: %d\n", ret);
+		return ret;
+	}
+
+	return 0;
+};
+
+static int mt8195_etdm_hw_params_fixup(struct snd_soc_pcm_runtime *rtd,
+				       struct snd_pcm_hw_params *params)
+{
+	/* fix BE i2s format to 32bit, clean param mask first */
+	snd_mask_reset_range(hw_param_mask(params, SNDRV_PCM_HW_PARAM_FORMAT),
+			     0, (__force unsigned int)SNDRV_PCM_FORMAT_LAST);
+
+	params_set_format(params, SNDRV_PCM_FORMAT_S24_LE);
+
+	return 0;
+}
+
+static int mt8195_hdmitx_dptx_startup(struct snd_pcm_substream *substream)
+{
+	static const unsigned int rates[] = {
+		48000
+	};
+	static const unsigned int channels[] = {
+		2, 4, 6, 8
+	};
+	static const struct snd_pcm_hw_constraint_list constraints_rates = {
+		.count = ARRAY_SIZE(rates),
+		.list  = rates,
+		.mask = 0,
+	};
+	static const struct snd_pcm_hw_constraint_list constraints_channels = {
+		.count = ARRAY_SIZE(channels),
+		.list  = channels,
+		.mask = 0,
+	};
+
+	struct snd_soc_pcm_runtime *rtd = asoc_substream_to_rtd(substream);
+	struct snd_pcm_runtime *runtime = substream->runtime;
+	int ret;
+
+	ret = snd_pcm_hw_constraint_list(runtime, 0,
+					 SNDRV_PCM_HW_PARAM_RATE,
+					 &constraints_rates);
+	if (ret < 0) {
+		dev_err(rtd->dev, "hw_constraint_list rate failed\n");
+		return ret;
+	}
+
+	ret = snd_pcm_hw_constraint_list(runtime, 0,
+					 SNDRV_PCM_HW_PARAM_CHANNELS,
+					 &constraints_channels);
+	if (ret < 0) {
+		dev_err(rtd->dev, "hw_constraint_list channel failed\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static const struct snd_soc_ops mt8195_hdmitx_dptx_playback_ops = {
+	.startup = mt8195_hdmitx_dptx_startup,
+};
+
+static int mt8195_dptx_hw_params(struct snd_pcm_substream *substream,
+				 struct snd_pcm_hw_params *params)
+{
+	struct snd_soc_pcm_runtime *rtd = substream->private_data;
+	struct snd_soc_dai *cpu_dai = asoc_rtd_to_cpu(rtd, 0);
+
+	return snd_soc_dai_set_sysclk(cpu_dai, 0, params_rate(params) * 256,
+				      SND_SOC_CLOCK_OUT);
+}
+
+static struct snd_soc_ops mt8195_dptx_ops = {
+	.hw_params = mt8195_dptx_hw_params,
+};
+
+static int mt8195_dptx_codec_init(struct snd_soc_pcm_runtime *rtd)
+{
+	struct mt8195_mt6359_rt1011_rt5682_priv *priv =
+		snd_soc_card_get_drvdata(rtd->card);
+	struct snd_soc_component *cmpnt_codec =
+		asoc_rtd_to_codec(rtd, 0)->component;
+	int ret;
+
+	ret = snd_soc_card_jack_new(rtd->card, "DP Jack", SND_JACK_LINEOUT,
+				    &priv->dp_jack, NULL, 0);
+	if (ret)
+		return ret;
+
+	return snd_soc_component_set_jack(cmpnt_codec, &priv->dp_jack, NULL);
+}
+
+static int mt8195_hdmi_codec_init(struct snd_soc_pcm_runtime *rtd)
+{
+	struct mt8195_mt6359_rt1011_rt5682_priv *priv =
+		snd_soc_card_get_drvdata(rtd->card);
+	struct snd_soc_component *cmpnt_codec =
+		asoc_rtd_to_codec(rtd, 0)->component;
+	int ret;
+
+	ret = snd_soc_card_jack_new(rtd->card, "HDMI Jack", SND_JACK_LINEOUT,
+				    &priv->hdmi_jack, NULL, 0);
+	if (ret)
+		return ret;
+
+	return snd_soc_component_set_jack(cmpnt_codec, &priv->hdmi_jack, NULL);
+}
+
+static int mt8195_dptx_hw_params_fixup(struct snd_soc_pcm_runtime *rtd,
+				       struct snd_pcm_hw_params *params)
+
+{
+	/* fix BE i2s format to 32bit, clean param mask first */
+	snd_mask_reset_range(hw_param_mask(params, SNDRV_PCM_HW_PARAM_FORMAT),
+			     0, (__force unsigned int)SNDRV_PCM_FORMAT_LAST);
+
+	params_set_format(params, SNDRV_PCM_FORMAT_S24_LE);
+
+	return 0;
+}
+
+static int mt8195_playback_startup(struct snd_pcm_substream *substream)
+{
+	static const unsigned int rates[] = {
+		48000
+	};
+	static const unsigned int channels[] = {
+		2
+	};
+	static const struct snd_pcm_hw_constraint_list constraints_rates = {
+		.count = ARRAY_SIZE(rates),
+		.list  = rates,
+		.mask = 0,
+	};
+	static const struct snd_pcm_hw_constraint_list constraints_channels = {
+		.count = ARRAY_SIZE(channels),
+		.list  = channels,
+		.mask = 0,
+	};
+
+	struct snd_soc_pcm_runtime *rtd = asoc_substream_to_rtd(substream);
+	struct snd_pcm_runtime *runtime = substream->runtime;
+	int ret;
+
+	ret = snd_pcm_hw_constraint_list(runtime, 0,
+					 SNDRV_PCM_HW_PARAM_RATE,
+					 &constraints_rates);
+	if (ret < 0) {
+		dev_err(rtd->dev, "hw_constraint_list rate failed\n");
+		return ret;
+	}
+
+	ret = snd_pcm_hw_constraint_list(runtime, 0,
+					 SNDRV_PCM_HW_PARAM_CHANNELS,
+					 &constraints_channels);
+	if (ret < 0) {
+		dev_err(rtd->dev, "hw_constraint_list channel failed\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static const struct snd_soc_ops mt8195_playback_ops = {
+	.startup = mt8195_playback_startup,
+};
+
+static int mt8195_capture_startup(struct snd_pcm_substream *substream)
+{
+	static const unsigned int rates[] = {
+		48000
+	};
+	static const unsigned int channels[] = {
+		1, 2
+	};
+	static const struct snd_pcm_hw_constraint_list constraints_rates = {
+		.count = ARRAY_SIZE(rates),
+		.list  = rates,
+		.mask = 0,
+	};
+	static const struct snd_pcm_hw_constraint_list constraints_channels = {
+		.count = ARRAY_SIZE(channels),
+		.list  = channels,
+		.mask = 0,
+	};
+
+	struct snd_soc_pcm_runtime *rtd = asoc_substream_to_rtd(substream);
+	struct snd_pcm_runtime *runtime = substream->runtime;
+	int ret;
+
+	ret = snd_pcm_hw_constraint_list(runtime, 0,
+					 SNDRV_PCM_HW_PARAM_RATE,
+					 &constraints_rates);
+	if (ret < 0) {
+		dev_err(rtd->dev, "hw_constraint_list rate failed\n");
+		return ret;
+	}
+
+	ret = snd_pcm_hw_constraint_list(runtime, 0,
+					 SNDRV_PCM_HW_PARAM_CHANNELS,
+					 &constraints_channels);
+	if (ret < 0) {
+		dev_err(rtd->dev, "hw_constraint_list channel failed\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static const struct snd_soc_ops mt8195_capture_ops = {
+	.startup = mt8195_capture_startup,
+};
+
+enum {
+	DAI_LINK_DL2_FE,
+	DAI_LINK_DL3_FE,
+	DAI_LINK_DL6_FE,
+	DAI_LINK_DL7_FE,
+	DAI_LINK_DL8_FE,
+	DAI_LINK_DL10_FE,
+	DAI_LINK_DL11_FE,
+	DAI_LINK_UL1_FE,
+	DAI_LINK_UL2_FE,
+	DAI_LINK_UL3_FE,
+	DAI_LINK_UL4_FE,
+	DAI_LINK_UL5_FE,
+	DAI_LINK_UL6_FE,
+	DAI_LINK_UL8_FE,
+	DAI_LINK_UL9_FE,
+	DAI_LINK_UL10_FE,
+	DAI_LINK_DL_SRC_BE,
+	DAI_LINK_DPTX_BE,
+	DAI_LINK_ETDM1_IN_BE,
+	DAI_LINK_ETDM2_IN_BE,
+	DAI_LINK_ETDM1_OUT_BE,
+	DAI_LINK_ETDM2_OUT_BE,
+	DAI_LINK_ETDM3_OUT_BE,
+	DAI_LINK_PCM1_BE,
+	DAI_LINK_UL_SRC1_BE,
+	DAI_LINK_UL_SRC2_BE,
+};
+
+/* FE */
+SND_SOC_DAILINK_DEFS(DL2_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("DL2")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(DL3_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("DL3")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(DL6_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("DL6")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(DL7_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("DL7")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(DL8_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("DL8")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(DL10_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("DL10")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(DL11_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("DL11")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL1_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL1")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL2_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL2")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL3_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL3")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL4_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL4")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL5_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL5")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL6_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL6")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL8_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL8")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL9_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL9")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL10_FE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL10")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+/* BE */
+SND_SOC_DAILINK_DEFS(DL_SRC_BE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("DL_SRC")),
+		     DAILINK_COMP_ARRAY(COMP_CODEC("mt6359-sound",
+						   "mt6359-snd-codec-aif1")),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(DPTX_BE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("DPTX")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(ETDM1_IN_BE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("ETDM1_IN")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(ETDM2_IN_BE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("ETDM2_IN")),
+		     DAILINK_COMP_ARRAY(COMP_CODEC(RT5682_DEV0_NAME,
+						   RT5682_CODEC_DAI)),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(ETDM1_OUT_BE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("ETDM1_OUT")),
+		     DAILINK_COMP_ARRAY(COMP_CODEC(RT5682_DEV0_NAME,
+						   RT5682_CODEC_DAI)),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(ETDM2_OUT_BE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("ETDM2_OUT")),
+		     DAILINK_COMP_ARRAY(COMP_CODEC(RT1011_DEV0_NAME,
+						   RT1011_CODEC_DAI),
+					COMP_CODEC(RT1011_DEV1_NAME,
+						   RT1011_CODEC_DAI)),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(ETDM3_OUT_BE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("ETDM3_OUT")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(PCM1_BE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("PCM1")),
+		     DAILINK_COMP_ARRAY(COMP_DUMMY()),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL_SRC1_BE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL_SRC1")),
+		     DAILINK_COMP_ARRAY(COMP_CODEC("mt6359-sound",
+						   "mt6359-snd-codec-aif1"),
+					COMP_CODEC("dmic-codec",
+						   "dmic-hifi")),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+SND_SOC_DAILINK_DEFS(UL_SRC2_BE,
+		     DAILINK_COMP_ARRAY(COMP_CPU("UL_SRC2")),
+		     DAILINK_COMP_ARRAY(COMP_CODEC("mt6359-sound",
+						   "mt6359-snd-codec-aif2")),
+		     DAILINK_COMP_ARRAY(COMP_EMPTY()));
+
+static struct snd_soc_dai_link mt8195_mt6359_rt1011_rt5682_dai_links[] = {
+	/* FE */
+	[DAI_LINK_DL2_FE] = {
+		.name = "DL2_FE",
+		.stream_name = "DL2 Playback",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_playback = 1,
+		.ops = &mt8195_playback_ops,
+		SND_SOC_DAILINK_REG(DL2_FE),
+	},
+	[DAI_LINK_DL3_FE] = {
+		.name = "DL3_FE",
+		.stream_name = "DL3 Playback",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_playback = 1,
+		.ops = &mt8195_playback_ops,
+		SND_SOC_DAILINK_REG(DL3_FE),
+	},
+	[DAI_LINK_DL6_FE] = {
+		.name = "DL6_FE",
+		.stream_name = "DL6 Playback",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_playback = 1,
+		.ops = &mt8195_playback_ops,
+		SND_SOC_DAILINK_REG(DL6_FE),
+	},
+	[DAI_LINK_DL7_FE] = {
+		.name = "DL7_FE",
+		.stream_name = "DL7 Playback",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_PRE,
+			SND_SOC_DPCM_TRIGGER_PRE,
+		},
+		.dynamic = 1,
+		.dpcm_playback = 1,
+		SND_SOC_DAILINK_REG(DL7_FE),
+	},
+	[DAI_LINK_DL8_FE] = {
+		.name = "DL8_FE",
+		.stream_name = "DL8 Playback",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_playback = 1,
+		.ops = &mt8195_playback_ops,
+		SND_SOC_DAILINK_REG(DL8_FE),
+	},
+	[DAI_LINK_DL10_FE] = {
+		.name = "DL10_FE",
+		.stream_name = "DL10 Playback",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_playback = 1,
+		.ops = &mt8195_hdmitx_dptx_playback_ops,
+		SND_SOC_DAILINK_REG(DL10_FE),
+	},
+	[DAI_LINK_DL11_FE] = {
+		.name = "DL11_FE",
+		.stream_name = "DL11 Playback",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_playback = 1,
+		.ops = &mt8195_playback_ops,
+		SND_SOC_DAILINK_REG(DL11_FE),
+	},
+	[DAI_LINK_UL1_FE] = {
+		.name = "UL1_FE",
+		.stream_name = "UL1 Capture",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_PRE,
+			SND_SOC_DPCM_TRIGGER_PRE,
+		},
+		.dynamic = 1,
+		.dpcm_capture = 1,
+		SND_SOC_DAILINK_REG(UL1_FE),
+	},
+	[DAI_LINK_UL2_FE] = {
+		.name = "UL2_FE",
+		.stream_name = "UL2 Capture",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_capture = 1,
+		.ops = &mt8195_capture_ops,
+		SND_SOC_DAILINK_REG(UL2_FE),
+	},
+	[DAI_LINK_UL3_FE] = {
+		.name = "UL3_FE",
+		.stream_name = "UL3 Capture",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_capture = 1,
+		.ops = &mt8195_capture_ops,
+		SND_SOC_DAILINK_REG(UL3_FE),
+	},
+	[DAI_LINK_UL4_FE] = {
+		.name = "UL4_FE",
+		.stream_name = "UL4 Capture",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_capture = 1,
+		.ops = &mt8195_capture_ops,
+		SND_SOC_DAILINK_REG(UL4_FE),
+	},
+	[DAI_LINK_UL5_FE] = {
+		.name = "UL5_FE",
+		.stream_name = "UL5 Capture",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_capture = 1,
+		.ops = &mt8195_capture_ops,
+		SND_SOC_DAILINK_REG(UL5_FE),
+	},
+	[DAI_LINK_UL6_FE] = {
+		.name = "UL6_FE",
+		.stream_name = "UL6 Capture",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_PRE,
+			SND_SOC_DPCM_TRIGGER_PRE,
+		},
+		.dynamic = 1,
+		.dpcm_capture = 1,
+		SND_SOC_DAILINK_REG(UL6_FE),
+	},
+	[DAI_LINK_UL8_FE] = {
+		.name = "UL8_FE",
+		.stream_name = "UL8 Capture",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_capture = 1,
+		.ops = &mt8195_capture_ops,
+		SND_SOC_DAILINK_REG(UL8_FE),
+	},
+	[DAI_LINK_UL9_FE] = {
+		.name = "UL9_FE",
+		.stream_name = "UL9 Capture",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_capture = 1,
+		.ops = &mt8195_capture_ops,
+		SND_SOC_DAILINK_REG(UL9_FE),
+	},
+	[DAI_LINK_UL10_FE] = {
+		.name = "UL10_FE",
+		.stream_name = "UL10 Capture",
+		.trigger = {
+			SND_SOC_DPCM_TRIGGER_POST,
+			SND_SOC_DPCM_TRIGGER_POST,
+		},
+		.dynamic = 1,
+		.dpcm_capture = 1,
+		.ops = &mt8195_capture_ops,
+		SND_SOC_DAILINK_REG(UL10_FE),
+	},
+	/* BE */
+	[DAI_LINK_DL_SRC_BE] = {
+		.name = "DL_SRC_BE",
+		.init = mt8195_mt6359_init,
+		.no_pcm = 1,
+		.dpcm_playback = 1,
+		SND_SOC_DAILINK_REG(DL_SRC_BE),
+	},
+	[DAI_LINK_DPTX_BE] = {
+		.name = "DPTX_BE",
+		.no_pcm = 1,
+		.dpcm_playback = 1,
+		.ops = &mt8195_dptx_ops,
+		.be_hw_params_fixup = mt8195_dptx_hw_params_fixup,
+		SND_SOC_DAILINK_REG(DPTX_BE),
+	},
+	[DAI_LINK_ETDM1_IN_BE] = {
+		.name = "ETDM1_IN_BE",
+		.no_pcm = 1,
+		.dai_fmt = SND_SOC_DAIFMT_I2S |
+			SND_SOC_DAIFMT_NB_NF |
+			SND_SOC_DAIFMT_CBS_CFS,
+		.dpcm_capture = 1,
+		SND_SOC_DAILINK_REG(ETDM1_IN_BE),
+	},
+	[DAI_LINK_ETDM2_IN_BE] = {
+		.name = "ETDM2_IN_BE",
+		.no_pcm = 1,
+		.dai_fmt = SND_SOC_DAIFMT_I2S |
+			SND_SOC_DAIFMT_NB_NF |
+			SND_SOC_DAIFMT_CBS_CFS,
+		.dpcm_capture = 1,
+		.init = mt8195_rt5682_init,
+		.ops = &mt8195_rt5682_etdm_ops,
+		.be_hw_params_fixup = mt8195_etdm_hw_params_fixup,
+		SND_SOC_DAILINK_REG(ETDM2_IN_BE),
+	},
+	[DAI_LINK_ETDM1_OUT_BE] = {
+		.name = "ETDM1_OUT_BE",
+		.no_pcm = 1,
+		.dai_fmt = SND_SOC_DAIFMT_I2S |
+			SND_SOC_DAIFMT_NB_NF |
+			SND_SOC_DAIFMT_CBS_CFS,
+		.dpcm_playback = 1,
+		.ops = &mt8195_rt5682_etdm_ops,
+		.be_hw_params_fixup = mt8195_etdm_hw_params_fixup,
+		SND_SOC_DAILINK_REG(ETDM1_OUT_BE),
+	},
+	[DAI_LINK_ETDM2_OUT_BE] = {
+		.name = "ETDM2_OUT_BE",
+		.no_pcm = 1,
+		.dai_fmt = SND_SOC_DAIFMT_I2S |
+			SND_SOC_DAIFMT_NB_NF |
+			SND_SOC_DAIFMT_CBS_CFS,
+		.dpcm_playback = 1,
+		.ops = &mt8195_rt1011_etdm_ops,
+		.be_hw_params_fixup = mt8195_etdm_hw_params_fixup,
+		SND_SOC_DAILINK_REG(ETDM2_OUT_BE),
+	},
+	[DAI_LINK_ETDM3_OUT_BE] = {
+		.name = "ETDM3_OUT_BE",
+		.no_pcm = 1,
+		.dai_fmt = SND_SOC_DAIFMT_I2S |
+			SND_SOC_DAIFMT_NB_NF |
+			SND_SOC_DAIFMT_CBS_CFS,
+		.dpcm_playback = 1,
+		SND_SOC_DAILINK_REG(ETDM3_OUT_BE),
+	},
+	[DAI_LINK_PCM1_BE] = {
+		.name = "PCM1_BE",
+		.no_pcm = 1,
+		.dai_fmt = SND_SOC_DAIFMT_I2S |
+			SND_SOC_DAIFMT_NB_NF |
+			SND_SOC_DAIFMT_CBS_CFS,
+		.dpcm_capture = 1,
+		SND_SOC_DAILINK_REG(PCM1_BE),
+	},
+	[DAI_LINK_UL_SRC1_BE] = {
+		.name = "UL_SRC1_BE",
+		.no_pcm = 1,
+		.dpcm_capture = 1,
+		SND_SOC_DAILINK_REG(UL_SRC1_BE),
+	},
+	[DAI_LINK_UL_SRC2_BE] = {
+		.name = "UL_SRC2_BE",
+		.no_pcm = 1,
+		.dpcm_capture = 1,
+		SND_SOC_DAILINK_REG(UL_SRC2_BE),
+	},
+};
+
+static struct snd_soc_codec_conf rt1011_amp_conf[] = {
+	{
+		.dlc = COMP_CODEC_CONF(RT1011_DEV0_NAME),
+		.name_prefix = "Left",
+	},
+	{
+		.dlc = COMP_CODEC_CONF(RT1011_DEV1_NAME),
+		.name_prefix = "Right",
+	},
+};
+
+static struct snd_soc_card mt8195_mt6359_rt1011_rt5682_soc_card = {
+	.name = "mt8195_r1011_5682",
+	.owner = THIS_MODULE,
+	.dai_link = mt8195_mt6359_rt1011_rt5682_dai_links,
+	.num_links = ARRAY_SIZE(mt8195_mt6359_rt1011_rt5682_dai_links),
+	.controls = mt8195_mt6359_rt1011_rt5682_controls,
+	.num_controls = ARRAY_SIZE(mt8195_mt6359_rt1011_rt5682_controls),
+	.dapm_widgets = mt8195_mt6359_rt1011_rt5682_widgets,
+	.num_dapm_widgets = ARRAY_SIZE(mt8195_mt6359_rt1011_rt5682_widgets),
+	.dapm_routes = mt8195_mt6359_rt1011_rt5682_routes,
+	.num_dapm_routes = ARRAY_SIZE(mt8195_mt6359_rt1011_rt5682_routes),
+	.codec_conf = rt1011_amp_conf,
+	.num_configs = ARRAY_SIZE(rt1011_amp_conf),
+};
+
+static int mt8195_mt6359_rt1011_rt5682_dev_probe(struct platform_device *pdev)
+{
+	struct snd_soc_card *card = &mt8195_mt6359_rt1011_rt5682_soc_card;
+	struct snd_soc_dai_link *dai_link;
+	struct mt8195_mt6359_rt1011_rt5682_priv *priv;
+	int ret, i;
+
+	card->dev = &pdev->dev;
+
+	priv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->platform_node = of_parse_phandle(pdev->dev.of_node,
+					       "mediatek,platform", 0);
+	if (!priv->platform_node) {
+		dev_dbg(&pdev->dev, "Property 'platform' missing or invalid\n");
+		return -EINVAL;
+	}
+
+	for_each_card_prelinks(card, i, dai_link) {
+		if (!dai_link->platforms->name)
+			dai_link->platforms->of_node = priv->platform_node;
+
+		if (strcmp(dai_link->name, "DPTX_BE") == 0) {
+			priv->dp_node =
+				of_parse_phandle(pdev->dev.of_node,
+						 "mediatek,dptx-codec", 0);
+
+			if (!priv->dp_node) {
+				dev_dbg(&pdev->dev, "No property 'dptx-codec'\n");
+			} else {
+				dai_link->codecs->of_node = priv->dp_node;
+				dai_link->codecs->name = NULL;
+				dai_link->codecs->dai_name = "i2s-hifi";
+				dai_link->init = mt8195_dptx_codec_init;
+			}
+		}
+
+		if (strcmp(dai_link->name, "ETDM3_OUT_BE") == 0) {
+			priv->hdmi_node =
+				of_parse_phandle(pdev->dev.of_node,
+						 "mediatek,hdmi-codec", 0);
+			if (!priv->hdmi_node) {
+				dev_dbg(&pdev->dev, "No property 'hdmi-codec'\n");
+			} else {
+				dai_link->codecs->of_node = priv->hdmi_node;
+				dai_link->codecs->name = NULL;
+				dai_link->codecs->dai_name = "i2s-hifi";
+				dai_link->init = mt8195_hdmi_codec_init;
+			}
+		}
+	}
+
+	snd_soc_card_set_drvdata(card, priv);
+
+	ret = devm_snd_soc_register_card(&pdev->dev, card);
+	if (ret) {
+		dev_err(&pdev->dev, "%s snd_soc_register_card fail %d\n",
+			__func__, ret);
+		of_node_put(priv->hdmi_node);
+		of_node_put(priv->dp_node);
+		of_node_put(priv->platform_node);
+	}
+
+	return ret;
+}
+
+static int mt8195_mt6359_rt1011_rt5682_dev_remove(struct platform_device *pdev)
+{
+	struct snd_soc_card *card = platform_get_drvdata(pdev);
+	struct mt8195_mt6359_rt1011_rt5682_priv *priv =
+		snd_soc_card_get_drvdata(card);
+
+	of_node_put(priv->hdmi_node);
+	of_node_put(priv->dp_node);
+	of_node_put(priv->platform_node);
+
+	return 0;
+}
+
+#ifdef CONFIG_OF
+static const struct of_device_id mt8195_mt6359_rt1011_rt5682_dt_match[] = {
+	{.compatible = "mediatek,mt8195_mt6359_rt1011_rt5682",},
+	{}
+};
+#endif
+
+static const struct dev_pm_ops mt8195_mt6359_rt1011_rt5682_pm_ops = {
+	.poweroff = snd_soc_poweroff,
+	.restore = snd_soc_resume,
+};
+
+static struct platform_driver mt8195_mt6359_rt1011_rt5682_driver = {
+	.driver = {
+		.name = "mt8195_mt6359_rt1011_rt5682",
+#ifdef CONFIG_OF
+		.of_match_table = mt8195_mt6359_rt1011_rt5682_dt_match,
+#endif
+		.pm = &mt8195_mt6359_rt1011_rt5682_pm_ops,
+	},
+	.probe = mt8195_mt6359_rt1011_rt5682_dev_probe,
+	.remove = mt8195_mt6359_rt1011_rt5682_dev_remove,
+};
+
+module_platform_driver(mt8195_mt6359_rt1011_rt5682_driver);
+
+/* Module information */
+MODULE_DESCRIPTION("MT8195-MT6359-RT1011-RT5682 ALSA SoC machine driver");
+MODULE_AUTHOR("Trevor Wu <trevor.wu@mediatek.com>");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("mt8195_mt6359_rt1011_rt5682 soc card");
diff -ruN a/sound/soc/mediatek/mt8195/mt8195-mt6359-rt1019-rt5682.c b/sound/soc/mediatek/mt8195/mt8195-mt6359-rt1019-rt5682.c
--- a/sound/soc/mediatek/mt8195/mt8195-mt6359-rt1019-rt5682.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/mediatek/mt8195/mt8195-mt6359-rt1019-rt5682.c	2021-12-23 08:36:03.000000000 +0100
@@ -26,6 +26,9 @@
 #define RT5682_DEV0_NAME	"rt5682.2-001a"
 
 struct mt8195_mt6359_rt1019_rt5682_priv {
+	struct device_node *platform_node;
+	struct device_node *hdmi_node;
+	struct device_node *dp_node;
 	struct snd_soc_jack headset_jack;
 	struct snd_soc_jack dp_jack;
 	struct snd_soc_jack hdmi_jack;
@@ -994,31 +997,36 @@
 static int mt8195_mt6359_rt1019_rt5682_dev_probe(struct platform_device *pdev)
 {
 	struct snd_soc_card *card = &mt8195_mt6359_rt1019_rt5682_soc_card;
-	struct device_node *platform_node;
 	struct snd_soc_dai_link *dai_link;
-	struct mt8195_mt6359_rt1019_rt5682_priv *priv = NULL;
+	struct mt8195_mt6359_rt1019_rt5682_priv *priv;
 	int ret, i;
 
 	card->dev = &pdev->dev;
 
-	platform_node = of_parse_phandle(pdev->dev.of_node,
-					 "mediatek,platform", 0);
-	if (!platform_node) {
+	priv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->platform_node = of_parse_phandle(pdev->dev.of_node,
+					       "mediatek,platform", 0);
+	if (!priv->platform_node) {
 		dev_dbg(&pdev->dev, "Property 'platform' missing or invalid\n");
 		return -EINVAL;
 	}
 
 	for_each_card_prelinks(card, i, dai_link) {
 		if (!dai_link->platforms->name)
-			dai_link->platforms->of_node = platform_node;
+			dai_link->platforms->of_node = priv->platform_node;
 
 		if (strcmp(dai_link->name, "DPTX_BE") == 0) {
-			dai_link->codecs->of_node =
+			priv->dp_node =
 				of_parse_phandle(pdev->dev.of_node,
 						 "mediatek,dptx-codec", 0);
-			if (!dai_link->codecs->of_node) {
+
+			if (!priv->dp_node) {
 				dev_dbg(&pdev->dev, "No property 'dptx-codec'\n");
 			} else {
+				dai_link->codecs->of_node = priv->dp_node;
 				dai_link->codecs->name = NULL;
 				dai_link->codecs->dai_name = "i2s-hifi";
 				dai_link->init = mt8195_dptx_codec_init;
@@ -1026,12 +1034,13 @@
 		}
 
 		if (strcmp(dai_link->name, "ETDM3_OUT_BE") == 0) {
-			dai_link->codecs->of_node =
+			priv->hdmi_node =
 				of_parse_phandle(pdev->dev.of_node,
 						 "mediatek,hdmi-codec", 0);
-			if (!dai_link->codecs->of_node) {
+			if (!priv->hdmi_node) {
 				dev_dbg(&pdev->dev, "No property 'hdmi-codec'\n");
 			} else {
+				dai_link->codecs->of_node = priv->hdmi_node;
 				dai_link->codecs->name = NULL;
 				dai_link->codecs->dai_name = "i2s-hifi";
 				dai_link->init = mt8195_hdmi_codec_init;
@@ -1039,23 +1048,33 @@
 		}
 	}
 
-	priv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);
-	if (!priv) {
-		of_node_put(platform_node);
-		return -ENOMEM;
-	}
-
 	snd_soc_card_set_drvdata(card, priv);
 
 	ret = devm_snd_soc_register_card(&pdev->dev, card);
-	if (ret)
+	if (ret) {
 		dev_err(&pdev->dev, "%s snd_soc_register_card fail %d\n",
 			__func__, ret);
+		of_node_put(priv->hdmi_node);
+		of_node_put(priv->dp_node);
+		of_node_put(priv->platform_node);
+	}
 
-	of_node_put(platform_node);
 	return ret;
 }
 
+static int mt8195_mt6359_rt1019_rt5682_dev_remove(struct platform_device *pdev)
+{
+	struct snd_soc_card *card = platform_get_drvdata(pdev);
+	struct mt8195_mt6359_rt1019_rt5682_priv *priv =
+		snd_soc_card_get_drvdata(card);
+
+	of_node_put(priv->hdmi_node);
+	of_node_put(priv->dp_node);
+	of_node_put(priv->platform_node);
+
+	return 0;
+}
+
 #ifdef CONFIG_OF
 static const struct of_device_id mt8195_mt6359_rt1019_rt5682_dt_match[] = {
 	{.compatible = "mediatek,mt8195_mt6359_rt1019_rt5682",},
@@ -1077,6 +1096,7 @@
 		.pm = &mt8195_mt6359_rt1019_rt5682_pm_ops,
 	},
 	.probe = mt8195_mt6359_rt1019_rt5682_dev_probe,
+	.remove = mt8195_mt6359_rt1019_rt5682_dev_remove,
 };
 
 module_platform_driver(mt8195_mt6359_rt1019_rt5682_driver);
diff -ruN a/sound/soc/qcom/lpass-cpu.c b/sound/soc/qcom/lpass-cpu.c
--- a/sound/soc/qcom/lpass-cpu.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/qcom/lpass-cpu.c	2021-12-23 08:36:03.000000000 +0100
@@ -779,6 +779,8 @@
 		return true;
 	if (reg == LPASS_HDMI_TX_LEGACY_ADDR(v))
 		return true;
+	if (reg == LPASS_HDMI_TX_PARITY_ADDR(v))
+		return true;
 
 	for (i = 0; i < v->hdmi_rdma_channels; ++i) {
 		if (reg == LPAIF_HDMI_RDMACURR_REG(v, i))
diff -ruN a/sound/soc/soc-acpi.c b/sound/soc/soc-acpi.c
--- a/sound/soc/soc-acpi.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/soc-acpi.c	2021-12-23 08:36:03.000000000 +0100
@@ -8,14 +8,34 @@
 #include <linux/module.h>
 #include <sound/soc-acpi.h>
 
+static bool snd_soc_acpi_id_present(struct snd_soc_acpi_mach *machine)
+{
+	const struct snd_soc_acpi_codecs *comp_ids = machine->comp_ids;
+	int i;
+
+	if (machine->id[0]) {
+		if (acpi_dev_present(machine->id, NULL, -1))
+			return true;
+	}
+
+	if (comp_ids) {
+		for (i = 0; i < comp_ids->num_codecs; i++) {
+			if (acpi_dev_present(comp_ids->codecs[i], NULL, -1))
+				return true;
+		}
+	}
+
+	return false;
+}
+
 struct snd_soc_acpi_mach *
 snd_soc_acpi_find_machine(struct snd_soc_acpi_mach *machines)
 {
 	struct snd_soc_acpi_mach *mach;
 	struct snd_soc_acpi_mach *mach_alt;
 
-	for (mach = machines; mach->id[0]; mach++) {
-		if (acpi_dev_present(mach->id, NULL, -1)) {
+	for (mach = machines; mach->id[0] || mach->comp_ids; mach++) {
+		if (snd_soc_acpi_id_present(mach)) {
 			if (mach->machine_quirk) {
 				mach_alt = mach->machine_quirk(mach);
 				if (!mach_alt)
diff -ruN a/sound/soc/soc-dapm.c b/sound/soc/soc-dapm.c
--- a/sound/soc/soc-dapm.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/soc-dapm.c	2021-12-23 08:36:03.000000000 +0100
@@ -1331,11 +1331,13 @@
 
 	return paths;
 }
+EXPORT_SYMBOL_GPL(snd_soc_dapm_dai_get_connected_widgets);
 
 void snd_soc_dapm_dai_free_widgets(struct snd_soc_dapm_widget_list **list)
 {
 	dapm_widget_list_free(list);
 }
+EXPORT_SYMBOL_GPL(snd_soc_dapm_dai_free_widgets);
 
 /*
  * Handler for regulator supply widget.
diff -ruN a/sound/soc/soc-pcm.c b/sound/soc/soc-pcm.c
--- a/sound/soc/soc-pcm.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/soc-pcm.c	2021-12-23 08:36:03.000000000 +0100
@@ -1262,8 +1262,7 @@
 	return 0;
 }
 
-static bool dpcm_end_walk_at_be(struct snd_soc_dapm_widget *widget,
-		enum snd_soc_dapm_direction dir)
+bool dpcm_end_walk_at_be(struct snd_soc_dapm_widget *widget, enum snd_soc_dapm_direction dir)
 {
 	struct snd_soc_card *card = widget->dapm->card;
 	struct snd_soc_pcm_runtime *rtd;
@@ -1281,6 +1280,7 @@
 
 	return false;
 }
+EXPORT_SYMBOL_GPL(dpcm_end_walk_at_be);
 
 int dpcm_path_get(struct snd_soc_pcm_runtime *fe,
 	int stream, struct snd_soc_dapm_widget_list **list)
diff -ruN a/sound/soc/soc-topology.c b/sound/soc/soc-topology.c
--- a/sound/soc/soc-topology.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/soc-topology.c	2021-12-23 08:36:03.000000000 +0100
@@ -78,7 +78,7 @@
 };
 
 static int soc_tplg_process_headers(struct soc_tplg *tplg);
-static void soc_tplg_complete(struct soc_tplg *tplg);
+static int soc_tplg_complete(struct soc_tplg *tplg);
 
 /* check we dont overflow the data for this control chunk */
 static int soc_tplg_check_elem_count(struct soc_tplg *tplg, size_t elem_size,
@@ -312,10 +312,12 @@
 }
 
 /* tell the component driver that all firmware has been loaded in this request */
-static void soc_tplg_complete(struct soc_tplg *tplg)
+static int soc_tplg_complete(struct soc_tplg *tplg)
 {
 	if (tplg->ops && tplg->ops->complete)
-		tplg->ops->complete(tplg->comp);
+		return tplg->ops->complete(tplg->comp);
+
+	return 0;
 }
 
 /* add a dynamic kcontrol */
@@ -2627,7 +2629,7 @@
 
 	ret = soc_tplg_process_headers(tplg);
 	if (ret == 0)
-		soc_tplg_complete(tplg);
+		return soc_tplg_complete(tplg);
 
 	return ret;
 }
diff -ruN a/sound/soc/sof/intel/hda.c b/sound/soc/sof/intel/hda.c
--- a/sound/soc/sof/intel/hda.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/intel/hda.c	2021-12-23 08:36:03.000000000 +0100
@@ -41,6 +41,92 @@
 #define EXCEPT_MAX_HDR_SIZE	0x400
 #define HDA_EXT_ROM_STATUS_SIZE 8
 
+int hda_ctrl_dai_widget_setup(struct snd_soc_dapm_widget *w)
+{
+	struct snd_sof_widget *swidget = w->dobj.private;
+	struct snd_soc_component *component = swidget->scomp;
+	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(component);
+	struct sof_ipc_dai_config *config;
+	struct snd_sof_dai *sof_dai;
+	struct sof_ipc_reply reply;
+	int ret;
+
+	sof_dai = swidget->private;
+
+	if (!sof_dai || !sof_dai->dai_config) {
+		dev_err(sdev->dev, "No config for DAI %s\n", w->name);
+		return -EINVAL;
+	}
+
+	config = &sof_dai->dai_config[sof_dai->current_config];
+
+	/*
+	 * For static pipelines, the DAI widget would already be set up and calling
+	 * sof_widget_setup() simply returns without doing anything.
+	 * For dynamic pipelines, the DAI widget will be set up now.
+	 */
+	ret = sof_widget_setup(sdev, swidget);
+	if (ret < 0) {
+		dev_err(sdev->dev, "error: failed setting up DAI widget %s\n", w->name);
+		return ret;
+	}
+
+	/* set HW_PARAMS flag */
+	config->flags = FIELD_PREP(SOF_DAI_CONFIG_FLAGS_MASK, SOF_DAI_CONFIG_FLAGS_HW_PARAMS);
+
+	/* send DAI_CONFIG IPC */
+	ret = sof_ipc_tx_message(sdev->ipc, config->hdr.cmd, config, config->hdr.size,
+				 &reply, sizeof(reply));
+	if (ret < 0) {
+		dev_err(sdev->dev, "error: failed setting DAI config for %s\n", w->name);
+		return ret;
+	}
+
+	sof_dai->configured = true;
+
+	return 0;
+}
+
+int hda_ctrl_dai_widget_free(struct snd_soc_dapm_widget *w)
+{
+	struct snd_sof_widget *swidget = w->dobj.private;
+	struct snd_soc_component *component = swidget->scomp;
+	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(component);
+	struct sof_ipc_dai_config *config;
+	struct snd_sof_dai *sof_dai;
+	struct sof_ipc_reply reply;
+	int ret;
+
+	sof_dai = swidget->private;
+
+	if (!sof_dai || !sof_dai->dai_config) {
+		dev_err(sdev->dev, "error: No config to free DAI %s\n", w->name);
+		return -EINVAL;
+	}
+
+	/* nothing to do if hw_free() is called without restarting the stream after resume. */
+	if (!sof_dai->configured)
+		return 0;
+
+	config = &sof_dai->dai_config[sof_dai->current_config];
+
+	/* set HW_FREE flag */
+	config->flags = FIELD_PREP(SOF_DAI_CONFIG_FLAGS_MASK, SOF_DAI_CONFIG_FLAGS_HW_FREE);
+
+	ret = sof_ipc_tx_message(sdev->ipc, config->hdr.cmd, config, config->hdr.size,
+				 &reply, sizeof(reply));
+	if (ret < 0)
+		dev_err(sdev->dev, "error: failed resetting DAI config for %s\n", w->name);
+
+	/*
+	 * Reset the configured_flag and free the widget even if the IPC fails to keep
+	 * the widget use_count balanced
+	 */
+	sof_dai->configured = false;
+
+	return sof_widget_free(sdev, swidget);
+}
+
 static const struct sof_intel_dsp_desc
 	*get_chip_info(struct snd_sof_pdata *pdata)
 {
@@ -64,67 +150,70 @@
 module_param(sdw_clock_stop_quirks, int, 0444);
 MODULE_PARM_DESC(sdw_clock_stop_quirks, "SOF SoundWire clock stop quirks");
 
+static int sdw_dai_config_ipc(struct snd_sof_dev *sdev,
+			      struct snd_soc_dapm_widget *w,
+			      int link_id, int alh_stream_id, int dai_id, bool setup)
+{
+	struct snd_sof_widget *swidget = w->dobj.private;
+	struct sof_ipc_dai_config *config;
+	struct snd_sof_dai *sof_dai;
+
+	if (!swidget) {
+		dev_err(sdev->dev, "error: No private data for widget %s\n", w->name);
+		return -EINVAL;
+	}
+
+	sof_dai = swidget->private;
+
+	if (!sof_dai || !sof_dai->dai_config) {
+		dev_err(sdev->dev, "error: No config for DAI %s\n", w->name);
+		return -EINVAL;
+	}
+
+	config = &sof_dai->dai_config[sof_dai->current_config];
+
+	/* update config with link and stream ID */
+	config->dai_index = (link_id << 8) | dai_id;
+	config->alh.stream_id = alh_stream_id;
+
+	if (setup)
+		return hda_ctrl_dai_widget_setup(w);
+
+	return hda_ctrl_dai_widget_free(w);
+}
+
 static int sdw_params_stream(struct device *dev,
 			     struct sdw_intel_stream_params_data *params_data)
 {
+	struct snd_pcm_substream *substream = params_data->substream;
 	struct snd_sof_dev *sdev = dev_get_drvdata(dev);
 	struct snd_soc_dai *d = params_data->dai;
-	struct sof_ipc_dai_config config;
-	struct sof_ipc_reply reply;
-	int link_id = params_data->link_id;
-	int alh_stream_id = params_data->alh_stream_id;
-	int ret;
-	u32 size = sizeof(config);
+	struct snd_soc_dapm_widget *w;
 
-	memset(&config, 0, size);
-	config.hdr.size = size;
-	config.hdr.cmd = SOF_IPC_GLB_DAI_MSG | SOF_IPC_DAI_CONFIG;
-	config.type = SOF_DAI_INTEL_ALH;
-	config.dai_index = (link_id << 8) | (d->id);
-	config.alh.stream_id = alh_stream_id;
-
-	/* send message to DSP */
-	ret = sof_ipc_tx_message(sdev->ipc,
-				 config.hdr.cmd, &config, size, &reply,
-				 sizeof(reply));
-	if (ret < 0) {
-		dev_err(sdev->dev,
-			"error: failed to set DAI hw_params for link %d dai->id %d ALH %d\n",
-			link_id, d->id, alh_stream_id);
-	}
+	if (substream->stream == SNDRV_PCM_STREAM_PLAYBACK)
+		w = d->playback_widget;
+	else
+		w = d->capture_widget;
 
-	return ret;
+	return sdw_dai_config_ipc(sdev, w, params_data->link_id, params_data->alh_stream_id,
+				  d->id, true);
 }
 
 static int sdw_free_stream(struct device *dev,
 			   struct sdw_intel_stream_free_data *free_data)
 {
+	struct snd_pcm_substream *substream = free_data->substream;
 	struct snd_sof_dev *sdev = dev_get_drvdata(dev);
 	struct snd_soc_dai *d = free_data->dai;
-	struct sof_ipc_dai_config config;
-	struct sof_ipc_reply reply;
-	int link_id = free_data->link_id;
-	int ret;
-	u32 size = sizeof(config);
+	struct snd_soc_dapm_widget *w;
 
-	memset(&config, 0, size);
-	config.hdr.size = size;
-	config.hdr.cmd = SOF_IPC_GLB_DAI_MSG | SOF_IPC_DAI_CONFIG;
-	config.type = SOF_DAI_INTEL_ALH;
-	config.dai_index = (link_id << 8) | d->id;
-	config.alh.stream_id = 0xFFFF; /* invalid value on purpose */
-
-	/* send message to DSP */
-	ret = sof_ipc_tx_message(sdev->ipc,
-				 config.hdr.cmd, &config, size, &reply,
-				 sizeof(reply));
-	if (ret < 0) {
-		dev_err(sdev->dev,
-			"error: failed to free stream for link %d dai->id %d\n",
-			link_id, d->id);
-	}
+	if (substream->stream == SNDRV_PCM_STREAM_PLAYBACK)
+		w = d->playback_widget;
+	else
+		w = d->capture_widget;
 
-	return ret;
+	/* send invalid stream_id */
+	return sdw_dai_config_ipc(sdev, w, free_data->link_id, 0xFFFF, d->id, false);
 }
 
 static const struct sdw_intel_ops sdw_callback = {
diff -ruN a/sound/soc/sof/intel/hda-dai.c b/sound/soc/sof/intel/hda-dai.c
--- a/sound/soc/sof/intel/hda-dai.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/intel/hda-dai.c	2021-12-23 08:36:03.000000000 +0100
@@ -152,49 +152,68 @@
 	return 0;
 }
 
-/* Send DAI_CONFIG IPC to the DAI that matches the dai_name and direction */
-static int hda_link_config_ipc(struct sof_intel_hda_stream *hda_stream,
-			       const char *dai_name, int channel, int dir)
+/* Update config for the DAI widget */
+static struct sof_ipc_dai_config *hda_dai_update_config(struct snd_soc_dapm_widget *w,
+							int channel)
 {
+	struct snd_sof_widget *swidget = w->dobj.private;
 	struct sof_ipc_dai_config *config;
 	struct snd_sof_dai *sof_dai;
+
+	if (!swidget)
+		return NULL;
+
+	sof_dai = swidget->private;
+
+	if (!sof_dai || !sof_dai->dai_config) {
+		dev_err(swidget->scomp->dev, "error: No config for DAI %s\n", w->name);
+		return NULL;
+	}
+
+	config = &sof_dai->dai_config[sof_dai->current_config];
+
+	/* update config with stream tag */
+	config->hda.link_dma_ch = channel;
+
+	return config;
+}
+
+static int hda_link_config_ipc(struct sof_intel_hda_stream *hda_stream,
+			       struct snd_soc_dapm_widget *w, int channel)
+{
+	struct snd_sof_dev *sdev = hda_stream->sdev;
+	struct sof_ipc_dai_config *config;
 	struct sof_ipc_reply reply;
-	int ret = 0;
 
-	list_for_each_entry(sof_dai, &hda_stream->sdev->dai_list, list) {
-		if (!sof_dai->cpu_dai_name)
-			continue;
-
-		if (!strcmp(dai_name, sof_dai->cpu_dai_name) &&
-		    dir == sof_dai->comp_dai.direction) {
-			config = sof_dai->dai_config;
-
-			if (!config) {
-				dev_err(hda_stream->sdev->dev,
-					"error: no config for DAI %s\n",
-					sof_dai->name);
-				return -EINVAL;
-			}
-
-			/* update config with stream tag */
-			config->hda.link_dma_ch = channel;
-
-			/* send IPC */
-			ret = sof_ipc_tx_message(hda_stream->sdev->ipc,
-						 config->hdr.cmd,
-						 config,
-						 config->hdr.size,
-						 &reply, sizeof(reply));
-
-			if (ret < 0)
-				dev_err(hda_stream->sdev->dev,
-					"error: failed to set dai config for %s\n",
-					sof_dai->name);
-			return ret;
-		}
+	config = hda_dai_update_config(w, channel);
+	if (!config) {
+		dev_err(sdev->dev, "error: no config for DAI %s\n", w->name);
+		return -ENOENT;
 	}
 
-	return -EINVAL;
+	/* send DAI_CONFIG IPC */
+	return sof_ipc_tx_message(sdev->ipc, config->hdr.cmd, config, config->hdr.size,
+				  &reply, sizeof(reply));
+}
+
+static int hda_link_dai_widget_update(struct sof_intel_hda_stream *hda_stream,
+				      struct snd_soc_dapm_widget *w,
+				      int channel, bool widget_setup)
+{
+	struct snd_sof_dev *sdev = hda_stream->sdev;
+	struct sof_ipc_dai_config *config;
+
+	config = hda_dai_update_config(w, channel);
+	if (!config) {
+		dev_err(sdev->dev, "error: no config for DAI %s\n", w->name);
+		return -ENOENT;
+	}
+
+	/* set up/free DAI widget and send DAI_CONFIG IPC */
+	if (widget_setup)
+		return hda_ctrl_dai_widget_setup(w);
+
+	return hda_ctrl_dai_widget_free(w);
 }
 
 static int hda_link_hw_params(struct snd_pcm_substream *substream,
@@ -208,6 +227,7 @@
 	struct snd_soc_dai *codec_dai = asoc_rtd_to_codec(rtd, 0);
 	struct sof_intel_hda_stream *hda_stream;
 	struct hda_pipe_params p_params = {0};
+	struct snd_soc_dapm_widget *w;
 	struct hdac_ext_link *link;
 	int stream_tag;
 	int ret;
@@ -226,9 +246,13 @@
 
 	hda_stream = hstream_to_sof_hda_stream(link_dev);
 
-	/* update the DSP with the new tag */
-	ret = hda_link_config_ipc(hda_stream, dai->name, stream_tag - 1,
-				  substream->stream);
+	if (substream->stream == SNDRV_PCM_STREAM_PLAYBACK)
+		w = dai->playback_widget;
+	else
+		w = dai->capture_widget;
+
+	/* set up the DAI widget and send the DAI_CONFIG with the new tag */
+	ret = hda_link_dai_widget_update(hda_stream, w, stream_tag - 1, true);
 	if (ret < 0)
 		return ret;
 
@@ -284,6 +308,7 @@
 				snd_soc_dai_get_dma_data(dai, substream);
 	struct sof_intel_hda_stream *hda_stream;
 	struct snd_soc_pcm_runtime *rtd;
+	struct snd_soc_dapm_widget *w;
 	struct hdac_ext_link *link;
 	struct hdac_stream *hstream;
 	struct hdac_bus *bus;
@@ -318,12 +343,16 @@
 		break;
 	case SNDRV_PCM_TRIGGER_SUSPEND:
 	case SNDRV_PCM_TRIGGER_STOP:
+		if (substream->stream == SNDRV_PCM_STREAM_PLAYBACK)
+			w = dai->playback_widget;
+		else
+			w = dai->capture_widget;
+
 		/*
 		 * clear link DMA channel. It will be assigned when
 		 * hw_params is set up again after resume.
 		 */
-		ret = hda_link_config_ipc(hda_stream, dai->name,
-					  DMA_CHAN_INVALID, substream->stream);
+		ret = hda_link_config_ipc(hda_stream, w, DMA_CHAN_INVALID);
 		if (ret < 0)
 			return ret;
 
@@ -354,6 +383,7 @@
 	struct hdac_stream *hstream;
 	struct snd_soc_pcm_runtime *rtd;
 	struct hdac_ext_stream *link_dev;
+	struct snd_soc_dapm_widget *w;
 	int ret;
 
 	hstream = substream->runtime->private_data;
@@ -369,9 +399,13 @@
 
 	hda_stream = hstream_to_sof_hda_stream(link_dev);
 
-	/* free the link DMA channel in the FW */
-	ret = hda_link_config_ipc(hda_stream, dai->name, DMA_CHAN_INVALID,
-				  substream->stream);
+	if (substream->stream == SNDRV_PCM_STREAM_PLAYBACK)
+		w = dai->playback_widget;
+	else
+		w = dai->capture_widget;
+
+	/* free the link DMA channel in the FW and the DAI widget */
+	ret = hda_link_dai_widget_update(hda_stream, w, DMA_CHAN_INVALID, false);
 	if (ret < 0)
 		return ret;
 
@@ -415,47 +449,129 @@
 #endif
 #endif
 
-static int ssp_dai_hw_params(struct snd_pcm_substream *substream,
-			     struct snd_pcm_hw_params *params,
-			     struct snd_soc_dai *dai)
+/* only one flag used so far to harden hw_params/hw_free/trigger/prepare */
+struct ssp_dai_dma_data {
+	bool setup;
+};
+
+static int ssp_dai_setup_or_free(struct snd_pcm_substream *substream, struct snd_soc_dai *dai,
+				 bool setup)
 {
-	struct snd_soc_pcm_runtime *rtd = asoc_substream_to_rtd(substream);
-	struct snd_soc_component *component = snd_soc_rtdcom_lookup(rtd, SOF_AUDIO_PCM_DRV_NAME);
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(component);
-	struct sof_ipc_fw_version *v = &sdev->fw_ready.version;
-	struct sof_ipc_dai_config *config;
-	struct snd_sof_dai *sof_dai;
-	struct sof_ipc_reply reply;
-	int ret;
+	struct snd_soc_component *component;
+	struct snd_sof_widget *swidget;
+	struct snd_soc_dapm_widget *w;
+	struct sof_ipc_fw_version *v;
+	struct snd_sof_dev *sdev;
+
+	if (substream->stream == SNDRV_PCM_STREAM_PLAYBACK)
+		w = dai->playback_widget;
+	else
+		w = dai->capture_widget;
+
+	swidget = w->dobj.private;
+	component = swidget->scomp;
+	sdev = snd_soc_component_get_drvdata(component);
+	v = &sdev->fw_ready.version;
 
 	/* DAI_CONFIG IPC during hw_params is not supported in older firmware */
 	if (v->abi_version < SOF_ABI_VER(3, 18, 0))
 		return 0;
 
-	list_for_each_entry(sof_dai, &sdev->dai_list, list) {
-		if (!sof_dai->cpu_dai_name || !sof_dai->dai_config)
-			continue;
-
-		if (!strcmp(dai->name, sof_dai->cpu_dai_name) &&
-		    substream->stream == sof_dai->comp_dai.direction) {
-			config = &sof_dai->dai_config[sof_dai->current_config];
-
-			/* send IPC */
-			ret = sof_ipc_tx_message(sdev->ipc, config->hdr.cmd, config,
-						 config->hdr.size, &reply, sizeof(reply));
-
-			if (ret < 0)
-				dev_err(sdev->dev, "error: failed to set DAI config for %s\n",
-					sof_dai->name);
-			return ret;
-		}
-	}
+	if (setup)
+		return hda_ctrl_dai_widget_setup(w);
+
+	return hda_ctrl_dai_widget_free(w);
+}
+
+static int ssp_dai_startup(struct snd_pcm_substream *substream,
+			   struct snd_soc_dai *dai)
+{
+	struct ssp_dai_dma_data *dma_data;
+
+	dma_data = kzalloc(sizeof(*dma_data), GFP_KERNEL);
+	if (!dma_data)
+		return -ENOMEM;
+
+	snd_soc_dai_set_dma_data(dai, substream, dma_data);
 
 	return 0;
 }
 
+static int ssp_dai_setup(struct snd_pcm_substream *substream,
+			 struct snd_soc_dai *dai,
+			 bool setup)
+{
+	struct ssp_dai_dma_data *dma_data;
+	int ret = 0;
+
+	dma_data = snd_soc_dai_get_dma_data(dai, substream);
+	if (!dma_data) {
+		dev_err(dai->dev, "%s: failed to get dma_data\n", __func__);
+		return -EIO;
+	}
+
+	if (dma_data->setup != setup) {
+		ret = ssp_dai_setup_or_free(substream, dai, setup);
+		if (!ret)
+			dma_data->setup = setup;
+	}
+	return ret;
+}
+
+static int ssp_dai_hw_params(struct snd_pcm_substream *substream,
+			     struct snd_pcm_hw_params *params,
+			     struct snd_soc_dai *dai)
+{
+	/* params are ignored for now */
+	return ssp_dai_setup(substream, dai, true);
+}
+
+static int ssp_dai_prepare(struct snd_pcm_substream *substream,
+			   struct snd_soc_dai *dai)
+{
+	/*
+	 * the SSP will only be reconfigured during resume operations and
+	 * not in case of xruns
+	 */
+	return ssp_dai_setup(substream, dai, true);
+}
+
+static int ssp_dai_trigger(struct snd_pcm_substream *substream,
+			   int cmd, struct snd_soc_dai *dai)
+{
+	if (cmd != SNDRV_PCM_TRIGGER_SUSPEND)
+		return 0;
+
+	return ssp_dai_setup(substream, dai, false);
+}
+
+static int ssp_dai_hw_free(struct snd_pcm_substream *substream,
+			   struct snd_soc_dai *dai)
+{
+	return ssp_dai_setup(substream, dai, false);
+}
+
+static void ssp_dai_shutdown(struct snd_pcm_substream *substream,
+			     struct snd_soc_dai *dai)
+{
+	struct ssp_dai_dma_data *dma_data;
+
+	dma_data = snd_soc_dai_get_dma_data(dai, substream);
+	if (!dma_data) {
+		dev_err(dai->dev, "%s: failed to get dma_data\n", __func__);
+		return;
+	}
+	snd_soc_dai_set_dma_data(dai, substream, NULL);
+	kfree(dma_data);
+}
+
 static const struct snd_soc_dai_ops ssp_dai_ops = {
+	.startup = ssp_dai_startup,
 	.hw_params = ssp_dai_hw_params,
+	.prepare = ssp_dai_prepare,
+	.trigger = ssp_dai_trigger,
+	.hw_free = ssp_dai_hw_free,
+	.shutdown = ssp_dai_shutdown,
 };
 
 /*
diff -ruN a/sound/soc/sof/intel/hda.h b/sound/soc/sof/intel/hda.h
--- a/sound/soc/sof/intel/hda.h	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/intel/hda.h	2021-12-23 08:36:03.000000000 +0100
@@ -774,4 +774,9 @@
 /* PCI driver selection and probe */
 int hda_pci_intel_probe(struct pci_dev *pci, const struct pci_device_id *pci_id);
 
+struct snd_sof_dai;
+struct sof_ipc_dai_config;
+int hda_ctrl_dai_widget_setup(struct snd_soc_dapm_widget *w);
+int hda_ctrl_dai_widget_free(struct snd_soc_dapm_widget *w);
+
 #endif
diff -ruN a/sound/soc/sof/ipc.c b/sound/soc/sof/ipc.c
--- a/sound/soc/sof/ipc.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/ipc.c	2021-12-23 08:36:03.000000000 +0100
@@ -672,9 +672,31 @@
 	struct sof_ipc_fw_ready *ready = &sdev->fw_ready;
 	struct sof_ipc_fw_version *v = &ready->version;
 	struct sof_ipc_ctrl_data_params sparams;
+	struct snd_sof_widget *swidget;
+	bool widget_found = false;
 	size_t send_bytes;
 	int err;
 
+	list_for_each_entry(swidget, &sdev->widget_list, list) {
+		if (swidget->comp_id == scontrol->comp_id) {
+			widget_found = true;
+			break;
+		}
+	}
+
+	if (!widget_found) {
+		dev_err(sdev->dev, "error: can't find widget with id %d\n", scontrol->comp_id);
+		return -EINVAL;
+	}
+
+	/*
+	 * Volatile controls should always be part of static pipelines and the widget use_count
+	 * would always be > 0 in this case. For the others, just return the cached value if the
+	 * widget is not set up.
+	 */
+	if (!swidget->use_count)
+		return 0;
+
 	/* read or write firmware volume */
 	if (scontrol->readback_offset != 0) {
 		/* write/read value header via mmaped region */
diff -ruN a/sound/soc/sof/pcm.c b/sound/soc/sof/pcm.c
--- a/sound/soc/sof/pcm.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/pcm.c	2021-12-23 08:36:03.000000000 +0100
@@ -116,6 +116,40 @@
 	return ret;
 }
 
+static int sof_pcm_setup_connected_widgets(struct snd_sof_dev *sdev,
+					   struct snd_soc_pcm_runtime *rtd,
+					   struct snd_sof_pcm *spcm, int dir)
+{
+	struct snd_soc_dai *dai;
+	int ret, j;
+
+	/* query DAPM for list of connected widgets and set them up */
+	for_each_rtd_cpu_dais(rtd, j, dai) {
+		struct snd_soc_dapm_widget_list *list;
+
+		ret = snd_soc_dapm_dai_get_connected_widgets(dai, dir, &list,
+							     dpcm_end_walk_at_be);
+		if (ret < 0) {
+			dev_err(sdev->dev, "error: dai %s has no valid %s path\n", dai->name,
+				dir == SNDRV_PCM_STREAM_PLAYBACK ? "playback" : "capture");
+			return ret;
+		}
+
+		spcm->stream[dir].list = list;
+
+		ret = sof_widget_list_setup(sdev, spcm, dir);
+		if (ret < 0) {
+			dev_err(sdev->dev, "error: failed widget list set up for pcm %d dir %d\n",
+				spcm->pcm.pcm_id, dir);
+			spcm->stream[dir].list = NULL;
+			snd_soc_dapm_dai_free_widgets(&list);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
 static int sof_pcm_hw_params(struct snd_soc_component *component,
 			     struct snd_pcm_substream *substream,
 			     struct snd_pcm_hw_params *params)
@@ -213,7 +247,14 @@
 
 	dev_dbg(component->dev, "stream_tag %d", pcm.params.stream_tag);
 
-	/* send IPC to the DSP */
+	/* if this is a repeated hw_params without hw_free, skip setting up widgets */
+	if (!spcm->stream[substream->stream].list) {
+		ret = sof_pcm_setup_connected_widgets(sdev, rtd, spcm, substream->stream);
+		if (ret < 0)
+			return ret;
+	}
+
+	/* send hw_params IPC to the DSP */
 	ret = sof_ipc_tx_message(sdev->ipc, pcm.hdr.cmd, &pcm, sizeof(pcm),
 				 &ipc_params_reply, sizeof(ipc_params_reply));
 	if (ret < 0) {
@@ -259,6 +300,10 @@
 			err = ret;
 	}
 
+	ret = sof_widget_list_free(sdev, spcm, substream->stream);
+	if (ret < 0)
+		err = ret;
+
 	cancel_work_sync(&spcm->stream[substream->stream].period_elapsed_work);
 
 	ret = snd_sof_pcm_platform_hw_free(sdev, substream);
@@ -316,6 +361,7 @@
 	struct sof_ipc_stream stream;
 	struct sof_ipc_reply reply;
 	bool reset_hw_params = false;
+	bool free_widget_list = false;
 	bool ipc_first = false;
 	int ret;
 
@@ -386,6 +432,7 @@
 			spcm->stream[substream->stream].suspend_ignored = true;
 			return 0;
 		}
+		free_widget_list = true;
 		fallthrough;
 	case SNDRV_PCM_TRIGGER_STOP:
 		stream.hdr.cmd |= SOF_IPC_STREAM_TRIG_STOP;
@@ -414,8 +461,15 @@
 		snd_sof_pcm_platform_trigger(sdev, substream, cmd);
 
 	/* free PCM if reset_hw_params is set and the STOP IPC is successful */
-	if (!ret && reset_hw_params)
+	if (!ret && reset_hw_params) {
 		ret = sof_pcm_dsp_pcm_free(substream, sdev, spcm);
+		if (ret < 0)
+			return ret;
+
+		/* free widget list only for SUSPEND trigger */
+		if (free_widget_list)
+			ret = sof_widget_list_free(sdev, spcm, substream->stream);
+	}
 
 	return ret;
 }
diff -ruN a/sound/soc/sof/pm.c b/sound/soc/sof/pm.c
--- a/sound/soc/sof/pm.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/pm.c	2021-12-23 08:36:03.000000000 +0100
@@ -157,7 +157,7 @@
 	}
 
 	/* restore pipelines */
-	ret = sof_restore_pipelines(sdev->dev);
+	ret = sof_set_up_pipelines(sdev->dev, false);
 	if (ret < 0) {
 		dev_err(sdev->dev,
 			"error: failed to restore pipeline after resume %d\n",
@@ -208,6 +208,8 @@
 	if (target_state == SOF_DSP_PM_D0)
 		goto suspend;
 
+	sof_tear_down_pipelines(dev, false);
+
 	/* release trace */
 	snd_sof_release_trace(sdev);
 
diff -ruN a/sound/soc/sof/sof-audio.c b/sound/soc/sof/sof-audio.c
--- a/sound/soc/sof/sof-audio.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/sof-audio.c	2021-12-23 08:36:03.000000000 +0100
@@ -8,9 +8,493 @@
 // Author: Ranjani Sridharan <ranjani.sridharan@linux.intel.com>
 //
 
+#include <linux/bitfield.h>
 #include "sof-audio.h"
 #include "ops.h"
 
+static int sof_kcontrol_setup(struct snd_sof_dev *sdev, struct snd_sof_control *scontrol)
+{
+	int ipc_cmd, ctrl_type;
+	int ret;
+
+	/* reset readback offset for scontrol */
+	scontrol->readback_offset = 0;
+
+	/* notify DSP of kcontrol values */
+	switch (scontrol->cmd) {
+	case SOF_CTRL_CMD_VOLUME:
+	case SOF_CTRL_CMD_ENUM:
+	case SOF_CTRL_CMD_SWITCH:
+		ipc_cmd = SOF_IPC_COMP_SET_VALUE;
+		ctrl_type = SOF_CTRL_TYPE_VALUE_CHAN_SET;
+		break;
+	case SOF_CTRL_CMD_BINARY:
+		ipc_cmd = SOF_IPC_COMP_SET_DATA;
+		ctrl_type = SOF_CTRL_TYPE_DATA_SET;
+		break;
+	default:
+		return 0;
+	}
+
+	ret = snd_sof_ipc_set_get_comp_data(scontrol, ipc_cmd, ctrl_type, scontrol->cmd, true);
+	if (ret < 0)
+		dev_err(sdev->dev, "error: failed kcontrol value set for widget: %d\n",
+			scontrol->comp_id);
+
+	return ret;
+}
+
+static int sof_dai_config_setup(struct snd_sof_dev *sdev, struct snd_sof_dai *dai)
+{
+	struct sof_ipc_dai_config *config;
+	struct sof_ipc_reply reply;
+	int ret;
+
+	config = &dai->dai_config[dai->current_config];
+	if (!config) {
+		dev_err(sdev->dev, "error: no config for DAI %s\n", dai->name);
+		return -EINVAL;
+	}
+
+	/* set NONE flag to clear all previous settings */
+	config->flags = FIELD_PREP(SOF_DAI_CONFIG_FLAGS_MASK, SOF_DAI_CONFIG_FLAGS_NONE);
+
+	ret = sof_ipc_tx_message(sdev->ipc, config->hdr.cmd, config, config->hdr.size,
+				 &reply, sizeof(reply));
+
+	if (ret < 0)
+		dev_err(sdev->dev, "error: failed to set dai config for %s\n", dai->name);
+
+	return ret;
+}
+
+static int sof_widget_kcontrol_setup(struct snd_sof_dev *sdev, struct snd_sof_widget *swidget)
+{
+	struct snd_sof_control *scontrol;
+	int ret;
+
+	/* set up all controls for the widget */
+	list_for_each_entry(scontrol, &sdev->kcontrol_list, list)
+		if (scontrol->comp_id == swidget->comp_id) {
+			ret = sof_kcontrol_setup(sdev, scontrol);
+			if (ret < 0) {
+				dev_err(sdev->dev, "error: fail to set up kcontrols for widget %s\n",
+					swidget->widget->name);
+				return ret;
+			}
+		}
+
+	return 0;
+}
+
+static void sof_reset_route_setup_status(struct snd_sof_dev *sdev, struct snd_sof_widget *widget)
+{
+	struct snd_sof_route *sroute;
+
+	list_for_each_entry(sroute, &sdev->route_list, list)
+		if (sroute->src_widget == widget || sroute->sink_widget == widget)
+			sroute->setup = false;
+}
+
+int sof_widget_free(struct snd_sof_dev *sdev, struct snd_sof_widget *swidget)
+{
+	struct sof_ipc_free ipc_free = {
+		.hdr = {
+			.size = sizeof(ipc_free),
+			.cmd = SOF_IPC_GLB_TPLG_MSG,
+		},
+		.id = swidget->comp_id,
+	};
+	struct sof_ipc_reply reply;
+	int ret;
+
+	if (!swidget->private)
+		return 0;
+
+	/* only free when use_count is 0 */
+	if (--swidget->use_count)
+		return 0;
+
+	switch (swidget->id) {
+	case snd_soc_dapm_scheduler:
+		ipc_free.hdr.cmd |= SOF_IPC_TPLG_PIPE_FREE;
+		break;
+	case snd_soc_dapm_buffer:
+		ipc_free.hdr.cmd |= SOF_IPC_TPLG_BUFFER_FREE;
+		break;
+	default:
+		ipc_free.hdr.cmd |= SOF_IPC_TPLG_COMP_FREE;
+		break;
+	}
+
+	ret = sof_ipc_tx_message(sdev->ipc, ipc_free.hdr.cmd, &ipc_free, sizeof(ipc_free),
+				 &reply, sizeof(reply));
+	if (ret < 0) {
+		dev_err(sdev->dev, "error: failed to free widget %s\n", swidget->widget->name);
+		swidget->use_count++;
+		return ret;
+	}
+
+	/* reset route setup status for all routes that contain this widget */
+	sof_reset_route_setup_status(sdev, swidget);
+	swidget->complete = 0;
+	dev_dbg(sdev->dev, "widget %s freed\n", swidget->widget->name);
+
+	return 0;
+}
+EXPORT_SYMBOL(sof_widget_free);
+
+int sof_widget_setup(struct snd_sof_dev *sdev, struct snd_sof_widget *swidget)
+{
+	struct sof_ipc_pipe_new *pipeline;
+	struct sof_ipc_comp_reply r;
+	struct sof_ipc_cmd_hdr *hdr;
+	struct sof_ipc_comp *comp;
+	struct snd_sof_dai *dai;
+	size_t ipc_size;
+	int ret;
+
+	/* skip if there is no private data */
+	if (!swidget->private)
+		return 0;
+
+	/* widget already set up */
+	if (++swidget->use_count > 1)
+		return 0;
+
+	ret = sof_pipeline_core_enable(sdev, swidget);
+	if (ret < 0) {
+		dev_err(sdev->dev, "error: failed to enable target core: %d for widget %s\n",
+			ret, swidget->widget->name);
+		goto use_count_dec;
+	}
+
+	switch (swidget->id) {
+	case snd_soc_dapm_dai_in:
+	case snd_soc_dapm_dai_out:
+		ipc_size = sizeof(struct sof_ipc_comp_dai) + sizeof(struct sof_ipc_comp_ext);
+		comp = kzalloc(ipc_size, GFP_KERNEL);
+		if (!comp)
+			return -ENOMEM;
+
+		dai = swidget->private;
+		dai->configured = false;
+		memcpy(comp, &dai->comp_dai, sizeof(struct sof_ipc_comp_dai));
+
+		/* append extended data to the end of the component */
+		memcpy((u8 *)comp + sizeof(struct sof_ipc_comp_dai), &swidget->comp_ext,
+		       sizeof(swidget->comp_ext));
+
+		ret = sof_ipc_tx_message(sdev->ipc, comp->hdr.cmd, comp, ipc_size, &r, sizeof(r));
+		kfree(comp);
+		if (ret < 0) {
+			dev_err(sdev->dev, "error: failed to load widget %s\n",
+				swidget->widget->name);
+			goto use_count_dec;
+		}
+
+		ret = sof_dai_config_setup(sdev, dai);
+		if (ret < 0) {
+			dev_err(sdev->dev, "error: failed to load dai config for DAI %s\n",
+				swidget->widget->name);
+			sof_widget_free(sdev, swidget);
+			return ret;
+		}
+		break;
+	case snd_soc_dapm_scheduler:
+		pipeline = swidget->private;
+		ret = sof_load_pipeline_ipc(sdev->dev, pipeline, &r);
+		break;
+	default:
+		hdr = swidget->private;
+		ret = sof_ipc_tx_message(sdev->ipc, hdr->cmd, swidget->private, hdr->size,
+					 &r, sizeof(r));
+		break;
+	}
+	if (ret < 0) {
+		dev_err(sdev->dev, "error: failed to load widget %s\n", swidget->widget->name);
+		goto use_count_dec;
+	}
+
+	/* restore kcontrols for widget */
+	ret = sof_widget_kcontrol_setup(sdev, swidget);
+	if (ret < 0) {
+		dev_err(sdev->dev, "error: failed to restore kcontrols for widget %s\n",
+			swidget->widget->name);
+		sof_widget_free(sdev, swidget);
+		return ret;
+	}
+
+	dev_dbg(sdev->dev, "widget %s setup complete\n", swidget->widget->name);
+
+	return 0;
+
+use_count_dec:
+	swidget->use_count--;
+	return ret;
+}
+EXPORT_SYMBOL(sof_widget_setup);
+
+static int sof_route_setup_ipc(struct snd_sof_dev *sdev, struct snd_sof_route *sroute)
+{
+	struct sof_ipc_pipe_comp_connect *connect;
+	struct sof_ipc_reply reply;
+	int ret;
+
+	/* skip if there's no private data */
+	if (!sroute->private)
+		return 0;
+
+	/* nothing to do if route is already set up */
+	if (sroute->setup)
+		return 0;
+
+	connect = sroute->private;
+
+	dev_dbg(sdev->dev, "setting up route %s -> %s\n",
+		sroute->src_widget->widget->name,
+		sroute->sink_widget->widget->name);
+
+	/* send ipc */
+	ret = sof_ipc_tx_message(sdev->ipc,
+				 connect->hdr.cmd,
+				 connect, sizeof(*connect),
+				 &reply, sizeof(reply));
+	if (ret < 0) {
+		dev_err(sdev->dev, "%s: route setup failed %d\n", __func__, ret);
+		return ret;
+	}
+
+	sroute->setup = true;
+
+	return 0;
+}
+
+static int sof_route_setup(struct snd_sof_dev *sdev, struct snd_soc_dapm_widget *wsource,
+			   struct snd_soc_dapm_widget *wsink)
+{
+	struct snd_sof_widget *src_widget = wsource->dobj.private;
+	struct snd_sof_widget *sink_widget = wsink->dobj.private;
+	struct snd_sof_route *sroute;
+	bool route_found = false;
+
+	/* ignore routes involving virtual widgets in topology */
+	switch (src_widget->id) {
+	case snd_soc_dapm_out_drv:
+	case snd_soc_dapm_output:
+	case snd_soc_dapm_input:
+		return 0;
+	default:
+		break;
+	}
+
+	switch (sink_widget->id) {
+	case snd_soc_dapm_out_drv:
+	case snd_soc_dapm_output:
+	case snd_soc_dapm_input:
+		return 0;
+	default:
+		break;
+	}
+
+	/* find route matching source and sink widgets */
+	list_for_each_entry(sroute, &sdev->route_list, list)
+		if (sroute->src_widget == src_widget && sroute->sink_widget == sink_widget) {
+			route_found = true;
+			break;
+		}
+
+	if (!route_found) {
+		dev_err(sdev->dev, "error: cannot find SOF route for source %s -> %s sink\n",
+			wsource->name, wsink->name);
+		return -EINVAL;
+	}
+
+	return sof_route_setup_ipc(sdev, sroute);
+}
+
+static int sof_setup_pipeline_connections(struct snd_sof_dev *sdev,
+					  struct snd_soc_dapm_widget_list *list, int dir)
+{
+	struct snd_soc_dapm_widget *widget;
+	struct snd_soc_dapm_path *p;
+	int ret;
+	int i;
+
+	/*
+	 * Set up connections between widgets in the sink/source paths based on direction.
+	 * Some non-SOF widgets exist in topology either for compatibility or for the
+	 * purpose of connecting a pipeline from a host to a DAI in order to receive the DAPM
+	 * events. But they are not handled by the firmware. So ignore them.
+	 */
+	if (dir == SNDRV_PCM_STREAM_PLAYBACK) {
+		for_each_dapm_widgets(list, i, widget) {
+			if (!widget->dobj.private)
+				continue;
+
+			snd_soc_dapm_widget_for_each_sink_path(widget, p)
+				if (p->sink->dobj.private) {
+					ret = sof_route_setup(sdev, widget, p->sink);
+					if (ret < 0)
+						return ret;
+				}
+		}
+	} else {
+		for_each_dapm_widgets(list, i, widget) {
+			if (!widget->dobj.private)
+				continue;
+
+			snd_soc_dapm_widget_for_each_source_path(widget, p)
+				if (p->source->dobj.private) {
+					ret = sof_route_setup(sdev, p->source, widget);
+					if (ret < 0)
+						return ret;
+				}
+		}
+	}
+
+	return 0;
+}
+
+int sof_widget_list_setup(struct snd_sof_dev *sdev, struct snd_sof_pcm *spcm, int dir)
+{
+	struct snd_soc_dapm_widget_list *list = spcm->stream[dir].list;
+	struct snd_soc_dapm_widget *widget;
+	int i, ret, num_widgets;
+
+	/* nothing to set up */
+	if (!list)
+		return 0;
+
+	/* set up widgets in the list */
+	for_each_dapm_widgets(list, num_widgets, widget) {
+		struct snd_sof_widget *swidget = widget->dobj.private;
+		struct snd_sof_widget *pipe_widget;
+
+		if (!swidget)
+			continue;
+
+		/*
+		 * The scheduler widget for a pipeline is not part of the connected DAPM
+		 * widget list and it needs to be set up before the widgets in the pipeline
+		 * are set up. The use_count for the scheduler widget is incremented for every
+		 * widget in a given pipeline to ensure that it is freed only after the last
+		 * widget in the pipeline is freed.
+		 */
+		pipe_widget = swidget->pipe_widget;
+		if (!pipe_widget) {
+			dev_err(sdev->dev, "error: no pipeline widget found for %s\n",
+				swidget->widget->name);
+			ret = -EINVAL;
+			goto widget_free;
+		}
+
+		ret = sof_widget_setup(sdev, pipe_widget);
+		if (ret < 0)
+			goto widget_free;
+
+		/* set up the widget */
+		ret = sof_widget_setup(sdev, swidget);
+		if (ret < 0) {
+			sof_widget_free(sdev, pipe_widget);
+			goto widget_free;
+		}
+	}
+
+	/*
+	 * error in setting pipeline connections will result in route status being reset for
+	 * routes that were successfully set up when the widgets are freed.
+	 */
+	ret = sof_setup_pipeline_connections(sdev, list, dir);
+	if (ret < 0)
+		goto widget_free;
+
+	/* complete pipelines */
+	for_each_dapm_widgets(list, i, widget) {
+		struct snd_sof_widget *swidget = widget->dobj.private;
+		struct snd_sof_widget *pipe_widget;
+
+		if (!swidget)
+			continue;
+
+		pipe_widget = swidget->pipe_widget;
+		if (!pipe_widget) {
+			dev_err(sdev->dev, "error: no pipeline widget found for %s\n",
+				swidget->widget->name);
+			ret = -EINVAL;
+			goto widget_free;
+		}
+
+		if (pipe_widget->complete)
+			continue;
+
+		pipe_widget->complete = snd_sof_complete_pipeline(sdev->dev, pipe_widget);
+		if (pipe_widget->complete < 0) {
+			ret = pipe_widget->complete;
+			goto widget_free;
+		}
+	}
+
+	return 0;
+
+widget_free:
+	/* free all widgets that have been set up successfully */
+	for_each_dapm_widgets(list, i, widget) {
+		struct snd_sof_widget *swidget = widget->dobj.private;
+
+		if (!swidget)
+			continue;
+
+		if (!num_widgets--)
+			break;
+
+		sof_widget_free(sdev, swidget);
+		sof_widget_free(sdev, swidget->pipe_widget);
+	}
+
+	return ret;
+}
+
+int sof_widget_list_free(struct snd_sof_dev *sdev, struct snd_sof_pcm *spcm, int dir)
+{
+	struct snd_soc_dapm_widget_list *list = spcm->stream[dir].list;
+	struct snd_soc_dapm_widget *widget;
+	int i, ret;
+	int ret1 = 0;
+
+	/* nothing to free */
+	if (!list)
+		return 0;
+
+	/*
+	 * Free widgets in the list. This can fail but continue freeing other widgets to keep
+	 * use_counts balanced.
+	 */
+	for_each_dapm_widgets(list, i, widget) {
+		struct snd_sof_widget *swidget = widget->dobj.private;
+
+		if (!swidget)
+			continue;
+
+		/*
+		 * free widget and its pipe_widget. Either of these can fail, but free as many as
+		 * possible before freeing the list and returning the error.
+		 */
+		ret = sof_widget_free(sdev, swidget);
+		if (ret < 0)
+			ret1 = ret;
+
+		ret = sof_widget_free(sdev, swidget->pipe_widget);
+		if (ret < 0)
+			ret1 = ret;
+	}
+
+	snd_soc_dapm_dai_free_widgets(&list);
+	spcm->stream[dir].list = NULL;
+
+	return ret1;
+}
+
 /*
  * helper to determine if there are only D0i3 compatible
  * streams active
@@ -93,55 +577,6 @@
 	return snd_sof_dsp_hw_params_upon_resume(sdev);
 }
 
-static int sof_restore_kcontrols(struct device *dev)
-{
-	struct snd_sof_dev *sdev = dev_get_drvdata(dev);
-	struct snd_sof_control *scontrol;
-	int ipc_cmd, ctrl_type;
-	int ret = 0;
-
-	/* restore kcontrol values */
-	list_for_each_entry(scontrol, &sdev->kcontrol_list, list) {
-		/* reset readback offset for scontrol after resuming */
-		scontrol->readback_offset = 0;
-
-		/* notify DSP of kcontrol values */
-		switch (scontrol->cmd) {
-		case SOF_CTRL_CMD_VOLUME:
-		case SOF_CTRL_CMD_ENUM:
-		case SOF_CTRL_CMD_SWITCH:
-			ipc_cmd = SOF_IPC_COMP_SET_VALUE;
-			ctrl_type = SOF_CTRL_TYPE_VALUE_CHAN_SET;
-			ret = snd_sof_ipc_set_get_comp_data(scontrol,
-							    ipc_cmd, ctrl_type,
-							    scontrol->cmd,
-							    true);
-			break;
-		case SOF_CTRL_CMD_BINARY:
-			ipc_cmd = SOF_IPC_COMP_SET_DATA;
-			ctrl_type = SOF_CTRL_TYPE_DATA_SET;
-			ret = snd_sof_ipc_set_get_comp_data(scontrol,
-							    ipc_cmd, ctrl_type,
-							    scontrol->cmd,
-							    true);
-			break;
-
-		default:
-			break;
-		}
-
-		if (ret < 0) {
-			dev_err(dev,
-				"error: failed kcontrol value set for widget: %d\n",
-				scontrol->comp_id);
-
-			return ret;
-		}
-	}
-
-	return 0;
-}
-
 const struct sof_ipc_pipe_new *snd_sof_pipeline_find(struct snd_sof_dev *sdev,
 						     int pipeline_id)
 {
@@ -158,142 +593,54 @@
 	return NULL;
 }
 
-int sof_restore_pipelines(struct device *dev)
+int sof_set_up_pipelines(struct device *dev, bool verify)
 {
 	struct snd_sof_dev *sdev = dev_get_drvdata(dev);
 	struct snd_sof_widget *swidget;
 	struct snd_sof_route *sroute;
-	struct sof_ipc_pipe_new *pipeline;
-	struct snd_sof_dai *dai;
-	struct sof_ipc_cmd_hdr *hdr;
-	struct sof_ipc_comp *comp;
-	size_t ipc_size;
 	int ret;
 
 	/* restore pipeline components */
 	list_for_each_entry_reverse(swidget, &sdev->widget_list, list) {
-		struct sof_ipc_comp_reply r;
-
-		/* skip if there is no private data */
-		if (!swidget->private)
+		/* only set up the widgets belonging to static pipelines */
+		if (!verify && swidget->dynamic_pipeline_widget)
 			continue;
 
-		ret = sof_pipeline_core_enable(sdev, swidget);
-		if (ret < 0) {
-			dev_err(dev,
-				"error: failed to enable target core: %d\n",
-				ret);
-
-			return ret;
-		}
+		/* update DAI config. The IPC will be sent in sof_widget_setup() */
+		if (WIDGET_IS_DAI(swidget->id)) {
+			struct snd_sof_dai *dai = swidget->private;
+			struct sof_ipc_dai_config *config;
 
-		switch (swidget->id) {
-		case snd_soc_dapm_dai_in:
-		case snd_soc_dapm_dai_out:
-			ipc_size = sizeof(struct sof_ipc_comp_dai) +
-				   sizeof(struct sof_ipc_comp_ext);
-			comp = kzalloc(ipc_size, GFP_KERNEL);
-			if (!comp)
-				return -ENOMEM;
-
-			dai = swidget->private;
-			memcpy(comp, &dai->comp_dai,
-			       sizeof(struct sof_ipc_comp_dai));
-
-			/* append extended data to the end of the component */
-			memcpy((u8 *)comp + sizeof(struct sof_ipc_comp_dai),
-			       &swidget->comp_ext, sizeof(swidget->comp_ext));
-
-			ret = sof_ipc_tx_message(sdev->ipc, comp->hdr.cmd,
-						 comp, ipc_size,
-						 &r, sizeof(r));
-			kfree(comp);
-			break;
-		case snd_soc_dapm_scheduler:
+			if (!dai || !dai->dai_config)
+				continue;
 
+			config = dai->dai_config;
 			/*
-			 * During suspend, all DSP cores are powered off.
-			 * Therefore upon resume, create the pipeline comp
-			 * and power up the core that the pipeline is
-			 * scheduled on.
+			 * The link DMA channel would be invalidated for running
+			 * streams but not for streams that were in the PAUSED
+			 * state during suspend. So invalidate it here before setting
+			 * the dai config in the DSP.
 			 */
-			pipeline = swidget->private;
-			ret = sof_load_pipeline_ipc(dev, pipeline, &r);
-			break;
-		default:
-			hdr = swidget->private;
-			ret = sof_ipc_tx_message(sdev->ipc, hdr->cmd,
-						 swidget->private, hdr->size,
-						 &r, sizeof(r));
-			break;
+			if (config->type == SOF_DAI_INTEL_HDA)
+				config->hda.link_dma_ch = DMA_CHAN_INVALID;
 		}
-		if (ret < 0) {
-			dev_err(dev,
-				"error: failed to load widget type %d with ID: %d\n",
-				swidget->widget->id, swidget->comp_id);
 
+		ret = sof_widget_setup(sdev, swidget);
+		if (ret < 0)
 			return ret;
-		}
 	}
 
 	/* restore pipeline connections */
-	list_for_each_entry_reverse(sroute, &sdev->route_list, list) {
-		struct sof_ipc_pipe_comp_connect *connect;
-		struct sof_ipc_reply reply;
-
-		/* skip if there's no private data */
-		if (!sroute->private)
-			continue;
-
-		connect = sroute->private;
-
-		/* send ipc */
-		ret = sof_ipc_tx_message(sdev->ipc,
-					 connect->hdr.cmd,
-					 connect, sizeof(*connect),
-					 &reply, sizeof(reply));
-		if (ret < 0) {
-			dev_err(dev,
-				"error: failed to load route sink %s control %s source %s\n",
-				sroute->route->sink,
-				sroute->route->control ? sroute->route->control
-					: "none",
-				sroute->route->source);
-
-			return ret;
-		}
-	}
+	list_for_each_entry(sroute, &sdev->route_list, list) {
 
-	/* restore dai links */
-	list_for_each_entry_reverse(dai, &sdev->dai_list, list) {
-		struct sof_ipc_reply reply;
-		struct sof_ipc_dai_config *config = &dai->dai_config[dai->current_config];
-
-		if (!config) {
-			dev_err(dev, "error: no config for DAI %s\n",
-				dai->name);
+		/* only set up routes belonging to static pipelines */
+		if (!verify && (sroute->src_widget->dynamic_pipeline_widget ||
+				sroute->sink_widget->dynamic_pipeline_widget))
 			continue;
-		}
-
-		/*
-		 * The link DMA channel would be invalidated for running
-		 * streams but not for streams that were in the PAUSED
-		 * state during suspend. So invalidate it here before setting
-		 * the dai config in the DSP.
-		 */
-		if (config->type == SOF_DAI_INTEL_HDA)
-			config->hda.link_dma_ch = DMA_CHAN_INVALID;
-
-		ret = sof_ipc_tx_message(sdev->ipc,
-					 config->hdr.cmd, config,
-					 config->hdr.size,
-					 &reply, sizeof(reply));
 
+		ret = sof_route_setup_ipc(sdev, sroute);
 		if (ret < 0) {
-			dev_err(dev,
-				"error: failed to set dai config for %s\n",
-				dai->name);
-
+			dev_err(sdev->dev, "%s: restore pipeline connections failed\n", __func__);
 			return ret;
 		}
 	}
@@ -302,6 +649,10 @@
 	list_for_each_entry(swidget, &sdev->widget_list, list) {
 		switch (swidget->id) {
 		case snd_soc_dapm_scheduler:
+			/* only complete static pipelines */
+			if (!verify && swidget->dynamic_pipeline_widget)
+				continue;
+
 			swidget->complete =
 				snd_sof_complete_pipeline(dev, swidget);
 			break;
@@ -310,13 +661,41 @@
 		}
 	}
 
-	/* restore pipeline kcontrols */
-	ret = sof_restore_kcontrols(dev);
-	if (ret < 0)
-		dev_err(dev,
-			"error: restoring kcontrols after resume\n");
+	return 0;
+}
 
-	return ret;
+/*
+ * This function doesn't free widgets during suspend. It only resets the set up status for all
+ * routes and use_count for all widgets.
+ */
+int sof_tear_down_pipelines(struct device *dev, bool verify)
+{
+	struct snd_sof_dev *sdev = dev_get_drvdata(dev);
+	struct snd_sof_widget *swidget;
+	struct snd_sof_route *sroute;
+	int ret;
+
+	/*
+	 * This function is called during suspend and for one-time topology verification during
+	 * first boot. In both cases, there is no need to protect swidget->use_count and
+	 * sroute->setup because during suspend all streams are suspended and during topology
+	 * loading the sound card unavailable to open PCMs.
+	 */
+	list_for_each_entry_reverse(swidget, &sdev->widget_list, list) {
+		if (!verify) {
+			swidget->use_count = 0;
+			continue;
+		}
+
+		ret = sof_widget_free(sdev, swidget);
+		if (ret < 0)
+			return ret;
+	}
+
+	list_for_each_entry(sroute, &sdev->route_list, list)
+		sroute->setup = false;
+
+	return 0;
 }
 
 /*
diff -ruN a/sound/soc/sof/sof-audio.h b/sound/soc/sof/sof-audio.h
--- a/sound/soc/sof/sof-audio.h	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/sof-audio.h	2021-12-23 08:36:03.000000000 +0100
@@ -28,6 +28,8 @@
 
 #define DMA_CHAN_INVALID	0xFFFFFFFF
 
+#define WIDGET_IS_DAI(id) ((id) == snd_soc_dapm_dai_in || (id) == snd_soc_dapm_dai_out)
+
 /* PCM stream, mapped to FW component  */
 struct snd_sof_pcm_stream {
 	u32 comp_id;
@@ -35,6 +37,7 @@
 	struct sof_ipc_stream_posn posn;
 	struct snd_pcm_substream *substream;
 	struct work_struct period_elapsed_work;
+	struct snd_soc_dapm_widget_list *list; /* list of connected DAPM widgets */
 	bool d0i3_compatible; /* DSP can be in D0I3 when this pcm is opened */
 	/*
 	 * flag to indicate that the DSP pipelines should be kept
@@ -66,6 +69,7 @@
 	int min_volume_step; /* min volume step for volume_table */
 	int max_volume_step; /* max volume step for volume_table */
 	int num_channels;
+	unsigned int access;
 	u32 readback_offset; /* offset to mmapped data if used */
 	struct sof_ipc_ctrl_data *control_data;
 	u32 size;	/* cdata size */
@@ -77,17 +81,31 @@
 	struct snd_sof_led_control led_ctl;
 };
 
+struct snd_sof_widget;
+
 /* ASoC SOF DAPM widget */
 struct snd_sof_widget {
 	struct snd_soc_component *scomp;
 	int comp_id;
 	int pipeline_id;
 	int complete;
+	int use_count; /* use_count will be protected by the PCM mutex held by the core */
 	int core;
 	int id;
 
+	/*
+	 * Flag indicating if the widget should be set up dynamically when a PCM is opened.
+	 * This flag is only set for the scheduler type widget in topology. During topology
+	 * loading, this flag is propagated to all the widgets belonging to the same pipeline.
+	 * When this flag is not set, a widget is set up at the time of topology loading
+	 * and retained until the DSP enters D3. It will need to be set up again when resuming
+	 * from D3.
+	 */
+	bool dynamic_pipeline_widget;
+
 	struct snd_soc_dapm_widget *widget;
 	struct list_head list;	/* list in sdev widget list */
+	struct snd_sof_widget *pipe_widget;
 
 	/* extended data for UUID components */
 	struct sof_ipc_comp_ext comp_ext;
@@ -101,6 +119,9 @@
 
 	struct snd_soc_dapm_route *route;
 	struct list_head list;	/* list in sdev route list */
+	struct snd_sof_widget *src_widget;
+	struct snd_sof_widget *sink_widget;
+	bool setup;
 
 	void *private;
 };
@@ -109,11 +130,11 @@
 struct snd_sof_dai {
 	struct snd_soc_component *scomp;
 	const char *name;
-	const char *cpu_dai_name;
 
 	struct sof_ipc_comp_dai comp_dai;
 	int number_configs;
 	int current_config;
+	bool configured; /* DAI configured during BE hw_params */
 	struct sof_ipc_dai_config *dai_config;
 	struct list_head list;	/* list in sdev dai list */
 };
@@ -220,7 +241,8 @@
 int sof_pcm_dai_link_fixup(struct snd_soc_pcm_runtime *rtd, struct snd_pcm_hw_params *params);
 
 /* PM */
-int sof_restore_pipelines(struct device *dev);
+int sof_set_up_pipelines(struct device *dev, bool verify);
+int sof_tear_down_pipelines(struct device *dev, bool verify);
 int sof_set_hw_params_upon_resume(struct device *dev);
 bool snd_sof_stream_suspend_ignored(struct snd_sof_dev *sdev);
 bool snd_sof_dsp_only_d0i3_compatible_stream_active(struct snd_sof_dev *sdev);
@@ -229,4 +251,10 @@
 int sof_machine_register(struct snd_sof_dev *sdev, void *pdata);
 void sof_machine_unregister(struct snd_sof_dev *sdev, void *pdata);
 
+int sof_widget_setup(struct snd_sof_dev *sdev, struct snd_sof_widget *swidget);
+int sof_widget_free(struct snd_sof_dev *sdev, struct snd_sof_widget *swidget);
+
+/* PCM */
+int sof_widget_list_setup(struct snd_sof_dev *sdev, struct snd_sof_pcm *spcm, int dir);
+int sof_widget_list_free(struct snd_sof_dev *sdev, struct snd_sof_pcm *spcm, int dir);
 #endif
diff -ruN a/sound/soc/sof/sof-pci-dev.c b/sound/soc/sof/sof-pci-dev.c
--- a/sound/soc/sof/sof-pci-dev.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/sof-pci-dev.c	2021-12-23 08:36:03.000000000 +0100
@@ -59,6 +59,15 @@
 		},
 		.driver_data = "sof-adl-rt5682-ssp0-max98373-ssp2.tplg",
 	},
+	{
+		.callback = sof_tplg_cb,
+		.matches = {
+			DMI_MATCH(DMI_PRODUCT_FAMILY, "Google_Brya"),
+			DMI_MATCH(DMI_OEM_STRING, "AUDIO-MAX98390_ALC5682I_I2S"),
+		},
+		.driver_data = "sof-adl-max98390-ssp2-rt5682-ssp0.tplg",
+	},
+
 	{}
 };
 
diff -ruN a/sound/soc/sof/sof-priv.h b/sound/soc/sof/sof-priv.h
--- a/sound/soc/sof/sof-priv.h	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/sof-priv.h	2021-12-23 08:36:03.000000000 +0100
@@ -23,6 +23,7 @@
 /* debug flags */
 #define SOF_DBG_ENABLE_TRACE	BIT(0)
 #define SOF_DBG_RETAIN_CTX	BIT(1)	/* prevent DSP D3 on FW exception */
+#define SOF_DBG_VERIFY_TPLG	BIT(2) /* verify topology during load */
 
 #define SOF_DBG_DUMP_REGS		BIT(0)
 #define SOF_DBG_DUMP_MBOX		BIT(1)
diff -ruN a/sound/soc/sof/topology.c b/sound/soc/sof/topology.c
--- a/sound/soc/sof/topology.c	2021-12-08 09:04:57.000000000 +0100
+++ b/sound/soc/sof/topology.c	2021-12-23 08:36:03.000000000 +0100
@@ -572,6 +572,12 @@
 		offsetof(struct sof_ipc_pipe_new, time_domain), 0},
 };
 
+static const struct sof_topology_token pipeline_tokens[] = {
+	{SOF_TKN_SCHED_DYNAMIC_PIPELINE, SND_SOC_TPLG_TUPLE_TYPE_BOOL, get_token_u16,
+		offsetof(struct snd_sof_widget, dynamic_pipeline_widget), 0},
+
+};
+
 /* volume */
 static const struct sof_topology_token volume_tokens[] = {
 	{SOF_TKN_VOLUME_RAMP_STEP_TYPE, SND_SOC_TPLG_TUPLE_TYPE_WORD,
@@ -1250,6 +1256,7 @@
 		return -ENOMEM;
 
 	scontrol->scomp = scomp;
+	scontrol->access = kc->access;
 
 	switch (le32_to_cpu(hdr->ops.info)) {
 	case SND_SOC_TPLG_CTL_VOLSW:
@@ -1512,10 +1519,8 @@
 static int sof_widget_load_dai(struct snd_soc_component *scomp, int index,
 			       struct snd_sof_widget *swidget,
 			       struct snd_soc_tplg_dapm_widget *tw,
-			       struct sof_ipc_comp_reply *r,
 			       struct snd_sof_dai *dai)
 {
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_comp_dai *comp_dai;
 	size_t ipc_size = sizeof(*comp_dai);
@@ -1552,10 +1557,7 @@
 		swidget->widget->name, comp_dai->type, comp_dai->dai_index);
 	sof_dbg_comp_config(scomp, &comp_dai->config);
 
-	ret = sof_ipc_tx_message(sdev->ipc, comp_dai->comp.hdr.cmd,
-				 comp_dai, ipc_size, r, sizeof(*r));
-
-	if (ret == 0 && dai) {
+	if (dai) {
 		dai->scomp = scomp;
 
 		/*
@@ -1577,10 +1579,8 @@
 
 static int sof_widget_load_buffer(struct snd_soc_component *scomp, int index,
 				  struct snd_sof_widget *swidget,
-				  struct snd_soc_tplg_dapm_widget *tw,
-				  struct sof_ipc_comp_reply *r)
+				  struct snd_soc_tplg_dapm_widget *tw)
 {
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_buffer *buffer;
 	int ret;
@@ -1612,15 +1612,7 @@
 
 	swidget->private = buffer;
 
-	ret = sof_ipc_tx_message(sdev->ipc, buffer->comp.hdr.cmd, buffer,
-				 sizeof(*buffer), r, sizeof(*r));
-	if (ret < 0) {
-		dev_err(scomp->dev, "error: buffer %s load failed\n",
-			swidget->widget->name);
-		kfree(buffer);
-	}
-
-	return ret;
+	return 0;
 }
 
 /* bind PCM ID to host component ID */
@@ -1649,10 +1641,8 @@
 static int sof_widget_load_pcm(struct snd_soc_component *scomp, int index,
 			       struct snd_sof_widget *swidget,
 			       enum sof_ipc_stream_direction dir,
-			       struct snd_soc_tplg_dapm_widget *tw,
-			       struct sof_ipc_comp_reply *r)
+			       struct snd_soc_tplg_dapm_widget *tw)
 {
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_comp_host *host;
 	size_t ipc_size = sizeof(*host);
@@ -1691,10 +1681,7 @@
 
 	swidget->private = host;
 
-	ret = sof_ipc_tx_message(sdev->ipc, host->comp.hdr.cmd, host,
-				 ipc_size, r, sizeof(*r));
-	if (ret >= 0)
-		return ret;
+	return 0;
 err:
 	kfree(host);
 	return ret;
@@ -1723,8 +1710,7 @@
 
 static int sof_widget_load_pipeline(struct snd_soc_component *scomp, int index,
 				    struct snd_sof_widget *swidget,
-				    struct snd_soc_tplg_dapm_widget *tw,
-				    struct sof_ipc_comp_reply *r)
+				    struct snd_soc_tplg_dapm_widget *tw)
 {
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_pipe_new *pipeline;
@@ -1764,16 +1750,22 @@
 		goto err;
 	}
 
+	ret = sof_parse_tokens(scomp, swidget, pipeline_tokens,
+			       ARRAY_SIZE(pipeline_tokens), private->array,
+			       le32_to_cpu(private->size));
+	if (ret != 0) {
+		dev_err(scomp->dev, "error: parse dynamic pipeline token failed %d\n",
+			private->size);
+		goto err;
+	}
+
 	dev_dbg(scomp->dev, "pipeline %s: period %d pri %d mips %d core %d frames %d\n",
 		swidget->widget->name, pipeline->period, pipeline->priority,
 		pipeline->period_mips, pipeline->core, pipeline->frames_per_sched);
 
 	swidget->private = pipeline;
 
-	/* send ipc's to create pipeline comp and power up schedule core */
-	ret = sof_load_pipeline_ipc(scomp->dev, pipeline, r);
-	if (ret >= 0)
-		return ret;
+	return 0;
 err:
 	kfree(pipeline);
 	return ret;
@@ -1785,10 +1777,8 @@
 
 static int sof_widget_load_mixer(struct snd_soc_component *scomp, int index,
 				 struct snd_sof_widget *swidget,
-				 struct snd_soc_tplg_dapm_widget *tw,
-				 struct sof_ipc_comp_reply *r)
+				 struct snd_soc_tplg_dapm_widget *tw)
 {
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_comp_mixer *mixer;
 	size_t ipc_size = sizeof(*mixer);
@@ -1817,12 +1807,7 @@
 
 	swidget->private = mixer;
 
-	ret = sof_ipc_tx_message(sdev->ipc, mixer->comp.hdr.cmd, mixer,
-				 ipc_size, r, sizeof(*r));
-	if (ret < 0)
-		kfree(mixer);
-
-	return ret;
+	return 0;
 }
 
 /*
@@ -1830,10 +1815,8 @@
  */
 static int sof_widget_load_mux(struct snd_soc_component *scomp, int index,
 			       struct snd_sof_widget *swidget,
-			       struct snd_soc_tplg_dapm_widget *tw,
-			       struct sof_ipc_comp_reply *r)
+			       struct snd_soc_tplg_dapm_widget *tw)
 {
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_comp_mux *mux;
 	size_t ipc_size = sizeof(*mux);
@@ -1862,12 +1845,7 @@
 
 	swidget->private = mux;
 
-	ret = sof_ipc_tx_message(sdev->ipc, mux->comp.hdr.cmd, mux,
-				 ipc_size, r, sizeof(*r));
-	if (ret < 0)
-		kfree(mux);
-
-	return ret;
+	return 0;
 }
 
 /*
@@ -1876,8 +1854,7 @@
 
 static int sof_widget_load_pga(struct snd_soc_component *scomp, int index,
 			       struct snd_sof_widget *swidget,
-			       struct snd_soc_tplg_dapm_widget *tw,
-			       struct sof_ipc_comp_reply *r)
+			       struct snd_soc_tplg_dapm_widget *tw)
 {
 	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_soc_tplg_private *private = &tw->priv;
@@ -1937,10 +1914,7 @@
 		}
 	}
 
-	ret = sof_ipc_tx_message(sdev->ipc, volume->comp.hdr.cmd, volume,
-				 ipc_size, r, sizeof(*r));
-	if (ret >= 0)
-		return ret;
+	return 0;
 err:
 	kfree(volume);
 	return ret;
@@ -1952,10 +1926,8 @@
 
 static int sof_widget_load_src(struct snd_soc_component *scomp, int index,
 			       struct snd_sof_widget *swidget,
-			       struct snd_soc_tplg_dapm_widget *tw,
-			       struct sof_ipc_comp_reply *r)
+			       struct snd_soc_tplg_dapm_widget *tw)
 {
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_comp_src *src;
 	size_t ipc_size = sizeof(*src);
@@ -1994,10 +1966,7 @@
 
 	swidget->private = src;
 
-	ret = sof_ipc_tx_message(sdev->ipc, src->comp.hdr.cmd, src,
-				 ipc_size, r, sizeof(*r));
-	if (ret >= 0)
-		return ret;
+	return 0;
 err:
 	kfree(src);
 	return ret;
@@ -2009,10 +1978,8 @@
 
 static int sof_widget_load_asrc(struct snd_soc_component *scomp, int index,
 				struct snd_sof_widget *swidget,
-				struct snd_soc_tplg_dapm_widget *tw,
-				struct sof_ipc_comp_reply *r)
+				struct snd_soc_tplg_dapm_widget *tw)
 {
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_comp_asrc *asrc;
 	size_t ipc_size = sizeof(*asrc);
@@ -2053,10 +2020,7 @@
 
 	swidget->private = asrc;
 
-	ret = sof_ipc_tx_message(sdev->ipc, asrc->comp.hdr.cmd, asrc,
-				 ipc_size, r, sizeof(*r));
-	if (ret >= 0)
-		return ret;
+	return 0;
 err:
 	kfree(asrc);
 	return ret;
@@ -2068,10 +2032,8 @@
 
 static int sof_widget_load_siggen(struct snd_soc_component *scomp, int index,
 				  struct snd_sof_widget *swidget,
-				  struct snd_soc_tplg_dapm_widget *tw,
-				  struct sof_ipc_comp_reply *r)
+				  struct snd_soc_tplg_dapm_widget *tw)
 {
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_comp_tone *tone;
 	size_t ipc_size = sizeof(*tone);
@@ -2110,10 +2072,7 @@
 
 	swidget->private = tone;
 
-	ret = sof_ipc_tx_message(sdev->ipc, tone->comp.hdr.cmd, tone,
-				 ipc_size, r, sizeof(*r));
-	if (ret >= 0)
-		return ret;
+	return 0;
 err:
 	kfree(tone);
 	return ret;
@@ -2195,10 +2154,8 @@
 static int sof_process_load(struct snd_soc_component *scomp, int index,
 			    struct snd_sof_widget *swidget,
 			    struct snd_soc_tplg_dapm_widget *tw,
-			    struct sof_ipc_comp_reply *r,
 			    int type)
 {
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_soc_dapm_widget *widget = swidget->widget;
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_comp_process *process;
@@ -2272,33 +2229,6 @@
 
 	process->size = ipc_data_size;
 	swidget->private = process;
-
-	ret = sof_ipc_tx_message(sdev->ipc, process->comp.hdr.cmd, process,
-				 ipc_size, r, sizeof(*r));
-
-	if (ret < 0) {
-		dev_err(scomp->dev, "error: create process failed\n");
-		goto err;
-	}
-
-	/* we sent the data in single message so return */
-	if (ipc_data_size)
-		goto out;
-
-	/* send control data with large message supported method */
-	for (i = 0; i < widget->num_kcontrols; i++) {
-		wdata[i].control->readback_offset = 0;
-		ret = snd_sof_ipc_set_get_comp_data(wdata[i].control,
-						    wdata[i].ipc_cmd,
-						    wdata[i].ctrl_type,
-						    wdata[i].control->cmd,
-						    true);
-		if (ret != 0) {
-			dev_err(scomp->dev, "error: send control failed\n");
-			break;
-		}
-	}
-
 err:
 	if (ret < 0)
 		kfree(process);
@@ -2314,8 +2244,7 @@
 
 static int sof_widget_load_process(struct snd_soc_component *scomp, int index,
 				   struct snd_sof_widget *swidget,
-				   struct snd_soc_tplg_dapm_widget *tw,
-				   struct sof_ipc_comp_reply *r)
+				   struct snd_soc_tplg_dapm_widget *tw)
 {
 	struct snd_soc_tplg_private *private = &tw->priv;
 	struct sof_ipc_comp_process config;
@@ -2341,8 +2270,7 @@
 	}
 
 	/* now load process specific data and send IPC */
-	ret = sof_process_load(scomp, index, swidget, tw, r,
-			       find_process_comp_type(config.type));
+	ret = sof_process_load(scomp, index, swidget, tw, find_process_comp_type(config.type));
 	if (ret < 0) {
 		dev_err(scomp->dev, "error: process loading failed\n");
 		return ret;
@@ -2391,8 +2319,6 @@
 	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
 	struct snd_sof_widget *swidget;
 	struct snd_sof_dai *dai;
-	struct sof_ipc_comp_reply reply;
-	struct snd_sof_control *scontrol;
 	struct sof_ipc_comp comp = {
 		.core = SOF_DSP_PRIMARY_CORE,
 	};
@@ -2409,7 +2335,6 @@
 	swidget->id = w->id;
 	swidget->pipeline_id = index;
 	swidget->private = NULL;
-	memset(&reply, 0, sizeof(reply));
 
 	dev_dbg(scomp->dev, "tplg: ready widget id %d pipe %d type %d name : %s stream %s\n",
 		swidget->comp_id, index, swidget->id, tw->name,
@@ -2428,14 +2353,6 @@
 
 	swidget->core = comp.core;
 
-	/* default is primary core, safe to call for already enabled cores */
-	ret = sof_core_enable(sdev, comp.core);
-	if (ret < 0) {
-		dev_err(scomp->dev, "error: enable core: %d\n", ret);
-		kfree(swidget);
-		return ret;
-	}
-
 	ret = sof_parse_tokens(scomp, &swidget->comp_ext, comp_ext_tokens,
 			       ARRAY_SIZE(comp_ext_tokens), tw->priv.array,
 			       le32_to_cpu(tw->priv.size));
@@ -2456,7 +2373,7 @@
 			return -ENOMEM;
 		}
 
-		ret = sof_widget_load_dai(scomp, index, swidget, tw, &reply, dai);
+		ret = sof_widget_load_dai(scomp, index, swidget, tw, dai);
 		if (ret == 0) {
 			sof_connect_dai_widget(scomp, w, tw, dai);
 			list_add(&dai->list, &sdev->dai_list);
@@ -2466,47 +2383,40 @@
 		}
 		break;
 	case snd_soc_dapm_mixer:
-		ret = sof_widget_load_mixer(scomp, index, swidget, tw, &reply);
+		ret = sof_widget_load_mixer(scomp, index, swidget, tw);
 		break;
 	case snd_soc_dapm_pga:
-		ret = sof_widget_load_pga(scomp, index, swidget, tw, &reply);
-		/* Find scontrol for this pga and set readback offset*/
-		list_for_each_entry(scontrol, &sdev->kcontrol_list, list) {
-			if (scontrol->comp_id == swidget->comp_id) {
-				scontrol->readback_offset = reply.offset;
-				break;
-			}
-		}
+		ret = sof_widget_load_pga(scomp, index, swidget, tw);
 		break;
 	case snd_soc_dapm_buffer:
-		ret = sof_widget_load_buffer(scomp, index, swidget, tw, &reply);
+		ret = sof_widget_load_buffer(scomp, index, swidget, tw);
 		break;
 	case snd_soc_dapm_scheduler:
-		ret = sof_widget_load_pipeline(scomp, index, swidget, tw, &reply);
+		ret = sof_widget_load_pipeline(scomp, index, swidget, tw);
 		break;
 	case snd_soc_dapm_aif_out:
 		ret = sof_widget_load_pcm(scomp, index, swidget,
-					  SOF_IPC_STREAM_CAPTURE, tw, &reply);
+					  SOF_IPC_STREAM_CAPTURE, tw);
 		break;
 	case snd_soc_dapm_aif_in:
 		ret = sof_widget_load_pcm(scomp, index, swidget,
-					  SOF_IPC_STREAM_PLAYBACK, tw, &reply);
+					  SOF_IPC_STREAM_PLAYBACK, tw);
 		break;
 	case snd_soc_dapm_src:
-		ret = sof_widget_load_src(scomp, index, swidget, tw, &reply);
+		ret = sof_widget_load_src(scomp, index, swidget, tw);
 		break;
 	case snd_soc_dapm_asrc:
-		ret = sof_widget_load_asrc(scomp, index, swidget, tw, &reply);
+		ret = sof_widget_load_asrc(scomp, index, swidget, tw);
 		break;
 	case snd_soc_dapm_siggen:
-		ret = sof_widget_load_siggen(scomp, index, swidget, tw, &reply);
+		ret = sof_widget_load_siggen(scomp, index, swidget, tw);
 		break;
 	case snd_soc_dapm_effect:
-		ret = sof_widget_load_process(scomp, index, swidget, tw, &reply);
+		ret = sof_widget_load_process(scomp, index, swidget, tw);
 		break;
 	case snd_soc_dapm_mux:
 	case snd_soc_dapm_demux:
-		ret = sof_widget_load_mux(scomp, index, swidget, tw, &reply);
+		ret = sof_widget_load_mux(scomp, index, swidget, tw);
 		break;
 	case snd_soc_dapm_switch:
 	case snd_soc_dapm_dai_link:
@@ -2517,12 +2427,12 @@
 	}
 
 	/* check IPC reply */
-	if (ret < 0 || reply.rhdr.error < 0) {
+	if (ret < 0) {
 		dev_err(scomp->dev,
-			"error: DSP failed to add widget id %d type %d name : %s stream %s reply %d\n",
+			"error: failed to add widget id %d type %d name : %s stream %s\n",
 			tw->shift, swidget->id, tw->name,
 			strnlen(tw->sname, SNDRV_CTL_ELEM_ID_NAME_MAXLEN) > 0
-				? tw->sname : "none", reply.rhdr.error);
+				? tw->sname : "none");
 		kfree(swidget);
 		return ret;
 	}
@@ -2837,9 +2747,6 @@
 			continue;
 
 		if (strcmp(link->name, dai->name) == 0) {
-			struct sof_ipc_reply reply;
-			int ret;
-
 			/*
 			 * the same dai config will be applied to all DAIs in
 			 * the same dai link. We have to ensure that the ipc
@@ -2851,18 +2758,6 @@
 
 			dev_dbg(sdev->dev, "set DAI config for %s index %d\n",
 				dai->name, config[curr_conf].dai_index);
-			/* send message to DSP */
-			ret = sof_ipc_tx_message(sdev->ipc,
-						 config[curr_conf].hdr.cmd,
-						 &config[curr_conf], size,
-						 &reply, sizeof(reply));
-
-			if (ret < 0) {
-				dev_err(sdev->dev,
-					"error: failed to set DAI config for %s index %d\n",
-					dai->name, config[curr_conf].dai_index);
-				return ret;
-			}
 
 			dai->number_configs = num_conf;
 			dai->current_config = curr_conf;
@@ -2870,9 +2765,6 @@
 			if (!dai->dai_config)
 				return -ENOMEM;
 
-			/* set cpu_dai_name */
-			dai->cpu_dai_name = link->cpus->dai_name;
-
 			found = 1;
 		}
 	}
@@ -3392,7 +3284,6 @@
 	struct snd_sof_widget *source_swidget, *sink_swidget;
 	struct snd_soc_dobj *dobj = &route->dobj;
 	struct snd_sof_route *sroute;
-	struct sof_ipc_reply reply;
 	int ret = 0;
 
 	/* allocate memory for sroute and connect */
@@ -3467,33 +3358,11 @@
 			route->source, route->sink);
 		goto err;
 	} else {
-		ret = sof_ipc_tx_message(sdev->ipc,
-					 connect->hdr.cmd,
-					 connect, sizeof(*connect),
-					 &reply, sizeof(reply));
-
-		/* check IPC return value */
-		if (ret < 0) {
-			dev_err(scomp->dev, "error: failed to add route sink %s control %s source %s\n",
-				route->sink,
-				route->control ? route->control : "none",
-				route->source);
-			goto err;
-		}
-
-		/* check IPC reply */
-		if (reply.error < 0) {
-			dev_err(scomp->dev, "error: DSP failed to add route sink %s control %s source %s result %d\n",
-				route->sink,
-				route->control ? route->control : "none",
-				route->source, reply.error);
-			ret = reply.error;
-			goto err;
-		}
-
 		sroute->route = route;
 		dobj->private = sroute;
 		sroute->private = connect;
+		sroute->src_widget = source_swidget;
+		sroute->sink_widget = sink_swidget;
 
 		/* add route to route list */
 		list_add(&sroute->list, &sdev->route_list);
@@ -3507,50 +3376,6 @@
 	return ret;
 }
 
-/* Function to set the initial value of SOF kcontrols.
- * The value will be stored in scontrol->control_data
- */
-static int snd_sof_cache_kcontrol_val(struct snd_soc_component *scomp)
-{
-	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
-	struct snd_sof_control *scontrol = NULL;
-	int ipc_cmd, ctrl_type;
-	int ret = 0;
-
-	list_for_each_entry(scontrol, &sdev->kcontrol_list, list) {
-
-		/* notify DSP of kcontrol values */
-		switch (scontrol->cmd) {
-		case SOF_CTRL_CMD_VOLUME:
-		case SOF_CTRL_CMD_ENUM:
-		case SOF_CTRL_CMD_SWITCH:
-			ipc_cmd = SOF_IPC_COMP_GET_VALUE;
-			ctrl_type = SOF_CTRL_TYPE_VALUE_CHAN_GET;
-			break;
-		case SOF_CTRL_CMD_BINARY:
-			ipc_cmd = SOF_IPC_COMP_GET_DATA;
-			ctrl_type = SOF_CTRL_TYPE_DATA_GET;
-			break;
-		default:
-			dev_err(scomp->dev,
-				"error: Invalid scontrol->cmd: %d\n",
-				scontrol->cmd);
-			return -EINVAL;
-		}
-		ret = snd_sof_ipc_set_get_comp_data(scontrol,
-						    ipc_cmd, ctrl_type,
-						    scontrol->cmd,
-						    false);
-		if (ret < 0) {
-			dev_warn(scomp->dev,
-				 "error: kcontrol value get for widget: %d\n",
-				 scontrol->comp_id);
-		}
-	}
-
-	return ret;
-}
-
 int snd_sof_complete_pipeline(struct device *dev,
 			      struct snd_sof_widget *swidget)
 {
@@ -3575,31 +3400,84 @@
 	return 1;
 }
 
+/**
+ * sof_set_pipe_widget - Set pipe_widget for a component
+ * @sdev: pointer to struct snd_sof_dev
+ * @pipe_widget: pointer to struct snd_sof_widget of type snd_soc_dapm_scheduler
+ * @swidget: pointer to struct snd_sof_widget that has the same pipeline ID as @pipe_widget
+ *
+ * Return: 0 if successful, -EINVAL on error.
+ * The function checks if @swidget is associated with any volatile controls. If so, setting
+ * the dynamic_pipeline_widget is disallowed.
+ */
+static int sof_set_pipe_widget(struct snd_sof_dev *sdev, struct snd_sof_widget *pipe_widget,
+			       struct snd_sof_widget *swidget)
+{
+	struct snd_sof_control *scontrol;
+
+	if (pipe_widget->dynamic_pipeline_widget) {
+		/* dynamic widgets cannot have volatile kcontrols */
+		list_for_each_entry(scontrol, &sdev->kcontrol_list, list)
+			if (scontrol->comp_id == swidget->comp_id &&
+			    (scontrol->access & SNDRV_CTL_ELEM_ACCESS_VOLATILE)) {
+				dev_err(sdev->dev,
+					"error: volatile control found for dynamic widget %s\n",
+					swidget->widget->name);
+				return -EINVAL;
+			}
+	}
+
+	/* set the pipe_widget and apply the dynamic_pipeline_widget_flag */
+	swidget->pipe_widget = pipe_widget;
+	swidget->dynamic_pipeline_widget = pipe_widget->dynamic_pipeline_widget;
+
+	return 0;
+}
+
 /* completion - called at completion of firmware loading */
-static void sof_complete(struct snd_soc_component *scomp)
+static int sof_complete(struct snd_soc_component *scomp)
 {
 	struct snd_sof_dev *sdev = snd_soc_component_get_drvdata(scomp);
-	struct snd_sof_widget *swidget;
+	struct snd_sof_widget *swidget, *comp_swidget;
+	int ret;
 
-	/* some widget types require completion notificattion */
+	/* set the pipe_widget and apply the dynamic_pipeline_widget_flag */
 	list_for_each_entry(swidget, &sdev->widget_list, list) {
-		if (swidget->complete)
-			continue;
-
 		switch (swidget->id) {
 		case snd_soc_dapm_scheduler:
-			swidget->complete =
-				snd_sof_complete_pipeline(scomp->dev, swidget);
+			/*
+			 * Apply the dynamic_pipeline_widget flag and set the pipe_widget field
+			 * for all widgets that have the same pipeline ID as the scheduler widget
+			 */
+			list_for_each_entry_reverse(comp_swidget, &sdev->widget_list, list)
+				if (comp_swidget->pipeline_id == swidget->pipeline_id) {
+					ret = sof_set_pipe_widget(sdev, swidget, comp_swidget);
+					if (ret < 0)
+						return ret;
+				}
 			break;
 		default:
 			break;
 		}
 	}
-	/*
-	 * cache initial values of SOF kcontrols by reading DSP value over
-	 * IPC. It may be overwritten by alsa-mixer after booting up
-	 */
-	snd_sof_cache_kcontrol_val(scomp);
+
+	/* verify topology components loading including dynamic pipelines */
+	if (sof_core_debug & SOF_DBG_VERIFY_TPLG) {
+		ret = sof_set_up_pipelines(scomp->dev, true);
+		if (ret < 0) {
+			dev_err(sdev->dev, "error: topology verification failed %d\n", ret);
+			return ret;
+		}
+
+		ret = sof_tear_down_pipelines(scomp->dev, true);
+		if (ret < 0) {
+			dev_err(sdev->dev, "error: topology tear down pipelines failed %d\n", ret);
+			return ret;
+		}
+	}
+
+	/* set up static pipelines */
+	return sof_set_up_pipelines(scomp->dev, false);
 }
 
 /* manifest - optional to inform component of manifest */
diff -ruN a/tools/mm/low-mem-test.c b/tools/mm/low-mem-test.c
--- a/tools/mm/low-mem-test.c	1970-01-01 01:00:00.000000000 +0100
+++ b/tools/mm/low-mem-test.c	2021-12-23 08:36:03.000000000 +0100
@@ -0,0 +1,178 @@
+/* Copyright (c) 2012 The Chromium OS Authors. All rights reserved.
+ * This program is free software, released under the GPL.
+ * Based on code by Minchan Kim
+ *
+ * User program that tests low-memory notifications.
+ *
+ * Compile with -lpthread
+ * for instance
+ * i686-pc-linux-gnu-gcc low-mem-test.c -o low-mem-test -lpthread
+ *
+ * Run as: low-mem-test <allocation size> <allocation interval (microseconds)>
+ *
+ * This program runs in two threads.  One thread continuously allocates memory
+ * in the given chunk size, waiting for the specified microsecond interval
+ * between allocations.  The other runs in a loop that waits for a low-memory
+ * notification, then frees some of the memory that the first thread has
+ * allocated.
+ */
+
+#include <poll.h>
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <stdio.h>
+#include <pthread.h>
+#include <stdlib.h>
+#include <string.h>
+
+int memory_chunk_size = 10000000;
+int wait_time_us = 10000;
+int autotesting;
+
+pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;
+
+struct node {
+	void *memory;
+	struct node *prev;
+	struct node *next;
+};
+
+struct node head, tail;
+
+void work(void)
+{
+	int i;
+
+	while (1) {
+		struct node *new = malloc(sizeof(struct node));
+		if (new == NULL) {
+			perror("allocating node");
+			exit(1);
+		}
+		new->memory = malloc(memory_chunk_size);
+		if (new->memory == NULL) {
+			perror("allocating chunk");
+			exit(1);
+		}
+
+		pthread_mutex_lock(&mutex);
+		new->next = &head;
+		new->prev = head.prev;
+		new->prev->next = new;
+		new->next->prev = new;
+		for (i = 0; i < memory_chunk_size / 4096; i++) {
+			/* touch page */
+			((unsigned char *) new->memory)[i * 4096] = 1;
+		}
+
+		pthread_mutex_unlock(&mutex);
+
+		if (!autotesting) {
+			printf("+");
+			fflush(stdout);
+		}
+
+		usleep(wait_time_us);
+	}
+}
+
+void free_memory(void)
+{
+	struct node *old;
+	pthread_mutex_lock(&mutex);
+	old = tail.next;
+	if (old == &head) {
+		fprintf(stderr, "no memory left to free\n");
+		exit(1);
+	}
+	old->prev->next = old->next;
+	old->next->prev = old->prev;
+	free(old->memory);
+	free(old);
+	pthread_mutex_unlock(&mutex);
+	if (!autotesting) {
+		printf("-");
+		fflush(stdout);
+	}
+}
+
+void *poll_thread(void *dummy)
+{
+	struct pollfd pfd;
+	int fd = open("/dev/chromeos-low-mem", O_RDONLY);
+	if (fd == -1) {
+		perror("/dev/chromeos-low-mem");
+		exit(1);
+	}
+
+	pfd.fd = fd;
+	pfd.events = POLLIN;
+
+	if (autotesting) {
+		/* Check that there is no memory shortage yet. */
+		poll(&pfd, 1, 0);
+		if (pfd.revents != 0) {
+			exit(0);
+		} else {
+			fprintf(stderr, "expected no events but "
+				"poll() returned 0x%x\n", pfd.revents);
+			exit(1);
+		}
+	}
+
+	while (1) {
+		poll(&pfd, 1, -1);
+		if (autotesting) {
+			/* Free several chunks and check that the notification
+			 * is gone. */
+			free_memory();
+			free_memory();
+			free_memory();
+			free_memory();
+			free_memory();
+			poll(&pfd, 1, 0);
+			if (pfd.revents == 0) {
+				exit(0);
+			} else {
+				fprintf(stderr, "expected no events but "
+					"poll() returned 0x%x\n", pfd.revents);
+				exit(1);
+			}
+		}
+		free_memory();
+	}
+}
+
+int main(int argc, char **argv)
+{
+	pthread_t threadid;
+
+	head.next = NULL;
+	head.prev = &tail;
+	tail.next = &head;
+	tail.prev = NULL;
+
+	if (argc != 3 && (argc != 2 || strcmp(argv[1], "autotesting"))) {
+		fprintf(stderr,
+			"usage: low-mem-test <alloc size in bytes> "
+			"<alloc interval in microseconds>\n"
+			"or:    low-mem-test autotesting\n");
+		exit(1);
+	}
+
+	if (argc == 2) {
+		autotesting = 1;
+	} else {
+		memory_chunk_size = atoi(argv[1]);
+		wait_time_us = atoi(argv[2]);
+	}
+
+	if (pthread_create(&threadid, NULL, poll_thread, NULL)) {
+		perror("pthread");
+		return 1;
+	}
+
+	work();
+	return 0;
+}
diff -ruN a/tools/testing/selftests/futex/functional/futex_swap.c b/tools/testing/selftests/futex/functional/futex_swap.c
--- a/tools/testing/selftests/futex/functional/futex_swap.c	1970-01-01 01:00:00.000000000 +0100
+++ b/tools/testing/selftests/futex/functional/futex_swap.c	2021-12-23 08:36:04.000000000 +0100
@@ -0,0 +1,209 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+
+#include <errno.h>
+#include <getopt.h>
+#include <pthread.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <time.h>
+#include "atomic.h"
+#include "futextest.h"
+
+/* The futex the main thread waits on. */
+futex_t futex_main = FUTEX_INITIALIZER;
+/* The futex the other thread wats on. */
+futex_t futex_other = FUTEX_INITIALIZER;
+
+/* The number of iterations to run (>1 => run benchmarks. */
+static int cfg_iterations = 1;
+
+/* If != 0, print diagnostic messages. */
+static int cfg_verbose;
+
+/* If == 0, do not use validation_counter. Useful for benchmarking. */
+static int cfg_validate = 1;
+
+/* How to swap threads. */
+#define SWAP_WAKE_WAIT 1
+#define SWAP_SWAP 2
+
+/* Futex values. */
+#define FUTEX_WAITING 0
+#define FUTEX_WAKEUP 1
+
+/* An atomic counter used to validate proper swapping. */
+static atomic_t validation_counter;
+
+void futex_swap_op(int mode, futex_t *futex_this, futex_t *futex_that)
+{
+	int ret;
+
+	switch (mode) {
+	case SWAP_WAKE_WAIT:
+		futex_set(futex_this, FUTEX_WAITING);
+		futex_set(futex_that, FUTEX_WAKEUP);
+		futex_wake(futex_that, 1, FUTEX_PRIVATE_FLAG);
+		futex_wait(futex_this, FUTEX_WAITING, NULL, FUTEX_PRIVATE_FLAG);
+		if (*futex_this != FUTEX_WAKEUP) {
+			fprintf(stderr, "unexpected futex_this value on wakeup\n");
+			exit(1);
+		}
+		break;
+
+	case SWAP_SWAP:
+		futex_set(futex_this, FUTEX_WAITING);
+		futex_set(futex_that, FUTEX_WAKEUP);
+		ret = futex_swap(futex_this, FUTEX_WAITING, NULL,
+				 futex_that, FUTEX_PRIVATE_FLAG);
+		if (ret < 0 && errno == ENOSYS) {
+			/* futex_swap not implemented */
+			perror("futex_swap");
+			exit(1);
+		}
+		if (*futex_this != FUTEX_WAKEUP) {
+			fprintf(stderr, "unexpected futex_this value on wakeup\n");
+			exit(1);
+		}
+		break;
+
+	default:
+		fprintf(stderr, "unknown mode in %s\n", __func__);
+		exit(1);
+	}
+}
+
+void *other_thread(void *arg)
+{
+	int mode = *((int *)arg);
+	int counter;
+
+	if (cfg_verbose)
+		printf("%s started\n", __func__);
+
+	futex_wait(&futex_other, 0, NULL, FUTEX_PRIVATE_FLAG);
+
+	for (counter = 0; counter < cfg_iterations; ++counter) {
+		if (cfg_validate) {
+			int prev = 2 * counter + 1;
+
+			if (prev != atomic_cmpxchg(&validation_counter, prev,
+						   prev + 1)) {
+				fprintf(stderr, "swap validation failed\n");
+				exit(1);
+			}
+		}
+		futex_swap_op(mode, &futex_other, &futex_main);
+	}
+
+	if (cfg_verbose)
+		printf("%s finished: %d iteration(s)\n", __func__, counter);
+
+	return NULL;
+}
+
+void run_test(int mode)
+{
+	struct timespec start, stop;
+	int ret, counter;
+	pthread_t thread;
+	uint64_t duration;
+
+	futex_set(&futex_other, FUTEX_WAITING);
+	atomic_set(&validation_counter, 0);
+	ret = pthread_create(&thread, NULL, &other_thread, &mode);
+	if (ret) {
+		perror("pthread_create");
+		exit(1);
+	}
+
+	ret = clock_gettime(CLOCK_MONOTONIC, &start);
+	if (ret) {
+		perror("clock_gettime");
+		exit(1);
+	}
+
+	for (counter = 0; counter < cfg_iterations; ++counter) {
+		if (cfg_validate) {
+			int prev = 2 * counter;
+
+			if (prev != atomic_cmpxchg(&validation_counter, prev,
+						   prev + 1)) {
+				fprintf(stderr, "swap validation failed\n");
+				exit(1);
+			}
+		}
+		futex_swap_op(mode, &futex_main, &futex_other);
+	}
+	if (cfg_validate && validation_counter.val != 2 * cfg_iterations) {
+		fprintf(stderr, "final swap validation failed\n");
+		exit(1);
+	}
+
+	ret = clock_gettime(CLOCK_MONOTONIC, &stop);
+	if (ret) {
+		perror("clock_gettime");
+		exit(1);
+	}
+
+	duration = (stop.tv_sec - start.tv_sec) * 1000000000LL +
+	stop.tv_nsec - start.tv_nsec;
+	if (cfg_verbose || cfg_iterations > 1) {
+		printf("completed %d swap and back iterations in %lu ns: %lu ns per swap\n",
+			cfg_iterations, duration,
+			duration / (cfg_iterations * 2));
+	}
+
+	/* The remote thread is blocked; send it the final wake. */
+	futex_set(&futex_other, FUTEX_WAKEUP);
+	futex_wake(&futex_other, 1, FUTEX_PRIVATE_FLAG);
+	if (pthread_join(thread, NULL)) {
+		perror("pthread_join");
+		exit(1);
+	}
+}
+
+void usage(char *prog)
+{
+	printf("Usage: %s\n", prog);
+	printf("  -h    Display this help message\n");
+	printf("  -i N  Use N iterations to benchmark\n");
+	printf("  -n    Do not validate swapping correctness\n");
+	printf("  -v    Print diagnostic messages\n");
+}
+
+int main(int argc, char *argv[])
+{
+	int c;
+
+	while ((c = getopt(argc, argv, "hi:nv")) != -1) {
+		switch (c) {
+		case 'h':
+			usage(basename(argv[0]));
+			exit(0);
+		case 'i':
+			cfg_iterations = atoi(optarg);
+			break;
+		case 'n':
+			cfg_validate = 0;
+			break;
+		case 'v':
+			cfg_verbose = 1;
+			break;
+		default:
+			usage(basename(argv[0]));
+			exit(1);
+		}
+	}
+
+	printf("\n\n------- running SWAP_WAKE_WAIT -----------\n\n");
+	run_test(SWAP_WAKE_WAIT);
+	printf("PASS\n");
+
+	printf("\n\n------- running SWAP_SWAP -----------\n\n");
+	run_test(SWAP_SWAP);
+	printf("PASS\n");
+
+	return 0;
+}
diff -ruN a/tools/testing/selftests/futex/functional/.gitignore b/tools/testing/selftests/futex/functional/.gitignore
--- a/tools/testing/selftests/futex/functional/.gitignore	2021-12-08 09:04:57.000000000 +0100
+++ b/tools/testing/selftests/futex/functional/.gitignore	2021-12-23 08:36:04.000000000 +0100
@@ -2,6 +2,7 @@
 futex_requeue_pi
 futex_requeue_pi_mismatched_ops
 futex_requeue_pi_signal_restart
+futex_swap
 futex_wait_private_mapped_file
 futex_wait_timeout
 futex_wait_uninitialized_heap
diff -ruN a/tools/testing/selftests/futex/functional/Makefile b/tools/testing/selftests/futex/functional/Makefile
--- a/tools/testing/selftests/futex/functional/Makefile	2021-12-08 09:04:57.000000000 +0100
+++ b/tools/testing/selftests/futex/functional/Makefile	2021-12-23 08:36:04.000000000 +0100
@@ -14,6 +14,7 @@
 	futex_requeue_pi \
 	futex_requeue_pi_signal_restart \
 	futex_requeue_pi_mismatched_ops \
+	futex_swap \
 	futex_wait_uninitialized_heap \
 	futex_wait_private_mapped_file \
 	futex_wait \
diff -ruN a/tools/testing/selftests/futex/include/futextest.h b/tools/testing/selftests/futex/include/futextest.h
--- a/tools/testing/selftests/futex/include/futextest.h	2021-12-08 09:04:57.000000000 +0100
+++ b/tools/testing/selftests/futex/include/futextest.h	2021-12-23 08:36:04.000000000 +0100
@@ -38,6 +38,9 @@
 #ifndef FUTEX_CMP_REQUEUE_PI
 #define FUTEX_CMP_REQUEUE_PI		12
 #endif
+#ifndef FUTEX_SWAP
+#define FUTEX_SWAP			13
+#endif
 #ifndef FUTEX_WAIT_REQUEUE_PI_PRIVATE
 #define FUTEX_WAIT_REQUEUE_PI_PRIVATE	(FUTEX_WAIT_REQUEUE_PI | \
 					 FUTEX_PRIVATE_FLAG)
@@ -46,6 +49,9 @@
 #define FUTEX_CMP_REQUEUE_PI_PRIVATE	(FUTEX_CMP_REQUEUE_PI | \
 					 FUTEX_PRIVATE_FLAG)
 #endif
+#ifndef FUTEX_SWAP_PRIVATE
+#define FUTEX_SWAP_PRIVATE		(FUTEX_WAIT_WAKE | FUTEX_PRIVATE_FLAG)
+#endif
 
 /**
  * futex() - SYS_futex syscall wrapper
@@ -205,6 +211,19 @@
 }
 
 /**
+ * futex_swap() - block on uaddr and wake one task blocked on uaddr2.
+ * @uaddr:	futex to block the current task on
+ * @timeout:	relative timeout for the current task block
+ * @uaddr2:	futex to wake tasks at (can be the same as uaddr)
+ */
+static inline int
+futex_swap(futex_t *uaddr, futex_t val, struct timespec *timeout,
+	   futex_t *uaddr2, int opflags)
+{
+	return futex(uaddr, FUTEX_SWAP, val, timeout, uaddr2, 0, opflags);
+}
+
+/**
  * futex_cmpxchg() - atomic compare and exchange
  * @uaddr:	The address of the futex to be modified
  * @oldval:	The expected value of the futex
diff -ruN a/unblocked_terms.txt b/unblocked_terms.txt
--- a/unblocked_terms.txt	1970-01-01 01:00:00.000000000 +0100
+++ b/unblocked_terms.txt	2021-12-23 08:36:05.000000000 +0100
@@ -0,0 +1 @@
+.*
diff -ruN a/virt/kvm/Kconfig b/virt/kvm/Kconfig
--- a/virt/kvm/Kconfig	2021-12-08 09:04:57.000000000 +0100
+++ b/virt/kvm/Kconfig	2021-12-23 08:36:05.000000000 +0100
@@ -47,7 +47,7 @@
 
 config KVM_COMPAT
        def_bool y
-       depends on KVM && COMPAT && !(S390 || ARM64)
+       depends on KVM && COMPAT && !S390
 
 config HAVE_KVM_IRQ_BYPASS
        bool
diff -ruN a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
--- a/virt/kvm/kvm_main.c	2021-12-08 09:04:57.000000000 +0100
+++ b/virt/kvm/kvm_main.c	2021-12-23 08:36:05.000000000 +0100
@@ -168,7 +168,7 @@
 	 * the device has been pinned, e.g. by get_user_pages().  WARN if the
 	 * page_count() is zero to help detect bad usage of this helper.
 	 */
-	if (!pfn_valid(pfn) || WARN_ON_ONCE(!page_count(pfn_to_page(pfn))))
+	if (!pfn_valid(pfn) || !page_count(pfn_to_page(pfn)))
 		return false;
 
 	return is_zone_device_page(pfn_to_page(pfn));
@@ -2238,9 +2238,9 @@
  * only part that runs if we can in atomic context.
  */
 static bool hva_to_pfn_fast(unsigned long addr, bool write_fault,
-			    bool *writable, kvm_pfn_t *pfn)
+			    bool *writable, kvm_pfn_t *pfn,
+			    struct page **page)
 {
-	struct page *page[1];
 
 	/*
 	 * Fast pin a writable pfn only if it is a write fault request
@@ -2251,7 +2251,7 @@
 		return false;
 
 	if (get_user_page_fast_only(addr, FOLL_WRITE, page)) {
-		*pfn = page_to_pfn(page[0]);
+		*pfn = page_to_pfn(*page);
 
 		if (writable)
 			*writable = true;
@@ -2266,10 +2266,9 @@
  * 1 indicates success, -errno is returned if error is detected.
  */
 static int hva_to_pfn_slow(unsigned long addr, bool *async, bool write_fault,
-			   bool *writable, kvm_pfn_t *pfn)
+			   bool *writable, kvm_pfn_t *pfn, struct page **page)
 {
 	unsigned int flags = FOLL_HWPOISON;
-	struct page *page;
 	int npages = 0;
 
 	might_sleep();
@@ -2282,7 +2281,7 @@
 	if (async)
 		flags |= FOLL_NOWAIT;
 
-	npages = get_user_pages_unlocked(addr, 1, &page, flags);
+	npages = get_user_pages_unlocked(addr, 1, page, flags);
 	if (npages != 1)
 		return npages;
 
@@ -2292,11 +2291,11 @@
 
 		if (get_user_page_fast_only(addr, FOLL_WRITE, &wpage)) {
 			*writable = true;
-			put_page(page);
-			page = wpage;
+			put_page(*page);
+			*page = wpage;
 		}
 	}
-	*pfn = page_to_pfn(page);
+	*pfn = page_to_pfn(*page);
 	return npages;
 }
 
@@ -2311,13 +2310,6 @@
 	return true;
 }
 
-static int kvm_try_get_pfn(kvm_pfn_t pfn)
-{
-	if (kvm_is_reserved_pfn(pfn))
-		return 1;
-	return get_page_unless_zero(pfn_to_page(pfn));
-}
-
 static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 			       unsigned long addr, bool *async,
 			       bool write_fault, bool *writable,
@@ -2357,26 +2349,6 @@
 		*writable = pte_write(*ptep);
 	pfn = pte_pfn(*ptep);
 
-	/*
-	 * Get a reference here because callers of *hva_to_pfn* and
-	 * *gfn_to_pfn* ultimately call kvm_release_pfn_clean on the
-	 * returned pfn.  This is only needed if the VMA has VM_MIXEDMAP
-	 * set, but the kvm_try_get_pfn/kvm_release_pfn_clean pair will
-	 * simply do nothing for reserved pfns.
-	 *
-	 * Whoever called remap_pfn_range is also going to call e.g.
-	 * unmap_mapping_range before the underlying pages are freed,
-	 * causing a call to our MMU notifier.
-	 *
-	 * Certain IO or PFNMAP mappings can be backed with valid
-	 * struct pages, but be allocated without refcounting e.g.,
-	 * tail pages of non-compound higher order allocations, which
-	 * would then underflow the refcount when the caller does the
-	 * required put_page. Don't allow those pages here.
-	 */ 
-	if (!kvm_try_get_pfn(pfn))
-		r = -EFAULT;
-
 out:
 	pte_unmap_unlock(ptep, ptl);
 	*p_pfn = pfn;
@@ -2398,8 +2370,9 @@
  * 2): @write_fault = false && @writable, @writable will tell the caller
  *     whether the mapping is writable.
  */
-static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
-			bool write_fault, bool *writable)
+static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic,
+			    bool *async, bool write_fault, bool *writable,
+			    struct page **page)
 {
 	struct vm_area_struct *vma;
 	kvm_pfn_t pfn = 0;
@@ -2408,13 +2381,14 @@
 	/* we can do it either atomically or asynchronously, not both */
 	BUG_ON(atomic && async);
 
-	if (hva_to_pfn_fast(addr, write_fault, writable, &pfn))
+	if (hva_to_pfn_fast(addr, write_fault, writable, &pfn, page))
 		return pfn;
 
 	if (atomic)
 		return KVM_PFN_ERR_FAULT;
 
-	npages = hva_to_pfn_slow(addr, async, write_fault, writable, &pfn);
+	npages = hva_to_pfn_slow(addr, async, write_fault, writable,
+				 &pfn, page);
 	if (npages == 1)
 		return pfn;
 
@@ -2446,12 +2420,14 @@
 	return pfn;
 }
 
-kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
-			       bool atomic, bool *async, bool write_fault,
-			       bool *writable, hva_t *hva)
+kvm_pfn_t __gfn_to_pfn_page_memslot(struct kvm_memory_slot *slot,
+				    gfn_t gfn, bool atomic, bool *async,
+				    bool write_fault, bool *writable,
+				    hva_t *hva, struct page **page)
 {
 	unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);
 
+	*page = NULL;
 	if (hva)
 		*hva = addr;
 
@@ -2474,45 +2450,153 @@
 	}
 
 	return hva_to_pfn(addr, atomic, async, write_fault,
-			  writable);
+			  writable, page);
+}
+EXPORT_SYMBOL_GPL(__gfn_to_pfn_page_memslot);
+
+kvm_pfn_t gfn_to_pfn_page_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
+			       bool *writable, struct page **page)
+{
+	return __gfn_to_pfn_page_memslot(gfn_to_memslot(kvm, gfn), gfn, false,
+					 NULL, write_fault, writable, NULL,
+					 page);
+}
+EXPORT_SYMBOL_GPL(gfn_to_pfn_page_prot);
+
+kvm_pfn_t gfn_to_pfn_page_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
+				  struct page **page)
+{
+	return __gfn_to_pfn_page_memslot(slot, gfn, false, NULL, true,
+					 NULL, NULL, page);
+}
+EXPORT_SYMBOL_GPL(gfn_to_pfn_page_memslot);
+
+kvm_pfn_t gfn_to_pfn_page_memslot_atomic(struct kvm_memory_slot *slot,
+					 gfn_t gfn, struct page **page)
+{
+	return __gfn_to_pfn_page_memslot(slot, gfn, true, NULL, true, NULL,
+					 NULL, page);
+}
+EXPORT_SYMBOL_GPL(gfn_to_pfn_page_memslot_atomic);
+
+kvm_pfn_t kvm_vcpu_gfn_to_pfn_page_atomic(struct kvm_vcpu *vcpu, gfn_t gfn,
+					  struct page **page)
+{
+	return gfn_to_pfn_page_memslot_atomic(
+			kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn, page);
+}
+EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn_page_atomic);
+
+kvm_pfn_t gfn_to_pfn_page(struct kvm *kvm, gfn_t gfn, struct page **page)
+{
+	return gfn_to_pfn_page_memslot(gfn_to_memslot(kvm, gfn), gfn, page);
+}
+EXPORT_SYMBOL_GPL(gfn_to_pfn_page);
+
+kvm_pfn_t kvm_vcpu_gfn_to_pfn_page(struct kvm_vcpu *vcpu, gfn_t gfn,
+				   struct page **page)
+{
+	return gfn_to_pfn_page_memslot(kvm_vcpu_gfn_to_memslot(vcpu, gfn),
+				       gfn, page);
+}
+EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn_page);
+
+static kvm_pfn_t ensure_pfn_ref(struct page *page, kvm_pfn_t pfn)
+{
+	if (page || is_error_pfn(pfn) || kvm_is_reserved_pfn(pfn))
+		return pfn;
+
+	/*
+	 * Certain IO or PFNMAP mappings can be backed with valid
+	 * struct pages, but be allocated without refcounting e.g.,
+	 * tail pages of non-compound higher order allocations, which
+	 * would then underflow the refcount when the caller does the
+	 * required put_page. Don't allow those pages here.
+	 */
+	if (get_page_unless_zero(pfn_to_page(pfn)))
+		return pfn;
+
+	return KVM_PFN_ERR_FAULT;
+}
+
+kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
+			       bool atomic, bool *async, bool write_fault,
+			       bool *writable, hva_t *hva)
+{
+	struct page *page;
+	kvm_pfn_t pfn;
+
+	pfn = __gfn_to_pfn_page_memslot(slot, gfn, atomic, async,
+					write_fault, writable, hva, &page);
+
+	return ensure_pfn_ref(page, pfn);
 }
 EXPORT_SYMBOL_GPL(__gfn_to_pfn_memslot);
 
 kvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,
 		      bool *writable)
 {
-	return __gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn, false, NULL,
-				    write_fault, writable, NULL);
+	struct page *page;
+	kvm_pfn_t pfn;
+
+	pfn = gfn_to_pfn_page_prot(kvm, gfn, write_fault, writable, &page);
+
+	return ensure_pfn_ref(page, pfn);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_prot);
 
 kvm_pfn_t gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn)
 {
-	return __gfn_to_pfn_memslot(slot, gfn, false, NULL, true, NULL, NULL);
+	struct page *page;
+	kvm_pfn_t pfn;
+
+	pfn = gfn_to_pfn_page_memslot(slot, gfn, &page);
+
+	return ensure_pfn_ref(page, pfn);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot);
 
 kvm_pfn_t gfn_to_pfn_memslot_atomic(struct kvm_memory_slot *slot, gfn_t gfn)
 {
-	return __gfn_to_pfn_memslot(slot, gfn, true, NULL, true, NULL, NULL);
+	struct page *page;
+	kvm_pfn_t pfn;
+
+	pfn = gfn_to_pfn_page_memslot_atomic(slot, gfn, &page);
+
+	return ensure_pfn_ref(page, pfn);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn_memslot_atomic);
 
 kvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
-	return gfn_to_pfn_memslot_atomic(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
+	struct page *page;
+	kvm_pfn_t pfn;
+
+	pfn = kvm_vcpu_gfn_to_pfn_page_atomic(vcpu, gfn, &page);
+
+	return ensure_pfn_ref(page, pfn);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn_atomic);
 
 kvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn)
 {
-	return gfn_to_pfn_memslot(gfn_to_memslot(kvm, gfn), gfn);
+	struct page *page;
+	kvm_pfn_t pfn;
+
+	pfn = gfn_to_pfn_page(kvm, gfn, &page);
+
+	return ensure_pfn_ref(page, pfn);
 }
 EXPORT_SYMBOL_GPL(gfn_to_pfn);
 
 kvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn)
 {
-	return gfn_to_pfn_memslot(kvm_vcpu_gfn_to_memslot(vcpu, gfn), gfn);
+	struct page *page;
+	kvm_pfn_t pfn;
+
+	pfn = kvm_vcpu_gfn_to_pfn_page(vcpu, gfn, &page);
+
+	return ensure_pfn_ref(page, pfn);
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_gfn_to_pfn);
 
